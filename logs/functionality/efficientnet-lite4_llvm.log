nohup: ignoring input
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_transpose_layout_transform"
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 224, 3), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_transpose = T.alloc_buffer([1, 3, 224, 224], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 224, 224):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax2, ax3, ax1])
                T.writes(T_transpose[ax0, ax1, ax2, ax3])
                T_transpose[ax0, ax1, ax2, ax3] = placeholder[ax0, ax2, ax3, ax1]
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_transpose[ax0, ax1 * 3 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, T_transpose[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 224, 3), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_transpose = T.alloc_buffer([1, 3, 224, 224], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 3, 224, 224):
                with T.block("T_transpose"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax2, ax3, ax1])
                    T.writes(T_transpose[ax0, ax1, ax2, ax3])
                    T_transpose[ax0, ax1, ax2, ax3] = placeholder[ax0, ax2, ax3, ax1]
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(T_transpose[ax0, ax1 * 3 + ax4, ax2, ax3])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, T_transpose[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_transpose", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"
[15:58:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 225, 225, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 225, 225, 3):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(0 <= i2_1 and i2_1 < 224 and 0 <= i3_1 and i3_1 < 224, placeholder[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 112, 112, 4, 3, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 225, 225, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 225, 225, 3):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 224 and 0 <= i3 and i3 < 224, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 28, 1, 3, 3, 1, 1, 2, 16, 2, 1, 1, 1, 3, 1, 1, 7, 2, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_2)
                        oh = T.axis.spatial(112, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(112, i3_1 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_1])
                        T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 16, 7])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 28, 2, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 225, 225, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 225, 225, 3):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 224 and 0 <= i3 and i3 < 224, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 1, 1, 1, 28, 1):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 2, 16, 2, 1, 1, 1, 3, 1, 1, 7, 2, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_2)
                            oh = T.axis.spatial(112, i2_2 * 7 + i2_3)
                            ow = T.axis.spatial(112, i3_1 * 4 + i3_2 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_1])
                            T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 112, 4, 2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(8, i1_0 * 2 + ax1)
                            i2 = T.axis.spatial(112, ax2)
                            i3 = T.axis.spatial(112, i3_1 * 4 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 16, 7])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 28, 2, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 28, 1, 3, 3, 1, 1, 2, 16, 2, 1, 1, 1, 3, 1, 1, 7, 2, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_2)
                        oh = T.axis.spatial(112, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(112, i3_1 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_1])
                        T.reads(placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 224 and 0 <= ow * 2 + kw and ow * 2 + kw < 224, placeholder[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 112, 112, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i1_0 * 2 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 16, 7])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 28, 2, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"
[15:58:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 114, 114, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 8, 112, 112, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 2, 7, 7, 1, 1, 2, 8, 2, 2, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 10, 2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + ax1)
                        i2 = T.axis.spatial(114, i2_0 * 16 + i2_1 * 2 + i5_0 + ax2)
                        i3 = T.axis.spatial(114, i3_0 * 16 + i3_1 * 8 + ax3)
                        i4 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 2, 1, 1, 1, 3, 1, 1, 1, 8, 2):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(112, i2_0 * 16 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(112, i3_0 * 16 + i3_1 * 8 + i3_3)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 8, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 2, 1, 8])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 2, 7, 7, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 18, 18, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + ax1)
                        i2 = T.axis.spatial(114, i2_0 * 16 + ax2)
                        i3 = T.axis.spatial(114, i3_0 * 16 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(8, 2, 2):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 2, 2, 1, 1, 1, 3, 1, 1, 1, 8, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + i1_2)
                            oh = T.axis.spatial(112, i2_0 * 16 + i2_1 * 2 + i2_2)
                            ow = T.axis.spatial(112, i3_0 * 16 + i3_1 * 8 + i3_3)
                            oci = T.axis.spatial(4, i4_1 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 8, 2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + ax1)
                            i2 = T.axis.spatial(112, i2_0 * 16 + i2_1 * 2 + ax2)
                            i3 = T.axis.spatial(112, i3_0 * 16 + i3_1 * 8 + ax3)
                            i4 = T.axis.spatial(4, i4_1 * 2 + ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 8, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 2, 1, 8])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 7, 7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 8, 2, 2, 3, 1, 1, 2, 2, 1, 1, 1, 3, 1, 1, 1, 8, 2):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(8, i1_0 * 4 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(112, i2_0 * 16 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(112, i3_0 * 16 + i3_1 * 8 + i3_3)
                        oci = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                        T.reads(placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh + kh and oh + kh < 113 and 1 <= ow + kw and ow + kw < 113, placeholder[b, oci // 4 + oco, oh + kh - 1, ow + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 16, 16, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i1_0 * 4 + ax1)
                        i2 = T.axis.spatial(112, i2_0 * 16 + ax2)
                        i3 = T.axis.spatial(112, i3_0 * 16 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 8, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 2, 1, 8])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_contrib_conv2d_NCHWc_add"
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(6, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 6, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 6, 112, 112, 4, 32, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [6, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 6, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(6, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 6, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 6, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 2, 1, 1, 1, 7, 1, 2, 2, 1, 1, 1, 3, 1, 4, 1, 16, 1, 1, 1, 2, 8, 14, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(6, i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(112, i2_0 * 56 + i2_1 * 8 + i2_3)
                    ow = T.axis.spatial(112, i3_0 * 56 + i3_2 * 14 + i3_3)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [6, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 6, 112, 112, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 3, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 7, 1, 8])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 14])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[2, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(6, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 6, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 6, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 2, 1, 1, 1, 7, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 3, 1, 4, 1, 16, 1, 1, 1, 2, 8, 14, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(112, i2_0 * 56 + i2_1 * 8 + i2_3)
                        ow = T.axis.spatial(112, i3_0 * 56 + i3_2 * 14 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [6, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 8, 56, 2):
                    with T.block("T_add"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(112, i2_0 * 56 + i2_1 * 8 + ax2)
                        ax3_1 = T.axis.spatial(112, i3_0 * 56 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 3, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 7, 1, 8])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 14])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[2, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(6, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 6, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 6, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 2, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 1, 2, 2, 1, 1, 1, 3, 1, 4, 1, 16, 1, 1, 1, 2, 8, 14, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(112, i2_0 * 56 + i2_1 * 8 + i2_3)
                        ow = T.axis.spatial(112, i3_0 * 56 + i3_2 * 14 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(32, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [6, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 56, 56, 4):
                    with T.block("T_add"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(112, i2_0 * 56 + ax2)
                        ax3_1 = T.axis.spatial(112, i3_0 * 56 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 3, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 7, 1, 8])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 14])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[2, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"
[15:58:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(36, 6, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], compute: T.Buffer[(1, 36, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 36, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 36, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 36, 112, 112, 4, 24, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 112, 112, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 112, 112, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(36, 6, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], compute: T.Buffer[(1, 36, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 36, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 14, 1, 1, 1, 6, 4, 28, 1, 2, 1, 1, 1, 1, 2, 4, 1, 12, 1, 1, 1, 2, 1, 1, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(36, i1_0 * 12 + i1_1 * 2 + i1_3)
                    oh = T.axis.spatial(112, i2_0 * 8 + i2_1 * 2 + i2_2)
                    ow = T.axis.spatial(112, i3_1 * 4 + i3_2)
                    oc_block = T.axis.spatial(4, i4_3)
                    ic = T.axis.reduce(24, i5_0 * 12 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 112, 112, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 36, 112, 112, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 6, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 4, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 28, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 12])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(36, 6, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], compute: T.Buffer[(1, 36, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 36, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 3, 14, 1, 1, 1, 6, 4, 28, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 1, 2, 4, 1, 12, 1, 1, 1, 2, 1, 1, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(36, i1_0 * 12 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(112, i2_0 * 8 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(112, i3_1 * 4 + i3_2)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(24, i5_0 * 12 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 112, 112, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 4, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(36, i1_0 * 12 + i1_1 * 2 + ax1)
                        i2 = T.axis.spatial(112, i2_0 * 8 + i2_1 * 2 + ax2)
                        i3 = T.axis.spatial(112, i3_1 * 4 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 6, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 4, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 28, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 12])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(36, 6, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], compute: T.Buffer[(1, 36, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 36, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 3, 14, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 6, 4, 28, 1, 2, 1, 1, 1, 1, 2, 4, 1, 12, 1, 1, 1, 2, 1, 1, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(36, i1_0 * 12 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(112, i2_0 * 8 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(112, i3_1 * 4 + i3_2)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(24, i5_0 * 12 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 112, 112, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 12, 8, 112, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(36, i1_0 * 12 + ax1)
                        i2 = T.axis.spatial(112, i2_0 * 8 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 6, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 4, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 28, 4, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 12])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], compute: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 113, 113, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(0 <= i2_1 and i2_1 < 112 and 0 <= i3_1 and i3_1 < 112, placeholder[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 36, 56, 56, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 112, 112, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 56, 56, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], compute: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 36, 113, 113, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 2, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 18, 29, 113, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(36, i1_0 * 18 + ax1)
                        i2 = T.axis.spatial(113, i2_0 * 28 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 3, 1, 28, 4, 1, 1, 1, 2, 14, 2, 1, 3, 3, 1, 3, 1, 1, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i1_0 * 18 + i1_1 * 6 + i1_2 * 3 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 14 + i2_2)
                        ow = T.axis.spatial(56, i3_1 * 2 + i3_2)
                        oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_1, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 112, 112, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 36, 56, 56, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 3, 2, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 28, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], compute: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 36, 113, 113, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 4, 1, 1, 1, 3, 1, 28, 4):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2 in T.grid(1, 1, 1, 2, 14, 2):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 3, 3, 3, 1):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(36, i1_0 * 18 + i1_1 * 6 + i1_2 * 3 + ax1)
                            i2 = T.axis.spatial(113, i2_0 * 28 + i2_2 * 2 + ax2)
                            i3 = T.axis.spatial(113, i3_1 * 4 + i3_2 * 2 + ax3)
                            i4 = T.axis.spatial(4, i4_1 + ax4)
                            T.reads(placeholder[i0, i1, i2, i3, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                    for i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 3, 1, 3, 1, 1, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(36, i1_0 * 18 + i1_1 * 6 + i1_2 * 3 + i1_3)
                            oh = T.axis.spatial(56, i2_0 * 14 + i2_2)
                            ow = T.axis.spatial(56, i3_1 * 2 + i3_2)
                            oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 112, 112, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 14, 2, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(36, i1_0 * 18 + i1_1 * 6 + ax1)
                        i2 = T.axis.spatial(56, i2_0 * 14 + ax2)
                        i3 = T.axis.spatial(56, i3_1 * 2 + ax3)
                        i4 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 3, 2, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 28, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=15)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], compute: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 36, 113, 113, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 18, 113, 113, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(36, i1_0 * 18 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0 in T.grid(4, 1, 1):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 28, 4, 1, 1, 1, 2, 14, 2, 1, 3, 3, 1, 3, 1, 1, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(36, i1_0 * 18 + i1_1 * 6 + i1_2 * 3 + i1_3)
                            oh = T.axis.spatial(56, i2_0 * 14 + i2_2)
                            ow = T.axis.spatial(56, i3_1 * 2 + i3_2)
                            oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 112, 112, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 18, 14, 56, 4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(36, i1_0 * 18 + ax1)
                            i2 = T.axis.spatial(56, i2_0 * 14 + ax2)
                            i3, i4 = T.axis.remap("SS", [ax3, ax4])
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 3, 2, 3])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 14, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 28, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"
[15:58:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 56, 56, 4, 144, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 7, 1, 1, 1, 1, 1, 4, 1, 36, 1, 1, 1, 2, 1, 14, 2, 4, 1, 1, 1, 1, 8, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_2)
                    oh = T.axis.spatial(56, i2_0 * 8 + i2_3)
                    ow = T.axis.spatial(56, i3_1 * 14 + i3_2)
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(144, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 56, 56, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 1, 1, 8])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 14, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[36, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 7, 1, 1, 1, 1, 1, 4, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(36, 1, 1, 1, 2, 1, 14, 2, 4, 1, 1, 1, 1, 8, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_2)
                        oh = T.axis.spatial(56, i2_0 * 8 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 14 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(144, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 8, 14, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 8 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_1 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 1, 1, 8])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 14, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[36, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 7, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 4, 1, 36, 1, 1, 1, 2, 1, 14, 2, 4, 1, 1, 1, 1, 8, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_2)
                        oh = T.axis.spatial(56, i2_0 * 8 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 14 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(144, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 8, 56, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 8 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 1, 1, 8])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 14, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[36, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 58, 58, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 48, 56, 56, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 48, 58, 58, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 6, 28, 7, 1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 4, 10, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(48, i1_0 * 8 + i1_1 * 2 + ax1)
                        i2 = T.axis.spatial(58, i2_0 * 2 + ax2)
                        i3 = T.axis.spatial(58, i3_0 * 8 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 3, 1, 1, 2, 2, 1, 3, 1, 1, 2, 1, 4, 4):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(48, i1_0 * 8 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 2 + i2_2)
                        ow = T.axis.spatial(56, i3_0 * 8 + i3_2 * 4 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_1, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[6, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[28, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 2, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 48, 58, 58, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 6, 28, 7, 1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 4, 10, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(48, i1_0 * 8 + i1_1 * 2 + ax1)
                        i2 = T.axis.spatial(58, i2_0 * 2 + ax2)
                        i3 = T.axis.spatial(58, i3_0 * 8 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(1, 1, 1):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 1, 2, 2, 1, 3, 1, 1, 2, 1, 4, 4):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i1_0 * 8 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(56, i2_0 * 2 + i2_2)
                            ow = T.axis.spatial(56, i3_0 * 8 + i3_2 * 4 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_1, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 8, 4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(48, i1_0 * 8 + i1_1 * 2 + ax1)
                            i2 = T.axis.spatial(56, i2_0 * 2 + ax2)
                            i3 = T.axis.spatial(56, i3_0 * 8 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[6, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[28, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 2, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 48, 58, 58, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 6, 28):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 4, 58, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(48, i1_0 * 8 + ax1)
                        i2 = T.axis.spatial(58, i2_0 * 2 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0 in T.grid(7, 1):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 1, 1, 3, 1, 1, 2, 2, 1, 3, 1, 1, 2, 1, 4, 4):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i1_0 * 8 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(56, i2_0 * 2 + i2_2)
                            ow = T.axis.spatial(56, i3_0 * 8 + i3_2 * 4 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_1, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 2, 8, 4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(48, i1_0 * 8 + ax1)
                            i2 = T.axis.spatial(56, i2_0 * 2 + ax2)
                            i3 = T.axis.spatial(56, i3_0 * 8 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[6, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[28, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 2, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 56, 56, 4, 192, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 56, 56, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 14, 4, 1, 1, 1, 1, 1, 96, 1, 1, 1, 1, 8, 1, 1, 2, 1, 1, 1, 1, 7, 4, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_0)
                    oh = T.axis.spatial(56, i2_2 * 7 + i2_3)
                    ow = T.axis.spatial(56, i3_0 * 4 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(192, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 56, 56, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 8, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[96, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 14, 4, 1, 1, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(96, 1, 1, 1, 1, 8, 1, 1, 2, 1, 1, 1, 1, 7, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0)
                        oh = T.axis.spatial(56, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(192, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 56, 4, 1):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 + ax1)
                        ax2_1 = T.axis.spatial(56, ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 8, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[96, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 1, 14, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 96, 1, 1, 1, 1, 8, 1, 1, 2, 1, 1, 1, 1, 7, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0)
                        oh = T.axis.spatial(56, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(192, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 56, 4, 1):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 + ax1)
                        ax2_1 = T.axis.spatial(56, ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 8, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[96, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #9: "fused_nn_contrib_conv2d_NCHWc_add_clip_2"
[15:58:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 48, 56, 56, 4, 32, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 56, 56, 4], "float32"], ["TENSOR", [48, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 2, 1, 2, 56, 1, 1, 32, 1, 1, 1, 8, 1, 2, 1, 1, 1, 1, 1, 3, 1, 4, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(48, i1_1 * 24 + i1_2 * 3 + i1_3)
                    oh = T.axis.spatial(56, i2_1)
                    ow = T.axis.spatial(56, i3_0 * 8 + i3_2 * 4 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(32, i5_0)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 56, 56, 4], "float32"], ["TENSOR", [48, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 8, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 56, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 7, 2, 1, 2, 56, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 8, 1, 2, 1, 1, 1, 1, 1, 3, 1, 4, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(48, i1_1 * 24 + i1_2 * 3 + i1_3)
                        oh = T.axis.spatial(56, i2_1)
                        ow = T.axis.spatial(56, i3_0 * 8 + i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(32, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 56, 56, 4], "float32"], ["TENSOR", [48, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 24, 1, 8, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(48, i1_1 * 24 + ax1)
                        i2 = T.axis.spatial(56, i2_1 + ax2)
                        i3 = T.axis.spatial(56, i3_0 * 8 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 8, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 56, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 7, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 56, 1, 1, 32, 1, 1, 1, 8, 1, 2, 1, 1, 1, 1, 1, 3, 1, 4, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(48, i1_1 * 24 + i1_2 * 3 + i1_3)
                        oh = T.axis.spatial(56, i2_1)
                        ow = T.axis.spatial(56, i3_0 * 8 + i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(32, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 56, 56, 4], "float32"], ["TENSOR", [48, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 48, 56, 8, 2):
                    with T.block("compute"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(56, i3_0 * 8 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 8, 3])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 56, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #10: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3"
[15:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 59, 59, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 59, 59, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 48, 28, 28, 4, 5, 5):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 5, 5, 1, 4], "float32"], [2, 2], [1, 1, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 48, 59, 59, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 6):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 59, 59, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(48, i1_0 * 8 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 2, 14, 28, 1, 1, 5, 1, 4, 2, 1, 2, 5, 1, 1, 1, 1, 1, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(48, i1_0 * 8 + i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(28, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_1)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 5, 5, 1, 4], "float32"], [2, 2], [1, 1, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 48, 28, 28, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[6, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 28, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[5, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 48, 59, 59, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 48, 59, 59, 4):
                with T.block("PaddedInput"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                    PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 6, 1, 1, 2, 1, 2, 14, 28, 1):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 5, 1, 4, 2, 1, 2, 5, 1, 1, 1, 1, 1, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(48, i1_0 * 8 + i1_1_1 * 4 + i1_2)
                        oh = T.axis.spatial(28, i2_1_1 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_1_1)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 5, 5, 1, 4], "float32"], [2, 2], [1, 1, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 2, 1, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(48, i1_0 * 8 + i1_1_1 * 4 + ax1)
                        i2 = T.axis.spatial(28, i2_1_1 * 2 + ax2)
                        i3 = T.axis.spatial(28, i3_1_1 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[6, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 28, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[5, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 48, 59, 59, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 48, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 6, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1 in T.grid(1, 2, 14, 28):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 7, 5, 2):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(48, i1_0 * 8 + i1_1 * 4 + ax1)
                            i2 = T.axis.spatial(59, i2_1 * 4 + ax2)
                            i3 = T.axis.spatial(59, i3_1 * 2 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 5, 1, 4, 2, 1, 2, 5, 1, 1, 1, 1, 1, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i1_0 * 8 + i1_1 * 4 + i1_2)
                            oh = T.axis.spatial(28, i2_1 * 2 + i2_2)
                            ow = T.axis.spatial(28, i3_1)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 5, 5, 1, 4], "float32"], [2, 2], [1, 1, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 28, 28, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(48, i1_0 * 8 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[6, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 28, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[5, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #11: "fused_nn_contrib_conv2d_NCHWc_add_2"
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(14, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 14, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 14, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 14, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 14, 28, 28, 4, 192, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [14, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 14, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(14, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 14, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 14, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 14, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 7, 1, 1, 1, 1, 1, 1, 2, 2, 24, 1, 1, 1, 1, 4, 7, 2, 8, 1, 1, 1, 2, 7, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(14, i1_0 * 2 + i1_3)
                    oh = T.axis.spatial(28, i2_2 * 7 + i2_3)
                    ow = T.axis.spatial(28, i3_1 * 14 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                    ic = T.axis.reduce(192, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [14, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 14, 28, 28, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[24, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(14, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 14, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 14, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 14, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 7, 1, 1, 1, 1, 1, 1, 2, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(24, 1, 1, 1, 1, 4, 7, 2, 8, 1, 1, 1, 2, 7, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(14, i1_0 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_1 * 14 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(192, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [14, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 28, 14, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(14, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(28, ax2)
                        ax3_1 = T.axis.spatial(28, i3_1 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[24, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(14, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 14, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 14, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 14, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 7, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 2, 24, 1, 1, 1, 1, 4, 7, 2, 8, 1, 1, 1, 2, 7, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(14, i1_0 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_1 * 14 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(192, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 28, 28, 4], "float32"], ["TENSOR", [14, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 28, 28, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(14, i1_0 * 2 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[24, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #12: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4"
[15:58:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(84, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 84, 1, 1, 4), "float32"], compute: T.Buffer[(1, 84, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 84, 32, 32, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 84, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 84, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 84, 32, 32, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(2 <= i2_1 and i2_1 < 30 and 2 <= i3_1 and i3_1 < 30, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 84, 28, 28, 4, 5, 5):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 84, 28, 28, 4], "float32"], ["TENSOR", [84, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 84, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 84, 28, 28, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(84, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 84, 1, 1, 4), "float32"], compute: T.Buffer[(1, 84, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 84, 32, 32, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 84, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 2, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 42, 18, 32, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(84, i1_0 * 42 + ax1)
                        i2 = T.axis.spatial(32, i2_0 * 14 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 2, i3 - 2, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(2 <= i2 and i2 < 30 and 2 <= i3 and i3 < 30, placeholder[i0, i1, i2 - 2, i3 - 2, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 6, 7, 1, 4, 5, 5, 1, 7, 1, 14, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(84, i1_0 * 42 + i1_2 * 7 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 14 + i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_1, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 84, 28, 28, 4], "float32"], ["TENSOR", [84, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 84, 28, 28, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 6, 7])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(84, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 84, 1, 1, 4), "float32"], compute: T.Buffer[(1, 84, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 84, 32, 32, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 84, 28, 28, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 42, 32, 32, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(84, i1_0 * 42 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 2, i3 - 2, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(2 <= i2 and i2 < 30 and 2 <= i3 and i3 < 30, placeholder[i0, i1, i2 - 2, i3 - 2, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 2, 1, 1, 1, 2, 1, 1):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 6, 7, 1, 4, 5, 5, 1, 7, 1, 14, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(84, i1_0 * 42 + i1_2 * 7 + i1_3)
                            oh = T.axis.spatial(28, i2_0 * 14 + i2_1 * 7 + i2_2)
                            ow = T.axis.spatial(28, i3_0 * 14 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 84, 28, 28, 4], "float32"], ["TENSOR", [84, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 42, 7, 14, 4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(84, i1_0 * 42 + ax1)
                            i2 = T.axis.spatial(28, i2_0 * 14 + i2_1 * 7 + ax2)
                            i3 = T.axis.spatial(28, i3_0 * 14 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 6, 7])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(84, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 84, 1, 1, 4), "float32"], compute: T.Buffer[(1, 84, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 84, 32, 32, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 84, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 2, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2 in T.grid(1, 1, 2, 1, 1, 1, 1, 1, 6):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 7, 11, 18, 4):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(84, i1_0 * 42 + i1_2 * 7 + ax1)
                            i2 = T.axis.spatial(32, i2_0 * 14 + i2_1 * 7 + ax2)
                            i3 = T.axis.spatial(32, i3_0 * 14 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 2, i3 - 2, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(2 <= i2 and i2 < 30 and 2 <= i3 and i3 < 30, placeholder[i0, i1, i2 - 2, i3 - 2, i4], T.float32(0), dtype="float32")
                    for i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(7, 1, 4, 5, 5, 1, 7, 1, 14, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(84, i1_0 * 42 + i1_2 * 7 + i1_3)
                            oh = T.axis.spatial(28, i2_0 * 14 + i2_1 * 7 + i2_2)
                            ow = T.axis.spatial(28, i3_0 * 14 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 84, 28, 28, 4], "float32"], ["TENSOR", [84, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 42, 14, 14, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(84, i1_0 * 42 + ax1)
                        i2 = T.axis.spatial(28, i2_0 * 14 + ax2)
                        i3 = T.axis.spatial(28, i3_0 * 14 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 6, 7])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=13)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #13: "fused_nn_contrib_conv2d_NCHWc_add_add_1"
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(14, 84, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 14, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 14, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 14, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 14, 28, 28, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 14, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 14, 28, 28, 4, 336, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 84, 28, 28, 4], "float32"], ["TENSOR", [14, 84, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 14, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 14, 28, 28, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(14, 84, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 14, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 14, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 14, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 14, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 7, 1, 7, 1, 1, 2, 2, 4, 2, 56, 1, 1, 1, 1, 7, 1, 1, 6, 1, 1, 1, 1, 2, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(14, i1_0 * 2 + i1_1)
                    oh = T.axis.spatial(28, i2_1 * 14 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(28, i3_0 * 4 + i3_1)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(336, i5_0 * 6 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 84, 28, 28, 4], "float32"], ["TENSOR", [14, 84, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 14, 28, 28, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 4, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[56, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(14, 84, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 14, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 14, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 14, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 14, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 7, 1, 7, 1, 1, 2, 2, 4, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(56, 1, 1, 1, 1, 7, 1, 1, 6, 1, 1, 1, 1, 2, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(14, i1_0 * 2 + i1_1)
                        oh = T.axis.spatial(28, i2_1 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 4 + i3_1)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(336, i5_0 * 6 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 84, 28, 28, 4], "float32"], ["TENSOR", [14, 84, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 14, 1, 2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(14, i1_0 * 2 + i1_1 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_1 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 4 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 4, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[56, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(14, 84, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 14, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 14, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 14, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 14, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 7, 1, 7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 4, 2, 56, 1, 1, 1, 1, 7, 1, 1, 6, 1, 1, 1, 1, 2, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(14, i1_0 * 2 + i1_1)
                        oh = T.axis.spatial(28, i2_1 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 4 + i3_1)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(336, i5_0 * 6 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 84, 28, 28, 4], "float32"], ["TENSOR", [14, 84, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 28, 4, 4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(14, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(28, ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 4, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[56, 6])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #14: "fused_nn_contrib_conv2d_NCHWc_add_clip_3"
[15:58:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 14, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(84, 14, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 84, 1, 1, 4), "float32"], compute: T.Buffer[(1, 84, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 84, 28, 28, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 84, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 84, 28, 28, 4, 56, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 14, 28, 28, 4], "float32"], ["TENSOR", [84, 14, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 84, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 84, 28, 28, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 14, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(84, 14, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 84, 1, 1, 4), "float32"], compute: T.Buffer[(1, 84, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 84, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 2, 1, 1, 7, 1, 2, 1, 56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 28, 7, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(84, i1_0 * 28 + i1_1 * 4 + i1_3)
                    oh = T.axis.spatial(28, i2_3)
                    ow = T.axis.spatial(28, i3_0 * 14 + i3_1 * 7 + i3_3)
                    oc_block, ic = T.axis.remap("SR", [i4_3, i5_0])
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 14, 28, 28, 4], "float32"], ["TENSOR", [84, 14, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 84, 28, 28, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 7, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 28])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[56, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 14, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(84, 14, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 84, 1, 1, 4), "float32"], compute: T.Buffer[(1, 84, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 84, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 3, 1, 2, 1, 1, 7, 1, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 28, 7, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(84, i1_0 * 28 + i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(28, i2_3)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_1 * 7 + i3_3)
                        oc_block, ic = T.axis.remap("SR", [i4_3, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 14, 28, 28, 4], "float32"], ["TENSOR", [84, 14, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 28, 7, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(84, i1_0 * 28 + i1_1 * 4 + ax1)
                        i2 = T.axis.spatial(28, ax2)
                        i3 = T.axis.spatial(28, i3_0 * 14 + i3_1 * 7 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 7, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 28])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[56, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 14, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(84, 14, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 84, 1, 1, 4), "float32"], compute: T.Buffer[(1, 84, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 84, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 3, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 7, 1, 2, 1, 56, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 28, 7, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(84, i1_0 * 28 + i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(28, i2_3)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_1 * 7 + i3_3)
                        oc_block, ic = T.axis.remap("SR", [i4_3, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 14, 28, 28, 4], "float32"], ["TENSOR", [84, 14, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 28, 28, 14, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(84, i1_0 * 28 + ax1)
                        i2 = T.axis.spatial(28, ax2)
                        i3 = T.axis.spatial(28, i3_0 * 14 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[3, 7, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 28])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[56, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #15: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5"
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(84, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 84, 1, 1, 4), "float32"], compute: T.Buffer[(1, 84, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 84, 29, 29, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 84, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 84, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 84, 29, 29, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(0 <= i2_1 and i2_1 < 28 and 0 <= i3_1 and i3_1 < 28, placeholder[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 84, 14, 14, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 84, 28, 28, 4], "float32"], ["TENSOR", [84, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 84, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 84, 14, 14, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(84, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 84, 1, 1, 4), "float32"], compute: T.Buffer[(1, 84, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 84, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 1, 1, 6, 1, 7, 1, 3, 1, 1, 14, 7, 1, 2, 1, 3, 1, 1, 2, 1, 2):
                with T.block("DepthwiseConv2d"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(84, i1_1 * 14 + i1_2)
                    oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(14, i3_0 * 7 + i3_1)
                    oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                    T.reads(placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 84, 28, 28, 4], "float32"], ["TENSOR", [84, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(0 <= oh * 2 + kh and oh * 2 + kh < 28 and 0 <= ow * 2 + kw and ow * 2 + kw < 28, placeholder[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 84, 14, 14, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 6, 14, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(84, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 84, 1, 1, 4), "float32"], compute: T.Buffer[(1, 84, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 84, 29, 29, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 84, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 1, 1, 6, 1, 7, 1):
                for i5_0 in T.serial(3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 14, 27, 3, 4):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(84, i1_1 * 14 + ax1)
                            i2 = T.axis.spatial(29, i5_0 + ax2)
                            i3 = T.axis.spatial(29, i3_0 * 14 + i3_1 * 2 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2, i3, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 28 and 0 <= i3 and i3 < 28, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 14, 7, 1, 2, 1, 3, 1, 1, 2, 1, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(84, i1_1 * 14 + i1_2)
                            oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(14, i3_0 * 7 + i3_1)
                            oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 84, 28, 28, 4], "float32"], ["TENSOR", [84, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 14, 14, 1, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(84, i1_1 * 14 + ax1)
                        i2 = T.axis.spatial(14, ax2)
                        i3 = T.axis.spatial(14, i3_0 * 7 + i3_1 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 6, 14, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(84, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 84, 1, 1, 4), "float32"], compute: T.Buffer[(1, 84, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 84, 29, 29, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 84, 14, 14, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 84, 29, 29, 4):
                with T.block("PaddedInput"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1, i3_1, i4_1])
                    T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                    PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(0 <= i2_1 and i2_1 < 28 and 0 <= i3_1 and i3_1 < 28, placeholder[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 2, 1):
                for i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 6, 1, 7, 1, 3, 1, 1, 14, 7, 1, 2, 1, 3, 1, 1, 2, 1, 2):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(84, i1_1_1 * 14 + i1_2)
                        oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_1_1)
                        oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 84, 28, 28, 4], "float32"], ["TENSOR", [84, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 84, 14, 7, 4):
                    with T.block("compute"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 6, 14, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #16: "fused_nn_contrib_conv2d_NCHWc_add_3"
[15:58:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(28, 84, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 28, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 28, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 28, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 28, 14, 14, 4, 336, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 84, 14, 14, 4], "float32"], ["TENSOR", [28, 84, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 28, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(28, 84, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 28, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 28, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 28, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 14, 1, 14, 1, 1, 2, 2, 1, 1, 21, 1, 1, 1, 1, 1, 1, 4, 16, 1, 1, 1, 1, 7, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(28, i1_0 * 2 + i1_1)
                    oh = T.axis.spatial(14, i2_1 * 7 + i2_3)
                    ow, oc_block = T.axis.remap("SS", [i3_0, i4_2])
                    ic = T.axis.reduce(336, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 84, 14, 14, 4], "float32"], ["TENSOR", [28, 84, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 28, 14, 14, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[21, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(28, 84, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 28, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 28, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 28, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 14, 1, 14, 1, 1, 2, 2, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(21, 1, 1, 1, 1, 1, 1, 4, 16, 1, 1, 1, 1, 7, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(28, i1_0 * 2 + i1_1)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_3)
                        ow, oc_block = T.axis.remap("SS", [i3_0, i4_2])
                        ic = T.axis.reduce(336, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 84, 14, 14, 4], "float32"], ["TENSOR", [28, 84, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 7, 1, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(28, i1_0 * 2 + i1_1 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_1 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[21, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 84, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(28, 84, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 28, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 28, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 28, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 14, 1, 14, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 1, 1, 21, 1, 1, 1, 1, 1, 1, 4, 16, 1, 1, 1, 1, 7, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(28, i1_0 * 2 + i1_1)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_3)
                        ow, oc_block = T.axis.remap("SS", [i3_0, i4_2])
                        ic = T.axis.reduce(336, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 84, 14, 14, 4], "float32"], ["TENSOR", [28, 84, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 14, 1, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(28, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[21, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #17: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6"
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(168, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], compute: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 168, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 16, 16, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 168, 14, 14, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [168, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(168, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], compute: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 168, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 14, 7, 1, 1, 1, 3, 2, 1, 4, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 16, 1):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(168, i1_0 * 12 + i1_1 * 4 + ax1)
                        i2 = T.axis.spatial(16, i2_0 * 2 + i2_1 + i5_0 + ax2)
                        i3 = T.axis.spatial(16, ax3)
                        i4 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 1, 7, 1, 1, 3, 1, 1, 1, 2, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(168, i1_0 * 12 + i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(14, i2_0 * 2 + i2_1)
                        ow = T.axis.spatial(14, i3_2 * 2 + i3_3)
                        oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_0, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [168, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 3, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(168, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], compute: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 168, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1 in T.grid(1, 14, 7, 1, 1, 1, 3, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 3, 16, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(168, i1_0 * 12 + i1_1 * 4 + ax1)
                        i2 = T.axis.spatial(16, i2_0 * 2 + i2_1 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_1, i4_1 in T.grid(1, 4):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 4, 1, 7, 1, 1, 3, 1, 1, 1, 2, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(168, i1_0 * 12 + i1_1 * 4 + i1_2)
                            oh = T.axis.spatial(14, i2_0 * 2 + i2_1)
                            ow = T.axis.spatial(14, i3_2 * 2 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_0, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [168, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 14, 1):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(168, i1_0 * 12 + i1_1 * 4 + ax1)
                            i2 = T.axis.spatial(14, i2_0 * 2 + i2_1 + ax2)
                            i3 = T.axis.spatial(14, ax3)
                            i4 = T.axis.spatial(4, i4_1 + ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 3, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(168, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], compute: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 168, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 14):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 12, 16, 16, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(168, i1_0 * 12 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0 in T.grid(7, 1, 1):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 2, 1, 4, 3, 1, 1, 4, 1, 7, 1, 1, 3, 1, 1, 1, 2, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(168, i1_0 * 12 + i1_1 * 4 + i1_2)
                            oh = T.axis.spatial(14, i2_0 * 2 + i2_1)
                            ow = T.axis.spatial(14, i3_2 * 2 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_0, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [168, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 12, 2, 14, 4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(168, i1_0 * 12 + ax1)
                            i2 = T.axis.spatial(14, i2_0 * 2 + ax2)
                            i3, i4 = T.axis.remap("SS", [ax3, ax4])
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 3, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"
[15:58:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(28, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 28, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 28, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 28, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 28, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 28, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 28, 14, 14, 4, 672, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [28, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 28, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 28, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(28, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 28, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 28, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 28, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 28, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 14, 7, 1, 4, 84, 1, 1, 1, 1, 2, 1, 1, 8, 1, 1, 1, 2, 1, 14, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(28, i1_1 * 2 + i1_3)
                    oh = T.axis.spatial(14, i2_1 * 2 + i2_2)
                    ow, oc_block = T.axis.remap("SS", [i3_3, i4_1])
                    ic = T.axis.reduce(672, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [28, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 28, 14, 14, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 14, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[84, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(28, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 28, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 28, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 28, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 28, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 14, 7, 1, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(84, 1, 1, 1, 1, 2, 1, 1, 8, 1, 1, 1, 2, 1, 14, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(28, i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_2)
                        ow, oc_block = T.axis.remap("SS", [i3_3, i4_1])
                        ic = T.axis.reduce(672, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [28, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 14, 1):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(28, i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 14, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[84, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(28, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 28, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 28, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 28, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 28, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 14, 7, 1, 4, 84, 1, 1, 1, 1, 2, 1, 1, 8, 1, 1, 1, 2, 1, 14, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(28, i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 2 + i2_2)
                        ow, oc_block = T.axis.remap("SS", [i3_3, i4_1])
                        ic = T.axis.reduce(672, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [28, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 28, 14, 14, 4):
                    with T.block("T_add_1"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 14, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[84, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #19: "fused_nn_contrib_conv2d_NCHWc_add_clip_4"
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 28, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(168, 28, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], compute: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 168, 14, 14, 4, 112, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 28, 14, 14, 4], "float32"], ["TENSOR", [168, 28, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 28, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(168, 28, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], compute: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 7, 1, 1, 1, 2, 7, 1, 4, 16, 1, 1, 1, 21, 1, 7, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(168, i1_0 * 84 + i1_1 * 42 + i1_2 * 21 + i1_3)
                    oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                    ow = T.axis.spatial(14, i3_0 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(112, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 28, 14, 14, 4], "float32"], ["TENSOR", [168, 28, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 21])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[7, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 28, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(168, 28, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], compute: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 2, 1, 1, 2, 2, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(7, 1, 1, 1, 2, 7, 1, 4, 16, 1, 1, 1, 21, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(168, i1_0 * 84 + i1_1 * 42 + i1_2 * 21 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(112, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 28, 14, 14, 4], "float32"], ["TENSOR", [168, 28, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 42, 7, 7, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(168, i1_0 * 84 + i1_1 * 42 + ax1)
                        i2 = T.axis.spatial(14, i2_1 * 7 + ax2)
                        i3 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 21])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[7, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 28, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(168, 28, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], compute: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 1, 1, 7, 1, 1, 1, 2, 7, 1, 4, 16, 1, 1, 1, 21, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(168, i1_0 * 84 + i1_1 * 42 + i1_2 * 21 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(112, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 28, 14, 14, 4], "float32"], ["TENSOR", [168, 28, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 84, 14, 7, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(168, i1_0 * 84 + ax1)
                        i2 = T.axis.spatial(14, ax2)
                        i3 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 21])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[7, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #20: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7"
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(168, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], compute: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 168, 18, 18, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 18, 18, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(2 <= i2_1 and i2_1 < 16 and 2 <= i3_1 and i3_1 < 16, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 168, 14, 14, 4, 5, 5):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [168, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(168, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], compute: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 168, 18, 18, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 42, 18, 18, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(168, i1_0 * 42 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 2, i3 - 2, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(2 <= i2 and i2 < 16 and 2 <= i3 and i3 < 16, placeholder[i0, i1, i2 - 2, i3 - 2, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 21, 1, 1, 1, 1, 5, 1, 1, 7, 7, 1, 5, 1, 1, 2, 2, 1, 4):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(168, i1_0 * 42 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                        oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_1, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [168, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 168, 14, 14, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 21, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[5, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(168, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], compute: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 168, 18, 18, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 4, 1, 2, 1, 1, 21):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 18, 11, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(168, i1_0 * 42 + i1_1 * 2 + ax1)
                        i2 = T.axis.spatial(18, ax2)
                        i3 = T.axis.spatial(18, i3_0 * 7 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 2, i3 - 2, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(2 <= i2 and i2 < 16 and 2 <= i3 and i3 < 16, placeholder[i0, i1, i2 - 2, i3 - 2, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(1, 1, 1):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 5, 1, 1, 7, 7, 1, 5, 1, 1, 2, 2, 1, 4):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(168, i1_0 * 42 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                            oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_1, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [168, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 14, 7, 4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(168, i1_0 * 42 + i1_1 * 2 + ax1)
                            i2 = T.axis.spatial(14, ax2)
                            i3 = T.axis.spatial(14, i3_0 * 7 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 21, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[5, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(168, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 168, 1, 1, 4), "float32"], compute: T.Buffer[(1, 168, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 168, 18, 18, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 168, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0 in T.grid(1, 21, 1, 1, 1, 1, 5):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 18, 7, 4):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(168, i1_0 * 42 + i1_1 * 2 + ax1)
                            i2 = T.axis.spatial(18, ax2)
                            i3 = T.axis.spatial(18, i3_0 * 7 + i6_0 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 2, i3 - 2, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(2 <= i2 and i2 < 16 and 2 <= i3 and i3 < 16, placeholder[i0, i1, i2 - 2, i3 - 2, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 7, 1, 5, 1, 1, 2, 2, 1, 4):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(168, i1_0 * 42 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                            oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_1, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [168, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 42, 14, 7, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(168, i1_0 * 42 + ax1)
                        i2 = T.axis.spatial(14, ax2)
                        i3 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 21, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[5, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #21: "fused_nn_contrib_conv2d_NCHWc_add_4"
[15:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(40, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 40, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 40, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 40, 14, 14, 4, 672, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [40, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(40, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 40, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 40, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 5, 2, 1, 4, 1, 8, 7, 7, 1, 14, 1, 1, 1, 1, 1, 1, 1, 48, 1, 1, 1, 1, 1, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(40, i1_0 * 8 + i1_1)
                    oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                    ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(672, i5_0 * 48 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [40, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 40, 14, 14, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[5, 8, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[14, 48])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(40, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 40, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 40, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 5, 2, 1, 4, 1, 8, 7, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(14, 1, 1, 1, 1, 1, 1, 1, 48, 1, 1, 1, 1, 1, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(40, i1_0 * 8 + i1_1)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(672, i5_0 * 48 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [40, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 2, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(40, i1_0 * 8 + i1_1 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + i2_1 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[5, 8, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[14, 48])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 168, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(40, 168, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 40, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 40, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 5, 2, 1, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 7, 7, 1, 14, 1, 1, 1, 1, 1, 1, 1, 48, 1, 1, 1, 1, 1, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(40, i1_0 * 8 + i1_1)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_1)
                        ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(672, i5_0 * 48 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 168, 14, 14, 4], "float32"], ["TENSOR", [40, 168, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 14, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(40, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[5, 8, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[14, 48])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8"
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], compute: T.Buffer[(1, 240, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 240, 18, 18, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 240, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 240, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 18, 18, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(2 <= i2_1 and i2_1 < 16 and 2 <= i3_1 and i3_1 < 16, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 240, 14, 14, 4, 5, 5):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [240, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 14, 14, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], compute: T.Buffer[(1, 240, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 240, 18, 18, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 240, 14, 14, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 240, 18, 18, 4):
                with T.block("PaddedInput"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2, i4_1])
                    T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                    PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(2 <= i2_1 and i2_1 < 16 and 2 <= i3_1 and i3_1 < 16, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 7, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 5, 1, 1, 2, 5, 5, 1, 12, 1, 7, 2):
                with T.block("DepthwiseConv2d"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(240, i1_0 * 60 + i1_2 * 12 + i1_3)
                    oh = T.axis.spatial(14, i2_0 * 2 + i2_1_1)
                    ow = T.axis.spatial(14, i3_1_1 * 7 + i3_3)
                    oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                    T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [240, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 240, 14, 14, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 5, 12])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], compute: T.Buffer[(1, 240, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 240, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 7, 1, 1, 1, 1, 2, 2, 1):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 5, 1, 1, 2, 5, 5, 1, 12, 1, 7, 2):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i1_0 * 60 + i1_2 * 12 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 2 + i2_1)
                        ow = T.axis.spatial(14, i3_1 * 7 + i3_3)
                        oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                        T.reads(placeholder[b, oci // 4 + oco, oh + kh - 2, ow + kw - 2, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [240, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(2 <= oh + kh and oh + kh < 16 and 2 <= ow + kw and ow + kw < 16, placeholder[b, oci // 4 + oco, oh + kh - 2, ow + kw - 2, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 60, 1, 7, 4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(240, i1_0 * 60 + ax1)
                        i2 = T.axis.spatial(14, i2_0 * 2 + i2_1 + ax2)
                        i3 = T.axis.spatial(14, i3_1 * 7 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 5, 12])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], compute: T.Buffer[(1, 240, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 240, 18, 18, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 240, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 4, 7):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 60, 6, 18, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(240, i1_0 * 60 + ax1)
                        i2 = T.axis.spatial(18, i2_0 * 2 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 2, i3 - 2, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(2 <= i2 and i2 < 16 and 2 <= i3 and i3 < 16, placeholder[i0, i1, i2 - 2, i3 - 2, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0 in T.grid(1, 1):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 2, 1, 1, 1, 1, 5, 1, 1, 2, 5, 5, 1, 12, 1, 7, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(240, i1_0 * 60 + i1_2 * 12 + i1_3)
                            oh = T.axis.spatial(14, i2_0 * 2 + i2_1)
                            ow = T.axis.spatial(14, i3_1 * 7 + i3_3)
                            oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [240, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 60, 2, 14, 4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(240, i1_0 * 60 + ax1)
                            i2 = T.axis.spatial(14, i2_0 * 2 + ax2)
                            i3, i4 = T.axis.remap("SS", [ax3, ax4])
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 5, 12])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 5])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #23: "fused_nn_contrib_conv2d_NCHWc_add_add_3"
[15:58:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(40, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 40, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 40, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 40, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 40, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 40, 14, 14, 4, 960, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [40, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 40, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(40, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 40, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 40, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 40, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 4, 1, 1, 2, 1, 1, 15, 1, 1, 1, 5, 7, 2, 1, 64, 1, 1, 1, 8, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(40, i1_2 * 8 + i1_3)
                    oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                    ow = T.axis.spatial(14, i3_0 * 2 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(960, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [40, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 40, 14, 14, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 5, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[15, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(40, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 40, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 40, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 40, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 7, 4, 1, 1, 2, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(15, 1, 1, 1, 5, 7, 2, 1, 64, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(40, i1_2 * 8 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(960, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [40, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 40, 7, 2, 1):
                    with T.block("T_add_1"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(14, i2_1 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 5, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[15, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(40, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 40, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 40, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 40, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 40, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 7, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 1, 15, 1, 1, 1, 5, 7, 2, 1, 64, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(40, i1_2 * 8 + i1_3)
                        oh = T.axis.spatial(14, i2_1 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(960, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [40, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 40, 14, 2, 1):
                    with T.block("T_add_1"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 5, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[15, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #24: "fused_nn_contrib_conv2d_NCHWc_add_clip_5"
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(240, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], compute: T.Buffer[(1, 240, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 240, 14, 14, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 240, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 240, 14, 14, 4, 160, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 14, 14, 4], "float32"], ["TENSOR", [240, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 14, 14, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(240, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], compute: T.Buffer[(1, 240, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 240, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 12, 2, 1, 2, 1, 5, 1, 1, 1, 16, 1, 1, 1, 2, 7, 14, 1, 10, 1, 1, 1, 2, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(240, i1_0 * 20 + i1_1 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(14, i2_0 * 7 + i2_2)
                    ow = T.axis.spatial(14, i3_2)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(160, i5_0 * 10 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 14, 14, 4], "float32"], ["TENSOR", [240, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 240, 14, 14, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[12, 5, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 10])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(240, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], compute: T.Buffer[(1, 240, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 240, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 12, 2, 1, 2, 1, 5, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 2, 7, 14, 1, 10, 1, 1, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(240, i1_0 * 20 + i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(160, i5_0 * 10 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 14, 14, 4], "float32"], ["TENSOR", [240, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 7, 14, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(240, i1_0 * 20 + i1_1 * 4 + ax1)
                        i2 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        i3 = T.axis.spatial(14, ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[12, 5, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 10])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 40, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(240, 40, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], compute: T.Buffer[(1, 240, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 240, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 12, 2, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 5, 1, 1, 1, 16, 1, 1, 1, 2, 7, 14, 1, 10, 1, 1, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(240, i1_0 * 20 + i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(160, i5_0 * 10 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 40, 14, 14, 4], "float32"], ["TENSOR", [240, 40, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 20, 7, 14, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(240, i1_0 * 20 + ax1)
                        i2 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        i3 = T.axis.spatial(14, ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[12, 5, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 10])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #25: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_9"
[15:58:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], compute: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 240, 17, 17, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 17, 17, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 240, 7, 7, 4, 5, 5):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [240, 1, 5, 5, 1, 4], "float32"], [2, 2], [1, 1, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], compute: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 240, 17, 17, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 2, 1, 120, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 17, 17, 1):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(240, i1_0 * 120 + i1_1 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(5, 1, 1, 1, 7, 1, 1, 1, 5, 1, 1, 1, 7, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i1_0 * 120 + i1_1)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [240, 1, 5, 5, 1, 4], "float32"], [2, 2], [1, 1, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 240, 7, 7, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 120, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[5, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], compute: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 240, 17, 17, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 240, 17, 17, 4):
                with T.block("PaddedInput"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                    PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 2, 1, 1, 2, 1, 120, 1, 1, 2):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(5, 1, 1, 1, 7, 1, 1, 1, 5, 1, 1, 1, 7, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(240, i1_0 * 120 + i1_1_1)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_1_1)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [240, 1, 5, 5, 1, 4], "float32"], [2, 2], [1, 1, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 7, 7, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(240, i1_0 * 120 + i1_1_1 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i4_0 * 2 + i4_1_1 + ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 120, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[5, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(240, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 240, 1, 1, 4), "float32"], compute: T.Buffer[(1, 240, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 240, 17, 17, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 240, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 120, 1, 1, 2, 5):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 13, 17, 1):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(240, i1_0 * 120 + i1_1 + ax1)
                            i2 = T.axis.spatial(17, i5_0 + ax2)
                            i3 = T.axis.spatial(17, ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 1, 1, 1, 5, 1, 1, 1, 7, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(240, i1_0 * 120 + i1_1)
                            oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 240, 14, 14, 4], "float32"], ["TENSOR", [240, 1, 5, 5, 1, 4], "float32"], [2, 2], [1, 1, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 120, 7, 7, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(240, i1_0 * 120 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 120, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[5, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #26: "fused_nn_contrib_conv2d_NCHWc_add_5"
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(68, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 68, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 68, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 68, 7, 7, 4, 960, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [68, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 68, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(68, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 68, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 68, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 480, 1, 1, 1, 2, 1, 7, 2, 2, 1, 1, 1, 34, 7, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(68, i1_2 * 34 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                    ic = T.axis.reduce(960, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [68, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 68, 7, 7, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 2, 34])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[480, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(68, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 68, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 68, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(480, 1, 1, 1, 2, 1, 7, 2, 2, 1, 1, 1, 34, 7, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(68, i1_2 * 34 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(960, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [68, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 68, 7, 7, 2):
                    with T.block("T_add"):
                        ax0_1, ax1_1, ax2_1, ax3_1 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 2, 34])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[480, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 240, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(68, 240, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 68, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 68, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 480, 1, 1, 1, 2, 1, 7, 2, 2, 1, 1, 1, 34, 7, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(68, i1_2 * 34 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(960, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 240, 7, 7, 4], "float32"], ["TENSOR", [68, 240, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 68, 7, 7, 4):
                    with T.block("T_add"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 2, 34])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[480, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_10"
[15:58:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(408, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 408, 1, 1, 4), "float32"], compute: T.Buffer[(1, 408, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 408, 11, 11, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 408, 11, 11, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(2 <= i2_1 and i2_1 < 9 and 2 <= i3_1 and i3_1 < 9, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 408, 7, 7, 4, 5, 5):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [408, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 408, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 408, 7, 7, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(408, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 408, 1, 1, 4), "float32"], compute: T.Buffer[(1, 408, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 408, 11, 11, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1 in T.grid(1, 4, 1, 1, 2, 1, 17, 7):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 5, 11, 2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(408, i1_0 * 102 + i1_1 * 6 + ax1)
                        i2 = T.axis.spatial(11, i2_1 + ax2)
                        i3 = T.axis.spatial(11, ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 2, i3 - 2, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(2 <= i2 and i2 < 9 and 2 <= i3 and i3 < 9, placeholder[i0, i1, i2 - 2, i3 - 2, i4], T.float32(0), dtype="float32")
                for i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 5, 1, 6, 1, 7, 2):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(408, i1_0 * 102 + i1_1 * 6 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_3])
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [408, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 408, 7, 7, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 17, 1, 6])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[5, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(408, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 408, 1, 1, 4), "float32"], compute: T.Buffer[(1, 408, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 408, 11, 11, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 102, 11, 11, 2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(408, i1_0 * 102 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 2, i3 - 2, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(2 <= i2 and i2 < 9 and 2 <= i3 and i3 < 9, placeholder[i0, i1, i2 - 2, i3 - 2, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 17, 7, 1, 1):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(5, 1, 1, 1, 1, 1, 1, 1, 5, 1, 6, 1, 7, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(408, i1_0 * 102 + i1_1 * 6 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_1, i3_3])
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [408, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 1, 7, 2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(408, i1_0 * 102 + i1_1 * 6 + ax1)
                            i2 = T.axis.spatial(7, i2_1 + ax2)
                            i3 = T.axis.spatial(7, ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 17, 1, 6])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[5, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(408, 1, 5, 5, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 408, 1, 1, 4), "float32"], compute: T.Buffer[(1, 408, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 408, 11, 11, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 102, 11, 11, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(408, i1_0 * 102 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 2, i3 - 2, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(2 <= i2 and i2 < 9 and 2 <= i3 and i3 < 9, placeholder[i0, i1, i2 - 2, i3 - 2, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0 in T.grid(1, 1, 2):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 17, 7, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 5, 1, 6, 1, 7, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(408, i1_0 * 102 + i1_1 * 6 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_1, i3_3])
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [408, 1, 5, 5, 1, 4], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 102, 7, 7, 2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(408, i1_0 * 102 + ax1)
                            i2, i3 = T.axis.remap("SS", [ax2, ax3])
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 17, 1, 6])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[5, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 5])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_4"
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(68, 408, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 68, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 68, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 68, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 68, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 68, 7, 7, 4, 1632, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [68, 408, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 68, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 68, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(68, 408, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 68, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 68, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 68, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 17, 7, 1, 1, 1, 4, 1, 7, 1, 136, 1, 1, 1, 1, 1, 1, 4, 12, 1, 1, 1, 1, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(68, i1_0 * 4 + i1_1)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_0, i3_1, i4_2])
                    ic = T.axis.reduce(1632, i5_0 * 12 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [68, 408, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 68, 7, 7, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[17, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[136, 12])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(68, 408, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 68, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 68, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 68, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 17, 7, 1, 1, 1, 4, 1, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(136, 1, 1, 1, 1, 1, 1, 4, 12, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(68, i1_0 * 4 + i1_1)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_0, i3_1, i4_2])
                        ic = T.axis.reduce(1632, i5_0 * 12 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [68, 408, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(68, i1_0 * 4 + i1_1 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(7, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[17, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[136, 12])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(68, 408, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 68, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 68, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 68, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 17, 7, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 7, 1, 136, 1, 1, 1, 1, 1, 1, 4, 12, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(68, i1_0 * 4 + i1_1)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_0, i3_1, i4_2])
                        ic = T.axis.reduce(1632, i5_0 * 12 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [68, 408, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 7, 4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(68, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[17, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[136, 12])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #29: "fused_nn_contrib_conv2d_NCHWc_add_clip_6"
[15:58:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(408, 68, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 408, 1, 1, 4), "float32"], compute: T.Buffer[(1, 408, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 408, 7, 7, 4, 272, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 68, 7, 7, 4], "float32"], ["TENSOR", [408, 68, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 408, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 408, 7, 7, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(408, 68, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 408, 1, 1, 4), "float32"], compute: T.Buffer[(1, 408, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 6, 1, 1, 2, 1, 2, 7, 1, 1, 16, 1, 1, 1, 2, 1, 7, 2, 17, 1, 1, 1, 17, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(408, i1_0 * 68 + i1_1 * 34 + i1_2 * 17 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_1, i3_2])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(272, i5_0 * 17 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 68, 7, 7, 4], "float32"], ["TENSOR", [408, 68, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 408, 7, 7, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[6, 2, 2, 17])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 17])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(408, 68, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 408, 1, 1, 4), "float32"], compute: T.Buffer[(1, 408, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 6, 1, 1, 2, 1, 2, 7, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 2, 1, 7, 2, 17, 1, 1, 1, 17, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(408, i1_0 * 68 + i1_1 * 34 + i1_2 * 17 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_2])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(272, i5_0 * 17 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 68, 7, 7, 4], "float32"], ["TENSOR", [408, 68, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 34, 1, 7, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(408, i1_0 * 68 + i1_1 * 34 + ax1)
                        i2 = T.axis.spatial(7, i2_1 + ax2)
                        i3 = T.axis.spatial(7, ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[6, 2, 2, 17])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 17])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(408, 68, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 408, 1, 1, 4), "float32"], compute: T.Buffer[(1, 408, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 6, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 1, 1, 16, 1, 1, 1, 2, 1, 7, 2, 17, 1, 1, 1, 17, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(408, i1_0 * 68 + i1_1 * 34 + i1_2 * 17 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_2])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(272, i5_0 * 17 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 68, 7, 7, 4], "float32"], ["TENSOR", [408, 68, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 68, 7, 7, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(408, i1_0 * 68 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[6, 2, 2, 17])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 17])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #30: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_11"
[15:58:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(408, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 408, 1, 1, 4), "float32"], compute: T.Buffer[(1, 408, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 408, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 408, 9, 9, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 408, 7, 7, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [408, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 408, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 408, 7, 7, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[15:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(408, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 408, 1, 1, 4), "float32"], compute: T.Buffer[(1, 408, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 408, 9, 9, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1 in T.grid(1, 2, 1, 1, 2, 1, 204, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 7, 7, 2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(408, i1_0 * 204 + i1_1 + ax1)
                        i2 = T.axis.spatial(9, i5_1 + ax2)
                        i3 = T.axis.spatial(9, i6_1 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 7, 2):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(408, i1_0 * 204 + i1_1)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [408, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 408, 7, 7, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(DepthwiseConv2d[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 204, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=18)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[15:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(408, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 408, 1, 1, 4), "float32"], compute: T.Buffer[(1, 408, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 408, 9, 9, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 204, 9, 9, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(408, i1_0 * 204 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 1, 204, 1, 1, 1):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 7, 7, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(408, i1_0 * 204 + i1_1)
                            oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [408, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 7, 7, 2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(408, i1_0 * 204 + i1_1 + ax1)
                            i2, i3 = T.axis.remap("SS", [ax2, ax3])
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 204, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(408, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 408, 1, 1, 4), "float32"], compute: T.Buffer[(1, 408, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 408, 9, 9, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 408, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 204, 9, 9, 2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(408, i1_0 * 204 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 204, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 7, 7, 2):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(408, i1_0 * 204 + i1_1)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [408, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 204, 7, 7, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(408, i1_0 * 204 + ax1)
                        i2, i3 = T.axis.remap("SS", [ax2, ax3])
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 204, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
[15:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #31: "fused_nn_contrib_conv2d_NCHWc_add_6"
[15:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(112, 408, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 112, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 112, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 112, 7, 7, 4, 1632, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [112, 408, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 112, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[15:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(112, 408, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 112, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 112, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 1, 1, 1, 7, 1, 7, 1, 204, 1, 1, 1, 1, 1, 1, 4, 8, 1, 1, 1, 2, 7, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(112, i1_0 * 14 + i1_1 * 2 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_1, i4_2])
                    ic = T.axis.reduce(1632, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [112, 408, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 112, 7, 7, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 7, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[204, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[15:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(112, 408, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 112, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 112, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 1, 1, 1, 7, 1, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(204, 1, 1, 1, 1, 1, 1, 4, 8, 1, 1, 1, 2, 7, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(112, i1_0 * 14 + i1_1 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_1, i4_2])
                        ic = T.axis.reduce(1632, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [112, 408, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 7, 1, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(112, i1_0 * 14 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(7, ax2)
                        ax3_1 = T.axis.spatial(7, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 7, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[204, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 408, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(112, 408, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 112, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 112, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 7, 1, 7, 1, 204, 1, 1, 1, 1, 1, 1, 4, 8, 1, 1, 1, 2, 7, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(112, i1_0 * 14 + i1_1 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_1, i4_2])
                        ic = T.axis.reduce(1632, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 408, 7, 7, 4], "float32"], ["TENSOR", [112, 408, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 14, 7, 7, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(112, i1_0 * 14 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 7, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[204, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #32: "fused_nn_contrib_conv2d_NCHWc_add_clip_7"
[15:58:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(320, 112, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1, 4), "float32"], compute: T.Buffer[(1, 320, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 320, 7, 7, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 320, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 320, 7, 7, 4, 448, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 112, 7, 7, 4], "float32"], ["TENSOR", [320, 112, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 7, 7, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[15:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(320, 112, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1, 4), "float32"], compute: T.Buffer[(1, 320, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 320, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 2, 1, 5, 1, 1, 1, 28, 1, 1, 1, 8, 7, 1, 2, 16, 1, 1, 1, 8, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(320, i1_1 * 64 + i1_2 * 8 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_2, i3_0])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(448, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 112, 7, 7, 4], "float32"], ["TENSOR", [320, 112, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 320, 7, 7, 4):
                with T.block("compute"):
                    i0_4, i1_4, i2_4, i3_4, i4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4], placeholder_2[i0_4, i1_4, 0, 0, i4_4])
                    T.writes(compute[i0_4, i1_4, i2_4, i3_4, i4_4])
                    compute[i0_4, i1_4, i2_4, i3_4, i4_4] = T.max(T.min(conv2d_NCHWc[i0_4, i1_4, i2_4, i3_4, i4_4] + placeholder_2[i0_4, i1_4, 0, 0, i4_4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 5, 8, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[28, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[15:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(320, 112, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1, 4), "float32"], compute: T.Buffer[(1, 320, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 320, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 7, 2, 1, 5, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(28, 1, 1, 1, 8, 7, 1, 2, 16, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(320, i1_1 * 64 + i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_0])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(448, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 112, 7, 7, 4], "float32"], ["TENSOR", [320, 112, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 7, 1, 2):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(320, i1_1 * 64 + ax1)
                        i2 = T.axis.spatial(7, ax2)
                        i3 = T.axis.spatial(7, i3_0 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 5, 8, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[28, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(320, 112, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1, 4), "float32"], compute: T.Buffer[(1, 320, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 320, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 7, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 5, 1, 1, 1, 28, 1, 1, 1, 8, 7, 1, 2, 16, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(320, i1_1 * 64 + i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_0])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(448, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 112, 7, 7, 4], "float32"], ["TENSOR", [320, 112, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 320, 7, 1, 2):
                    with T.block("compute"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(7, i3_0 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 5, 8, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[28, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[15:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #33: "fused_nn_avg_pool2d"
[15:58:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 320, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 320, 1, 1, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 320, 1, 1, 4, 7, 7):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
                T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 320, 1, 1, 4):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2 + 6, 6) + 1 - T.max(0, ax2)) * (T.min(ax3 + 6, 6) + 1 - T.max(0, ax3)), 1), "float32")
    

[15:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 320, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 320, 1, 1, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 320, 1, 1, 4, 7], dtype="float32")
            for i0, i1 in T.grid(1, 320):
                for ax0 in T.serial(7):
                    for ax0_1, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 1, 1, 1, 1, 4, 7):
                        with T.block("tensor_rf"):
                            vi5_i6_fused_0 = T.axis.spatial(7, ax0 + ax0_1)
                            ax0_2 = T.axis.spatial(1, ax1)
                            ax1_1 = T.axis.spatial(320, i1 + ax2)
                            ax2_1, ax3_1, ax4_1, vi5_i6_fused_1 = T.axis.remap("SSSR", [ax3, ax4, ax5, ax6])
                            T.reads(placeholder[ax0_2, ax1_1, ax2_1 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3_1 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4_1])
                            T.writes(tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0])
                            with T.init():
                                tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] = T.float32(0)
                            tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] = tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_0] + placeholder[ax0_2, ax1_1, ax2_1 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3_1 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4_1]
                    for ax1, ax2, ax3, ax4, ax5 in T.grid(1, 1, 1, 1, 4):
                        with T.block("tensor"):
                            vi5_i6_fused_0, ax0_3 = T.axis.remap("RS", [ax0, ax1])
                            ax1_2 = T.axis.spatial(320, i1 + ax2)
                            ax2_2, ax3_2, ax4_2 = T.axis.remap("SSS", [ax3, ax4, ax5])
                            T.reads(tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0])
                            T.writes(tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2])
                            with T.init():
                                tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] = T.float32(0)
                            tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] = tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] + tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0]
                for i2, i3, i4 in T.grid(1, 1, 4):
                    with T.block("tensor_1"):
                        ax0_4, ax1_3, ax2_3, ax3_3, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(tensor_1[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3])
                        T.writes(tensor[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3])
                        tensor[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3] = tensor_1[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3] / T.cast(T.max((T.min(ax2_3 + 6, 6) + 1 - T.max(0, ax2_3)) * (T.min(ax3_3 + 6, 6) + 1 - T.max(0, ax3_3)), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[7, 7])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=2)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
[15:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 320, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 320, 1, 1, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 320, 1, 1, 4, 7], dtype="float32")
            for i0, i1 in T.grid(1, 320):
                for ax0 in T.serial(7):
                    for ax0_1, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 1, 1, 1, 1, 4, 7):
                        with T.block("tensor_rf"):
                            vi5_i6_fused_1 = T.axis.spatial(7, ax0 + ax0_1)
                            ax0_2 = T.axis.spatial(1, ax1)
                            ax1_1 = T.axis.spatial(320, i1 + ax2)
                            ax2_1, ax3_1, ax4_1, vi5_i6_fused_0 = T.axis.remap("SSSR", [ax3, ax4, ax5, ax6])
                            T.reads(placeholder[ax0_2, ax1_1, ax2_1 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3_1 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4_1])
                            T.writes(tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1])
                            with T.init():
                                tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1] = T.float32(0)
                            tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1] = tensor_rf[ax0_2, ax1_1, ax2_1, ax3_1, ax4_1, vi5_i6_fused_1] + placeholder[ax0_2, ax1_1, ax2_1 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) // 7, ax3_1 + (vi5_i6_fused_0 * 7 + vi5_i6_fused_1) % 7, ax4_1]
                    for ax1, ax2, ax3, ax4, ax5 in T.grid(1, 1, 1, 1, 4):
                        with T.block("tensor"):
                            vi5_i6_fused_1, ax0_3 = T.axis.remap("RS", [ax0, ax1])
                            ax1_2 = T.axis.spatial(320, i1 + ax2)
                            ax2_2, ax3_2, ax4_2 = T.axis.remap("SSS", [ax3, ax4, ax5])
                            T.reads(tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_1])
                            T.writes(tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2])
                            with T.init():
                                tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] = T.float32(0)
                            tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] = tensor_1[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2] + tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_1]
                for i2, i3, i4 in T.grid(1, 1, 4):
                    with T.block("tensor_1"):
                        ax0_4, ax1_3, ax2_3, ax3_3, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(tensor_1[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3])
                        T.writes(tensor[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3])
                        tensor[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3] = tensor_1[ax0_4, ax1_3, ax2_3, ax3_3, ax4_3] / T.cast(T.max((T.min(ax2_3 + 6, 6) + 1 - T.max(0, ax2_3)) * (T.min(ax3_3 + 6, 6) + 1 - T.max(0, ax3_3)), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b0)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[7, 7])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=2)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
[15:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 320, 1, 1, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 320, 1, 1, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 320, 1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 1, 1, 1, 1, 7, 7):
                    with T.block("tensor"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(320, i1 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4 + ax4)
                        rv0, rv1 = T.axis.remap("RR", [ax5, ax6])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 + rv0, ax3_1 + rv1, ax4_1])
                        T.writes(tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        with T.init():
                            tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.float32(0)
                        tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder[ax0_1, ax1_1, ax2_1 + rv0, ax3_1 + rv1, ax4_1]
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2 + 6, 6) + 1 - T.max(0, ax2)) * (T.min(ax3 + 6, 6) + 1 - T.max(0, ax3)), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[15:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #34: "fused_squeeze_layout_transform"
[15:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 1, 1, 4), "float32"], T_layout_trans: T.Buffer[(1, 1280), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_squeeze = T.alloc_buffer([1, 320, 4], dtype="float32")
        for i0, i1, i2 in T.grid(1, 320, 4):
            with T.block("T_squeeze"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder[ax0, ax1, 0, 0, ax2])
                T.writes(T_squeeze[ax0, ax1, ax2])
                T_squeeze[ax0, ax1, ax2] = placeholder[ax0, ax1, 0, 0, ax2]
        for i0, i1 in T.grid(1, 1280):
            with T.block("T_layout_trans"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_squeeze[ax0, ax1 // 4, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1])
                T_layout_trans[ax0, ax1] = T.if_then_else(ax0 < 1 and ax1 < 1280, T_squeeze[ax0, ax1 // 4, ax1 % 4], T.float32(0), dtype="float32")
    

[15:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[15:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 1, 1, 4), "float32"], T_layout_trans: T.Buffer[(1, 1280), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1 in T.grid(1, 1280):
                with T.block("T_layout_trans"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[ax0, ax1 // 4, 0, 0, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1])
                    T_layout_trans[ax0, ax1] = T.if_then_else(ax0 < 1 and ax1 < 1280, placeholder[ax0, ax1 // 4, 0, 0, ax1 % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_squeeze", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[15:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #35: "fused_nn_dense_add"
[15:58:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280), "float32"], placeholder_1: T.Buffer[(1000, 1280), "float32"], placeholder_2: T.Buffer[(1, 1000), "float32"], T_add: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True, "layout_free_placeholders": [1]})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 1000], dtype="float32")
        for i0, i1, i2 in T.grid(1, 1000, 1280):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 3 design space(s) generated
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280), "float32"], placeholder_1: T.Buffer[(1000, 1280), "float32"], placeholder_2: T.Buffer[(1, 1000), "float32"], T_add: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True, "layout_free_placeholders": [1]})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 1000], dtype="float32")
            for i0_0, i1_0, i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 1, 1, 10, 160, 1, 2, 8, 1, 50):
                with T.block("T_matmul_NT"):
                    i = T.axis.spatial(1, 0)
                    j = T.axis.spatial(1000, i1_1 * 100 + i1_2 * 50 + i1_3)
                    k = T.axis.reduce(1280, i2_0 * 8 + i2_1)
                    T.reads(placeholder[i, k], placeholder_1[j, k])
                    T.writes(T_matmul_NT[i, j])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        T_matmul_NT[i, j] = T.float32(0)
                    T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_add"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                    T.writes(T_add[ax0, ax1])
                    T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 10, 2, 50])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[160, 8])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v25 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v25)
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280), "float32"], placeholder_1: T.Buffer[(1000, 1280), "float32"], placeholder_2: T.Buffer[(1, 1000), "float32"], T_add: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True, "layout_free_placeholders": [1]})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 1000], dtype="float32")
            for i0_0, i1_0, i0_1, i1_1 in T.grid(1, 1, 1, 10):
                for i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(160, 1, 2, 8, 1, 50):
                    with T.block("T_matmul_NT"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(1000, i1_1 * 100 + i1_2 * 50 + i1_3)
                        k = T.axis.reduce(1280, i2_0 * 8 + i2_1)
                        T.reads(placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
                for ax0, ax1 in T.grid(1, 100):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(1000, i1_1 * 100 + ax1)
                        T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                        T.writes(T_add[ax0_1, ax1_1])
                        T_add[ax0_1, ax1_1] = T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 10, 2, 50])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[160, 8])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
b25, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b25, loop=l18, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280), "float32"], placeholder_1: T.Buffer[(1000, 1280), "float32"], placeholder_2: T.Buffer[(1, 1000), "float32"], T_add: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True, "layout_free_placeholders": [1]})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 1000], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 1):
                for i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 10, 160, 1, 2, 8, 1, 50):
                    with T.block("T_matmul_NT"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(1000, i1_1 * 100 + i1_2 * 50 + i1_3)
                        k = T.axis.reduce(1280, i2_0 * 8 + i2_1)
                        T.reads(placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
                for ax0, ax1 in T.grid(1, 1000):
                    with T.block("T_add"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                        T.writes(T_add[ax0_1, ax1_1])
                        T_add[ax0_1, ax1_1] = T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 10, 2, 50])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[160, 8])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
b25, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b25, loop=l17, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #36: "fused_nn_softmax"
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
        T_softmax_exp = T.alloc_buffer([1, 1000], dtype="float32")
        T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_softmax_maxelem"):
                i0_1, k = T.axis.remap("SR", [i0, i1])
                T.reads(placeholder[i0_1, k])
                T.writes(T_softmax_maxelem[i0_1])
                with T.init():
                    T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_softmax_exp"):
                i0_2, i1_1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[i0_2, i1_1], T_softmax_maxelem[i0_2])
                T.writes(T_softmax_exp[i0_2, i1_1])
                T_softmax_exp[i0_2, i1_1] = T.exp(placeholder[i0_2, i1_1] - T_softmax_maxelem[i0_2], dtype="float32")
        for i0_3, i1 in T.grid(1, 1000):
            with T.block("T_softmax_expsum"):
                i0_4, k = T.axis.remap("SR", [i0_3, i1])
                T.reads(T_softmax_exp[i0_4, k])
                T.writes(T_softmax_expsum[i0_4])
                with T.init():
                    T_softmax_expsum[i0_4] = T.float32(0)
                T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T_softmax_exp[i0_4, k]
        for i0_5, i1 in T.grid(1, 1000):
            with T.block("T_softmax_norm"):
                i0_6, i1_2 = T.axis.remap("SS", [i0_5, i1])
                T.reads(T_softmax_exp[i0_6, i1_2], T_softmax_expsum[i0_6])
                T.writes(T_softmax_norm[i0_6, i1_2])
                T.block_attr({"axis":1})
                T_softmax_norm[i0_6, i1_2] = T_softmax_exp[i0_6, i1_2] / T_softmax_expsum[i0_6]
    

[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 9 design space(s) generated
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_exp = T.alloc_buffer([1, 1000], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 500], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 50], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 50, 20):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_0 = T.axis.spatial(50, i1_0)
                    i0_1 = T.axis.spatial(1, 0)
                    vi1_1 = T.axis.reduce(20, i1_1)
                    T.reads(placeholder[i0_1, vi1_0 * 20 + vi1_1])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_0])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_0] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_0] = T.max(T_softmax_maxelem_rf[i0_1, vi1_0], placeholder[i0_1, vi1_0 * 20 + vi1_1])
            for i0, i1_0 in T.grid(1, 50):
                with T.block("T_softmax_maxelem"):
                    vi1_0 = T.axis.reduce(50, i1_0)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem_rf[i0_2, vi1_0])
                    T.writes(T_softmax_maxelem[i0_2])
                    with T.init():
                        T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_0])
            for i0_3, i1 in T.grid(1, 1000):
                with T.block("T_softmax_exp"):
                    i0_4, i1_2 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(placeholder[i0_4, i1_2], T_softmax_maxelem[i0_4])
                    T.writes(T_softmax_exp[i0_4, i1_2])
                    T_softmax_exp[i0_4, i1_2] = T.exp(placeholder[i0_4, i1_2] - T_softmax_maxelem[i0_4], dtype="float32")
            for i0_5, i1_0, i1_1 in T.grid(1, 500, 2):
                with T.block("T_softmax_expsum_rf"):
                    vi1_0 = T.axis.spatial(500, i1_0)
                    i0_6 = T.axis.spatial(1, 0)
                    vi1_1 = T.axis.reduce(2, i1_1)
                    T.reads(T_softmax_exp[i0_6, vi1_0 * 2 + vi1_1])
                    T.writes(T_softmax_expsum_rf[i0_6, vi1_0])
                    with T.init():
                        T_softmax_expsum_rf[i0_6, vi1_0] = T.float32(0)
                    T_softmax_expsum_rf[i0_6, vi1_0] = T_softmax_expsum_rf[i0_6, vi1_0] + T_softmax_exp[i0_6, vi1_0 * 2 + vi1_1]
            for i0_7, i1_0 in T.grid(1, 500):
                with T.block("T_softmax_expsum"):
                    vi1_0 = T.axis.reduce(500, i1_0)
                    i0_8 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum_rf[i0_8, vi1_0])
                    T.writes(T_softmax_expsum[i0_8])
                    with T.init():
                        T_softmax_expsum[i0_8] = T.float32(0)
                    T_softmax_expsum[i0_8] = T_softmax_expsum[i0_8] + T_softmax_expsum_rf[i0_8, vi1_0]
            for i0_9, i1_3 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_10, i1_4 = T.axis.remap("SS", [i0_9, i1_3])
                    T.reads(T_softmax_exp[i0_10, i1_4], T_softmax_expsum[i0_10])
                    T.writes(T_softmax_norm[i0_10, i1_4])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_10, i1_4] = T_softmax_exp[i0_10, i1_4] / T_softmax_expsum[i0_10]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[500, 2])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l8, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
l11, l12 = sch.get_loops(block=b0)
v13, v14 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[50, 20])
l15, l16 = sch.split(loop=l12, factors=[v13, v14])
b17 = sch.rfactor(loop=l15, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
b19, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l20 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b19, decision=-1)
sch.compute_at(block=b19, loop=l21, preserve_unit_loops=True)
l22 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l22, preserve_unit_loops=True)
b23, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l24 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l24, preserve_unit_loops=True)
l25 = sch.sample_compute_location(block=b23, decision=-1)
sch.compute_at(block=b23, loop=l25, preserve_unit_loops=True)
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_exp = T.alloc_buffer([1, 1000], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 500], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 20], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 50, 20):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_1 = T.axis.spatial(20, i1_1)
                    i0_1 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(50, i1_0)
                    T.reads(placeholder[i0_1, vi1_0 * 20 + vi1_1])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_1])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_1] = T.max(T_softmax_maxelem_rf[i0_1, vi1_1], placeholder[i0_1, vi1_0 * 20 + vi1_1])
            for i0, i1_1 in T.grid(1, 20):
                with T.block("T_softmax_maxelem"):
                    vi1_1 = T.axis.reduce(20, i1_1)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem_rf[i0_2, vi1_1])
                    T.writes(T_softmax_maxelem[i0_2])
                    with T.init():
                        T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_1])
            for i0_3, i1 in T.grid(1, 1000):
                with T.block("T_softmax_exp"):
                    i0_4, i1_2 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(placeholder[i0_4, i1_2], T_softmax_maxelem[i0_4])
                    T.writes(T_softmax_exp[i0_4, i1_2])
                    T_softmax_exp[i0_4, i1_2] = T.exp(placeholder[i0_4, i1_2] - T_softmax_maxelem[i0_4], dtype="float32")
            for i0_5, i1_0, i1_1 in T.grid(1, 500, 2):
                with T.block("T_softmax_expsum_rf"):
                    vi1_0 = T.axis.spatial(500, i1_0)
                    i0_6 = T.axis.spatial(1, 0)
                    vi1_1 = T.axis.reduce(2, i1_1)
                    T.reads(T_softmax_exp[i0_6, vi1_0 * 2 + vi1_1])
                    T.writes(T_softmax_expsum_rf[i0_6, vi1_0])
                    with T.init():
                        T_softmax_expsum_rf[i0_6, vi1_0] = T.float32(0)
                    T_softmax_expsum_rf[i0_6, vi1_0] = T_softmax_expsum_rf[i0_6, vi1_0] + T_softmax_exp[i0_6, vi1_0 * 2 + vi1_1]
            for i0_7, i1_0 in T.grid(1, 500):
                with T.block("T_softmax_expsum"):
                    vi1_0 = T.axis.reduce(500, i1_0)
                    i0_8 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum_rf[i0_8, vi1_0])
                    T.writes(T_softmax_expsum[i0_8])
                    with T.init():
                        T_softmax_expsum[i0_8] = T.float32(0)
                    T_softmax_expsum[i0_8] = T_softmax_expsum[i0_8] + T_softmax_expsum_rf[i0_8, vi1_0]
            for i0_9, i1_3 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_10, i1_4 = T.axis.remap("SS", [i0_9, i1_3])
                    T.reads(T_softmax_exp[i0_10, i1_4], T_softmax_expsum[i0_10])
                    T.writes(T_softmax_norm[i0_10, i1_4])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_10, i1_4] = T_softmax_exp[i0_10, i1_4] / T_softmax_expsum[i0_10]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[500, 2])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l8, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
l11, l12 = sch.get_loops(block=b0)
v13, v14 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[50, 20])
l15, l16 = sch.split(loop=l12, factors=[v13, v14])
b17 = sch.rfactor(loop=l16, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
b19, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l20 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b19, decision=-1)
sch.compute_at(block=b19, loop=l21, preserve_unit_loops=True)
l22 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l22, preserve_unit_loops=True)
b23, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l24 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l24, preserve_unit_loops=True)
l25 = sch.sample_compute_location(block=b23, decision=-1)
sch.compute_at(block=b23, loop=l25, preserve_unit_loops=True)
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 500], dtype="float32")
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1_0, i1_1 in T.grid(1, 500, 2):
                with T.block("T_softmax_expsum_rf"):
                    vi1_0 = T.axis.spatial(500, i1_0)
                    i0_2 = T.axis.spatial(1, 0)
                    vi1_1 = T.axis.reduce(2, i1_1)
                    T.reads(placeholder[i0_2, vi1_0 * 2 + vi1_1], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_expsum_rf[i0_2, vi1_0])
                    with T.init():
                        T_softmax_expsum_rf[i0_2, vi1_0] = T.float32(0)
                    T_softmax_expsum_rf[i0_2, vi1_0] = T_softmax_expsum_rf[i0_2, vi1_0] + T.exp(placeholder[i0_2, vi1_0 * 2 + vi1_1] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1_0 in T.grid(1, 500):
                with T.block("T_softmax_expsum"):
                    vi1_0 = T.axis.reduce(500, i1_0)
                    i0_4 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum_rf[i0_4, vi1_0])
                    T.writes(T_softmax_expsum[i0_4])
                    with T.init():
                        T_softmax_expsum[i0_4] = T.float32(0)
                    T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T_softmax_expsum_rf[i0_4, vi1_0]
            for i0_5, i1 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_6, i1_2 = T.axis.remap("SS", [i0_5, i1])
                    T.reads(placeholder[i0_6, i1_2], T_softmax_maxelem[i0_6], T_softmax_expsum[i0_6])
                    T.writes(T_softmax_norm[i0_6, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_6, i1_2] = T.exp(placeholder[i0_6, i1_2] - T_softmax_maxelem[i0_6], dtype="float32") / T_softmax_expsum[i0_6]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[500, 2])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l8, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v11 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v11)
b12, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l13 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l13, preserve_unit_loops=True)
l14 = sch.sample_compute_location(block=b12, decision=-1)
sch.compute_at(block=b12, loop=l14, preserve_unit_loops=True)
l15 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l15, preserve_unit_loops=True)
l16 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l16, preserve_unit_loops=True)
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #3:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 2], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 1000, 1):
                with T.block("T_softmax_maxelem"):
                    vi1_0 = T.axis.reduce(1000, i1_0)
                    i0_1 = T.axis.spatial(1, 0)
                    T.reads(placeholder[i0_1, vi1_0])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, vi1_0])
            for i0, i1_0, i1_1 in T.grid(1, 500, 2):
                with T.block("T_softmax_expsum_rf"):
                    vi1_1 = T.axis.spatial(2, i1_1)
                    i0_2 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(500, i1_0)
                    T.reads(placeholder[i0_2, vi1_0 * 2 + vi1_1], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_expsum_rf[i0_2, vi1_1])
                    with T.init():
                        T_softmax_expsum_rf[i0_2, vi1_1] = T.float32(0)
                    T_softmax_expsum_rf[i0_2, vi1_1] = T_softmax_expsum_rf[i0_2, vi1_1] + T.exp(placeholder[i0_2, vi1_0 * 2 + vi1_1] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1_1 in T.grid(1, 2):
                with T.block("T_softmax_expsum"):
                    vi1_1 = T.axis.reduce(2, i1_1)
                    i0_4 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum_rf[i0_4, vi1_1])
                    T.writes(T_softmax_expsum[i0_4])
                    with T.init():
                        T_softmax_expsum[i0_4] = T.float32(0)
                    T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T_softmax_expsum_rf[i0_4, vi1_1]
            for i0_5, i1 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_6, i1_2 = T.axis.remap("SS", [i0_5, i1])
                    T.reads(placeholder[i0_6, i1_2], T_softmax_maxelem[i0_6], T_softmax_expsum[i0_6])
                    T.writes(T_softmax_norm[i0_6, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_6, i1_2] = T.exp(placeholder[i0_6, i1_2] - T_softmax_maxelem[i0_6], dtype="float32") / T_softmax_expsum[i0_6]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[500, 2])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l9, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
l11, l12 = sch.get_loops(block=b0)
v13, v14 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1000, 1])
l15, l16 = sch.split(loop=l12, factors=[v13, v14])
b17 = sch.rfactor(loop=l15, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
b19, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l20 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b19, decision=-1)
sch.compute_at(block=b19, loop=l21, preserve_unit_loops=True)
l22 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l22, preserve_unit_loops=True)
b23, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l24 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l24, preserve_unit_loops=True)
l25 = sch.sample_compute_location(block=b23, decision=-2)
sch.compute_at(block=b23, loop=l25, preserve_unit_loops=True)
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #4:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 2], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 1], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 1000, 1):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_1 = T.axis.spatial(1, i1_1)
                    i0_1 = T.axis.spatial(1, 0)
                    k = T.axis.reduce(1000, i1_0)
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_1])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_1] = T.max(T_softmax_maxelem_rf[i0_1, vi1_1], placeholder[i0_1, k])
            for i0, i1_1 in T.grid(1, 1):
                with T.block("T_softmax_maxelem"):
                    vi1_1 = T.axis.reduce(1, i1_1)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem_rf[i0_2, vi1_1])
                    T.writes(T_softmax_maxelem[i0_2])
                    with T.init():
                        T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_1])
            for i0_3, i1_0, i1_1 in T.grid(1, 500, 2):
                with T.block("T_softmax_expsum_rf"):
                    vi1_1 = T.axis.spatial(2, i1_1)
                    i0_4 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(500, i1_0)
                    T.reads(placeholder[i0_4, vi1_0 * 2 + vi1_1], T_softmax_maxelem[i0_4])
                    T.writes(T_softmax_expsum_rf[i0_4, vi1_1])
                    with T.init():
                        T_softmax_expsum_rf[i0_4, vi1_1] = T.float32(0)
                    T_softmax_expsum_rf[i0_4, vi1_1] = T_softmax_expsum_rf[i0_4, vi1_1] + T.exp(placeholder[i0_4, vi1_0 * 2 + vi1_1] - T_softmax_maxelem[i0_4], dtype="float32")
            for i0_5, i1_1 in T.grid(1, 2):
                with T.block("T_softmax_expsum"):
                    vi1_1 = T.axis.reduce(2, i1_1)
                    i0_6 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum_rf[i0_6, vi1_1])
                    T.writes(T_softmax_expsum[i0_6])
                    with T.init():
                        T_softmax_expsum[i0_6] = T.float32(0)
                    T_softmax_expsum[i0_6] = T_softmax_expsum[i0_6] + T_softmax_expsum_rf[i0_6, vi1_1]
            for i0_7, i1 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_8, i1_2 = T.axis.remap("SS", [i0_7, i1])
                    T.reads(placeholder[i0_8, i1_2], T_softmax_maxelem[i0_8], T_softmax_expsum[i0_8])
                    T.writes(T_softmax_norm[i0_8, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_8, i1_2] = T.exp(placeholder[i0_8, i1_2] - T_softmax_maxelem[i0_8], dtype="float32") / T_softmax_expsum[i0_8]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[500, 2])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l9, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
l11, l12 = sch.get_loops(block=b0)
v13, v14 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1000, 1])
l15, l16 = sch.split(loop=l12, factors=[v13, v14])
b17 = sch.rfactor(loop=l16, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
b19, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l20 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b19, decision=-1)
sch.compute_at(block=b19, loop=l21, preserve_unit_loops=True)
l22 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l22, preserve_unit_loops=True)
b23, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l24 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l24, preserve_unit_loops=True)
l25 = sch.sample_compute_location(block=b23, decision=-1)
sch.compute_at(block=b23, loop=l25, preserve_unit_loops=True)
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #5:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_exp = T.alloc_buffer([1, 1000], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 2], dtype="float32")
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_exp"):
                    i0_2, i1_1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[i0_2, i1_1], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_exp[i0_2, i1_1])
                    T_softmax_exp[i0_2, i1_1] = T.exp(placeholder[i0_2, i1_1] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1_0, i1_1_1 in T.grid(1, 500, 2):
                with T.block("T_softmax_expsum_rf"):
                    vi1_1 = T.axis.spatial(2, i1_1_1)
                    i0_4 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(500, i1_0)
                    T.reads(T_softmax_exp[i0_4, vi1_0 * 2 + vi1_1])
                    T.writes(T_softmax_expsum_rf[i0_4, vi1_1])
                    with T.init():
                        T_softmax_expsum_rf[i0_4, vi1_1] = T.float32(0)
                    T_softmax_expsum_rf[i0_4, vi1_1] = T_softmax_expsum_rf[i0_4, vi1_1] + T_softmax_exp[i0_4, vi1_0 * 2 + vi1_1]
            for i0_5, i1_1_2 in T.grid(1, 2):
                with T.block("T_softmax_expsum"):
                    vi1_1 = T.axis.reduce(2, i1_1_2)
                    i0_6 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum_rf[i0_6, vi1_1])
                    T.writes(T_softmax_expsum[i0_6])
                    with T.init():
                        T_softmax_expsum[i0_6] = T.float32(0)
                    T_softmax_expsum[i0_6] = T_softmax_expsum[i0_6] + T_softmax_expsum_rf[i0_6, vi1_1]
            for i0_7, i1 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_8, i1_2 = T.axis.remap("SS", [i0_7, i1])
                    T.reads(T_softmax_exp[i0_8, i1_2], T_softmax_expsum[i0_8])
                    T.writes(T_softmax_norm[i0_8, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_8, i1_2] = T_softmax_exp[i0_8, i1_2] / T_softmax_expsum[i0_8]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[500, 2])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l9, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v11 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v11)
b12, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l13 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l13, preserve_unit_loops=True)
l14 = sch.sample_compute_location(block=b12, decision=-1)
sch.compute_at(block=b12, loop=l14, preserve_unit_loops=True)
l15 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l15, preserve_unit_loops=True)
l16 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l16, preserve_unit_loops=True)
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #6:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_exp = T.alloc_buffer([1, 1000], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 50], dtype="float32")
            for i0, i1 in T.grid(1, 1000):
                for ax0 in T.serial(50):
                    for ax0_1, ax1, ax2 in T.grid(1, 1, 20):
                        with T.block("T_softmax_maxelem_rf"):
                            vi1_0 = T.axis.spatial(50, ax0 + ax0_1)
                            i0_1, vi1_1 = T.axis.remap("SR", [ax1, ax2])
                            T.reads(placeholder[i0_1, vi1_0 * 20 + vi1_1])
                            T.writes(T_softmax_maxelem_rf[i0_1, vi1_0])
                            with T.init():
                                T_softmax_maxelem_rf[i0_1, vi1_0] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_rf[i0_1, vi1_0] = T.max(T_softmax_maxelem_rf[i0_1, vi1_0], placeholder[i0_1, vi1_0 * 20 + vi1_1])
                    for ax1 in T.serial(1):
                        with T.block("T_softmax_maxelem"):
                            vi1_0, i0_2 = T.axis.remap("RS", [ax0, ax1])
                            T.reads(T_softmax_maxelem_rf[i0_2, vi1_0])
                            T.writes(T_softmax_maxelem[i0_2])
                            with T.init():
                                T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_0])
                for ax0, ax1 in T.grid(1, 1000):
                    with T.block("T_softmax_exp"):
                        i0_3, i1_1 = T.axis.remap("SS", [ax0, ax1])
                        T.reads(placeholder[i0_3, i1_1], T_softmax_maxelem[i0_3])
                        T.writes(T_softmax_exp[i0_3, i1_1])
                        T_softmax_exp[i0_3, i1_1] = T.exp(placeholder[i0_3, i1_1] - T_softmax_maxelem[i0_3], dtype="float32")
                for ax0, ax1 in T.grid(1, 1000):
                    with T.block("T_softmax_expsum"):
                        i0_4, k = T.axis.remap("SR", [ax0, ax1])
                        T.reads(T_softmax_exp[i0_4, k])
                        T.writes(T_softmax_expsum[i0_4])
                        with T.init():
                            T_softmax_expsum[i0_4] = T.float32(0)
                        T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T_softmax_exp[i0_4, k]
                with T.block("T_softmax_norm"):
                    i0_5, i1_2 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_softmax_exp[i0_5, i1_2], T_softmax_expsum[i0_5])
                    T.writes(T_softmax_norm[i0_5, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_5, i1_2] = T_softmax_exp[i0_5, i1_2] / T_softmax_expsum[i0_5]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b0)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[50, 20])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l8, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v11 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v11)
l12 = sch.sample_compute_location(block=b2, decision=1)
sch.compute_at(block=b2, loop=l12, preserve_unit_loops=True)
l13 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l13, preserve_unit_loops=True)
b14, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l15 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l15, preserve_unit_loops=True)
l16 = sch.sample_compute_location(block=b14, decision=2)
sch.compute_at(block=b14, loop=l16, preserve_unit_loops=True)
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #7:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 20], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 50, 20):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_1 = T.axis.spatial(20, i1_1)
                    i0_1 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(50, i1_0)
                    T.reads(placeholder[i0_1, vi1_0 * 20 + vi1_1])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_1])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_1] = T.max(T_softmax_maxelem_rf[i0_1, vi1_1], placeholder[i0_1, vi1_0 * 20 + vi1_1])
            for i0, i1_1 in T.grid(1, 20):
                with T.block("T_softmax_maxelem"):
                    vi1_1 = T.axis.reduce(20, i1_1)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem_rf[i0_2, vi1_1])
                    T.writes(T_softmax_maxelem[i0_2])
                    with T.init():
                        T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_1])
            for i0_3, i1 in T.grid(1, 1000):
                with T.block("T_softmax_expsum"):
                    i0_4, k = T.axis.remap("SR", [i0_3, i1])
                    T.reads(placeholder[i0_4, k], T_softmax_maxelem[i0_4])
                    T.writes(T_softmax_expsum[i0_4])
                    with T.init():
                        T_softmax_expsum[i0_4] = T.float32(0)
                    T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T.exp(placeholder[i0_4, k] - T_softmax_maxelem[i0_4], dtype="float32")
            for i0_5, i1 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_6, i1_2 = T.axis.remap("SS", [i0_5, i1])
                    T.reads(placeholder[i0_6, i1_2], T_softmax_maxelem[i0_6], T_softmax_expsum[i0_6])
                    T.writes(T_softmax_norm[i0_6, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_6, i1_2] = T.exp(placeholder[i0_6, i1_2] - T_softmax_maxelem[i0_6], dtype="float32") / T_softmax_expsum[i0_6]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b0)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[50, 20])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l9, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v11 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v11)
l12 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l12, preserve_unit_loops=True)
l13 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l13, preserve_unit_loops=True)
b14, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l15 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l15, preserve_unit_loops=True)
l16 = sch.sample_compute_location(block=b14, decision=-1)
sch.compute_at(block=b14, loop=l16, preserve_unit_loops=True)
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #8:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1 in T.grid(1, 1000):
                for ax0, ax1 in T.grid(1, 1000):
                    with T.block("T_softmax_expsum"):
                        i0_2, k = T.axis.remap("SR", [ax0, ax1])
                        T.reads(placeholder[i0_2, k], T_softmax_maxelem[i0_2])
                        T.writes(T_softmax_expsum[i0_2])
                        with T.init():
                            T_softmax_expsum[i0_2] = T.float32(0)
                        T_softmax_expsum[i0_2] = T_softmax_expsum[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem[i0_2], dtype="float32")
                with T.block("T_softmax_norm"):
                    i0_3, i1_1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[i0_3, i1_1], T_softmax_maxelem[i0_3], T_softmax_expsum[i0_3])
                    T.writes(T_softmax_norm[i0_3, i1_1])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_3, i1_1] = T.exp(placeholder[i0_3, i1_1] - T_softmax_maxelem[i0_3], dtype="float32") / T_softmax_expsum[i0_3]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=1)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                                                Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                    fused_transpose_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |              fused_nn_contrib_conv2d_NCHWc_add_clip | 22880256 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |    fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |  8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                   fused_nn_contrib_conv2d_NCHWc_add | 19568640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |            fused_nn_contrib_conv2d_NCHWc_add_clip_1 | 92123136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |  9483264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                 fused_nn_contrib_conv2d_NCHWc_add_1 | 29001728 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_contrib_conv2d_NCHWc_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |            fused_nn_contrib_conv2d_NCHWc_add_clip_2 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                 fused_nn_contrib_conv2d_NCHWc_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |             fused_nn_contrib_conv2d_NCHWc_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                 fused_nn_contrib_conv2d_NCHWc_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |             fused_nn_contrib_conv2d_NCHWc_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_contrib_conv2d_NCHWc_add_clip_4 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                 fused_nn_contrib_conv2d_NCHWc_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |             fused_nn_contrib_conv2d_NCHWc_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 |            fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_9 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                 fused_nn_contrib_conv2d_NCHWc_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_10 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_contrib_conv2d_NCHWc_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 |            fused_nn_contrib_conv2d_NCHWc_add_clip_6 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_11 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                 fused_nn_contrib_conv2d_NCHWc_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |            fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                      fused_squeeze_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                    fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[15:58:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_transpose_layout_transform"
[15:58:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:58:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:58:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28aefea8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f27bb2cf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28b32038)]: 0 failure(s)
[15:58:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[15:59:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28aefea8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f27bb2cf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28b32038)]: 0 failure(s)
[15:59:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28aefea8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f27bb2cf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28b32038)]: 0 failure(s)
[15:59:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28aefea8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f27bb2cf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28b32038)]: 0 failure(s)
[15:59:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28aefea8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f27bb2cf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28b32038)]: 0 failure(s)
[15:59:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 24 candidates:
[1 : 16]:	0.9991  0.9686  0.9658  0.9169  0.8956  0.8832  0.8304  0.8239  0.8175  0.8141  0.7698  0.7099  0.6062  0.5898  0.5885  0.5316
[17 : 24]:	0.3496  0.3361  0.2983  0.2177  0.1334  0.1245  0.1010  0.0083
[15:59:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 24 candidate(s) with evolutionary search
[15:59:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 24 candidates(s) for measurement
[15:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 24 sample(s) to builder
[16:00:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 24 sample(s) to runner
[16:00:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"
[16:00:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:00:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:01:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f268a6498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28b5b9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28e1e038)]: 0 failure(s)
[16:01:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:01:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f268a6498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28b5b9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28e1e038)]: 0 failure(s)
[16:02:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f268a6498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28b5b9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28e1e038)]: 0 failure(s)
[16:02:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f268a6498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28b5b9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28e1e038)]: 0 failure(s)
[16:03:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f268a6498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28b5b9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28e1e038)]: 0 failure(s)
[16:03:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9999  0.9995  0.9994  0.9994  0.9993  0.9993  0.9991  0.9989  0.9989  0.9988  0.9988  0.9986  0.9985  0.9982
[17 : 32]:	0.9981  0.9980  0.9978  0.9977  0.9975  0.9975  0.9974  0.9974  0.9972  0.9972  0.9970  0.9970  0.9970  0.9969  0.9969  0.9969
[16:03:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:03:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:03:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:03:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:04:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"
[16:04:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:04:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:04:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f27190118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28edfe28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28193ba8)]: 0 failure(s)
[16:04:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:04:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f27190118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28edfe28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28193ba8)]: 0 failure(s)
[16:05:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f27190118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28edfe28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28193ba8)]: 0 failure(s)
[16:05:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f27190118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28edfe28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28193ba8)]: 0 failure(s)
[16:06:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f27190118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28edfe28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28193ba8)]: 0 failure(s)
[16:06:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9995  0.9995  0.9994  0.9994  0.9993  0.9993  0.9992  0.9991  0.9990  0.9989  0.9989  0.9986  0.9986  0.9984
[17 : 32]:	0.9984  0.9984  0.9983  0.9983  0.9982  0.9980  0.9980  0.9980  0.9979  0.9978  0.9978  0.9976  0.9975  0.9974  0.9974  0.9972
[16:06:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:06:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:06:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:07:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_contrib_conv2d_NCHWc_add"
[16:07:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:07:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:07:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f277d2178)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28e12da8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f268ee7b8)]: 0 failure(s)
[16:07:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:07:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f277d2178)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28e12da8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f268ee7b8)]: 0 failure(s)
[16:08:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f277d2178)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28e12da8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f268ee7b8)]: 0 failure(s)
[16:08:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f277d2178)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28e12da8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f268ee7b8)]: 0 failure(s)
[16:08:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f277d2178)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28e12da8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f268ee7b8)]: 0 failure(s)
[16:08:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9995  0.9995  0.9993  0.9992  0.9992  0.9991  0.9990  0.9989  0.9988  0.9983  0.9983  0.9981  0.9980  0.9979  0.9979
[17 : 32]:	0.9974  0.9974  0.9974  0.9973  0.9972  0.9971  0.9970  0.9969  0.9968  0.9967  0.9965  0.9965  0.9965  0.9963  0.9963  0.9963
[16:08:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:08:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:08:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:09:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:09:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"
[16:09:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:09:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:09:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28d34938)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f26789d08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26791ca8)]: 0 failure(s)
[16:09:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:09:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28d34938)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f26789d08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26791ca8)]: 0 failure(s)
[16:10:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28d34938)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f26789d08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26791ca8)]: 0 failure(s)
[16:10:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28d34938)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f26789d08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26791ca8)]: 0 failure(s)
[16:10:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28d34938)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f26789d08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26791ca8)]: 0 failure(s)
[16:10:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9997  0.9997  0.9996  0.9995  0.9994  0.9992  0.9992  0.9989  0.9988  0.9988  0.9983  0.9980  0.9980  0.9979
[17 : 32]:	0.9979  0.9978  0.9976  0.9976  0.9973  0.9969  0.9968  0.9967  0.9966  0.9965  0.9965  0.9963  0.9963  0.9962  0.9961  0.9960
[16:10:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:10:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:10:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:11:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:11:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"
[16:11:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:11:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:12:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f26e94af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f2ae0eab8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26e46978)]: 0 failure(s)
[16:12:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:12:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f26e94af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f2ae0eab8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26e46978)]: 0 failure(s)
[16:13:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f26e94af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f2ae0eab8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26e46978)]: 0 failure(s)
[16:14:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f26e94af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f2ae0eab8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26e46978)]: 0 failure(s)
[16:14:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f26e94af8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f2ae0eab8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26e46978)]: 0 failure(s)
[16:14:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9996  0.9995  0.9993  0.9993  0.9993  0.9992  0.9991  0.9990  0.9989  0.9988  0.9987  0.9987  0.9985  0.9985
[17 : 32]:	0.9984  0.9984  0.9983  0.9981  0.9980  0.9980  0.9980  0.9978  0.9975  0.9975  0.9974  0.9973  0.9972  0.9969  0.9969  0.9969
[16:14:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:14:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:14:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:15:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:15:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"
[16:15:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:15:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:16:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28ec2458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28e5a0f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26e28598)]: 0 failure(s)
[16:16:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:16:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28ec2458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28e5a0f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26e28598)]: 0 failure(s)
[16:16:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28ec2458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28e5a0f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26e28598)]: 0 failure(s)
[16:16:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28ec2458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28e5a0f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26e28598)]: 0 failure(s)
[16:16:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f28ec2458)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28e5a0f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f26e28598)]: 0 failure(s)
[16:17:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9994  0.9992  0.9991  0.9991  0.9990  0.9990  0.9989  0.9989  0.9989  0.9988  0.9984  0.9984  0.9981  0.9979
[17 : 32]:	0.9973  0.9972  0.9970  0.9969  0.9968  0.9968  0.9968  0.9968  0.9967  0.9966  0.9965  0.9965  0.9962  0.9962  0.9962  0.9961
[16:17:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:17:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:17:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:17:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:17:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"
[16:17:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:17:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:18:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f278707e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28dcde38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28ee3f68)]: 0 failure(s)
[16:18:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:19:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f278707e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28dcde38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28ee3f68)]: 0 failure(s)
[16:20:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f278707e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28dcde38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28ee3f68)]: 0 failure(s)
[16:20:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f278707e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28dcde38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28ee3f68)]: 0 failure(s)
[16:21:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f278707e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28dcde38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28ee3f68)]: 0 failure(s)
[16:21:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9993  0.9991  0.9991  0.9990  0.9990  0.9989  0.9987  0.9987  0.9987  0.9986  0.9985  0.9982  0.9979  0.9978  0.9977
[17 : 32]:	0.9977  0.9976  0.9976  0.9974  0.9973  0.9973  0.9970  0.9970  0.9969  0.9969  0.9968  0.9968  0.9967  0.9967  0.9964  0.9964
[16:21:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:21:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:21:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:21:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:22:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"
[16:22:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[16:22:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[16:22:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f268d9608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28dcfe48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28ec51f8)]: 0 failure(s)
[16:22:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[16:22:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f268d9608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28dcfe48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28ec51f8)]: 0 failure(s)
[16:23:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f268d9608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28dcfe48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28ec51f8)]: 0 failure(s)
[16:23:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f268d9608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28dcfe48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28ec51f8)]: 0 failure(s)
[16:23:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559f268d9608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x559f28dcfe48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x559f28ec51f8)]: 0 failure(s)
[16:23:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9998  0.9994  0.9993  0.9993  0.9991  0.9990  0.9987  0.9986  0.9983  0.9983  0.9982  0.9982  0.9981  0.9979  0.9976
[17 : 32]:	0.9975  0.9974  0.9973  0.9973  0.9971  0.9971  0.9969  0.9968  0.9967  0.9966  0.9965  0.9963  0.9963  0.9963  0.9961  0.9960
[16:23:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[16:23:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[16:23:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:24:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #0: GFLOPs: 0.0000. Time: 4.9338 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #1: GFLOPs: 0.0000. Time: 1.0440 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #2: GFLOPs: 0.0000. Time: 7.8060 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #3: GFLOPs: 0.0000. Time: 2.7434 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #4: GFLOPs: 0.0000. Time: 0.6994 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #5: GFLOPs: 0.0000. Time: 4.1743 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #6: GFLOPs: 0.0000. Time: 4.4608 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #7: GFLOPs: 0.0000. Time: 2.2268 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #8: GFLOPs: 0.0000. Time: 7.9974 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #9: GFLOPs: 0.0000. Time: 3.5146 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #10: GFLOPs: 0.0000. Time: 1.8388 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #11: GFLOPs: 0.0000. Time: 0.2783 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #12: GFLOPs: 0.0000. Time: 1.5317 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #13: GFLOPs: 0.0000. Time: 3.3361 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #14: GFLOPs: 0.0000. Time: 3.3075 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #15: GFLOPs: 0.0000. Time: 0.0406 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #16: GFLOPs: 0.0000. Time: 0.1545 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #17: GFLOPs: 0.0000. Time: 0.0703 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #18: GFLOPs: 0.0000. Time: 0.0428 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #19: GFLOPs: 0.0000. Time: 0.2577 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #20: GFLOPs: 0.0000. Time: 0.0237 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #21: GFLOPs: 0.0000. Time: 0.1285 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #22: GFLOPs: 0.0000. Time: 0.0459 ms. Best GFLOPs: 0.0000
[16:24:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose_layout_transform"] Trial #23: GFLOPs: 0.0000. Time: 0.2284 ms. Best GFLOPs: 0.0000
[16:24:38] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_transpose_layout_transform"
 ID |                                                Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                    fused_transpose_layout_transform |        1 |      1 |         0.0000 |      23.7343 |               23.7343 |     24 |            
  1 |              fused_nn_contrib_conv2d_NCHWc_add_clip | 22880256 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |    fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |  8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                   fused_nn_contrib_conv2d_NCHWc_add | 19568640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |            fused_nn_contrib_conv2d_NCHWc_add_clip_1 | 92123136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |  9483264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                 fused_nn_contrib_conv2d_NCHWc_add_1 | 29001728 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_contrib_conv2d_NCHWc_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |            fused_nn_contrib_conv2d_NCHWc_add_clip_2 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                 fused_nn_contrib_conv2d_NCHWc_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |             fused_nn_contrib_conv2d_NCHWc_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                 fused_nn_contrib_conv2d_NCHWc_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |             fused_nn_contrib_conv2d_NCHWc_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_contrib_conv2d_NCHWc_add_clip_4 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                 fused_nn_contrib_conv2d_NCHWc_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |             fused_nn_contrib_conv2d_NCHWc_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 |            fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_9 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                 fused_nn_contrib_conv2d_NCHWc_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_10 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_contrib_conv2d_NCHWc_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 |            fused_nn_contrib_conv2d_NCHWc_add_clip_6 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_11 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                 fused_nn_contrib_conv2d_NCHWc_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |            fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                      fused_squeeze_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                    fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 24
Total latency (us): 23.7343

[16:24:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #0 has finished. Remaining task(s): 36
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #0: GFLOPs: 2.3254. Time: 9.8391 ms. Best GFLOPs: 2.3254
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #1: GFLOPs: 14.8718. Time: 1.5385 ms. Best GFLOPs: 14.8718
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #2: GFLOPs: 20.8192. Time: 1.0990 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #3: GFLOPs: 2.4210. Time: 9.4508 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #4: GFLOPs: 4.2558. Time: 5.3762 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #5: GFLOPs: 4.0042. Time: 5.7141 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #6: GFLOPs: 1.9593. Time: 11.6781 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #7: GFLOPs: 5.2306. Time: 4.3743 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #8: GFLOPs: 2.0734. Time: 11.0351 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #9: GFLOPs: 10.5468. Time: 2.1694 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #10: GFLOPs: 3.7987. Time: 6.0231 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #11: GFLOPs: 7.2931. Time: 3.1373 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #12: GFLOPs: 3.3816. Time: 6.7661 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #13: GFLOPs: 5.8915. Time: 3.8836 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #14: GFLOPs: 7.3917. Time: 3.0954 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #15: GFLOPs: 5.7162. Time: 4.0027 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #16: GFLOPs: 4.8858. Time: 4.6830 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #17: GFLOPs: 2.5912. Time: 8.8299 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #18: GFLOPs: 8.2079. Time: 2.7876 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #19: GFLOPs: 5.7155. Time: 4.0032 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #20: GFLOPs: 3.1134. Time: 7.3489 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #21: GFLOPs: 4.9570. Time: 4.6158 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #22: GFLOPs: 4.0423. Time: 5.6602 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 225, 225, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(225, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(225):
                for i4_fused in T.vectorized(3):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(1, 0)
                        i2, i3_1, i4 = T.axis.remap("SSS", [i0_i1_i2_fused, i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2, i3_1, i4])
                        T.writes(data_pad[i0, i1, i2, i3_1, i4])
                        data_pad[i0, i1, i2, i3_1, i4] = T.if_then_else(0 <= i2 and i2 < 224 and 0 <= i3_1 and i3_1 < 224, placeholder[i0, i1, i2, i3_1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(16, 2, 8, 8, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_3_init)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 2 * 8 + i2_3_init)
                    ow = T.axis.spatial(112, i3_2_init * 7 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 1, 1, 16, 2, 1, 3, 3, 1, 8, 8, 7, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_3)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 14 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 14 // 2 * 8 + i2_3)
                    ow = T.axis.spatial(112, i3_2 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                    ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_1, i7_1])
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(8, i0_i1_i2_fused // 112)
                        i2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        i3_4, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_4, i4])
                        compute[i0, i1, i2, i3_4, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 8])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 16, 7])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74 = sch.get_loops(block=b67)
l75 = sch.fuse(l70, l71, l72)
sch.parallel(loop=l75)
l76 = sch.fuse(l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b68)
l103 = sch.fuse(l77, l78, l79, l80, l81, l82, l83, l84, l85, l86)
sch.parallel(loop=l103)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108 = sch.get_loops(block=b69)
l109 = sch.fuse(l104, l105, l106)
sch.parallel(loop=l109)
l110 = sch.fuse(l108)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
b129 = sch.decompose_reduction(block=b111, loop=l113)
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #24: GFLOPs: 3.8756. Time: 5.9037 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #25: GFLOPs: 10.9332. Time: 2.0927 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #26: GFLOPs: 14.6123. Time: 1.5658 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #27: GFLOPs: 11.9426. Time: 1.9159 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #28: GFLOPs: 4.5928. Time: 4.9818 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #29: GFLOPs: 10.4694. Time: 2.1854 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #30: GFLOPs: 1.9299. Time: 11.8556 ms. Best GFLOPs: 20.8192
[16:24:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"] Trial #31: GFLOPs: 4.2497. Time: 5.3840 ms. Best GFLOPs: 20.8192
[16:24:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_contrib_conv2d_NCHWc_add_clip"
 ID |                                                Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                    fused_transpose_layout_transform |        1 |      1 |         0.0000 |      23.7343 |               23.7343 |     24 |          Y 
  1 |              fused_nn_contrib_conv2d_NCHWc_add_clip | 22880256 |      1 |        20.8192 |    1098.9969 |             1098.9969 |     32 |            
  2 |    fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |  8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                   fused_nn_contrib_conv2d_NCHWc_add | 19568640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |            fused_nn_contrib_conv2d_NCHWc_add_clip_1 | 92123136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |  9483264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                 fused_nn_contrib_conv2d_NCHWc_add_1 | 29001728 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_contrib_conv2d_NCHWc_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |            fused_nn_contrib_conv2d_NCHWc_add_clip_2 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                 fused_nn_contrib_conv2d_NCHWc_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |             fused_nn_contrib_conv2d_NCHWc_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                 fused_nn_contrib_conv2d_NCHWc_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |             fused_nn_contrib_conv2d_NCHWc_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_contrib_conv2d_NCHWc_add_clip_4 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                 fused_nn_contrib_conv2d_NCHWc_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |             fused_nn_contrib_conv2d_NCHWc_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 |            fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_9 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                 fused_nn_contrib_conv2d_NCHWc_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_10 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_contrib_conv2d_NCHWc_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 |            fused_nn_contrib_conv2d_NCHWc_add_clip_6 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_11 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                 fused_nn_contrib_conv2d_NCHWc_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |            fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                      fused_squeeze_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                    fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 56
Total latency (us): 1122.73

[16:24:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #1 has finished. Remaining task(s): 35
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #0: GFLOPs: 1.6190. Time: 5.2067 ms. Best GFLOPs: 1.6190
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #1: GFLOPs: 0.8541. Time: 9.8697 ms. Best GFLOPs: 1.6190
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #2: GFLOPs: 0.6819. Time: 12.3618 ms. Best GFLOPs: 1.6190
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #3: GFLOPs: 1.2545. Time: 6.7196 ms. Best GFLOPs: 1.6190
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #4: GFLOPs: 1.8685. Time: 4.5114 ms. Best GFLOPs: 1.8685
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #5: GFLOPs: 3.2171. Time: 2.6202 ms. Best GFLOPs: 3.2171
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #6: GFLOPs: 1.8486. Time: 4.5600 ms. Best GFLOPs: 3.2171
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #7: GFLOPs: 2.8641. Time: 2.9432 ms. Best GFLOPs: 3.2171
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #8: GFLOPs: 2.0397. Time: 4.1328 ms. Best GFLOPs: 3.2171
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #9: GFLOPs: 1.8376. Time: 4.5873 ms. Best GFLOPs: 3.2171
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #10: GFLOPs: 9.5042. Time: 0.8869 ms. Best GFLOPs: 9.5042
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #11: GFLOPs: 4.9399. Time: 1.7064 ms. Best GFLOPs: 9.5042
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #12: GFLOPs: 7.5871. Time: 1.1110 ms. Best GFLOPs: 9.5042
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #13: GFLOPs: 5.6676. Time: 1.4873 ms. Best GFLOPs: 9.5042
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #14: GFLOPs: 18.2420. Time: 0.4621 ms. Best GFLOPs: 18.2420
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #15: GFLOPs: 4.8914. Time: 1.7234 ms. Best GFLOPs: 18.2420
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #16: GFLOPs: 2.9774. Time: 2.8312 ms. Best GFLOPs: 18.2420
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #17: GFLOPs: 0.4543. Time: 18.5550 ms. Best GFLOPs: 18.2420
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #18: GFLOPs: 0.8910. Time: 9.4609 ms. Best GFLOPs: 18.2420
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #19: GFLOPs: 1.0290. Time: 8.1917 ms. Best GFLOPs: 18.2420
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #20: GFLOPs: 0.6622. Time: 12.7291 ms. Best GFLOPs: 18.2420
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #21: GFLOPs: 0.8097. Time: 10.4111 ms. Best GFLOPs: 18.2420
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #22: GFLOPs: 1.5840. Time: 5.3216 ms. Best GFLOPs: 18.2420
[16:24:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 2, 114, 58):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_fused % 8 // 2 * 2 + ax1)
                        i2 = T.axis.spatial(114, ax2)
                        i3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i4_0 in T.serial(1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 7, 4):
                    for i2_3_init, i3_3_init in T.grid(56, 4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i1_2)
                            oh = T.axis.spatial(112, i2_2 * 56 + i2_3_init)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i3_1 * 28 + i3_2 * 4 + i3_3_init)
                            oci = T.axis.spatial(4, i4_2)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 56, 4, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i1_2)
                            oh = T.axis.spatial(112, i2_2 * 56 + i2_3)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i3_1 * 28 + i3_2 * 4 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i4_2, i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 2, 112, 56):
                    for ax4_fused in T.vectorized(4):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + ax1)
                            i2 = T.axis.spatial(112, ax2)
                            i3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ax3)
                            i4 = T.axis.spatial(4, ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 56])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74 = sch.get_loops(block=b63)
l75 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l75)
l76 = sch.fuse(l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
l105 = sch.fuse(l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b106)
b128 = sch.decompose_reduction(block=b106, loop=l121)
[16:24:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #24: GFLOPs: 1.5301. Time: 5.5091 ms. Best GFLOPs: 18.2420
[16:24:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #25: GFLOPs: 0.5776. Time: 14.5931 ms. Best GFLOPs: 18.2420
[16:24:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #26: GFLOPs: 4.5551. Time: 1.8506 ms. Best GFLOPs: 18.2420
[16:24:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #27: GFLOPs: 7.1004. Time: 1.1872 ms. Best GFLOPs: 18.2420
[16:24:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #28: GFLOPs: 2.9592. Time: 2.8486 ms. Best GFLOPs: 18.2420
[16:24:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #29: GFLOPs: 8.8153. Time: 0.9562 ms. Best GFLOPs: 18.2420
[16:24:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #30: GFLOPs: 2.0313. Time: 4.1499 ms. Best GFLOPs: 18.2420
[16:24:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"] Trial #31: GFLOPs: 1.6341. Time: 5.1584 ms. Best GFLOPs: 18.2420
[16:24:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip"
 ID |                                                Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                    fused_transpose_layout_transform |        1 |      1 |         0.0000 |      23.7343 |               23.7343 |     24 |          Y 
  1 |              fused_nn_contrib_conv2d_NCHWc_add_clip | 22880256 |      1 |        20.8192 |    1098.9969 |             1098.9969 |     32 |          Y 
  2 |    fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |  8429568 |      1 |        18.2420 |     462.0955 |              462.0955 |     32 |            
  3 |                   fused_nn_contrib_conv2d_NCHWc_add | 19568640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |            fused_nn_contrib_conv2d_NCHWc_add_clip_1 | 92123136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |  9483264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                 fused_nn_contrib_conv2d_NCHWc_add_1 | 29001728 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_contrib_conv2d_NCHWc_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |            fused_nn_contrib_conv2d_NCHWc_add_clip_2 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                 fused_nn_contrib_conv2d_NCHWc_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |             fused_nn_contrib_conv2d_NCHWc_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                 fused_nn_contrib_conv2d_NCHWc_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |             fused_nn_contrib_conv2d_NCHWc_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_contrib_conv2d_NCHWc_add_clip_4 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                 fused_nn_contrib_conv2d_NCHWc_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |             fused_nn_contrib_conv2d_NCHWc_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 |            fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_9 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                 fused_nn_contrib_conv2d_NCHWc_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_10 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_contrib_conv2d_NCHWc_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 |            fused_nn_contrib_conv2d_NCHWc_add_clip_6 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_11 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                 fused_nn_contrib_conv2d_NCHWc_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |            fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                      fused_squeeze_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                    fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 88
Total latency (us): 1584.83

[16:24:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #2 has finished. Remaining task(s): 34
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #0: GFLOPs: 1.9833. Time: 9.8667 ms. Best GFLOPs: 1.9833
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #1: GFLOPs: 2.8976. Time: 6.7533 ms. Best GFLOPs: 2.8976
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #2: GFLOPs: 1.8266. Time: 10.7132 ms. Best GFLOPs: 2.8976
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #3: GFLOPs: 4.0492. Time: 4.8327 ms. Best GFLOPs: 4.0492
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #4: GFLOPs: 6.0011. Time: 3.2608 ms. Best GFLOPs: 6.0011
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #5: GFLOPs: 2.8561. Time: 6.8515 ms. Best GFLOPs: 6.0011
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #6: GFLOPs: 15.0105. Time: 1.3037 ms. Best GFLOPs: 15.0105
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #7: GFLOPs: 16.0932. Time: 1.2160 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #8: GFLOPs: 1.7883. Time: 10.9425 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #9: GFLOPs: 1.5678. Time: 12.4816 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #10: GFLOPs: 1.3806. Time: 14.1735 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #11: GFLOPs: 4.2451. Time: 4.6097 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #12: GFLOPs: 1.5779. Time: 12.4020 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #13: GFLOPs: 4.2182. Time: 4.6391 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #14: GFLOPs: 3.8255. Time: 5.1154 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #15: GFLOPs: 4.8631. Time: 4.0239 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #16: GFLOPs: 2.6043. Time: 7.5140 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #17: GFLOPs: 4.6377. Time: 4.2195 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #18: GFLOPs: 2.3122. Time: 8.4633 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #19: GFLOPs: 4.7231. Time: 4.1432 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #20: GFLOPs: 2.2003. Time: 8.8935 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #21: GFLOPs: 2.1744. Time: 8.9996 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #22: GFLOPs: 2.2660. Time: 8.6356 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #23: GFLOPs: 2.4160. Time: 8.0997 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #24: GFLOPs: 4.8831. Time: 4.0074 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #25: GFLOPs: 4.7127. Time: 4.1523 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #26: GFLOPs: 3.5812. Time: 5.4643 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #27: GFLOPs: 3.3708. Time: 5.8054 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #28: GFLOPs: 1.5290. Time: 12.7987 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #29: GFLOPs: 1.6936. Time: 11.5543 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #30: GFLOPs: 3.2621. Time: 5.9988 ms. Best GFLOPs: 16.0932
[16:24:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #31: GFLOPs: 3.3818. Time: 5.7865 ms. Best GFLOPs: 16.0932
[16:24:43] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_contrib_conv2d_NCHWc_add"
 ID |                                                Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                    fused_transpose_layout_transform |        1 |      1 |         0.0000 |      23.7343 |               23.7343 |     24 |          Y 
  1 |              fused_nn_contrib_conv2d_NCHWc_add_clip | 22880256 |      1 |        20.8192 |    1098.9969 |             1098.9969 |     32 |          Y 
  2 |    fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |  8429568 |      1 |        18.2420 |     462.0955 |              462.0955 |     32 |          Y 
  3 |                   fused_nn_contrib_conv2d_NCHWc_add | 19568640 |      1 |        16.0932 |    1215.9573 |             1215.9573 |     32 |            
  4 |            fused_nn_contrib_conv2d_NCHWc_add_clip_1 | 92123136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |  9483264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                 fused_nn_contrib_conv2d_NCHWc_add_1 | 29001728 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_contrib_conv2d_NCHWc_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |            fused_nn_contrib_conv2d_NCHWc_add_clip_2 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                 fused_nn_contrib_conv2d_NCHWc_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |             fused_nn_contrib_conv2d_NCHWc_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                 fused_nn_contrib_conv2d_NCHWc_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |             fused_nn_contrib_conv2d_NCHWc_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_contrib_conv2d_NCHWc_add_clip_4 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                 fused_nn_contrib_conv2d_NCHWc_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |             fused_nn_contrib_conv2d_NCHWc_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 |            fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_9 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                 fused_nn_contrib_conv2d_NCHWc_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_10 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_contrib_conv2d_NCHWc_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 |            fused_nn_contrib_conv2d_NCHWc_add_clip_6 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_11 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                 fused_nn_contrib_conv2d_NCHWc_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |            fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                      fused_squeeze_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                    fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 120
Total latency (us): 2800.78

[16:24:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #3 has finished. Remaining task(s): 33
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #0: GFLOPs: 18.6892. Time: 4.9292 ms. Best GFLOPs: 18.6892
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #1: GFLOPs: 9.4822. Time: 9.7154 ms. Best GFLOPs: 18.6892
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #2: GFLOPs: 14.2305. Time: 6.4736 ms. Best GFLOPs: 18.6892
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #3: GFLOPs: 14.7033. Time: 6.2655 ms. Best GFLOPs: 18.6892
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #4: GFLOPs: 15.5538. Time: 5.9229 ms. Best GFLOPs: 18.6892
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #5: GFLOPs: 23.9169. Time: 3.8518 ms. Best GFLOPs: 23.9169
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #6: GFLOPs: 16.7985. Time: 5.4840 ms. Best GFLOPs: 23.9169
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #7: GFLOPs: 33.5736. Time: 2.7439 ms. Best GFLOPs: 33.5736
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #8: GFLOPs: 12.3978. Time: 7.4306 ms. Best GFLOPs: 33.5736
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #9: GFLOPs: 19.9038. Time: 4.6284 ms. Best GFLOPs: 33.5736
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #10: GFLOPs: 15.0445. Time: 6.1234 ms. Best GFLOPs: 33.5736
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #11: GFLOPs: 18.1537. Time: 5.0746 ms. Best GFLOPs: 33.5736
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #12: GFLOPs: 30.4947. Time: 3.0210 ms. Best GFLOPs: 33.5736
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #13: GFLOPs: 13.0061. Time: 7.0831 ms. Best GFLOPs: 33.5736
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #14: GFLOPs: 15.5266. Time: 5.9333 ms. Best GFLOPs: 33.5736
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #15: GFLOPs: 10.1121. Time: 9.1102 ms. Best GFLOPs: 33.5736
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #16: GFLOPs: 15.3348. Time: 6.0075 ms. Best GFLOPs: 33.5736
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #17: GFLOPs: 11.0904. Time: 8.3066 ms. Best GFLOPs: 33.5736
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #18: GFLOPs: 50.7019. Time: 1.8170 ms. Best GFLOPs: 50.7019
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #19: GFLOPs: 24.6414. Time: 3.7386 ms. Best GFLOPs: 50.7019
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #20: GFLOPs: 10.5423. Time: 8.7384 ms. Best GFLOPs: 50.7019
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #21: GFLOPs: 22.2631. Time: 4.1379 ms. Best GFLOPs: 50.7019
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #22: GFLOPs: 26.1175. Time: 3.5273 ms. Best GFLOPs: 50.7019
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #23: GFLOPs: 24.0289. Time: 3.8338 ms. Best GFLOPs: 50.7019
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #24: GFLOPs: 16.5766. Time: 5.5574 ms. Best GFLOPs: 50.7019
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #25: GFLOPs: 16.8191. Time: 5.4773 ms. Best GFLOPs: 50.7019
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #26: GFLOPs: 18.9515. Time: 4.8610 ms. Best GFLOPs: 50.7019
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #27: GFLOPs: 19.4914. Time: 4.7263 ms. Best GFLOPs: 50.7019
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #28: GFLOPs: 22.1425. Time: 4.1605 ms. Best GFLOPs: 50.7019
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #29: GFLOPs: 24.2172. Time: 3.8040 ms. Best GFLOPs: 50.7019
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #30: GFLOPs: 23.3979. Time: 3.9372 ms. Best GFLOPs: 50.7019
[16:24:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"] Trial #31: GFLOPs: 12.0826. Time: 7.6245 ms. Best GFLOPs: 50.7019
[16:24:51] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_contrib_conv2d_NCHWc_add_clip_1"
 ID |                                                Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                    fused_transpose_layout_transform |        1 |      1 |         0.0000 |      23.7343 |               23.7343 |     24 |          Y 
  1 |              fused_nn_contrib_conv2d_NCHWc_add_clip | 22880256 |      1 |        20.8192 |    1098.9969 |             1098.9969 |     32 |          Y 
  2 |    fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |  8429568 |      1 |        18.2420 |     462.0955 |              462.0955 |     32 |          Y 
  3 |                   fused_nn_contrib_conv2d_NCHWc_add | 19568640 |      1 |        16.0932 |    1215.9573 |             1215.9573 |     32 |          Y 
  4 |            fused_nn_contrib_conv2d_NCHWc_add_clip_1 | 92123136 |      1 |        50.7019 |    1816.9571 |             1816.9571 |     32 |            
  5 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |  9483264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                 fused_nn_contrib_conv2d_NCHWc_add_1 | 29001728 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_contrib_conv2d_NCHWc_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |            fused_nn_contrib_conv2d_NCHWc_add_clip_2 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                 fused_nn_contrib_conv2d_NCHWc_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |             fused_nn_contrib_conv2d_NCHWc_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                 fused_nn_contrib_conv2d_NCHWc_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |             fused_nn_contrib_conv2d_NCHWc_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_contrib_conv2d_NCHWc_add_clip_4 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                 fused_nn_contrib_conv2d_NCHWc_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |             fused_nn_contrib_conv2d_NCHWc_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 |            fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_9 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                 fused_nn_contrib_conv2d_NCHWc_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_10 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_contrib_conv2d_NCHWc_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 |            fused_nn_contrib_conv2d_NCHWc_add_clip_6 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_11 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                 fused_nn_contrib_conv2d_NCHWc_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |            fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                      fused_squeeze_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                    fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 152
Total latency (us): 4617.74

[16:24:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #4 has finished. Remaining task(s): 32
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #0: GFLOPs: 2.8399. Time: 3.3392 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #1: GFLOPs: 1.7026. Time: 5.5699 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #2: GFLOPs: 1.1129. Time: 8.5211 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #3: GFLOPs: 0.8033. Time: 11.8053 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #4: GFLOPs: 1.2930. Time: 7.3341 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #5: GFLOPs: 1.1744. Time: 8.0753 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #6: GFLOPs: 2.4985. Time: 3.7955 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #7: GFLOPs: 2.2410. Time: 4.2317 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], compute: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(9, 2, 14, 2, 2, 4):
                with T.block("DepthwiseConv2d_init"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 18 + i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 4 + i2_2_init * 2 + i2_3_init)
                    ow = T.axis.spatial(56, i3_2_init * 4 + i3_3_init)
                    oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads()
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 112, 112, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
            for i5_0, i6_0 in T.grid(3, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 18, 7, 111, 1):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 18 + ax1)
                        i2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 112 // 16 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 8 + i5_0 + ax2)
                        i3 = T.axis.spatial(113, i6_0 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 + ax4)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 9, 2, 14, 1, 1, 1, 1, 2, 2, 4, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 18 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_2 * 4 + i3_3)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 112, 112, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 18, 4, 56, 1):
                with T.block("compute"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 18 + ax1)
                    i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 4 + ax2)
                    i3 = T.axis.spatial(56, ax3)
                    i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                    T.writes(compute[i0, i1, i2, i3, i4])
                    compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 9, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 14, 4])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b63)
l83 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l83)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l84, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l84, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b65)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b105 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b105)
b121 = sch.decompose_reduction(block=b105, loop=l107)
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #9: GFLOPs: 1.4130. Time: 6.7114 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #10: GFLOPs: 1.2992. Time: 7.2991 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #11: GFLOPs: 2.0837. Time: 4.5513 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #12: GFLOPs: 2.1598. Time: 4.3909 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #13: GFLOPs: 2.7289. Time: 3.4751 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #14: GFLOPs: 0.5379. Time: 17.6306 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #15: GFLOPs: 0.9662. Time: 9.8151 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #16: GFLOPs: 2.7789. Time: 3.4126 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #17: GFLOPs: 0.9220. Time: 10.2851 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #18: GFLOPs: 2.0299. Time: 4.6717 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #19: GFLOPs: 1.8926. Time: 5.0108 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #20: GFLOPs: 0.9659. Time: 9.8176 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #21: GFLOPs: 1.7015. Time: 5.5734 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #22: GFLOPs: 0.8876. Time: 10.6839 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #23: GFLOPs: 2.6246. Time: 3.6133 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #24: GFLOPs: 0.6944. Time: 13.6575 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #25: GFLOPs: 1.4757. Time: 6.4264 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #26: GFLOPs: 0.5776. Time: 16.4198 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], compute: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1 in T.serial(4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 9, 9, 15, 1):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 32 // 8 * 9 + ax1)
                        i2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 448 // 32 * 8 + ax2)
                        i3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 8 * 14 + ax3)
                        i4 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i1_2_init, i3_2_init, i2_3_init in T.grid(9, 7, 4):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 32 // 8 * 9 + i1_2_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 32 * 4 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 8 * 7 + i3_2_init)
                        oci = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 112, 112, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 9, 1, 7, 1, 1, 1, 1, 1, 4, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 32 // 8 * 9 + i1_2)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 32 * 4 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 8 * 7 + i3_2)
                        oci, kh, kw = T.axis.remap("SRR", [i4_1, i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 112, 112, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(2016, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(36, i0_i1_i2_fused // 56)
                        i2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3_1, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_1, i4])
                        compute[i0, i1, i2, i3_1, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3_1, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 9, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 1, 1, 4])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 8, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b62)
l80 = sch.fuse(l65, l66, l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l80)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b64)
l102 = sch.fuse(l97, l98, l99)
sch.parallel(loop=l102)
l103 = sch.fuse(l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b104)
b121 = sch.decompose_reduction(block=b104, loop=l107)
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #28: GFLOPs: 0.7026. Time: 13.4981 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #29: GFLOPs: 0.8354. Time: 11.3519 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #30: GFLOPs: 0.8807. Time: 10.7676 ms. Best GFLOPs: 2.8399
[16:24:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"] Trial #31: GFLOPs: 1.5915. Time: 5.9586 ms. Best GFLOPs: 2.8399
[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1"
 ID |                                                Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                    fused_transpose_layout_transform |        1 |      1 |         0.0000 |      23.7343 |               23.7343 |     24 |          Y 
  1 |              fused_nn_contrib_conv2d_NCHWc_add_clip | 22880256 |      1 |        20.8192 |    1098.9969 |             1098.9969 |     32 |          Y 
  2 |    fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |  8429568 |      1 |        18.2420 |     462.0955 |              462.0955 |     32 |          Y 
  3 |                   fused_nn_contrib_conv2d_NCHWc_add | 19568640 |      1 |        16.0932 |    1215.9573 |             1215.9573 |     32 |          Y 
  4 |            fused_nn_contrib_conv2d_NCHWc_add_clip_1 | 92123136 |      1 |        50.7019 |    1816.9571 |             1816.9571 |     32 |          Y 
  5 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |  9483264 |      1 |         2.8399 |    3339.2407 |             3339.2407 |     32 |            
  6 |                 fused_nn_contrib_conv2d_NCHWc_add_1 | 29001728 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_contrib_conv2d_NCHWc_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |            fused_nn_contrib_conv2d_NCHWc_add_clip_2 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                 fused_nn_contrib_conv2d_NCHWc_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |             fused_nn_contrib_conv2d_NCHWc_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                 fused_nn_contrib_conv2d_NCHWc_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |             fused_nn_contrib_conv2d_NCHWc_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_contrib_conv2d_NCHWc_add_clip_4 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                 fused_nn_contrib_conv2d_NCHWc_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |             fused_nn_contrib_conv2d_NCHWc_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 |            fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_9 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                 fused_nn_contrib_conv2d_NCHWc_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_10 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_contrib_conv2d_NCHWc_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 |            fused_nn_contrib_conv2d_NCHWc_add_clip_6 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_11 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                 fused_nn_contrib_conv2d_NCHWc_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |            fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                      fused_squeeze_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                    fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 184
Total latency (us): 7956.98

[16:24:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #5 has finished. Remaining task(s): 31
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #0: GFLOPs: 5.0031. Time: 5.7968 ms. Best GFLOPs: 5.0031
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #1: GFLOPs: 4.5918. Time: 6.3160 ms. Best GFLOPs: 5.0031
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #2: GFLOPs: 4.7798. Time: 6.0675 ms. Best GFLOPs: 5.0031
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #3: GFLOPs: 2.8277. Time: 10.2564 ms. Best GFLOPs: 5.0031
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #4: GFLOPs: 7.0542. Time: 4.1113 ms. Best GFLOPs: 7.0542
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #5: GFLOPs: 8.0780. Time: 3.5902 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #6: GFLOPs: 5.2514. Time: 5.5226 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #7: GFLOPs: 3.3672. Time: 8.6131 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #8: GFLOPs: 1.6116. Time: 17.9952 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #9: GFLOPs: 3.1728. Time: 9.1408 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(2, 28, 2, 4, 2):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_2_init * 4 + i1_3_init)
                    oh = T.axis.spatial(56, i2_2_init * 2 + i2_3_init)
                    ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 56)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 56 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(144, 1, 1, 1, 2, 28, 1, 2, 1, 1, 1, 1, 4, 2, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(56, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 56)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 56 * 2 + i4_2)
                    ic = T.axis.reduce(144, i5_0)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 56)
                        ax2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 2, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 28, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 56, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[144, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97 = sch.get_loops(block=b65)
l98 = sch.fuse(l93, l94, l95)
sch.parallel(loop=l98)
l99 = sch.fuse(l97)
sch.vectorize(loop=l99)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b100)
b118 = sch.decompose_reduction(block=b100, loop=l102)
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #11: GFLOPs: 6.6774. Time: 4.3433 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #12: GFLOPs: 5.7116. Time: 5.0777 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #13: GFLOPs: 6.5778. Time: 4.4090 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #14: GFLOPs: 3.6597. Time: 7.9247 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #15: GFLOPs: 4.1476. Time: 6.9925 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #16: GFLOPs: 2.9466. Time: 9.8423 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #17: GFLOPs: 6.9972. Time: 4.1448 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #18: GFLOPs: 2.9578. Time: 9.8050 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #19: GFLOPs: 1.5598. Time: 18.5932 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #20: GFLOPs: 5.3389. Time: 5.4321 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #21: GFLOPs: 5.5863. Time: 5.1916 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #22: GFLOPs: 5.7148. Time: 5.0749 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #23: GFLOPs: 3.4910. Time: 8.3075 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #24: GFLOPs: 6.4453. Time: 4.4997 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #25: GFLOPs: 6.2141. Time: 4.6671 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #26: GFLOPs: 3.5445. Time: 8.1823 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #27: GFLOPs: 3.7233. Time: 7.7892 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #28: GFLOPs: 7.2781. Time: 3.9848 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #29: GFLOPs: 5.8575. Time: 4.9512 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #30: GFLOPs: 6.9365. Time: 4.1811 ms. Best GFLOPs: 8.0780
[16:24:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #31: GFLOPs: 3.1607. Time: 9.1759 ms. Best GFLOPs: 8.0780
[16:24:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_contrib_conv2d_NCHWc_add_1"
 ID |                                                Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                    fused_transpose_layout_transform |        1 |      1 |         0.0000 |      23.7343 |               23.7343 |     24 |          Y 
  1 |              fused_nn_contrib_conv2d_NCHWc_add_clip | 22880256 |      1 |        20.8192 |    1098.9969 |             1098.9969 |     32 |          Y 
  2 |    fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |  8429568 |      1 |        18.2420 |     462.0955 |              462.0955 |     32 |          Y 
  3 |                   fused_nn_contrib_conv2d_NCHWc_add | 19568640 |      1 |        16.0932 |    1215.9573 |             1215.9573 |     32 |          Y 
  4 |            fused_nn_contrib_conv2d_NCHWc_add_clip_1 | 92123136 |      1 |        50.7019 |    1816.9571 |             1816.9571 |     32 |          Y 
  5 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |  9483264 |      1 |         2.8399 |    3339.2407 |             3339.2407 |     32 |          Y 
  6 |                 fused_nn_contrib_conv2d_NCHWc_add_1 | 29001728 |      1 |         8.0780 |    3590.2095 |             3590.2095 |     32 |            
  7 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_contrib_conv2d_NCHWc_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |            fused_nn_contrib_conv2d_NCHWc_add_clip_2 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                 fused_nn_contrib_conv2d_NCHWc_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |             fused_nn_contrib_conv2d_NCHWc_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                 fused_nn_contrib_conv2d_NCHWc_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |             fused_nn_contrib_conv2d_NCHWc_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_contrib_conv2d_NCHWc_add_clip_4 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                 fused_nn_contrib_conv2d_NCHWc_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |             fused_nn_contrib_conv2d_NCHWc_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 |            fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_9 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                 fused_nn_contrib_conv2d_NCHWc_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_10 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_contrib_conv2d_NCHWc_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 |            fused_nn_contrib_conv2d_NCHWc_add_clip_6 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_11 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                 fused_nn_contrib_conv2d_NCHWc_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |            fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                      fused_squeeze_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                    fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 216
Total latency (us): 11547.2

[16:24:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #6 has finished. Remaining task(s): 30
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #0: GFLOPs: 2.0134. Time: 6.2801 ms. Best GFLOPs: 2.0134
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #1: GFLOPs: 1.7468. Time: 7.2388 ms. Best GFLOPs: 2.0134
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #2: GFLOPs: 0.9502. Time: 13.3072 ms. Best GFLOPs: 2.0134
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #3: GFLOPs: 0.8090. Time: 15.6287 ms. Best GFLOPs: 2.0134
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #4: GFLOPs: 1.7839. Time: 7.0880 ms. Best GFLOPs: 2.0134
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #5: GFLOPs: 1.4289. Time: 8.8488 ms. Best GFLOPs: 2.0134
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #6: GFLOPs: 2.1422. Time: 5.9024 ms. Best GFLOPs: 2.1422
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #7: GFLOPs: 2.3157. Time: 5.4602 ms. Best GFLOPs: 2.3157
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #8: GFLOPs: 1.0920. Time: 11.5787 ms. Best GFLOPs: 2.3157
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #9: GFLOPs: 0.5882. Time: 21.4981 ms. Best GFLOPs: 2.3157
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #10: GFLOPs: 1.0197. Time: 12.4001 ms. Best GFLOPs: 2.3157
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #11: GFLOPs: 1.2338. Time: 10.2484 ms. Best GFLOPs: 2.3157
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #12: GFLOPs: 2.3457. Time: 5.3905 ms. Best GFLOPs: 2.3457
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #13: GFLOPs: 0.7560. Time: 16.7259 ms. Best GFLOPs: 2.3457
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #14: GFLOPs: 0.8196. Time: 15.4266 ms. Best GFLOPs: 2.3457
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #15: GFLOPs: 2.2858. Time: 5.5318 ms. Best GFLOPs: 2.3457
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #16: GFLOPs: 0.9784. Time: 12.9231 ms. Best GFLOPs: 2.3457
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #17: GFLOPs: 2.1075. Time: 5.9997 ms. Best GFLOPs: 2.3457
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #18: GFLOPs: 1.9342. Time: 6.5374 ms. Best GFLOPs: 2.3457
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #19: GFLOPs: 2.7097. Time: 4.6663 ms. Best GFLOPs: 2.7097
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #20: GFLOPs: 0.9151. Time: 13.8169 ms. Best GFLOPs: 2.7097
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #21: GFLOPs: 2.3468. Time: 5.3878 ms. Best GFLOPs: 2.7097
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #22: GFLOPs: 1.7694. Time: 7.1461 ms. Best GFLOPs: 2.7097
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(12, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 30, 30):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused % 12 // 4 * 16 + ax1)
                        i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + ax2)
                        i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 1, 2, 1, 2, 1):
                for i5_0 in T.serial(1):
                    for i1_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(4, 2, 2, 2, 28, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i1_1 * 8 + i1_2_init * 2 + i1_3_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + i2_3_init)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i3_1 * 14 + i3_2_init * 7 + i3_3_init)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_2_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 4, 1, 2, 2, 3, 1, 1, 2, 28, 7, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + i2_3)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i3_1 * 14 + i3_2 * 7 + i3_3)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 8, 28, 14):
                    for ax4_fused in T.vectorized(2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i1_1 * 8 + ax1)
                            i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + ax2)
                            i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i3_1 * 14 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 2, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 28])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74 = sch.get_loops(block=b63)
l75 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l75)
l76 = sch.fuse(l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b65)
l110 = sch.fuse(l109)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b111)
b133 = sch.decompose_reduction(block=b111, loop=l120)
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #24: GFLOPs: 1.9835. Time: 6.3748 ms. Best GFLOPs: 2.7097
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #25: GFLOPs: 19.3934. Time: 0.6520 ms. Best GFLOPs: 19.3934
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #26: GFLOPs: 10.1804. Time: 1.2420 ms. Best GFLOPs: 19.3934
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #27: GFLOPs: 7.6052. Time: 1.6626 ms. Best GFLOPs: 19.3934
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(24, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 30, 30):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused % 24 // 4 * 8 + ax1)
                        i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + ax2)
                        i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 1, 2, 1, 2, 1):
                for i5_0 in T.serial(1):
                    for i1_2_init, i3_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(4, 2, 2, 28, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i1_1 * 4 + i1_2_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + i2_3_init)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i3_1 * 14 + i3_2_init * 7 + i3_3_init)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_2_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 4, 1, 2, 2, 3, 1, 1, 1, 28, 7, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i1_1 * 4 + i1_2)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + i2_3)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i3_1 * 14 + i3_2 * 7 + i3_3)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 28, 14):
                    for ax4_fused in T.vectorized(2):
                        with T.block("compute"):
                            i0 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(48, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i1_1 * 4 + ax1)
                            i2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + ax2)
                            i3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i3_1 * 14 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4_fused)
                            T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                            T.writes(compute[i0, i1, i2, i3, i4])
                            compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[6, 2, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 28])
l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 2, 7])
l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l9, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l10, factors=[v55, v56])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)
b59, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v60)
l61 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)
sch.enter_postproc()
b62 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b62, ann_key="meta_schedule.unroll_explicit")
b63, b64, b65 = sch.get_child_blocks(b62)
l66, l67, l68, l69, l70, l71, l72, l73, l74 = sch.get_loops(block=b63)
l75 = sch.fuse(l66, l67, l68, l69)
sch.parallel(loop=l75)
l76 = sch.fuse(l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b65)
l110 = sch.fuse(l109)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b111)
b133 = sch.decompose_reduction(block=b111, loop=l120)
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #29: GFLOPs: 4.5489. Time: 2.7796 ms. Best GFLOPs: 19.3934
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #30: GFLOPs: 1.4847. Time: 8.5163 ms. Best GFLOPs: 19.3934
[16:24:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"] Trial #31: GFLOPs: 0.6932. Time: 18.2399 ms. Best GFLOPs: 19.3934
[16:25:01] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2"
 ID |                                                Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                    fused_transpose_layout_transform |        1 |      1 |         0.0000 |      23.7343 |               23.7343 |     24 |          Y 
  1 |              fused_nn_contrib_conv2d_NCHWc_add_clip | 22880256 |      1 |        20.8192 |    1098.9969 |             1098.9969 |     32 |          Y 
  2 |    fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |  8429568 |      1 |        18.2420 |     462.0955 |              462.0955 |     32 |          Y 
  3 |                   fused_nn_contrib_conv2d_NCHWc_add | 19568640 |      1 |        16.0932 |    1215.9573 |             1215.9573 |     32 |          Y 
  4 |            fused_nn_contrib_conv2d_NCHWc_add_clip_1 | 92123136 |      1 |        50.7019 |    1816.9571 |             1816.9571 |     32 |          Y 
  5 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |  9483264 |      1 |         2.8399 |    3339.2407 |             3339.2407 |     32 |          Y 
  6 |                 fused_nn_contrib_conv2d_NCHWc_add_1 | 29001728 |      1 |         8.0780 |    3590.2095 |             3590.2095 |     32 |          Y 
  7 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 | 12644352 |      3 |        19.3934 |     651.9930 |             1955.9789 |     32 |            
  8 |               fused_nn_contrib_conv2d_NCHWc_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |            fused_nn_contrib_conv2d_NCHWc_add_clip_2 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                 fused_nn_contrib_conv2d_NCHWc_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |             fused_nn_contrib_conv2d_NCHWc_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                 fused_nn_contrib_conv2d_NCHWc_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |             fused_nn_contrib_conv2d_NCHWc_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_contrib_conv2d_NCHWc_add_clip_4 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                 fused_nn_contrib_conv2d_NCHWc_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |             fused_nn_contrib_conv2d_NCHWc_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 |            fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_9 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                 fused_nn_contrib_conv2d_NCHWc_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_10 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_contrib_conv2d_NCHWc_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 |            fused_nn_contrib_conv2d_NCHWc_add_clip_6 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_11 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                 fused_nn_contrib_conv2d_NCHWc_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |            fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                      fused_squeeze_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                    fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 248
Total latency (us): 13503.2

[16:25:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #7 has finished. Remaining task(s): 29
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #0: GFLOPs: 2.8666. Time: 13.5129 ms. Best GFLOPs: 2.8666
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #1: GFLOPs: 7.5417. Time: 5.1362 ms. Best GFLOPs: 7.5417
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(14, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 2, 1):
                for i2_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(4, 4, 2, 2, 14):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_1 * 2 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 8 + i2_2_init * 2 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 28 + i3_1 * 14 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 1, 4, 1, 4, 3, 1, 1, 1, 2, 2, 14, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 8 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 28 + i3_1 * 14 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(192, i5_0 * 3 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 8, 28):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, ax1)
                        ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 2 * 8 + ax2)
                        ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 28 + ax3)
                        ax4 = T.axis.spatial(4, ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 4, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[64, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #3: GFLOPs: 3.5342. Time: 10.9602 ms. Best GFLOPs: 7.5417
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #4: GFLOPs: 8.5552. Time: 4.5278 ms. Best GFLOPs: 8.5552
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #5: GFLOPs: 7.4932. Time: 5.1695 ms. Best GFLOPs: 8.5552
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #6: GFLOPs: 3.7492. Time: 10.3318 ms. Best GFLOPs: 8.5552
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #7: GFLOPs: 4.7392. Time: 8.1736 ms. Best GFLOPs: 8.5552
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #8: GFLOPs: 9.2059. Time: 4.2077 ms. Best GFLOPs: 9.2059
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #9: GFLOPs: 5.3525. Time: 7.2370 ms. Best GFLOPs: 9.2059
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #10: GFLOPs: 7.7964. Time: 4.9684 ms. Best GFLOPs: 9.2059
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #11: GFLOPs: 3.5253. Time: 10.9879 ms. Best GFLOPs: 9.2059
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #12: GFLOPs: 8.1266. Time: 4.7665 ms. Best GFLOPs: 9.2059
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #13: GFLOPs: 4.2957. Time: 9.0175 ms. Best GFLOPs: 9.2059
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #14: GFLOPs: 4.2324. Time: 9.1522 ms. Best GFLOPs: 9.2059
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #15: GFLOPs: 7.2407. Time: 5.3497 ms. Best GFLOPs: 9.2059
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(1, 2, 1):
                for i2_2_init, i3_2_init, i4_2_init, i2_3_init in T.grid(2, 28, 2, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 56 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 56 // 4 * 4 + i2_2_init * 2 + i2_3_init)
                        ow = T.axis.spatial(56, i3_1 * 28 + i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 // 2 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(96, 1, 1, 1, 1, 2, 28, 2, 2, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 56 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 2)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 56 // 4 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 28 + i3_2)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 // 2 * 2 + i4_2)
                        ic = T.axis.reduce(192, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 56)
                        ax2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 28, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[96, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b66)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b101)
b122 = sch.decompose_reduction(block=b101, loop=l106)
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #17: GFLOPs: 27.0599. Time: 1.4315 ms. Best GFLOPs: 27.0599
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #18: GFLOPs: 13.5823. Time: 2.8519 ms. Best GFLOPs: 27.0599
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #19: GFLOPs: 30.8390. Time: 1.2561 ms. Best GFLOPs: 30.8390
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #20: GFLOPs: 7.8341. Time: 4.9445 ms. Best GFLOPs: 30.8390
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #21: GFLOPs: 28.7585. Time: 1.3469 ms. Best GFLOPs: 30.8390
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #22: GFLOPs: 17.4418. Time: 2.2209 ms. Best GFLOPs: 30.8390
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #23: GFLOPs: 5.4315. Time: 7.1317 ms. Best GFLOPs: 30.8390
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #24: GFLOPs: 22.7306. Time: 1.7041 ms. Best GFLOPs: 30.8390
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #25: GFLOPs: 24.5974. Time: 1.5748 ms. Best GFLOPs: 30.8390
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #26: GFLOPs: 20.1506. Time: 1.9223 ms. Best GFLOPs: 30.8390
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #27: GFLOPs: 12.3635. Time: 3.1331 ms. Best GFLOPs: 30.8390
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #28: GFLOPs: 25.0054. Time: 1.5491 ms. Best GFLOPs: 30.8390
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #29: GFLOPs: 8.8604. Time: 4.3718 ms. Best GFLOPs: 30.8390
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #30: GFLOPs: 3.9734. Time: 9.7489 ms. Best GFLOPs: 30.8390
[16:25:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #31: GFLOPs: 5.9305. Time: 6.5316 ms. Best GFLOPs: 30.8390
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_contrib_conv2d_NCHWc_add_add"
 ID |                                                Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                    fused_transpose_layout_transform |        1 |      1 |         0.0000 |      23.7343 |               23.7343 |     24 |          Y 
  1 |              fused_nn_contrib_conv2d_NCHWc_add_clip | 22880256 |      1 |        20.8192 |    1098.9969 |             1098.9969 |     32 |          Y 
  2 |    fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip |  8429568 |      1 |        18.2420 |     462.0955 |              462.0955 |     32 |          Y 
  3 |                   fused_nn_contrib_conv2d_NCHWc_add | 19568640 |      1 |        16.0932 |    1215.9573 |             1215.9573 |     32 |          Y 
  4 |            fused_nn_contrib_conv2d_NCHWc_add_clip_1 | 92123136 |      1 |        50.7019 |    1816.9571 |             1816.9571 |     32 |          Y 
  5 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1 |  9483264 |      1 |         2.8399 |    3339.2407 |             3339.2407 |     32 |          Y 
  6 |                 fused_nn_contrib_conv2d_NCHWc_add_1 | 29001728 |      1 |         8.0780 |    3590.2095 |             3590.2095 |     32 |          Y 
  7 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2 | 12644352 |      3 |        19.3934 |     651.9930 |             1955.9789 |     32 |          Y 
  8 |               fused_nn_contrib_conv2d_NCHWc_add_add | 38735872 |      3 |        30.8390 |    1256.0664 |             3768.1993 |     32 |            
  9 |            fused_nn_contrib_conv2d_NCHWc_add_clip_2 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                 fused_nn_contrib_conv2d_NCHWc_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |             fused_nn_contrib_conv2d_NCHWc_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |            fused_nn_contrib_conv2d_NCHWc_add_clip_3 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                 fused_nn_contrib_conv2d_NCHWc_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |             fused_nn_contrib_conv2d_NCHWc_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 |            fused_nn_contrib_conv2d_NCHWc_add_clip_4 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                 fused_nn_contrib_conv2d_NCHWc_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |             fused_nn_contrib_conv2d_NCHWc_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 |            fused_nn_contrib_conv2d_NCHWc_add_clip_5 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 |  fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_9 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                 fused_nn_contrib_conv2d_NCHWc_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_10 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |             fused_nn_contrib_conv2d_NCHWc_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 |            fused_nn_contrib_conv2d_NCHWc_add_clip_6 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_11 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                 fused_nn_contrib_conv2d_NCHWc_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |            fused_nn_contrib_conv2d_NCHWc_add_clip_7 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                                 fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                      fused_squeeze_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |                                    fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 280
Total latency (us): 17271.4

[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #8 has finished. Remaining task(s): 28
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #9 has finished. Remaining task(s): 27
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #10 has finished. Remaining task(s): 26
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #11 has finished. Remaining task(s): 25
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #12 has finished. Remaining task(s): 24
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #13 has finished. Remaining task(s): 23
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #14 has finished. Remaining task(s): 22
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #15 has finished. Remaining task(s): 21
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #16 has finished. Remaining task(s): 20
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #17 has finished. Remaining task(s): 19
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #18 has finished. Remaining task(s): 18
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #19 has finished. Remaining task(s): 17
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #20 has finished. Remaining task(s): 16
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #21 has finished. Remaining task(s): 15
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #22 has finished. Remaining task(s): 14
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #23 has finished. Remaining task(s): 13
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #24 has finished. Remaining task(s): 12
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #25 has finished. Remaining task(s): 11
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #26 has finished. Remaining task(s): 10
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #27 has finished. Remaining task(s): 9
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #28 has finished. Remaining task(s): 8
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #29 has finished. Remaining task(s): 7
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #30 has finished. Remaining task(s): 6
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #31 has finished. Remaining task(s): 5
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #32 has finished. Remaining task(s): 4
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #33 has finished. Remaining task(s): 3
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #34 has finished. Remaining task(s): 2
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #35 has finished. Remaining task(s): 1
[16:25:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #36 has finished. Remaining task(s): 0
[16:26:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 224, 3), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_transpose = T.alloc_buffer([1, 3, 224, 224], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 224, 224):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax2, ax3, ax1])
                T.writes(T_transpose[ax0, ax1, ax2, ax3])
                T_transpose[ax0, ax1, ax2, ax3] = placeholder[ax0, ax2, ax3, ax1]
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_transpose[ax0, ax1 * 3 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, T_transpose[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[16:26:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:26:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_transpose_layout_transform(placeholder: T.Buffer[(1, 224, 224, 3), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_transpose = T.alloc_buffer([1, 3, 224, 224], dtype="float32")
        for i0_i1_i2_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i3 in T.serial(224):
                for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 1):
                    with T.block("T_transpose"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3_1 = T.axis.remap("SSS", [ax1, i0_i1_i2_fused, i3])
                        T.reads(placeholder[ax0_1, ax2_1, ax3_1, ax1_1])
                        T.writes(T_transpose[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_transpose[ax0_1, ax1_1, ax2_1, ax3_1] = placeholder[ax0_1, ax2_1, ax3_1, ax1_1]
                for i4 in T.serial(3):
                    with T.block("T_layout_trans"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(1, 0)
                        ax2, ax3, ax4 = T.axis.remap("SSS", [i0_i1_i2_fused, i3, i4])
                        T.reads(T_transpose[ax0, ax1 * 3 + ax4, ax2, ax3])
                        T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                        T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, T_transpose[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[16:26:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:26:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_transpose\", func_name=\"main\")", "b1 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v2)", "l3 = sch.sample_compute_location(block=b0, decision=3)", "sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)", "sch.enter_postproc()", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b4, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b4, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\")", "b5, b6 = sch.get_child_blocks(b4)", "l7, l8, l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b5)", "l15 = sch.fuse(l7, l8, l9)", "sch.parallel(loop=l15)", "sch.annotate(block_or_loop=l15, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l15, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l16, l17, l18 = sch.get_loops(block=b6)", "sch.annotate(block_or_loop=l16, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l16, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[16:26:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 225, 225, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 225, 225, 3):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(0 <= i2_1 and i2_1 < 224 and 0 <= i3_1 and i3_1 < 224, placeholder[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 112, 112, 4, 3, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[16:26:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:26:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_clip(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 225, 225, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(224, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2 in T.grid(1, 1, 1, 1, 1, 2, 8, 4):
                for ax0, ax1, ax2 in T.grid(1, 1, 15):
                    for ax3_ax4_fused in T.vectorized(15):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 112 + i2_2 * 14 + ax2)
                            i3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 // 28 * 112 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7 * 16 + i3_2 * 4 + ax3_ax4_fused // 3)
                            i4 = T.axis.spatial(3, ax3_ax4_fused % 3)
                            T.reads(placeholder[i0, i1, i2, i3, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 224 and 0 <= i3 and i3 < 224, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
                for i4_2 in T.serial(2):
                    for i2_3_init, i3_3_init in T.grid(7, 2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 56 * 2 + i1_2)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 56 + i2_2 * 7 + i2_3_init)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 // 28 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i3_2 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 2 + i4_2)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 3, 1, 1, 7, 2, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 56 * 2 + i1_2)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 56 + i2_2 * 7 + i2_3)
                            ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 56 // 28 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i3_2 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 2 + i4_2)
                            ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_1])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(8, i0_i1_i2_fused // 112)
                        i2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3_1, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_1, i4])
                        compute[i0, i1, i2, i3_1, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3_1, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

[16:26:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:26:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"data_pad\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b2)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])", "v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])", "l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])", "v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 8, 7])", "l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])", "v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 7, 4, 2])", "l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])", "v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])", "l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])", "v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])", "l54, l55 = sch.split(loop=l9, factors=[v52, v53])", "v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])", "l58, l59 = sch.split(loop=l10, factors=[v56, v57])", "v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])", "l62, l63 = sch.split(loop=l11, factors=[v60, v61])", "sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v64)", "l65 = sch.sample_compute_location(block=b0, decision=16)", "sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)", "sch.enter_postproc()", "b66 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b66, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b66, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b66, ann_key=\"meta_schedule.unroll_explicit\")", "b67, b68, b69 = sch.get_child_blocks(b66)", "l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b67)", "l92 = sch.fuse(l70, l71, l72, l73, l74, l75, l76, l77, l78)", "sch.parallel(loop=l92)", "l93 = sch.fuse(l90, l91)", "sch.vectorize(loop=l93)", "sch.annotate(block_or_loop=l92, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l92, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b68)", "sch.annotate(block_or_loop=l94, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l94, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l112, l113, l114, l115, l116 = sch.get_loops(block=b69)", "l117 = sch.fuse(l112, l113, l114)", "sch.parallel(loop=l117)", "l118 = sch.fuse(l116)", "sch.vectorize(loop=l118)", "sch.annotate(block_or_loop=l117, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l117, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b119 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137 = sch.get_loops(block=b119)", "b138 = sch.decompose_reduction(block=b119, loop=l130)"]
[16:26:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 114, 114, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 8, 112, 112, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 112, 112, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[16:26:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:26:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], compute: T.Buffer[(1, 8, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 8, 114, 114, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 8, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 6, 114):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i0_0_i1_0_i2_0_fused % 56 // 28 * 4 + ax1)
                        i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_fused % 28 * 4 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 2, 1, 1, 1, 1, 2):
                for i5_0 in T.serial(1):
                    for i1_2_init, i2_2_init, i3_2_init, i3_3_init in T.grid(4, 4, 7, 8):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i0_0_i1_0_i2_0_fused // 28 * 4 + i1_2_init)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_fused % 28 * 4 + i2_2_init)
                            ow = T.axis.spatial(112, i3_0 * 56 + i3_2_init * 8 + i3_3_init)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 4, 4, 7, 1, 3, 1, 1, 1, 1, 8, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(8, i0_0_i1_0_i2_0_fused // 28 * 4 + i1_2)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_fused % 28 * 4 + i2_2)
                            ow = T.axis.spatial(112, i3_0 * 56 + i3_2 * 8 + i3_3)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [8, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 4, 56, 1):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(8, i0_0_i1_0_i2_0_fused // 28 * 4 + ax1)
                        i2 = T.axis.spatial(112, i0_0_i1_0_i2_0_fused % 28 * 4 + ax2)
                        i3 = T.axis.spatial(112, i3_0 * 56 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

[16:26:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:26:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"PaddedInput\", func_name=\"main\")", "b1 = sch.get_block(name=\"DepthwiseConv2d\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b2)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)", "v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])", "v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 1])", "l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])", "v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[28, 1, 4, 1])", "l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])", "v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 7, 8])", "l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])", "v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])", "l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])", "v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])", "l53, l54 = sch.split(loop=l9, factors=[v51, v52])", "v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])", "l57, l58 = sch.split(loop=l10, factors=[v55, v56])", "sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)", "b59, = sch.get_consumers(block=b1)", "sch.reverse_compute_at(block=b59, loop=l48, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v60 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v60)", "l61 = sch.sample_compute_location(block=b0, decision=2)", "sch.compute_at(block=b0, loop=l61, preserve_unit_loops=True)", "sch.enter_postproc()", "b62 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b62, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b62, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b62, ann_key=\"meta_schedule.unroll_explicit\")", "b63, b64, b65 = sch.get_child_blocks(b62)", "l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b63)", "l74 = sch.fuse(l66, l67, l68)", "sch.parallel(loop=l74)", "l75 = sch.fuse(l73)", "sch.vectorize(loop=l75)", "sch.annotate(block_or_loop=l74, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l74, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b64)", "sch.annotate(block_or_loop=l76, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l76, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b65)", "sch.annotate(block_or_loop=l98, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l98, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b111 = sch.get_block(name=\"DepthwiseConv2d\", func_name=\"main\")", "l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b111)", "b134 = sch.decompose_reduction(block=b111, loop=l121)"]
[16:26:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(6, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 6, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 6, 112, 112, 4, 32, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [6, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 6, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:26:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:26:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add(placeholder: T.Buffer[(1, 8, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(6, 8, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 6, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(224):
            for i4_1 in T.serial(1):
                for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i1_3_init in T.grid(3, 14, 4, 4, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 112 // 14 * 14 + i2_2_init)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 112 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 * 4 + i3_2_init)
                        oc_block = T.axis.spatial(4, i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [6, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 3, 14, 4, 4, 4, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 112 // 14 * 14 + i2_2)
                        ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 112 * 56 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 * 4 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(32, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 112, 112, 4], "float32"], ["TENSOR", [6, 8, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(672):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(6, i0_i1_i2_fused // 112)
                        ax2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:26:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:26:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)", "v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])", "v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 3, 2])", "l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])", "v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 14, 1])", "l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])", "v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 14, 4, 1])", "l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])", "v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])", "l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])", "v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 4])", "l52, l53 = sch.split(loop=l7, factors=[v50, v51])", "v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])", "l56, l57 = sch.split(loop=l8, factors=[v54, v55])", "v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l60, l61 = sch.split(loop=l9, factors=[v58, v59])", "sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v62)", "sch.enter_postproc()", "b63 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b63, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b63, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b63, ann_key=\"meta_schedule.unroll_explicit\")", "b64, b65 = sch.get_child_blocks(b63)", "l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)", "l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74)", "sch.parallel(loop=l92)", "l93, l94, l95, l96, l97 = sch.get_loops(block=b65)", "l98 = sch.fuse(l93, l94, l95)", "sch.parallel(loop=l98)", "l99 = sch.fuse(l97)", "sch.vectorize(loop=l99)", "b100 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b100)", "b119 = sch.decompose_reduction(block=b100, loop=l103)"]
[16:26:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(36, 6, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], compute: T.Buffer[(1, 36, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 36, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 36, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 36, 112, 112, 4, 24, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 112, 112, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 112, 112, 4):
            with T.block("compute"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(compute[i0_1, i1_1, i2_1, i3_1, i4_1])
                compute[i0_1, i1_1, i2_1, i3_1, i4_1] = T.max(T.min(T_add[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(6)), T.float32(0))
    

[16:26:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:26:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_clip_1(placeholder: T.Buffer[(1, 6, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(36, 6, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], compute: T.Buffer[(1, 36, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 36, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(7, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 8, 4, 1):
                for i1_2_init, i2_2_init, i3_2_init, i3_3_init in T.grid(18, 2, 7, 4):
                    for i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(36, i1_1 * 18 + i1_2_init)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 16 + i2_1 * 2 + i2_2_init)
                            ow = T.axis.spatial(112, i3_1 * 28 + i3_2_init * 4 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 112, 112, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(6, 1, 1, 1, 18, 2, 7, 1, 4, 1, 1, 1, 1, 1, 4):
                    for i4_3_fused in T.vectorized(4):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(36, i1_1 * 18 + i1_2)
                            oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 16 + i2_1 * 2 + i2_2)
                            ow = T.axis.spatial(112, i3_1 * 28 + i3_2 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i4_3_fused)
                            ic = T.axis.reduce(24, i5_0 * 4 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 6, 112, 112, 4], "float32"], ["TENSOR", [36, 6, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 36, 16, 112):
                for ax4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(36, ax1)
                        i2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 16 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[i0, i1, i2, i3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3, i4])
                        compute[i0, i1, i2, i3, i4] = T.max(T.min(conv2d_NCHWc[i0, i1, i2, i3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

[16:26:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:26:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b1)", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)", "v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])", "v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 18, 1])", "l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])", "v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 8, 2, 1])", "l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])", "v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 7, 4])", "l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])", "v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])", "l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])", "v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[6, 4])", "l53, l54 = sch.split(loop=l8, factors=[v51, v52])", "v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l57, l58 = sch.split(loop=l9, factors=[v55, v56])", "v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])", "l61, l62 = sch.split(loop=l10, factors=[v59, v60])", "sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)", "b63, = sch.get_consumers(block=b0)", "sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v64)", "sch.enter_postproc()", "b65 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.unroll_explicit\")", "b66, b67 = sch.get_child_blocks(b65)", "l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)", "l94 = sch.fuse(l68, l69, l70, l71, l72)", "sch.parallel(loop=l94)", "l95 = sch.fuse(l93)", "sch.vectorize(loop=l95)", "sch.annotate(block_or_loop=l94, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l94, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)", "l102 = sch.fuse(l101)", "sch.vectorize(loop=l102)", "sch.annotate(block_or_loop=l96, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l96, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b103 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)", "b126 = sch.decompose_reduction(block=b103, loop=l110)"]
[16:26:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], compute: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 113, 113, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(0 <= i2_1 and i2_1 < 112 and 0 <= i3_1 and i3_1 < 112, placeholder[i0_1, i1_1, i2_1, i3_1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 36, 56, 56, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 112, 112, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 36, 56, 56, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[16:26:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:26:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_1(placeholder: T.Buffer[(1, 36, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(36, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 36, 1, 1, 4), "float32"], compute: T.Buffer[(1, 36, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 36, 113, 113, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 36, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(3, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 12, 113, 113):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(36, i0_0_i1_0_fused * 12 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2, i3, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(0 <= i2 and i2 < 112 and 0 <= i3 and i3 < 112, placeholder[i0, i1, i2, i3, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 1, 2, 1, 1, 7, 2, 1):
                for i1_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(3, 14, 2, 4, 4, 2):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_fused * 12 + i1_2_init * 4 + i1_3_init)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 4 + i2_3_init)
                        ow = T.axis.spatial(56, i3_1 * 28 + i3_2_init * 2 + i3_3_init)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 112, 112, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 3, 1, 14, 2, 1, 1, 1, 4, 4, 2, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(36, i0_0_i1_0_fused * 12 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 4 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 28 + i3_2 * 2 + i3_3)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 36, 112, 112, 4], "float32"], ["TENSOR", [36, 1, 3, 3, 1, 4], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(2016, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(36, i0_i1_i2_fused // 56)
                        i2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        i3_4, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3_4, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_4, i4])
                        compute[i0, i1, i2, i3_4, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3_4, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

[16:26:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:26:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"PaddedInput\", func_name=\"main\")", "b1 = sch.get_block(name=\"DepthwiseConv2d\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b2)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)", "v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])", "v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 1, 3, 4])", "l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])", "v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 7, 1, 4])", "l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])", "v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 14, 2])", "l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])", "v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])", "l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])", "v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])", "l53, l54 = sch.split(loop=l9, factors=[v51, v52])", "v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])", "l57, l58 = sch.split(loop=l10, factors=[v55, v56])", "sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v59)", "l60 = sch.sample_compute_location(block=b0, decision=1)", "sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)", "sch.enter_postproc()", "b61 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b61, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b61, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b61, ann_key=\"meta_schedule.unroll_explicit\")", "b62, b63, b64 = sch.get_child_blocks(b61)", "l65, l66, l67, l68, l69, l70, l71 = sch.get_loops(block=b62)", "l72 = sch.fuse(l65, l66)", "sch.parallel(loop=l72)", "l73 = sch.fuse(l71)", "sch.vectorize(loop=l73)", "sch.annotate(block_or_loop=l72, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l72, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)", "sch.annotate(block_or_loop=l74, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l74, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l97, l98, l99, l100, l101 = sch.get_loops(block=b64)", "l102 = sch.fuse(l97, l98, l99)", "sch.parallel(loop=l102)", "l103 = sch.fuse(l101)", "sch.vectorize(loop=l103)", "sch.annotate(block_or_loop=l102, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l102, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b104 = sch.get_block(name=\"DepthwiseConv2d\", func_name=\"main\")", "l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127 = sch.get_loops(block=b104)", "b128 = sch.decompose_reduction(block=b104, loop=l114)"]
[16:26:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 56, 56, 4, 144, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:26:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:26:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_1(placeholder: T.Buffer[(1, 36, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 36, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(196, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 8, 2, 1):
                for i3_2_init, i1_3_init in T.grid(2, 8):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_3_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 8 + i2_1)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 4 + i3_1 * 2 + i3_2_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(24, 1, 1, 1, 1, 1, 2, 1, 6, 1, 1, 1, 8):
                    for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_3)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 8 + i2_1)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 4 + i3_1 * 2 + i3_2)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i2_3_i3_3_i4_3_fused)
                            ic = T.axis.reduce(144, i5_0 * 6 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 36, 56, 56, 4], "float32"], ["TENSOR", [8, 36, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 8, 4):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, ax1)
                        ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 28 * 8 + ax2)
                        ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 28 // 2 * 4 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

[16:26:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:26:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)", "v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])", "v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 8])", "l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])", "v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 8, 1, 1])", "l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])", "v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 2, 2, 1])", "l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])", "v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])", "l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])", "v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[24, 6])", "l52, l53 = sch.split(loop=l7, factors=[v50, v51])", "v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])", "l56, l57 = sch.split(loop=l8, factors=[v54, v55])", "v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l60, l61 = sch.split(loop=l9, factors=[v58, v59])", "sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)", "b62, = sch.get_consumers(block=b0)", "sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v63)", "sch.enter_postproc()", "b64 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b64, ann_key=\"meta_schedule.unroll_explicit\")", "b65, b66 = sch.get_child_blocks(b64)", "l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)", "l93 = sch.fuse(l67, l68, l69, l70, l71)", "sch.parallel(loop=l93)", "l94 = sch.fuse(l90, l91, l92)", "sch.vectorize(loop=l94)", "sch.annotate(block_or_loop=l93, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l93, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b66)", "l101 = sch.fuse(l100)", "sch.vectorize(loop=l101)", "sch.annotate(block_or_loop=l95, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l95, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b102 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b102)", "b123 = sch.decompose_reduction(block=b102, loop=l109)"]
[16:26:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_clip_2
[16:26:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 58, 58, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 48, 56, 56, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 48, 56, 56, 4):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2, i4_2 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2, i4_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2, i4_2])
                compute[i0_2, i1_2, i2_2, i3_2, i4_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2, i4_2], T.float32(6)), T.float32(0))
    

[16:26:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:26:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_2(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(48, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 48, 1, 1, 4), "float32"], compute: T.Buffer[(1, 48, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 48, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 48, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(12, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 16, 58):
                for ax4_fused in T.vectorized(4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(48, i0_0_i1_0_i2_0_fused % 12 // 4 * 16 + ax1)
                        i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_fused % 4 * 14 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(28, 1, 1, 1, 14, 2, 1, 1, 1, 1, 1, 1, 1, 1):
                for i1_3_init in T.serial(16):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i0_0_i1_0_i2_0_fused // 4 * 16 + i1_3_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_fused % 4 * 14 + i2_1)
                            ow = T.axis.spatial(56, i3_0 * 2 + i3_1)
                            oci = T.axis.spatial(4, i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_1, i6_1, i0_3, i1_3 in T.grid(3, 3, 1, 16):
                    for i2_3_i3_3_i4_3_fused in T.vectorized(4):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(48, i0_0_i1_0_i2_0_fused // 4 * 16 + i1_3)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_fused % 4 * 14 + i2_1)
                            ow = T.axis.spatial(56, i3_0 * 2 + i3_1)
                            oci, kh, kw = T.axis.remap("SRR", [i2_3_i3_3_i4_3_fused, i5_1, i6_1])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [48, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(2688, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for i4_fused in T.vectorized(4):
                    with T.block("compute"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(48, i0_i1_i2_fused // 56)
                        i2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        i3_3, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(DepthwiseConv2d[i0, i1, i2, i3_3, i4], placeholder_2[i0, i1, 0, 0, i4])
                        T.writes(compute[i0, i1, i2, i3_3, i4])
                        compute[i0, i1, i2, i3_3, i4] = T.max(T.min(DepthwiseConv2d[i0, i1, i2, i3_3, i4] + placeholder_2[i0, i1, 0, 0, i4], T.float32(6)), T.float32(0))
    

[16:26:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:26:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"PaddedInput\", func_name=\"main\")", "b1 = sch.get_block(name=\"DepthwiseConv2d\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b2)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)", "v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l15, l16, l17, l18 = sch.split(loop=l4, factors=[v11, v12, v13, v14])", "v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[3, 1, 1, 16])", "l23, l24, l25, l26 = sch.split(loop=l5, factors=[v19, v20, v21, v22])", "v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 14, 1, 1])", "l31, l32, l33, l34 = sch.split(loop=l6, factors=[v27, v28, v29, v30])", "v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[28, 2, 1, 1])", "l39, l40, l41, l42 = sch.split(loop=l7, factors=[v35, v36, v37, v38])", "v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])", "l47, l48, l49, l50 = sch.split(loop=l8, factors=[v43, v44, v45, v46])", "v51, v52 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])", "l53, l54 = sch.split(loop=l9, factors=[v51, v52])", "v55, v56 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])", "l57, l58 = sch.split(loop=l10, factors=[v55, v56])", "sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49, l54, l58, l18, l26, l34, l42, l50)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v59)", "l60 = sch.sample_compute_location(block=b0, decision=2)", "sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)", "sch.enter_postproc()", "b61 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b61, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b61, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b61, ann_key=\"meta_schedule.unroll_explicit\")", "b62, b63, b64 = sch.get_child_blocks(b61)", "l65, l66, l67, l68, l69, l70, l71, l72 = sch.get_loops(block=b62)", "l73 = sch.fuse(l65, l66, l67)", "sch.parallel(loop=l73)", "l74 = sch.fuse(l72)", "sch.vectorize(loop=l74)", "sch.annotate(block_or_loop=l73, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l73, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b63)", "l97 = sch.fuse(l94, l95, l96)", "sch.vectorize(loop=l97)", "sch.annotate(block_or_loop=l75, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l75, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l98, l99, l100, l101, l102 = sch.get_loops(block=b64)", "l103 = sch.fuse(l98, l99, l100)", "sch.parallel(loop=l103)", "l104 = sch.fuse(l102)", "sch.vectorize(loop=l104)", "sch.annotate(block_or_loop=l103, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l103, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b105 = sch.get_block(name=\"DepthwiseConv2d\", func_name=\"main\")", "l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b105)", "b126 = sch.decompose_reduction(block=b105, loop=l121)"]
[16:26:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 56, 56, 4, 192, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 56, 56, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[16:26:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[16:26:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add(placeholder: T.Buffer[(1, 48, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(8, 48, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 8, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 8, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused_fused in T.parallel(224):
            for i4_1 in T.serial(1):
                for i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 4, 7, 8):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused_fused % 14 // 7 * 4 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused_fused // 56 * 14 + i2_2_init * 7 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused_fused % 7 * 8 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused_fused % 56 // 14)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(48, 1, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1, 4, 7, 8, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused_fused % 14 // 7 * 4 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused_fused // 56 * 14 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused_fused % 7 * 8 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused_fused % 56 // 14)
                        ic = T.axis.reduce(192, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 48, 56, 56, 4], "float32"], ["TENSOR", [8, 48, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 14, 8, 1):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused_fused % 14 // 7 * 4 + ax1)
                        ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused_fused // 56 * 14 + ax2)
                        ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused_fused % 7 * 8 + ax3)
                        ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused_fused % 56 // 14)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

[16:26:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[16:26:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b2 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.compute_inline(block=b1)", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSRSRS\")", "l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)", "v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])", "l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])", "v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 4])", "l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])", "v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 2, 7])", "l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])", "v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 8])", "l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])", "v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])", "l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])", "v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[48, 4])", "l53, l54 = sch.split(loop=l8, factors=[v51, v52])", "v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])", "l57, l58 = sch.split(loop=l9, factors=[v55, v56])", "v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])", "l61, l62 = sch.split(loop=l10, factors=[v59, v60])", "sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)", "b63, = sch.get_consumers(block=b0)", "sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.parallel\", ann_val=128)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.vectorize\", ann_val=64)", "v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b2, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v64)", "sch.enter_postproc()", "b65 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.parallel\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.vectorize\")", "sch.unannotate(block_or_loop=b65, ann_key=\"meta_schedule.unroll_explicit\")", "b66, b67 = sch.get_child_blocks(b65)", "l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)", "l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76)", "sch.parallel(loop=l94)", "l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)", "l102 = sch.fuse(l95)", "sch.parallel(loop=l102)", "b103 = sch.get_block(name=\"conv2d_NCHWc\", func_name=\"main\")", "l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b103)", "b122 = sch.decompose_reduction(block=b103, loop=l106)"]
[16:26:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_3
[16:26:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_2
[16:26:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_clip_3
[16:26:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_4
[16:26:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_1
[16:26:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_5
[16:26:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_3
[16:26:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_clip_4
[16:26:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_6
[16:26:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_2
[16:26:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_7
[16:26:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_4
[16:26:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_clip_5
[16:26:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_8
[16:26:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_3
[16:26:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_9
[16:26:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_5
[16:26:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_clip_6
[16:26:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_10
[16:26:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_4
[16:26:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_depthwise_conv2d_NCHWc_add_clip_11
[16:26:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_6
[16:26:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_clip_7
[16:26:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_avg_pool2d
[16:26:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_squeeze_layout_transform
[16:26:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_dense_add
[16:26:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_softmax
https://storage.cloud.google.com/octoml-aquarium-models/onnx_model_zoo/vision_classification_efficientnet-lite4-11.onnx
file existed. Skipping downloading.
/home/yj/models/efficientnet-lite4.onnx
Starting to build with relay.
/home/yj/anaconda3/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
The result is correct!
