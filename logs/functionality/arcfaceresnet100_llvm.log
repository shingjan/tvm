nohup: ignoring input
{'name': 'data', 'dtype': 'float32', 'shape': [1, 3, 112, 112]}
Starting to build with relay.
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
[14:57:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #0: "fused_nn_contrib_conv2d_NCHWc_add"
[14:57:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 7, 7, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[14:57:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 7, 1, 1, 1, 7, 1, 4, 4, 1, 1, 1, 8, 1, 1, 1, 64, 1, 1, 1, 8, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_2 * 8 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_0, i4_1])
                    ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 8, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[4, 64])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[14:57:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 7, 1, 1, 1, 7, 1, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 8, 1, 1, 1, 64, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_2 * 8 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_0, i4_1])
                        ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 1, 1, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(7, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 8, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[4, 64])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[14:57:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 1, 4, 4, 1, 1, 1, 8, 1, 1, 1, 64, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_2 * 8 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_0, i4_1])
                        ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 7, 1, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + ax1)
                        ax2_1 = T.axis.spatial(7, ax2)
                        ax3_1 = T.axis.spatial(7, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 8, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[4, 64])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[14:57:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"
[14:57:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 14, 14, 4, 128, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 1, 2, 1, 8, 1, 2, 2, 64, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 4, 1, 7, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1 * 4 + i1_3)
                    oh = T.axis.spatial(14, i2_0 * 2 + i2_2)
                    ow = T.axis.spatial(14, i3_1 * 7 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 8, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 7, 1, 2, 1, 8, 1, 2, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 4, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 2 + i2_2)
                        ow = T.axis.spatial(14, i3_1 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 2, 7, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_1 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 8, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 7, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 2, 2, 64, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 4, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 2 + i2_2)
                        ow = T.axis.spatial(14, i3_1 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 2, 14, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 8, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"
[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 64, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 2, 2, 1, 1, 4, 7, 1, 8, 1, 1, 1, 4, 1, 2, 1, 8, 1, 1, 1, 2, 7, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(28, i2_1 * 7 + i2_3)
                    ow = T.axis.spatial(28, i3_0 * 14 + i3_1 * 2 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                    ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 2, 2, 1, 1, 4, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 4, 1, 2, 1, 8, 1, 1, 1, 2, 7, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 2, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_1 * 7 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 14 + i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 2, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 7, 1, 8, 1, 1, 1, 4, 1, 2, 1, 8, 1, 1, 1, 2, 7, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_1 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 28, 14, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #3: "fused_layout_transform"
[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], T_layout_trans: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 112 and ax3 < 112, placeholder[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], T_layout_trans: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1 * 4 + ax4, ax2, ax3])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 112 and ax3 < 112, placeholder[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"
[14:57:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 56, 56, 4, 64, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 4, 1, 1, 1, 1, 7, 7, 2, 16, 1, 1, 1, 8, 2, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_3)
                    oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 14 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                    ic = T.axis.reduce(64, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 1, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 7, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 7, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[4, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 2, 2, 1, 1, 1, 2, 2, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 1, 7, 7, 2, 16, 1, 1, 1, 8, 2, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(64, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 14, 14, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 28 + i2_1 * 14 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 1, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 7, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 7, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[4, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 2, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 2, 2, 4, 1, 1, 1, 1, 7, 7, 2, 16, 1, 1, 1, 8, 2, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 28 + i3_1 * 14 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(64, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 28, 28, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 28 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 28 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 1, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 7, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 7, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[4, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #5: "fused_copy_subtract_multiply_layout_transform"
[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 112, 112), "float32"], placeholder_1: T.Buffer[(1,), "float32"], placeholder_2: T.Buffer[(1,), "float32"], T_layout_trans: T.Buffer[(1, 1, 112, 112, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_identity = T.alloc_buffer([1, 3, 112, 112], dtype="float32")
        T_subtract = T.alloc_buffer([1, 3, 112, 112], dtype="float32")
        T_multiply = T.alloc_buffer([1, 3, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 112, 112):
            with T.block("T_identity"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_identity[ax0, ax1, ax2, ax3])
                T_identity[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 3, 112, 112):
            with T.block("T_subtract"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_identity[ax0, ax1, ax2, ax3], placeholder_1[0])
                T.writes(T_subtract[ax0, ax1, ax2, ax3])
                T_subtract[ax0, ax1, ax2, ax3] = T_identity[ax0, ax1, ax2, ax3] - placeholder_1[0]
        for i0, i1, i2, i3 in T.grid(1, 3, 112, 112):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_subtract[ax0, ax1, ax2, ax3], placeholder_2[0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = T_subtract[ax0, ax1, ax2, ax3] * placeholder_2[0]
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 112, 112, 3):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1 * 3 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 112 and ax3 < 112, T_multiply[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 112, 112), "float32"], placeholder_1: T.Buffer[(1,), "float32"], placeholder_2: T.Buffer[(1,), "float32"], T_layout_trans: T.Buffer[(1, 1, 112, 112, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 112, 112, 3):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], placeholder_1[0], placeholder_2[0])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 112 and ax3 < 112, (placeholder[ax0, ax1 * 3 + ax4, ax2, ax3] - placeholder_1[0]) * placeholder_2[0], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_identity", func_name="main")
b1 = sch.get_block(name="T_subtract", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"
[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 112, 112, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 114, 114, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 114, 114, 3):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 112, 112, 4, 3, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 112, 112, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 112, 112, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 114, 114, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1 in T.grid(1, 4, 1, 1, 1, 1, 2, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 30, 114, 3):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(114, i2_1 * 28 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 3, 3, 1, 1, 1, 2, 14, 1, 1, 1, 3, 1, 2, 14, 2, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(112, i2_1 * 28 + i2_2 * 14 + i2_3)
                        ow = T.axis.spatial(112, i3_1 * 28 + i3_2 * 2 + i3_3)
                        oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_3, i5_0, i6_0, i7_1])
                        T.reads(data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 112, 112, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 2, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 14, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 112, 112, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 114, 114, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 114, 114, 3):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2, 4, 4, 1):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 1, 2, 14, 1, 1, 1, 3, 1, 2, 14, 2, 4):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(112, i2_1 * 28 + i2_2 * 14 + i2_3)
                            ow = T.axis.spatial(112, i3_1 * 28 + i3_2 * 2 + i3_3)
                            oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_3, i5_0, i6_0, i7_1])
                            T.reads(data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 112, 112, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 28, 28, 4):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(16, i1_0 * 4 + i1_1 * 2 + ax1)
                            ax2_1 = T.axis.spatial(112, i2_1 * 28 + ax2)
                            ax3_1 = T.axis.spatial(112, i3_1 * 28 + ax3)
                            ax4_1 = T.axis.spatial(4, ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 2, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 14, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 112, 112, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 114, 114, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 2, 4, 4, 1, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 30, 30, 1):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(114, i2_1 * 28 + ax2)
                            i3 = T.axis.spatial(114, i3_1 * 28 + ax3)
                            i4 = T.axis.spatial(3, i5_0 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 2, 14, 1, 1, 1, 3, 1, 2, 14, 2, 4):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(112, i2_1 * 28 + i2_2 * 14 + i2_3)
                            ow = T.axis.spatial(112, i3_1 * 28 + i3_2 * 2 + i3_3)
                            oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_3, i5_0, i6_0, i7_1])
                            T.reads(data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 112, 112, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 112, 112, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 4 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 2, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 14, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"
[14:57:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(802816,), "float32"], T_reshape: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
        T_reshape_1 = T.alloc_buffer([802816], dtype="float32")
        T_prelu = T.alloc_buffer([802816], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 112 and ax3 < 112, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0 in T.serial(802816):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(802816, i0)
                T.reads(T_layout_trans[0, ax0 % 802816 // 12544, ax0 % 12544 // 112, ax0 % 112])
                T.writes(T_reshape_1[ax0])
                T_reshape_1[ax0] = T_layout_trans[0, ax0 % 802816 // 12544, ax0 % 12544 // 112, ax0 % 112]
        for i0 in T.serial(802816):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(802816, i0)
                T.reads(T_reshape_1[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape_1[ax0], T_reshape_1[ax0], T_reshape_1[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 12544 + ax2 * 112 + ax3) % 802816])
                T.writes(T_reshape[ax0, ax1, ax2, ax3])
                T_reshape[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 12544 + ax2 * 112 + ax3) % 802816]
    

[14:57:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(802816,), "float32"], T_reshape: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
            T_reshape_1 = T.alloc_buffer([802816], dtype="float32")
            T_prelu = T.alloc_buffer([802816], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 112 and ax3 < 112, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
            for i0 in T.serial(802816):
                with T.block("T_reshape"):
                    ax0 = T.axis.spatial(802816, i0)
                    T.reads(T_layout_trans[0, ax0 % 802816 // 12544, ax0 % 12544 // 112, ax0 % 112])
                    T.writes(T_reshape_1[ax0])
                    T_reshape_1[ax0] = T_layout_trans[0, ax0 % 802816 // 12544, ax0 % 12544 // 112, ax0 % 112]
            for i0 in T.serial(802816):
                with T.block("T_prelu"):
                    ax0 = T.axis.spatial(802816, i0)
                    T.reads(T_reshape_1[ax0], placeholder_1[ax0])
                    T.writes(T_prelu[ax0])
                    T_prelu[ax0] = T.Select(T.float32(0) < T_reshape_1[ax0], T_reshape_1[ax0], T_reshape_1[ax0] * placeholder_1[ax0])
            for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
                with T.block("T_reshape_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(T_prelu[(ax1 * 12544 + ax2 * 112 + ax3) % 802816])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3])
                    T_reshape[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 12544 + ax2 * 112 + ax3) % 802816]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_prelu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
[14:57:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #8: "fused_add_layout_transform"
[14:57:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 1), "float32"], T_layout_trans: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_add = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] + placeholder_1[ax1, 0, 0]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 112 and ax3 < 112, T_add[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[14:57:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], placeholder_1: T.Buffer[(64, 1, 1), "float32"], T_layout_trans: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1 * 4 + ax4, ax2, ax3], placeholder_1[ax1 * 4 + ax4, 0, 0])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 112 and ax3 < 112, placeholder[ax0, ax1 * 4 + ax4, ax2, ax3] + placeholder_1[ax1 * 4 + ax4, 0, 0], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[14:57:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"
[14:57:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 114, 114, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 114, 114, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 112, 112, 4, 64, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[14:57:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 114, 114, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 114, 114, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(14, 1, 4, 1, 1, 1, 14, 1, 8, 1, 1, 1, 1, 4, 8, 1, 8, 3, 3, 1, 8, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_3)
                        oh = T.axis.spatial(112, i2_0 * 8 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(112, i3_1 * 8 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 4, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 8, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[14:57:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 114, 114, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1 in T.grid(1, 2, 14, 1, 4, 1, 1, 1, 14):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 10, 10, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(114, i2_0 * 8 + ax2)
                        i3 = T.axis.spatial(114, i3_1 * 8 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1 in T.serial(1):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 4, 8, 1, 8, 3, 3, 1, 8, 2, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_3)
                            oh = T.axis.spatial(112, i2_0 * 8 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(112, i3_1 * 8 + i3_2)
                            oc_block = T.axis.spatial(4, i4_0)
                            ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 8, 8, 1):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                            ax2_1 = T.axis.spatial(112, i2_0 * 8 + ax2)
                            ax3_1 = T.axis.spatial(112, i3_1 * 8 + ax3)
                            ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 4, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 8, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 114, 114, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 2, 14):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 10, 114, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(114, i2_0 * 8 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0 in T.grid(1, 4):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 14, 1, 8, 1, 1, 1, 1, 4, 8, 1, 8, 3, 3, 1, 8, 2, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_3)
                            oh = T.axis.spatial(112, i2_0 * 8 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(112, i3_1 * 8 + i3_2)
                            oc_block = T.axis.spatial(4, i4_0)
                            ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 8, 112, 1):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                            ax2_1 = T.axis.spatial(112, i2_0 * 8 + ax2)
                            ax3_1 = T.axis.spatial(112, ax3)
                            ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 4, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 8, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"
[14:57:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(802816,), "float32"], T_layout_trans: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans_1 = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
        T_reshape = T.alloc_buffer([802816], dtype="float32")
        T_prelu = T.alloc_buffer([802816], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3])
                T_layout_trans_1[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 112 and ax3 < 112, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0 in T.serial(802816):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(802816, i0)
                T.reads(T_layout_trans_1[0, ax0 % 802816 // 12544, ax0 % 12544 // 112, ax0 % 112])
                T.writes(T_reshape[ax0])
                T_reshape[ax0] = T_layout_trans_1[0, ax0 % 802816 // 12544, ax0 % 12544 // 112, ax0 % 112]
        for i0 in T.serial(802816):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(802816, i0)
                T.reads(T_reshape[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape[ax0], T_reshape[ax0], T_reshape[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 12544 + ax2 * 112 + ax3) % 802816])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 12544 + ax2 * 112 + ax3) % 802816]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 112 and ax3 < 112, T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[14:57:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(802816,), "float32"], T_layout_trans: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_layout_trans_1 = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
            T_reshape = T.alloc_buffer([802816], dtype="float32")
            T_prelu = T.alloc_buffer([802816], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3])
                    T_layout_trans_1[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 112 and ax3 < 112, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
            for i0 in T.serial(802816):
                for ax0 in T.serial(1):
                    with T.block("T_reshape"):
                        ax0_1 = T.axis.spatial(802816, i0 + ax0)
                        T.reads(T_layout_trans_1[0, ax0_1 % 802816 // 12544, ax0_1 % 12544 // 112, ax0_1 % 112])
                        T.writes(T_reshape[ax0_1])
                        T_reshape[ax0_1] = T_layout_trans_1[0, ax0_1 % 802816 // 12544, ax0_1 % 12544 // 112, ax0_1 % 112]
                with T.block("T_prelu"):
                    ax0 = T.axis.spatial(802816, i0)
                    T.reads(T_reshape[ax0], placeholder_1[ax0])
                    T.writes(T_prelu[ax0])
                    T_prelu[ax0] = T.Select(T.float32(0) < T_reshape[ax0], T_reshape[ax0], T_reshape[ax0] * placeholder_1[ax0])
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 112, 112, 4):
                with T.block("T_layout_trans_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(T_prelu[(ax1 * 50176 + ax4 * 12544 + ax2 * 112 + ax3) % 802816])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 112 and ax3 < 112, T_prelu[((ax1 * 4 + ax4) * 12544 + ax2 * 112 + ax3) % 802816], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_prelu", func_name="main")
b3 = sch.get_block(name="T_reshape_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=-2)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b1, decision=0)
sch.compute_at(block=b1, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)
[14:57:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"
[14:57:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 114, 114, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 114, 114, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 56, 56, 4, 64, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[14:57:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 114, 114, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 114, 114, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 2, 1, 1, 1, 2, 1, 1, 1, 8, 1, 3, 1, 1, 28, 1, 4, 8, 3, 1, 1, 1, 1, 56, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 2 + i1_1_1)
                    oh = T.axis.spatial(56, i2_0 * 28 + i2_2)
                    ow, oc_block = T.axis.remap("SS", [i3_3, i4_2])
                    ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                    T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 2, 1, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 28, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 56])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[8, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 114, 114, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 8, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 57, 113, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(114, i2_0 * 56 + ax2)
                        i3 = T.axis.spatial(114, ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 1, 1, 1):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 3, 1, 1, 28, 1, 4, 8, 3, 1, 1, 1, 1, 56, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_0 * 2 + i1_1)
                            oh = T.axis.spatial(56, i2_0 * 28 + i2_2)
                            ow, oc_block = T.axis.remap("SS", [i3_3, i4_2])
                            ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 28, 56, 4):
                        with T.block("T_add_1"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(16, i1_0 * 2 + i1_1 + ax1)
                            ax2_1 = T.axis.spatial(56, i2_0 * 28 + ax2)
                            ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 2, 1, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 28, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 56])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[8, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 114, 114, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 2, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0 in T.grid(1, 2, 1, 1, 1, 8, 1, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 57, 111, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(16, i5_0 * 2 + ax1)
                            i2 = T.axis.spatial(114, i2_0 * 56 + ax2)
                            i3 = T.axis.spatial(114, i7_0 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 28, 1, 4, 8, 3, 1, 1, 1, 1, 56, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_0 * 2 + i1_1)
                            oh = T.axis.spatial(56, i2_0 * 28 + i2_2)
                            ow, oc_block = T.axis.remap("SS", [i3_3, i4_2])
                            ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 28, 56, 4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 28 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 2, 1, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 28, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 56])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[8, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"
[14:57:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 58, 58, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 58, 58, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 56, 56, 4, 64, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[14:57:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 58, 58, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1, 1, 2, 14, 8, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 6, 9, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(58, i2_1 * 4 + ax2)
                        i3 = T.axis.spatial(58, i3_1 * 7 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 3, 1, 1, 1, 2, 1, 1, 16, 1, 3, 1, 4, 2, 7, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(56, i2_1 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(64, i5_0 * 16 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 8, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[14:57:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1, 1, 2, 14, 8, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 3, 1, 1, 1, 2, 1, 1, 16, 1, 3, 1, 4, 2, 7, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(56, i2_1 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(64, i5_0 * 16 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 57 and 1 <= ow + kw and ow + kw < 57, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 4, 7, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_1 * 4 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_1 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 8, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 14, 8, 2, 4, 3, 1, 1, 1, 2, 1, 1, 16, 1, 3, 1, 4, 2, 7, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(56, i2_1 * 4 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 7 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(64, i5_0 * 16 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 57 and 1 <= ow + kw and ow + kw < 57, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 56, 56, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 14, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 8, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"
[14:57:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(200704,), "float32"], T_layout_trans: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans_1 = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        T_reshape = T.alloc_buffer([200704], dtype="float32")
        T_prelu = T.alloc_buffer([200704], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3])
                T_layout_trans_1[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 56 and ax3 < 56, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0 in T.serial(200704):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(200704, i0)
                T.reads(T_layout_trans_1[0, ax0 % 200704 // 3136, ax0 % 3136 // 56, ax0 % 56])
                T.writes(T_reshape[ax0])
                T_reshape[ax0] = T_layout_trans_1[0, ax0 % 200704 // 3136, ax0 % 3136 // 56, ax0 % 56]
        for i0 in T.serial(200704):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(200704, i0)
                T.reads(T_reshape[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape[ax0], T_reshape[ax0], T_reshape[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 3136 + ax2 * 56 + ax3) % 200704])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 3136 + ax2 * 56 + ax3) % 200704]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 56 and ax3 < 56, T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[14:57:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(200704,), "float32"], T_layout_trans: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_layout_trans_1 = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
            T_reshape = T.alloc_buffer([200704], dtype="float32")
            T_reshape_1 = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
            for i0, i1, i2 in T.grid(1, 16, 56):
                for ax0, ax1 in T.grid(1, 64):
                    for ax2, ax3 in T.grid(i2 + 1, 56):
                        with T.block("T_layout_trans"):
                            ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                            ax2_1 = T.axis.spatial(56, ax2)
                            ax3_1 = T.axis.spatial(56, ax3)
                            T.reads(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                            T.writes(T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1])
                            T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 64 and ax2_1 < 56 and ax3_1 < 56, placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], T.float32(0), dtype="float32")
                for i3 in T.serial(56):
                    for ax0, ax1 in T.grid(1, 4):
                        for ax0_2 in T.serial(1):
                            with T.block("T_reshape"):
                                ax0_3 = T.axis.spatial(200704, i1 * 12544 + ax1 * 3136 + i2 * 56 + i3 + ax0_2)
                                T.reads(T_layout_trans_1[0, ax0_3 % 200704 // 3136, ax0_3 % 3136 // 56, ax0_3 % 56])
                                T.writes(T_reshape[ax0_3])
                                T_reshape[ax0_3] = T_layout_trans_1[0, ax0_3 % 200704 // 3136, ax0_3 % 3136 // 56, ax0_3 % 56]
                        for ax2, ax3 in T.grid(1, 1):
                            with T.block("T_reshape_1"):
                                ax0_4 = T.axis.spatial(1, ax0)
                                ax1_2 = T.axis.spatial(64, i1 * 4 + ax1)
                                ax2_2 = T.axis.spatial(56, i2 + ax2)
                                ax3_2 = T.axis.spatial(56, i3 + ax3)
                                T.reads(T_reshape[(ax1_2 * 3136 + ax2_2 * 56 + ax3_2) % 200704], placeholder_1[(ax1_2 * 3136 + ax2_2 * 56 + ax3_2) % 200704])
                                T.writes(T_reshape_1[ax0_4, ax1_2, ax2_2, ax3_2])
                                T_reshape_1[ax0_4, ax1_2, ax2_2, ax3_2] = T.Select(T.float32(0) < T_reshape[(ax1_2 * 3136 + ax2_2 * 56 + ax3_2) % 200704], T_reshape[(ax1_2 * 3136 + ax2_2 * 56 + ax3_2) % 200704], T_reshape[(ax1_2 * 3136 + ax2_2 * 56 + ax3_2) % 200704] * placeholder_1[(ax1_2 * 3136 + ax2_2 * 56 + ax3_2) % 200704])
                    for i4 in T.serial(4):
                        with T.block("T_layout_trans_1"):
                            ax0_5, ax1_3, ax2_3, ax3_3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                            T.reads(T_reshape_1[ax0_5, ax1_3 * 4 + ax4, ax2_3, ax3_3])
                            T.writes(T_layout_trans[ax0_5, ax1_3, ax2_3, ax3_3, ax4])
                            T_layout_trans[ax0_5, ax1_3, ax2_3, ax3_3, ax4] = T.if_then_else(ax0_5 < 1 and ax1_3 * 4 + ax4 < 64 and ax2_3 < 56 and ax3_3 < 56, T_reshape_1[ax0_5, ax1_3 * 4 + ax4, ax2_3, ax3_3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_prelu", func_name="main")
b3 = sch.get_block(name="T_reshape_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=-2)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b1, decision=5)
sch.compute_at(block=b1, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)
[14:57:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"
[14:57:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 58, 58, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 58, 58, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 56, 56, 4, 64, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[14:57:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 58, 58, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 1, 14, 7, 1, 1, 4, 2, 4, 1, 32):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 4, 4, 2):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(16, i5_0 // 2 + ax1)
                        i2 = T.axis.spatial(58, i2_0 * 4 + i2_1 * 2 + ax2)
                        i3 = T.axis.spatial(58, i3_0 * 8 + i3_1 * 2 + ax3)
                        i4 = T.axis.spatial(4, i5_0 % 2 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 4, 1, 2, 4, 2, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(56, i2_0 * 4 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 8 + i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(64, i5_0 * 2 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 4, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 2, 1, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 4, 2, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 14, 7, 1, 1, 4, 2, 4, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 3, 3, 1, 4, 1, 2, 4, 2, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(56, i2_0 * 4 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 8 + i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(64, i5_0 * 2 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 57 and 1 <= ow + kw and ow + kw < 57, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 2, 2, 4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 4 + i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 8 + i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 4, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 2, 1, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 4, 2, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 14, 7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 2, 4, 1, 32, 3, 3, 1, 4, 1, 2, 4, 2, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(56, i2_0 * 4 + i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 8 + i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(64, i5_0 * 2 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 57 and 1 <= ow + kw and ow + kw < 57, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 4, 8, 4):
                    with T.block("T_add_1"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(56, i2_0 * 4 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 8 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 4, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 2, 1, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 4, 2, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #15: "fused_add"
[14:57:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] + placeholder_1[ax0, ax1, 0, 0, ax4]
    

[14:57:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] + placeholder_1[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:57:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"
[14:57:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 58, 58, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 58, 58, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 56, 56, 4, 64, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[14:57:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 2, 1, 1, 1, 8, 1, 1, 1, 1, 1, 16, 4, 7, 2, 64, 3, 3, 1, 2, 7, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(56, i2_0 * 28 + i2_2 * 7 + i2_3)
                    ow = T.axis.spatial(56, i3_1 * 7 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_1])
                    T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 57 and 1 <= ow + kw and ow + kw < 57, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 16, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 8, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[14:57:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 58, 58, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 58, 58, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 1, 2, 1, 2, 1, 1, 1, 8, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 16, 4, 7, 2, 64, 3, 3, 1, 2, 7, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_1_1 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 28, 7, 2):
                    with T.block("T_add"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(56, i2_0 * 28 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_1_1 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 16, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 8, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 58, 58, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 2, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 30, 58, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(58, i2_0 * 28 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 8, 1, 1, 1, 1, 1, 16, 4, 7, 2, 64, 3, 3, 1, 2, 7, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(56, i3_1 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 28, 56, 2):
                    with T.block("T_add"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(56, i2_0 * 28 + ax2)
                        ax3_1 = T.axis.spatial(56, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 16, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 8, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"
[14:57:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(401408,), "float32"], T_layout_trans: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans_1 = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        T_reshape = T.alloc_buffer([401408], dtype="float32")
        T_prelu = T.alloc_buffer([401408], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3])
                T_layout_trans_1[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 128 and ax2 < 56 and ax3 < 56, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0 in T.serial(401408):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(401408, i0)
                T.reads(T_layout_trans_1[0, ax0 % 401408 // 3136, ax0 % 3136 // 56, ax0 % 56])
                T.writes(T_reshape[ax0])
                T_reshape[ax0] = T_layout_trans_1[0, ax0 % 401408 // 3136, ax0 % 3136 // 56, ax0 % 56]
        for i0 in T.serial(401408):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(401408, i0)
                T.reads(T_reshape[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape[ax0], T_reshape[ax0], T_reshape[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 3136 + ax2 * 56 + ax3) % 401408])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 3136 + ax2 * 56 + ax3) % 401408]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 56, 56, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 128 and ax2 < 56 and ax3 < 56, T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[14:57:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(401408,), "float32"], T_layout_trans: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_reshape = T.alloc_buffer([401408], dtype="float32")
            T_prelu = T.alloc_buffer([401408], dtype="float32")
            T_reshape_1 = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
            for i0 in T.serial(401408):
                with T.block("T_reshape"):
                    ax0 = T.axis.spatial(401408, i0)
                    T.reads(placeholder[0, ax0 % 401408 // 12544, ax0 % 3136 // 56, ax0 % 56, ax0 % 12544 // 3136])
                    T.writes(T_reshape[ax0])
                    T_reshape[ax0] = T.if_then_else(0 < 1 and ax0 % 401408 // 3136 < 128 and ax0 % 3136 // 56 < 56 and ax0 % 56 < 56, placeholder[0, ax0 % 401408 // 3136 // 4, ax0 % 3136 // 56, ax0 % 56, ax0 % 401408 // 3136 % 4], T.float32(0), dtype="float32")
            for i0, i1 in T.grid(1, 32):
                for ax0 in T.serial(12544):
                    with T.block("T_prelu"):
                        ax0_1 = T.axis.spatial(401408, i1 * 12544 + ax0)
                        T.reads(T_reshape[ax0_1], placeholder_1[ax0_1])
                        T.writes(T_prelu[ax0_1])
                        T_prelu[ax0_1] = T.Select(T.float32(0) < T_reshape[ax0_1], T_reshape[ax0_1], T_reshape[ax0_1] * placeholder_1[ax0_1])
                for i2, i3, i4 in T.grid(56, 56, 4):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("T_reshape_1"):
                            ax0_2 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(128, i1 * 4 + i4 + ax1)
                            ax2_1 = T.axis.spatial(56, i2 + ax2)
                            ax3_1 = T.axis.spatial(56, i3 + ax3)
                            T.reads(T_prelu[(ax1_1 * 3136 + ax2_1 * 56 + ax3_1) % 401408])
                            T.writes(T_reshape_1[ax0_2, ax1_1, ax2_1, ax3_1])
                            T_reshape_1[ax0_2, ax1_1, ax2_1, ax3_1] = T_prelu[(ax1_1 * 3136 + ax2_1 * 56 + ax3_1) % 401408]
                    with T.block("T_layout_trans_1"):
                        ax0_3, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(T_reshape_1[ax0_3, ax1 * 4 + ax4, ax2, ax3])
                        T.writes(T_layout_trans[ax0_3, ax1, ax2, ax3, ax4])
                        T_layout_trans[ax0_3, ax1, ax2, ax3, ax4] = T.if_then_else(ax0_3 < 1 and ax1 * 4 + ax4 < 128 and ax2 < 56 and ax3 < 56, T_reshape_1[ax0_3, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_prelu", func_name="main")
b3 = sch.get_block(name="T_reshape_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=4)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=1)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)
[14:57:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"
[14:57:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 58, 58, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 128, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[14:57:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 1, 1, 2, 2, 1, 16):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 57, 29, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(58, ax2)
                        i3 = T.axis.spatial(58, i3_0 * 28 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 14, 1, 16, 1, 3, 1, 2, 7, 1, 2, 8, 3, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(28, i2_1 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 14, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 58, 58, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 1, 1, 2, 2, 1, 16, 2, 14, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 3, 1, 2, 7, 1, 2, 8, 3, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1_1 * 2 + i1_2)
                        oh = T.axis.spatial(28, i2_1_1 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_1_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 14, 1, 2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_1_1 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 14 + i3_1_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 14, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 58, 58, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 2, 2):
                for i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 2, 14, 1, 16, 1, 3, 1, 2, 7, 1, 2, 8, 3, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1_1 * 2 + i1_2)
                        oh = T.axis.spatial(28, i2_1_1 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 14 + i3_1_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 28, 14, 2):
                    with T.block("T_add_1"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(28, i3_0 * 14 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 16, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 7, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 14, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"
[14:57:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 30, 30, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 128, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[14:57:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0 in T.grid(1, 8, 1, 7):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 30, 6, 4):
                    with T.block("data_pad"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(30, i3_0 * 4 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 2, 4, 128, 3, 3, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 7, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(28, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 4 + i3_1 * 2 + i3_3)
                        oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_1, i5_0, i6_0, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[14:57:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1 in T.grid(1, 8, 1, 7, 1, 1, 2, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 30, 4, 4):
                    with T.block("data_pad"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(30, i3_0 * 4 + i3_1 * 2 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1 in T.serial(4):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 3, 3, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 7, 2, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(28, i2_2 * 7 + i2_3)
                            ow = T.axis.spatial(28, i3_0 * 4 + i3_1 * 2 + i3_3)
                            oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_1, i5_0, i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 28, 2, 1):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + ax1)
                            ax2_1 = T.axis.spatial(28, ax2)
                            ax3_1 = T.axis.spatial(28, i3_0 * 4 + i3_1 * 2 + ax3)
                            ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 1, 7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 2, 1, 2, 4, 128):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 30, 4, 1):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(32, i5_0 // 4 + ax1)
                            i2 = T.axis.spatial(30, ax2)
                            i3 = T.axis.spatial(30, i3_0 * 4 + i3_1 * 2 + ax3)
                            i4 = T.axis.spatial(4, i5_0 % 4 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 7, 2, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(28, i2_2 * 7 + i2_3)
                            ow = T.axis.spatial(28, i3_0 * 4 + i3_1 * 2 + i3_3)
                            oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_1, i5_0, i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 28, 4, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(28, ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 2, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 4, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"
[14:57:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(100352,), "float32"], T_layout_trans: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans_1 = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_reshape = T.alloc_buffer([100352], dtype="float32")
        T_prelu = T.alloc_buffer([100352], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3])
                T_layout_trans_1[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 128 and ax2 < 28 and ax3 < 28, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0 in T.serial(100352):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(100352, i0)
                T.reads(T_layout_trans_1[0, ax0 % 100352 // 784, ax0 % 784 // 28, ax0 % 28])
                T.writes(T_reshape[ax0])
                T_reshape[ax0] = T_layout_trans_1[0, ax0 % 100352 // 784, ax0 % 784 // 28, ax0 % 28]
        for i0 in T.serial(100352):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(100352, i0)
                T.reads(T_reshape[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape[ax0], T_reshape[ax0], T_reshape[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 784 + ax2 * 28 + ax3) % 100352])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 784 + ax2 * 28 + ax3) % 100352]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 128 and ax2 < 28 and ax3 < 28, T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[14:57:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(100352,), "float32"], T_layout_trans: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_layout_trans_1 = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
            T_reshape = T.alloc_buffer([100352], dtype="float32")
            T_prelu = T.alloc_buffer([100352], dtype="float32")
            T_reshape_1 = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3])
                    T_layout_trans_1[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 128 and ax2 < 28 and ax3 < 28, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
            for i0, i1 in T.grid(1, 32):
                for ax0 in T.serial(100352):
                    with T.block("T_reshape"):
                        ax0_1 = T.axis.spatial(100352, ax0)
                        T.reads(T_layout_trans_1[0, ax0_1 % 100352 // 784, ax0_1 % 784 // 28, ax0_1 % 28])
                        T.writes(T_reshape[ax0_1])
                        T_reshape[ax0_1] = T_layout_trans_1[0, ax0_1 % 100352 // 784, ax0_1 % 784 // 28, ax0_1 % 28]
                for i2, i3 in T.grid(28, 28):
                    for ax0 in T.serial(100352):
                        with T.block("T_prelu"):
                            ax0_2 = T.axis.spatial(100352, ax0)
                            T.reads(T_reshape[ax0_2], placeholder_1[ax0_2])
                            T.writes(T_prelu[ax0_2])
                            T_prelu[ax0_2] = T.Select(T.float32(0) < T_reshape[ax0_2], T_reshape[ax0_2], T_reshape[ax0_2] * placeholder_1[ax0_2])
                    for ax0_3, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("T_reshape_1"):
                            ax0_4 = T.axis.spatial(1, ax0_3)
                            ax1_1 = T.axis.spatial(128, i1 * 4 + ax1)
                            ax2_1 = T.axis.spatial(28, i2 + ax2)
                            ax3_1 = T.axis.spatial(28, i3 + ax3)
                            T.reads(T_prelu[(ax1_1 * 784 + ax2_1 * 28 + ax3_1) % 100352])
                            T.writes(T_reshape_1[ax0_4, ax1_1, ax2_1, ax3_1])
                            T_reshape_1[ax0_4, ax1_1, ax2_1, ax3_1] = T_prelu[(ax1_1 * 784 + ax2_1 * 28 + ax3_1) % 100352]
                    for i4 in T.serial(4):
                        with T.block("T_layout_trans_1"):
                            ax0_5, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                            T.reads(T_reshape_1[ax0_5, ax1 * 4 + ax4, ax2, ax3])
                            T.writes(T_layout_trans[ax0_5, ax1, ax2, ax3, ax4])
                            T_layout_trans[ax0_5, ax1, ax2, ax3, ax4] = T.if_then_else(ax0_5 < 1 and ax1 * 4 + ax4 < 128 and ax2 < 28 and ax3 < 28, T_reshape_1[ax0_5, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_prelu", func_name="main")
b3 = sch.get_block(name="T_reshape_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=3)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)
[14:57:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"
[14:57:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 30, 30, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 28, 28, 4, 128, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[14:57:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 14, 1, 1, 2, 14, 2, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 4, 3, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(30, i2_1 * 2 + ax2)
                        i3 = T.axis.spatial(30, i3_0 * 2 + i3_1 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 16, 2, 1, 2, 2, 3, 3, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2)
                        oh = T.axis.spatial(28, i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_0 * 2 + i3_1)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 16, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 1, 1, 14, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 30, 4, 4):
                    with T.block("data_pad"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(30, i3_0 * 2 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(14, 2, 2):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 16, 2, 1, 2, 2, 3, 3, 1, 1, 1, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2)
                            oh = T.axis.spatial(28, i2_1 * 2 + i2_2)
                            ow = T.axis.spatial(28, i3_0 * 2 + i3_1)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 2, 1, 2):
                        with T.block("T_add_1"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(32, i1_1 * 16 + ax1)
                            ax2_1 = T.axis.spatial(28, i2_1 * 2 + ax2)
                            ax3_1 = T.axis.spatial(28, i3_0 * 2 + i3_1 + ax3)
                            ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 16, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0 in T.grid(1, 1, 1, 14):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 30, 4, 4):
                    with T.block("data_pad"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(30, i3_0 * 2 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_0 in T.serial(1):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 14, 2, 2, 64, 1, 1, 1, 16, 2, 1, 2, 2, 3, 3, 1, 1, 1, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_2)
                            oh = T.axis.spatial(28, i2_1 * 2 + i2_2)
                            ow = T.axis.spatial(28, i3_0 * 2 + i3_1)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 28, 2, 4):
                        with T.block("T_add_1"):
                            ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                            ax3_1 = T.axis.spatial(28, i3_0 * 2 + ax3)
                            ax4_1 = T.axis.spatial(4, ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 16, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 2, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #22: "fused_add_1"
[14:57:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] + placeholder_1[ax0, ax1, 0, 0, ax4]
    

[14:57:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 28, 28, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] + placeholder_1[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:57:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"
[14:57:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 30, 30, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 28, 28, 4, 128, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[14:57:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 2, 14, 1, 1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 4, 30, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(30, i2_0 * 2 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 4, 1, 3, 1, 1, 2, 7, 1, 32, 3, 1, 1, 8, 1, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1 * 8 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[14:57:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 2, 14, 1, 1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 4, 30, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(30, i2_0 * 2 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(1, 1, 4):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 3, 1, 1, 2, 7, 1, 32, 3, 1, 1, 8, 1, 4, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1 * 8 + i1_3)
                            oh = T.axis.spatial(28, i2_0 * 2 + i2_2)
                            ow = T.axis.spatial(28, i3_2 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1)
                            ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 2, 28, 1):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(64, i1_0 * 32 + i1_1 * 8 + ax1)
                            ax2_1 = T.axis.spatial(28, i2_0 * 2 + ax2)
                            ax3_1 = T.axis.spatial(28, ax3)
                            ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 14, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 4, 4, 1, 3, 1, 1, 2, 7, 1, 32, 3, 1, 1, 8, 1, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1 * 8 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 29 and 1 <= ow + kw and ow + kw < 29, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 2, 28, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 2 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"
[14:57:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(200704,), "float32"], T_layout_trans: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans_1 = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        T_reshape = T.alloc_buffer([200704], dtype="float32")
        T_prelu = T.alloc_buffer([200704], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3])
                T_layout_trans_1[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 256 and ax2 < 28 and ax3 < 28, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0 in T.serial(200704):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(200704, i0)
                T.reads(T_layout_trans_1[0, ax0 % 200704 // 784, ax0 % 784 // 28, ax0 % 28])
                T.writes(T_reshape[ax0])
                T_reshape[ax0] = T_layout_trans_1[0, ax0 % 200704 // 784, ax0 % 784 // 28, ax0 % 28]
        for i0 in T.serial(200704):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(200704, i0)
                T.reads(T_reshape[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape[ax0], T_reshape[ax0], T_reshape[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 784 + ax2 * 28 + ax3) % 200704])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 784 + ax2 * 28 + ax3) % 200704]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 28 and ax3 < 28, T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[14:57:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(200704,), "float32"], T_layout_trans: T.Buffer[(1, 64, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_layout_trans_1 = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
            T_reshape = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
            for i0, i1 in T.grid(1, 256):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 28, 28):
                    with T.block("T_layout_trans"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                        T.writes(T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 256 and ax2_1 < 28 and ax3_1 < 28, placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], T.float32(0), dtype="float32")
                for i2, i3 in T.grid(28, 28):
                    with T.block("T_reshape_1"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(T_layout_trans_1[0, (ax1 * 784 + ax2 * 28 + ax3) % 200704 // 784, (ax2 * 28 + ax3) % 784 // 28, ax3 % 28], placeholder_1[(ax1 * 784 + ax2 * 28 + ax3) % 200704])
                        T.writes(T_reshape[ax0, ax1, ax2, ax3])
                        T_reshape[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < T_layout_trans_1[0, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 200704 // 784, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 784 // 28, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 28], T_layout_trans_1[0, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 200704 // 784, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 784 // 28, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 28], T_layout_trans_1[0, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 200704 // 784, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 784 // 28, (ax1 * 784 + ax2 * 28 + ax3) % 200704 % 28] * placeholder_1[(ax1 * 784 + ax2 * 28 + ax3) % 200704])
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
                with T.block("T_layout_trans_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(T_reshape[ax0, ax1 * 4 + ax4, ax2, ax3])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 28 and ax3 < 28, T_reshape[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_prelu", func_name="main")
b3 = sch.get_block(name="T_reshape_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=-1)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=-2)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)
[14:57:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"
[14:57:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 30, 30, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 30, 30, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 14, 14, 4, 256, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[14:57:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 30, 30, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 2, 1, 2, 1, 1, 1, 14, 7, 2, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 3, 3, 4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(64, i5_0 * 16 + ax1)
                        i2 = T.axis.spatial(30, i2_1 * 2 + ax2)
                        i3 = T.axis.spatial(30, i3_0 * 14 + i3_1 * 2 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 32, 1, 1, 2, 64, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_2)
                        oh = T.axis.spatial(14, i2_1)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_1)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 32, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[4, 64])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 30, 30, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 2, 1, 1, 1, 14, 7, 2):
                for i5_0 in T.serial(4):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 3, 3, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(64, i5_0 * 16 + ax1)
                            i2 = T.axis.spatial(30, i2_1 * 2 + ax2)
                            i3 = T.axis.spatial(30, i3_0 * 14 + i3_1 * 2 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 32, 1, 1, 2, 64, 1, 1, 1, 1, 1, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_2)
                            oh = T.axis.spatial(14, i2_1)
                            ow = T.axis.spatial(14, i3_0 * 7 + i3_1)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 1, 1, 2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 32, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[4, 64])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 14, 7, 2, 4, 3, 3, 1, 32, 1, 1, 2, 64, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_2)
                        oh = T.axis.spatial(14, i2_1)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_1)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(placeholder[n, ic // 4, oh * 2 + kh - 1, ow * 2 + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 28, 28, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh * 2 + kh and oh * 2 + kh < 29 and 1 <= ow * 2 + kw and ow * 2 + kw < 29, placeholder[n, ic // 4, oh * 2 + kh - 1, ow * 2 + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 14, 7, 4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 32, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[4, 64])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"
[14:57:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 16, 16, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 16, 16, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 14, 14, 4, 256, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[14:57:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 16, 16, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 14, 1, 2, 1, 4, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 3, 16, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(16, i2_0 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 3, 3, 1, 1, 1, 1, 1, 32, 1, 1, 1, 1, 1, 14, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 4 + i1_1)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_3])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[14:57:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 16, 16, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 14, 1, 2, 1, 4, 1, 1, 2):
                for i5_0 in T.serial(8):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 3, 16, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(64, i5_0 * 8 + ax1)
                            i2 = T.axis.spatial(16, i2_0 + ax2)
                            i3, i4 = T.axis.remap("SS", [ax3, ax4])
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 1, 1, 1, 32, 1, 1, 1, 1, 1, 14, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_0 * 4 + i1_1)
                            oh, ow = T.axis.remap("SS", [i2_0, i3_3])
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 14, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 4 + i1_1 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 16, 16, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 16, 14, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0 in T.grid(1, 4, 1, 1, 2, 8, 3, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 14, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(64, i5_0 * 8 + ax1)
                            i2 = T.axis.spatial(16, i2_0 + i6_0 + ax2)
                            i3 = T.axis.spatial(16, i7_0 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 32, 1, 1, 1, 1, 1, 14, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_0 * 4 + i1_1)
                            oh, ow = T.axis.remap("SS", [i2_0, i3_3])
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 14, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 4, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"
[14:57:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(50176,), "float32"], T_layout_trans: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans_1 = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        T_reshape = T.alloc_buffer([50176], dtype="float32")
        T_prelu = T.alloc_buffer([50176], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3])
                T_layout_trans_1[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 256 and ax2 < 14 and ax3 < 14, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0 in T.serial(50176):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(50176, i0)
                T.reads(T_layout_trans_1[0, ax0 % 50176 // 196, ax0 % 196 // 14, ax0 % 14])
                T.writes(T_reshape[ax0])
                T_reshape[ax0] = T_layout_trans_1[0, ax0 % 50176 // 196, ax0 % 196 // 14, ax0 % 14]
        for i0 in T.serial(50176):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(50176, i0)
                T.reads(T_reshape[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape[ax0], T_reshape[ax0], T_reshape[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 196 + ax2 * 14 + ax3) % 50176])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 196 + ax2 * 14 + ax3) % 50176]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 14 and ax3 < 14, T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[14:57:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(50176,), "float32"], T_layout_trans: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_layout_trans_1 = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
            T_reshape = T.alloc_buffer([50176], dtype="float32")
            T_reshape_1 = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
            for i0, i1 in T.grid(1, 64):
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 14, 14):
                    with T.block("T_layout_trans"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1 * 4 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                        T.writes(T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 256 and ax2_1 < 14 and ax3_1 < 14, placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], T.float32(0), dtype="float32")
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 14, 14):
                    for ax0_2 in T.serial(1):
                        with T.block("T_reshape"):
                            ax0_3 = T.axis.spatial(50176, i1 * 784 + ax1 * 196 + ax2 * 14 + ax3 + ax0_2)
                            T.reads(T_layout_trans_1[0, ax0_3 % 50176 // 196, ax0_3 % 196 // 14, ax0_3 % 14])
                            T.writes(T_reshape[ax0_3])
                            T_reshape[ax0_3] = T_layout_trans_1[0, ax0_3 % 50176 // 196, ax0_3 % 196 // 14, ax0_3 % 14]
                    with T.block("T_reshape_1"):
                        ax0_4 = T.axis.spatial(1, ax0)
                        ax1_2 = T.axis.spatial(256, i1 * 4 + ax1)
                        ax2_2, ax3_2 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(T_reshape[(ax1_2 * 196 + ax2_2 * 14 + ax3_2) % 50176], placeholder_1[(ax1_2 * 196 + ax2_2 * 14 + ax3_2) % 50176])
                        T.writes(T_reshape_1[ax0_4, ax1_2, ax2_2, ax3_2])
                        T_reshape_1[ax0_4, ax1_2, ax2_2, ax3_2] = T.Select(T.float32(0) < T_reshape[(ax1_2 * 196 + ax2_2 * 14 + ax3_2) % 50176], T_reshape[(ax1_2 * 196 + ax2_2 * 14 + ax3_2) % 50176], T_reshape[(ax1_2 * 196 + ax2_2 * 14 + ax3_2) % 50176] * placeholder_1[(ax1_2 * 196 + ax2_2 * 14 + ax3_2) % 50176])
                for i2, i3, i4 in T.grid(14, 14, 4):
                    with T.block("T_layout_trans_1"):
                        ax0_5, ax1_3, ax2_3, ax3_3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(T_reshape_1[ax0_5, ax1_3 * 4 + ax4, ax2_3, ax3_3])
                        T.writes(T_layout_trans[ax0_5, ax1_3, ax2_3, ax3_3, ax4])
                        T_layout_trans[ax0_5, ax1_3, ax2_3, ax3_3, ax4] = T.if_then_else(ax0_5 < 1 and ax1_3 * 4 + ax4 < 256 and ax2_3 < 14 and ax3_3 < 14, T_reshape_1[ax0_5, ax1_3 * 4 + ax4, ax2_3, ax3_3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_prelu", func_name="main")
b3 = sch.get_block(name="T_reshape_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=1)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=-2)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b1, decision=5)
sch.compute_at(block=b1, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)
[14:57:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"
[14:57:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 16, 16, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 16, 16, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 14, 14, 4, 256, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[14:57:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 16, 16, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 16, 16, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 2, 7, 1, 1, 1, 1, 2, 2, 256, 3, 3, 1, 2, 7, 1, 2, 1, 1, 1, 1, 16, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_2 * 16 + i1_3)
                    oh = T.axis.spatial(14, i2_0 * 7 + i2_2)
                    ow = T.axis.spatial(14, i3_0 * 2 + i3_1_1)
                    oc_block = T.axis.spatial(4, i4_1_1 * 2 + i4_2)
                    ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_0])
                    T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 16])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[256, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 2, 7, 1, 1, 1, 1, 2, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 3, 3, 1, 2, 7, 1, 2, 1, 1, 1, 1, 16, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_2 * 16 + i1_3)
                        oh = T.axis.spatial(14, i2_0 * 7 + i2_2)
                        ow = T.axis.spatial(14, i3_0 * 2 + i3_1)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_0])
                        T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 15 and 1 <= ow + kw and ow + kw < 15, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 1, 2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 16])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[256, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 16, 16, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 2, 7, 1):
                for i0_1, i1_1, i2_1, i3_1 in T.grid(1, 1, 1, 2):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 9, 3, 4):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(16, i2_0 * 7 + ax2)
                            i3 = T.axis.spatial(16, i3_0 * 2 + i3_1 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 256, 3, 3, 1, 2, 7, 1, 2, 1, 1, 1, 1, 16, 1, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_2 * 16 + i1_3)
                            oh = T.axis.spatial(14, i2_0 * 7 + i2_2)
                            ow = T.axis.spatial(14, i3_0 * 2 + i3_1)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 2, 4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 * 7 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 16])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[256, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #29: "fused_add_2"
[14:57:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] + placeholder_1[ax0, ax1, 0, 0, ax4]
    

[14:57:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 14, 14, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] + placeholder_1[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:57:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"
[14:57:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 16, 16, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 16, 16, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 14, 14, 4, 256, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[14:57:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 16, 16, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 16, 16, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 2, 1, 1, 2, 7, 1, 2, 32, 3, 3, 1, 1, 2, 7, 2, 8, 1, 1, 1, 32, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1_1 * 32 + i1_3)
                    oh = T.axis.spatial(14, i2_1_1 * 2 + i2_2)
                    ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                    oc_block = T.axis.spatial(4, i4_1_1 * 2 + i4_2)
                    ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                    T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 32])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[14:57:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 16, 16, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 2, 1, 1, 2, 7, 1, 2):
                for i5_0, i6_0 in T.grid(32, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 9, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(64, i5_0 * 2 + ax1)
                            i2 = T.axis.spatial(16, i2_1 * 2 + i6_0 + ax2)
                            i3 = T.axis.spatial(16, i3_0 * 7 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 2, 7, 2, 8, 1, 1, 1, 32, 1, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 32 + i1_3)
                            oh = T.axis.spatial(14, i2_1 * 2 + i2_2)
                            ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 2, 7, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + i1_1 * 32 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 32])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 16, 16, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 2, 1):
                for i0_1, i1_1 in T.grid(1, 2):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 16, 9, 4):
                        with T.block("data_pad"):
                            i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                            i3 = T.axis.spatial(16, i3_0 * 7 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(7, 1, 2, 32, 3, 3, 1, 1, 2, 7, 2, 8, 1, 1, 1, 32, 1, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 32 + i1_3)
                            oh = T.axis.spatial(14, i2_1 * 2 + i2_2)
                            ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 14, 7, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 32])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 7, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"
[14:57:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(100352,), "float32"], T_layout_trans: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans_1 = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
        T_reshape = T.alloc_buffer([100352], dtype="float32")
        T_prelu = T.alloc_buffer([100352], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3])
                T_layout_trans_1[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 512 and ax2 < 14 and ax3 < 14, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0 in T.serial(100352):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(100352, i0)
                T.reads(T_layout_trans_1[0, ax0 % 100352 // 196, ax0 % 196 // 14, ax0 % 14])
                T.writes(T_reshape[ax0])
                T_reshape[ax0] = T_layout_trans_1[0, ax0 % 100352 // 196, ax0 % 196 // 14, ax0 % 14]
        for i0 in T.serial(100352):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(100352, i0)
                T.reads(T_reshape[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape[ax0], T_reshape[ax0], T_reshape[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 196 + ax2 * 14 + ax3) % 100352])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 196 + ax2 * 14 + ax3) % 100352]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 14, 14, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 512 and ax2 < 14 and ax3 < 14, T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[14:57:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(100352,), "float32"], T_layout_trans: T.Buffer[(1, 128, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_layout_trans_1 = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
            T_reshape = T.alloc_buffer([100352], dtype="float32")
            T_reshape_1 = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
            for i0, i1, i2 in T.grid(1, 128, 14):
                for ax0, ax1 in T.grid(1, 512):
                    for ax2, ax3 in T.grid(i2 + 1, 14):
                        with T.block("T_layout_trans"):
                            ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                            ax2_1 = T.axis.spatial(14, ax2)
                            ax3_1 = T.axis.spatial(14, ax3)
                            T.reads(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                            T.writes(T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1])
                            T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 512 and ax2_1 < 14 and ax3_1 < 14, placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], T.float32(0), dtype="float32")
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 14):
                    for ax0_2 in T.serial(1):
                        with T.block("T_reshape"):
                            ax0_3 = T.axis.spatial(100352, i1 * 784 + ax1 * 196 + i2 * 14 + ax3 + ax0_2)
                            T.reads(T_layout_trans_1[0, ax0_3 % 100352 // 196, ax0_3 % 196 // 14, ax0_3 % 14])
                            T.writes(T_reshape[ax0_3])
                            T_reshape[ax0_3] = T_layout_trans_1[0, ax0_3 % 100352 // 196, ax0_3 % 196 // 14, ax0_3 % 14]
                    with T.block("T_reshape_1"):
                        ax0_4 = T.axis.spatial(1, ax0)
                        ax1_2 = T.axis.spatial(512, i1 * 4 + ax1)
                        ax2_2 = T.axis.spatial(14, i2 + ax2)
                        ax3_2 = T.axis.spatial(14, ax3)
                        T.reads(T_reshape[(ax1_2 * 196 + ax2_2 * 14 + ax3_2) % 100352], placeholder_1[(ax1_2 * 196 + ax2_2 * 14 + ax3_2) % 100352])
                        T.writes(T_reshape_1[ax0_4, ax1_2, ax2_2, ax3_2])
                        T_reshape_1[ax0_4, ax1_2, ax2_2, ax3_2] = T.Select(T.float32(0) < T_reshape[(ax1_2 * 196 + ax2_2 * 14 + ax3_2) % 100352], T_reshape[(ax1_2 * 196 + ax2_2 * 14 + ax3_2) % 100352], T_reshape[(ax1_2 * 196 + ax2_2 * 14 + ax3_2) % 100352] * placeholder_1[(ax1_2 * 196 + ax2_2 * 14 + ax3_2) % 100352])
                for i3, i4 in T.grid(14, 4):
                    with T.block("T_layout_trans_1"):
                        ax0_5, ax1_3, ax2_3, ax3_3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(T_reshape_1[ax0_5, ax1_3 * 4 + ax4, ax2_3, ax3_3])
                        T.writes(T_layout_trans[ax0_5, ax1_3, ax2_3, ax3_3, ax4])
                        T_layout_trans[ax0_5, ax1_3, ax2_3, ax3_3, ax4] = T.if_then_else(ax0_5 < 1 and ax1_3 * 4 + ax4 < 512 and ax2_3 < 14 and ax3_3 < 14, T_reshape_1[ax0_5, ax1_3 * 4 + ax4, ax2_3, ax3_3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_prelu", func_name="main")
b3 = sch.get_block(name="T_reshape_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=2)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=-2)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b1, decision=6)
sch.compute_at(block=b1, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)
[14:57:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"
[14:57:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 16, 16, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 16, 16, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 7, 7, 4, 512, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 16, 16, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 16, 16, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 1, 1, 1, 1, 1, 7, 1, 16, 1, 1, 1, 8, 1, 1, 2, 32, 3, 3, 1, 8, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_2 * 8 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_0, i3_1_1])
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                    T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 8, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 16, 16, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 2, 7):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 3, 15, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(16, i2_0 * 2 + ax2)
                        i3 = T.axis.spatial(16, ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 7, 1):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 8, 1, 1, 2, 32, 3, 3, 1, 8, 1, 1, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_2 * 8 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 1, 1, 4):
                        with T.block("T_add_1"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(128, i1_0 * 64 + ax1)
                            ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                            ax3_1 = T.axis.spatial(7, i3_1 + ax3)
                            ax4_1 = T.axis.spatial(4, ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 8, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 7, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 1, 16, 1, 1, 1, 8, 1, 1, 2, 32, 3, 3, 1, 8, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(placeholder[n, ic // 4, oh * 2 + kh - 1, ow * 2 + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh * 2 + kh and oh * 2 + kh < 15 and 1 <= ow * 2 + kw and ow * 2 + kw < 15, placeholder[n, ic // 4, oh * 2 + kh - 1, ow * 2 + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 1, 7, 4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 8, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"
[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 9, 9, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 9, 9, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 7, 7, 4, 512, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 9, 9, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 2, 1, 1, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 9, 9, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 3, 1, 2, 1, 1, 1, 4, 3, 1, 1, 16, 7, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_2 * 16 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(512, i5_0 * 4 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 2, 16])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[128, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 9, 9, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 9, 9, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 1, 1, 1, 1, 2):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 3, 1, 2, 1, 1, 1, 4, 3, 1, 1, 16, 7, 7, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_2 * 16 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            ic = T.axis.reduce(512, i5_0 * 4 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 7, 1):
                        with T.block("T_add_1"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(128, i1_0 * 32 + ax1)
                            ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                            ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 2, 16])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[128, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 9, 9, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 9, 9, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 128, 1, 3, 1, 2, 1, 1, 1, 4, 3, 1, 1, 16, 7, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_2 * 16 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(512, i5_0 * 4 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 7, 7, 2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 32 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 2, 16])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[128, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #34: "fused_add_3"
[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] + placeholder_1[ax0, ax1, 0, 0, ax4]
    

[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] + placeholder_1[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"
[14:57:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 9, 9, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 9, 9, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 7, 7, 4, 512, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[14:57:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:57:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 1, 1, 1, 1, 2, 1, 1, 1, 128, 3, 3, 1, 1, 1, 1, 2, 4, 1, 1, 1, 4, 7, 7, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_1 * 4 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(512, i5_0 * 4 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                    T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 8 and 1 <= ow + kw and ow + kw < 8, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[14:57:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 9, 9, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 16):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 9, 9, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2, 1, 1, 1):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 3, 3, 1, 1, 1, 1, 2, 4, 1, 1, 1, 4, 7, 7, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_1 * 4 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            ic = T.axis.reduce(512, i5_0 * 4 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 7, 7, 4):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(128, i1_0 * 8 + i1_1 * 4 + ax1)
                            ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 9, 9, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 16):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 9, 9, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0 in T.grid(1, 1, 1):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 128, 3, 3, 1, 1, 1, 1, 2, 4, 1, 1, 1, 4, 7, 7, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_1 * 4 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            ic = T.axis.reduce(512, i5_0 * 4 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 7, 4):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(128, i1_0 * 8 + ax1)
                            ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 4])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[14:57:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"
[14:57:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(25088,), "float32"], T_layout_trans: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans_1 = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        T_reshape = T.alloc_buffer([25088], dtype="float32")
        T_prelu = T.alloc_buffer([25088], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3])
                T_layout_trans_1[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 512 and ax2 < 7 and ax3 < 7, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0 in T.serial(25088):
            with T.block("T_reshape"):
                ax0 = T.axis.spatial(25088, i0)
                T.reads(T_layout_trans_1[0, ax0 % 25088 // 49, ax0 % 49 // 7, ax0 % 7])
                T.writes(T_reshape[ax0])
                T_reshape[ax0] = T_layout_trans_1[0, ax0 % 25088 // 49, ax0 % 49 // 7, ax0 % 7]
        for i0 in T.serial(25088):
            with T.block("T_prelu"):
                ax0 = T.axis.spatial(25088, i0)
                T.reads(T_reshape[ax0], placeholder_1[ax0])
                T.writes(T_prelu[ax0])
                T_prelu[ax0] = T.Select(T.float32(0) < T_reshape[ax0], T_reshape[ax0], T_reshape[ax0] * placeholder_1[ax0])
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_prelu[(ax1 * 49 + ax2 * 7 + ax3) % 25088])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_prelu[(ax1 * 49 + ax2 * 7 + ax3) % 25088]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 512 and ax2 < 7 and ax3 < 7, T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[14:57:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:57:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(25088,), "float32"], T_layout_trans: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_reshape = T.alloc_buffer([25088], dtype="float32")
            T_prelu = T.alloc_buffer([25088], dtype="float32")
            T_reshape_1 = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
            for i0, i1 in T.grid(1, 128):
                for ax0 in T.serial(25088):
                    with T.block("T_reshape"):
                        ax0_1 = T.axis.spatial(25088, ax0)
                        T.reads(placeholder[0, ax0_1 % 25088 // 196, ax0_1 % 49 // 7, ax0_1 % 7, ax0_1 % 196 // 49])
                        T.writes(T_reshape[ax0_1])
                        T_reshape[ax0_1] = T.if_then_else(0 < 1 and ax0_1 % 25088 // 49 < 512 and ax0_1 % 49 // 7 < 7 and ax0_1 % 7 < 7, placeholder[0, ax0_1 % 25088 // 49 // 4, ax0_1 % 49 // 7, ax0_1 % 7, ax0_1 % 25088 // 49 % 4], T.float32(0), dtype="float32")
                for i2 in T.serial(7):
                    for ax0 in T.serial(25088):
                        with T.block("T_prelu"):
                            ax0_2 = T.axis.spatial(25088, ax0)
                            T.reads(T_reshape[ax0_2], placeholder_1[ax0_2])
                            T.writes(T_prelu[ax0_2])
                            T_prelu[ax0_2] = T.Select(T.float32(0) < T_reshape[ax0_2], T_reshape[ax0_2], T_reshape[ax0_2] * placeholder_1[ax0_2])
                    for ax0_3, ax1, ax2, ax3 in T.grid(1, 4, 1, 7):
                        with T.block("T_reshape_1"):
                            ax0_4 = T.axis.spatial(1, ax0_3)
                            ax1_1 = T.axis.spatial(512, i1 * 4 + ax1)
                            ax2_1 = T.axis.spatial(7, i2 + ax2)
                            ax3_1 = T.axis.spatial(7, ax3)
                            T.reads(T_prelu[(ax1_1 * 49 + ax2_1 * 7 + ax3_1) % 25088])
                            T.writes(T_reshape_1[ax0_4, ax1_1, ax2_1, ax3_1])
                            T_reshape_1[ax0_4, ax1_1, ax2_1, ax3_1] = T_prelu[(ax1_1 * 49 + ax2_1 * 7 + ax3_1) % 25088]
                    for i3, i4 in T.grid(7, 4):
                        with T.block("T_layout_trans_1"):
                            ax0_5, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                            T.reads(T_reshape_1[ax0_5, ax1 * 4 + ax4, ax2, ax3])
                            T.writes(T_layout_trans[ax0_5, ax1, ax2, ax3, ax4])
                            T_layout_trans[ax0_5, ax1, ax2, ax3, ax4] = T.if_then_else(ax0_5 < 1 and ax1 * 4 + ax4 < 512 and ax2 < 7 and ax3 < 7, T_reshape_1[ax0_5, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_prelu", func_name="main")
b3 = sch.get_block(name="T_reshape_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=2)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=2)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)
[14:57:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"
[14:57:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_4: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_5: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 9, 9, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        T_add_2 = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 9, 9, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 7, 7, 4, 512, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add_2[ax0, ax1, ax2, ax3, ax4])
                T_add_2[ax0, ax1, ax2, ax3, ax4] = T_add_1[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_2[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = T_add_2[ax0, ax1, ax2, ax3, ax4] * placeholder_4[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
            with T.block("T_add_2"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1, ax2, ax3, ax4], placeholder_5[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = T_multiply[ax0, ax1, ax2, ax3, ax4] + placeholder_5[ax0, ax1, 0, 0, ax4]
    

[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_4: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_5: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 1, 1, 1, 2, 1, 1, 2, 32, 1, 3, 1, 1, 1, 7, 1, 16, 3, 1, 1, 32, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 32 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_0, i3_2])
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(512, i5_0 * 16 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                    T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 8 and 1 <= ow + kw and ow + kw < 8, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 7, 7, 4):
                with T.block("T_add_2"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, ax2, ax3, ax4], placeholder_4[ax0, ax1, 0, 0, ax4], placeholder_5[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, ax2, ax3, ax4]) * placeholder_4[ax0, ax1, 0, 0, ax4] + placeholder_5[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l18, l19, l20, l21 = sch.split(loop=l6, factors=[v14, v15, v16, v17])
v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 32])
l26, l27, l28, l29 = sch.split(loop=l7, factors=[v22, v23, v24, v25])
v30, v31, v32, v33 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l34, l35, l36, l37 = sch.split(loop=l8, factors=[v30, v31, v32, v33])
v38, v39, v40, v41 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l42, l43, l44, l45 = sch.split(loop=l9, factors=[v38, v39, v40, v41])
v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l50, l51, l52, l53 = sch.split(loop=l10, factors=[v46, v47, v48, v49])
v54, v55 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[32, 16])
l56, l57 = sch.split(loop=l11, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1, 3])
l60, l61 = sch.split(loop=l12, factors=[v58, v59])
v62, v63 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[3, 1])
l64, l65 = sch.split(loop=l13, factors=[v62, v63])
sch.reorder(l18, l26, l34, l42, l50, l19, l27, l35, l43, l51, l56, l60, l64, l20, l28, l36, l44, l52, l57, l61, l65, l21, l29, l37, l45, l53)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v66 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v66)
l67 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l67, preserve_unit_loops=True)
[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_4: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_5: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 9, 9, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 9, 9, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(7, 1, 1, 1, 2, 1, 1, 2):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 3, 1, 1, 1, 7, 1, 16, 3, 1, 1, 32, 1, 1, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 32 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_0, i3_2])
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                            ic = T.axis.reduce(512, i5_0 * 16 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 1, 7, 2):
                        with T.block("T_add_2"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(128, i1_0 * 64 + i1_1 * 32 + ax1)
                            ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                            ax3_1 = T.axis.spatial(7, ax3)
                            ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_4[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_5[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]) * placeholder_4[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_5[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l18, l19, l20, l21 = sch.split(loop=l6, factors=[v14, v15, v16, v17])
v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 32])
l26, l27, l28, l29 = sch.split(loop=l7, factors=[v22, v23, v24, v25])
v30, v31, v32, v33 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l34, l35, l36, l37 = sch.split(loop=l8, factors=[v30, v31, v32, v33])
v38, v39, v40, v41 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l42, l43, l44, l45 = sch.split(loop=l9, factors=[v38, v39, v40, v41])
v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l50, l51, l52, l53 = sch.split(loop=l10, factors=[v46, v47, v48, v49])
v54, v55 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[32, 16])
l56, l57 = sch.split(loop=l11, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1, 3])
l60, l61 = sch.split(loop=l12, factors=[v58, v59])
v62, v63 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[3, 1])
l64, l65 = sch.split(loop=l13, factors=[v62, v63])
sch.reorder(l18, l26, l34, l42, l50, l19, l27, l35, l43, l51, l56, l60, l64, l20, l28, l36, l44, l52, l57, l61, l65, l21, l29, l37, l45, l53)
b66, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b66, loop=l51, preserve_unit_loops=True)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v67 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v67)
l68 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l68, preserve_unit_loops=True)
[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_4: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_5: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 9, 9, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 9, 9, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 7, 1, 1):
                for i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 2, 32, 1, 3, 1, 1, 1, 7, 1, 16, 3, 1, 1, 32, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1_1 * 32 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_2])
                        oc_block = T.axis.spatial(4, i4_1_1 * 2 + i4_3)
                        ic = T.axis.reduce(512, i5_0 * 16 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 1, 7, 4):
                    with T.block("T_add_2"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_4[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_5[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]) * placeholder_4[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_5[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l18, l19, l20, l21 = sch.split(loop=l6, factors=[v14, v15, v16, v17])
v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 32])
l26, l27, l28, l29 = sch.split(loop=l7, factors=[v22, v23, v24, v25])
v30, v31, v32, v33 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l34, l35, l36, l37 = sch.split(loop=l8, factors=[v30, v31, v32, v33])
v38, v39, v40, v41 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l42, l43, l44, l45 = sch.split(loop=l9, factors=[v38, v39, v40, v41])
v46, v47, v48, v49 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l50, l51, l52, l53 = sch.split(loop=l10, factors=[v46, v47, v48, v49])
v54, v55 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[32, 16])
l56, l57 = sch.split(loop=l11, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1, 3])
l60, l61 = sch.split(loop=l12, factors=[v58, v59])
v62, v63 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[3, 1])
l64, l65 = sch.split(loop=l13, factors=[v62, v63])
sch.reorder(l18, l26, l34, l42, l50, l19, l27, l35, l43, l51, l56, l60, l64, l20, l28, l36, l44, l52, l57, l61, l65, l21, l29, l37, l45, l53)
b66, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b66, loop=l50, preserve_unit_loops=True)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v67 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v67)
l68 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l68, preserve_unit_loops=True)
[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #38: "fused_layout_transform_nn_batch_flatten"
[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 25088), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 512 and ax2 < 7 and ax3 < 7, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1 in T.grid(1, 25088):
            with T.block("tensor"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_layout_trans[ax0, ax1 % 25088 // 49, ax1 % 49 // 7, ax1 % 7])
                T.writes(tensor[ax0, ax1])
                tensor[ax0, ax1] = T_layout_trans[ax0, ax1 % 25088 // 49, ax1 % 49 // 7, ax1 % 7]
    

[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], tensor: T.Buffer[(1, 25088), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 512 and ax2 < 7 and ax3 < 7, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
            for i0, i1 in T.grid(1, 25088):
                with T.block("tensor"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_layout_trans[ax0, ax1 % 25088 // 49, ax1 % 49 // 7, ax1 % 7])
                    T.writes(tensor[ax0, ax1])
                    tensor[ax0, ax1] = T_layout_trans[ax0, ax1 % 25088 // 49, ax1 % 49 // 7, ax1 % 7]
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #39: "fused_nn_dense_add"
[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 25088), "float32"], placeholder_1: T.Buffer[(512, 25088), "float32"], placeholder_2: T.Buffer[(1, 512), "float32"], T_add: T.Buffer[(1, 512), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 512], dtype="float32")
        for i0, i1, i2 in T.grid(1, 512, 25088):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0, i1 in T.grid(1, 512):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 25088), "float32"], placeholder_1: T.Buffer[(512, 25088), "float32"], placeholder_2: T.Buffer[(1, 512), "float32"], T_add: T.Buffer[(1, 512), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 512], dtype="float32")
            for i0_0, i1_0, i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 1, 1, 16, 3136, 1, 1, 8, 1, 32):
                with T.block("T_matmul_NT"):
                    i = T.axis.spatial(1, 0)
                    j = T.axis.spatial(512, i1_1 * 32 + i1_3)
                    k = T.axis.reduce(25088, i2_0 * 8 + i2_1)
                    T.reads(placeholder[i, k], placeholder_1[j, k])
                    T.writes(T_matmul_NT[i, j])
                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        T_matmul_NT[i, j] = T.float32(0)
                    T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
            for i0, i1 in T.grid(1, 512):
                with T.block("T_add"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                    T.writes(T_add[ax0, ax1])
                    T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 16, 1, 32])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[3136, 8])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v25 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v25)
[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 25088), "float32"], placeholder_1: T.Buffer[(512, 25088), "float32"], placeholder_2: T.Buffer[(1, 512), "float32"], T_add: T.Buffer[(1, 512), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 512], dtype="float32")
            for i0_0, i1_0, i0_1, i1_1 in T.grid(1, 1, 1, 16):
                for i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(3136, 1, 1, 8, 1, 32):
                    with T.block("T_matmul_NT"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(512, i1_1 * 32 + i1_3)
                        k = T.axis.reduce(25088, i2_0 * 8 + i2_1)
                        T.reads(placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
                for ax0, ax1 in T.grid(1, 32):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(512, i1_1 * 32 + ax1)
                        T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                        T.writes(T_add[ax0_1, ax1_1])
                        T_add[ax0_1, ax1_1] = T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 16, 1, 32])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[3136, 8])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
b25, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b25, loop=l18, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 25088), "float32"], placeholder_1: T.Buffer[(512, 25088), "float32"], placeholder_2: T.Buffer[(1, 512), "float32"], T_add: T.Buffer[(1, 512), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 512], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 1):
                for i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 16, 3136, 1, 1, 8, 1, 32):
                    with T.block("T_matmul_NT"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(512, i1_1 * 32 + i1_3)
                        k = T.axis.reduce(25088, i2_0 * 8 + i2_1)
                        T.reads(placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
                for ax0, ax1 in T.grid(1, 512):
                    with T.block("T_add"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                        T.writes(T_add[ax0_1, ax1_1])
                        T_add[ax0_1, ax1_1] = T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 16, 1, 32])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[3136, 8])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
b25, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b25, loop=l17, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                                             fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_nn_contrib_conv2d_NCHWc_add"
[14:58:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:58:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[14:58:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"
[14:58:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:58:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[14:59:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"
[14:59:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:59:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[14:59:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_layout_transform"
[14:59:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:59:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[14:59:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"
[14:59:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:00:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:00:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_copy_subtract_multiply_layout_transform"
[15:00:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:00:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:00:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"
[15:00:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:01:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:01:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"
[15:01:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:01:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:01:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_add_layout_transform"
[15:01:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:01:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:02:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"
[15:02:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:02:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:02:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"
[15:02:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:02:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:05:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"
[15:05:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:05:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:05:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"
[15:05:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:05:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:06:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"
[15:06:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:07:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"
[15:07:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:07:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:07:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #15: "fused_add"
[15:07:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:07:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:08:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"
[15:08:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:08:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:08:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"
[15:08:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:09:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:10:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"
[15:10:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:10:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:11:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"
[15:11:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:11:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:11:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"
[15:11:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:11:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:12:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"
[15:12:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:12:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:12:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #22: "fused_add_1"
[15:12:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:12:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:12:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"
[15:12:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:13:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:13:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"
[15:13:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:13:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:13:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"
[15:13:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:14:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"
[15:14:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:14:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:14:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"
[15:14:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:14:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:14:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"
[15:14:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:15:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:15:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #29: "fused_add_2"
[15:15:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:15:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:15:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"
[15:15:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:15:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:16:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"
[15:16:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:16:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:16:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"
[15:16:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:16:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:17:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"
[15:17:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:17:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:17:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #34: "fused_add_3"
[15:17:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:17:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:18:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"
[15:18:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:18:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:18:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"
[15:18:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:18:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:18:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"
[15:18:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:19:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:19:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #38: "fused_layout_transform_nn_batch_flatten"
[15:19:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:19:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:19:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #39: "fused_nn_dense_add"
[15:19:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:19:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #0: GFLOPs: 10.7281. Time: 1.1997 ms. Best GFLOPs: 10.7281
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #1: GFLOPs: 119.9741. Time: 0.1073 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #2: GFLOPs: 56.9877. Time: 0.2258 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #3: GFLOPs: 62.1697. Time: 0.2070 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #4: GFLOPs: 67.4532. Time: 0.1908 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #5: GFLOPs: 70.7934. Time: 0.1818 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #6: GFLOPs: 22.9550. Time: 0.5607 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #7: GFLOPs: 61.1714. Time: 0.2104 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #8: GFLOPs: 12.0748. Time: 1.0659 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #9: GFLOPs: 47.4227. Time: 0.2714 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #10: GFLOPs: 82.8856. Time: 0.1553 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #11: GFLOPs: 18.5167. Time: 0.6951 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #12: GFLOPs: 5.2683. Time: 2.4429 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #13: GFLOPs: 29.9933. Time: 0.4291 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #14: GFLOPs: 31.6566. Time: 0.4066 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #15: GFLOPs: 2.3616. Time: 5.4497 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #16: GFLOPs: 36.8691. Time: 0.3491 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #17: GFLOPs: 10.1747. Time: 1.2649 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #18: GFLOPs: 35.3735. Time: 0.3638 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #19: GFLOPs: 7.1229. Time: 1.8069 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #20: GFLOPs: 27.1977. Time: 0.4732 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #21: GFLOPs: 8.7348. Time: 1.4734 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #22: GFLOPs: 22.9529. Time: 0.5607 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #23: GFLOPs: 108.0205. Time: 0.1191 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #24: GFLOPs: 5.7652. Time: 2.2324 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #25: GFLOPs: 11.4439. Time: 1.1246 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #26: GFLOPs: 9.7307. Time: 1.3226 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #27: GFLOPs: 10.8317. Time: 1.1882 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #28: GFLOPs: 61.3121. Time: 0.2099 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #29: GFLOPs: 29.4248. Time: 0.4374 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #30: GFLOPs: 6.3688. Time: 2.0208 ms. Best GFLOPs: 119.9741
[15:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #31: GFLOPs: 6.2673. Time: 2.0536 ms. Best GFLOPs: 119.9741
/home/yj/anaconda3/lib/python3.9/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
[15:20:05] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_contrib_conv2d_NCHWc_add"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                                             fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 107.274

[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #0: GFLOPs: 4.3599. Time: 2.9577 ms. Best GFLOPs: 4.3599
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #1: GFLOPs: 3.9332. Time: 3.2786 ms. Best GFLOPs: 4.3599
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #2: GFLOPs: 4.6566. Time: 2.7692 ms. Best GFLOPs: 4.6566
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #3: GFLOPs: 19.3771. Time: 0.6655 ms. Best GFLOPs: 19.3771
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #4: GFLOPs: 8.5815. Time: 1.5027 ms. Best GFLOPs: 19.3771
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #5: GFLOPs: 24.1095. Time: 0.5349 ms. Best GFLOPs: 24.1095
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #6: GFLOPs: 13.6602. Time: 0.9440 ms. Best GFLOPs: 24.1095
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #7: GFLOPs: 17.4657. Time: 0.7383 ms. Best GFLOPs: 24.1095
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #8: GFLOPs: 19.1244. Time: 0.6743 ms. Best GFLOPs: 24.1095
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #9: GFLOPs: 20.9945. Time: 0.6142 ms. Best GFLOPs: 24.1095
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #10: GFLOPs: 16.2599. Time: 0.7931 ms. Best GFLOPs: 24.1095
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #11: GFLOPs: 40.2442. Time: 0.3204 ms. Best GFLOPs: 40.2442
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #12: GFLOPs: 35.0861. Time: 0.3675 ms. Best GFLOPs: 40.2442
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #13: GFLOPs: 43.5576. Time: 0.2961 ms. Best GFLOPs: 43.5576
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #14: GFLOPs: 33.3054. Time: 0.3872 ms. Best GFLOPs: 43.5576
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #15: GFLOPs: 38.4212. Time: 0.3356 ms. Best GFLOPs: 43.5576
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #16: GFLOPs: 61.1458. Time: 0.2109 ms. Best GFLOPs: 61.1458
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #17: GFLOPs: 39.9507. Time: 0.3228 ms. Best GFLOPs: 61.1458
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #18: GFLOPs: 27.9003. Time: 0.4622 ms. Best GFLOPs: 61.1458
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #19: GFLOPs: 45.6522. Time: 0.2825 ms. Best GFLOPs: 61.1458
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #20: GFLOPs: 56.3861. Time: 0.2287 ms. Best GFLOPs: 61.1458
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #21: GFLOPs: 71.1961. Time: 0.1811 ms. Best GFLOPs: 71.1961
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #22: GFLOPs: 1.0538. Time: 12.2365 ms. Best GFLOPs: 71.1961
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #23: GFLOPs: 20.2987. Time: 0.6353 ms. Best GFLOPs: 71.1961
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #24: GFLOPs: 1.7857. Time: 7.2213 ms. Best GFLOPs: 71.1961
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #25: GFLOPs: 8.3796. Time: 1.5389 ms. Best GFLOPs: 71.1961
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #26: GFLOPs: 21.3557. Time: 0.6038 ms. Best GFLOPs: 71.1961
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #27: GFLOPs: 71.1017. Time: 0.1814 ms. Best GFLOPs: 71.1961
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #28: GFLOPs: 54.0004. Time: 0.2388 ms. Best GFLOPs: 71.1961
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(14, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(2, 4, 64, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk, oh = T.axis.remap("SS", [i1_3_init, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused])
                    ow = T.axis.spatial(14, i3_2_init * 7 + i3_3_init)
                    oc_block = T.axis.spatial(4, i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 1, 1, 2, 4, 1, 1, 1, 1, 64, 1, 7, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk, oh = T.axis.remap("SS", [i1_3, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused])
                    ow = T.axis.spatial(14, i3_2 * 7 + i3_3)
                    oc_block, ic = T.axis.remap("SR", [i4_2, i5_0])
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [64, 32, 1, 1, 4, 4], "float32"], [2, 2], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1 in T.grid(1, 64):
                for ax2_ax3_ax4_fused in T.vectorized(56):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2 = T.axis.remap("SS", [ax1, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused])
                        ax3 = T.axis.spatial(14, ax2_ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax2_ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 64])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l97, l98, l99)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #30: GFLOPs: 45.8736. Time: 0.2811 ms. Best GFLOPs: 71.1961
[15:20:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #31: GFLOPs: 0.7067. Time: 18.2464 ms. Best GFLOPs: 71.1961
[15:20:06] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                                             fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 64
Total latency (us): 288.397

[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #0: GFLOPs: 2.0125. Time: 6.4324 ms. Best GFLOPs: 2.0125
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #1: GFLOPs: 4.8436. Time: 2.6727 ms. Best GFLOPs: 4.8436
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #2: GFLOPs: 5.3786. Time: 2.4068 ms. Best GFLOPs: 5.3786
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #3: GFLOPs: 14.7198. Time: 0.8795 ms. Best GFLOPs: 14.7198
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #4: GFLOPs: 6.1001. Time: 2.1222 ms. Best GFLOPs: 14.7198
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #5: GFLOPs: 13.2615. Time: 0.9762 ms. Best GFLOPs: 14.7198
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #6: GFLOPs: 25.8462. Time: 0.5009 ms. Best GFLOPs: 25.8462
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #7: GFLOPs: 32.4925. Time: 0.3984 ms. Best GFLOPs: 32.4925
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #8: GFLOPs: 11.7602. Time: 1.1008 ms. Best GFLOPs: 32.4925
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #9: GFLOPs: 56.2440. Time: 0.2302 ms. Best GFLOPs: 56.2440
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #10: GFLOPs: 31.2857. Time: 0.4138 ms. Best GFLOPs: 56.2440
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #11: GFLOPs: 24.9224. Time: 0.5194 ms. Best GFLOPs: 56.2440
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #12: GFLOPs: 38.4733. Time: 0.3365 ms. Best GFLOPs: 56.2440
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #13: GFLOPs: 55.5718. Time: 0.2329 ms. Best GFLOPs: 56.2440
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #14: GFLOPs: 77.9595. Time: 0.1661 ms. Best GFLOPs: 77.9595
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #15: GFLOPs: 71.8330. Time: 0.1802 ms. Best GFLOPs: 77.9595
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #16: GFLOPs: 25.3491. Time: 0.5107 ms. Best GFLOPs: 77.9595
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #17: GFLOPs: 138.8828. Time: 0.0932 ms. Best GFLOPs: 138.8828
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #18: GFLOPs: 6.2658. Time: 2.0661 ms. Best GFLOPs: 138.8828
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #19: GFLOPs: 34.2698. Time: 0.3777 ms. Best GFLOPs: 138.8828
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #20: GFLOPs: 6.8531. Time: 1.8890 ms. Best GFLOPs: 138.8828
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #21: GFLOPs: 106.6307. Time: 0.1214 ms. Best GFLOPs: 138.8828
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #22: GFLOPs: 51.3821. Time: 0.2519 ms. Best GFLOPs: 138.8828
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #23: GFLOPs: 48.0371. Time: 0.2695 ms. Best GFLOPs: 138.8828
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #24: GFLOPs: 108.1616. Time: 0.1197 ms. Best GFLOPs: 138.8828
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #25: GFLOPs: 66.9069. Time: 0.1935 ms. Best GFLOPs: 138.8828
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #26: GFLOPs: 177.7715. Time: 0.0728 ms. Best GFLOPs: 177.7715
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #27: GFLOPs: 37.0261. Time: 0.3496 ms. Best GFLOPs: 177.7715
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #28: GFLOPs: 58.4391. Time: 0.2215 ms. Best GFLOPs: 177.7715
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #29: GFLOPs: 53.8192. Time: 0.2405 ms. Best GFLOPs: 177.7715
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #30: GFLOPs: 59.5251. Time: 0.2175 ms. Best GFLOPs: 177.7715
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #31: GFLOPs: 54.8997. Time: 0.2358 ms. Best GFLOPs: 177.7715
[15:20:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 96
Total latency (us): 361.218

[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #0: GFLOPs: 0.0000. Time: 0.0227 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #1: GFLOPs: 0.0000. Time: 0.0238 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #2: GFLOPs: 0.0000. Time: 0.0216 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #3: GFLOPs: 0.0000. Time: 0.0465 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #4: GFLOPs: 0.0000. Time: 0.0213 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #5: GFLOPs: 0.0000. Time: 0.0248 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #6: GFLOPs: 0.0000. Time: 0.0488 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #7: GFLOPs: 0.0000. Time: 0.0476 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #8: GFLOPs: 0.0000. Time: 0.0415 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #9: GFLOPs: 0.0000. Time: 0.0223 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #10: GFLOPs: 0.0000. Time: 0.0217 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #11: GFLOPs: 0.0000. Time: 0.0446 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #12: GFLOPs: 0.0000. Time: 0.0212 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #13: GFLOPs: 0.0000. Time: 0.0212 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #14: GFLOPs: 0.0000. Time: 0.0463 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #15: GFLOPs: 0.0000. Time: 0.0235 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #16: GFLOPs: 0.0000. Time: 0.3621 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #17: GFLOPs: 0.0000. Time: 0.0212 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #18: GFLOPs: 0.0000. Time: 1.1747 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #19: GFLOPs: 0.0000. Time: 0.3908 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #20: GFLOPs: 0.0000. Time: 0.5587 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #21: GFLOPs: 0.0000. Time: 0.2465 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #22: GFLOPs: 0.0000. Time: 0.1915 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #23: GFLOPs: 0.0000. Time: 0.2061 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #24: GFLOPs: 0.0000. Time: 0.1243 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #25: GFLOPs: 0.0000. Time: 0.1308 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #26: GFLOPs: 0.0000. Time: 0.0744 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #27: GFLOPs: 0.0000. Time: 0.1363 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #28: GFLOPs: 0.0000. Time: 0.0744 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #29: GFLOPs: 0.0000. Time: 0.0708 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #30: GFLOPs: 0.0000. Time: 0.0752 ms. Best GFLOPs: 0.0000
[15:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform"] Trial #31: GFLOPs: 0.0000. Time: 0.0655 ms. Best GFLOPs: 0.0000
[15:20:08] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_layout_transform"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 128
Total latency (us): 382.37

[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #0: GFLOPs: 53.6245. Time: 0.4828 ms. Best GFLOPs: 53.6245
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #1: GFLOPs: 43.1622. Time: 0.5998 ms. Best GFLOPs: 53.6245
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #2: GFLOPs: 46.7469. Time: 0.5539 ms. Best GFLOPs: 53.6245
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #3: GFLOPs: 59.1586. Time: 0.4377 ms. Best GFLOPs: 59.1586
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #4: GFLOPs: 26.9451. Time: 0.9609 ms. Best GFLOPs: 59.1586
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #5: GFLOPs: 59.9113. Time: 0.4322 ms. Best GFLOPs: 59.9113
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #6: GFLOPs: 51.9991. Time: 0.4979 ms. Best GFLOPs: 59.9113
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #7: GFLOPs: 22.5985. Time: 1.1457 ms. Best GFLOPs: 59.9113
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #8: GFLOPs: 28.6392. Time: 0.9040 ms. Best GFLOPs: 59.9113
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #9: GFLOPs: 43.3539. Time: 0.5972 ms. Best GFLOPs: 59.9113
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #10: GFLOPs: 66.4724. Time: 0.3895 ms. Best GFLOPs: 66.4724
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #11: GFLOPs: 40.0351. Time: 0.6467 ms. Best GFLOPs: 66.4724
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #12: GFLOPs: 37.7168. Time: 0.6865 ms. Best GFLOPs: 66.4724
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #13: GFLOPs: 56.3620. Time: 0.4594 ms. Best GFLOPs: 66.4724
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #14: GFLOPs: 93.6708. Time: 0.2764 ms. Best GFLOPs: 93.6708
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #15: GFLOPs: 137.5179. Time: 0.1883 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #16: GFLOPs: 94.3128. Time: 0.2745 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #17: GFLOPs: 51.5749. Time: 0.5020 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #18: GFLOPs: 45.4582. Time: 0.5696 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #19: GFLOPs: 22.2335. Time: 1.1645 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #20: GFLOPs: 69.6337. Time: 0.3718 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #21: GFLOPs: 47.7061. Time: 0.5427 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #22: GFLOPs: 58.3295. Time: 0.4439 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #23: GFLOPs: 34.6125. Time: 0.7480 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #24: GFLOPs: 23.1585. Time: 1.1180 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #25: GFLOPs: 30.6822. Time: 0.8438 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #26: GFLOPs: 69.3986. Time: 0.3731 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #27: GFLOPs: 21.6405. Time: 1.1964 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #28: GFLOPs: 2.2430. Time: 11.5429 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #29: GFLOPs: 0.6316. Time: 40.9912 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #30: GFLOPs: 2.8273. Time: 9.1575 ms. Best GFLOPs: 137.5179
[15:20:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #31: GFLOPs: 39.3348. Time: 0.6582 ms. Best GFLOPs: 137.5179
[15:20:09] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 160
Total latency (us): 570.642

[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #0: GFLOPs: 9.1083. Time: 0.0083 ms. Best GFLOPs: 9.1083
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #1: GFLOPs: 9.5312. Time: 0.0079 ms. Best GFLOPs: 9.5312
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #2: GFLOPs: 9.1804. Time: 0.0082 ms. Best GFLOPs: 9.5312
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #3: GFLOPs: 9.7082. Time: 0.0078 ms. Best GFLOPs: 9.7082
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #4: GFLOPs: 9.4651. Time: 0.0080 ms. Best GFLOPs: 9.7082
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #5: GFLOPs: 9.7121. Time: 0.0077 ms. Best GFLOPs: 9.7121
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #6: GFLOPs: 9.4245. Time: 0.0080 ms. Best GFLOPs: 9.7121
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #7: GFLOPs: 9.5236. Time: 0.0079 ms. Best GFLOPs: 9.7121
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #8: GFLOPs: 9.1387. Time: 0.0082 ms. Best GFLOPs: 9.7121
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #9: GFLOPs: 9.6020. Time: 0.0078 ms. Best GFLOPs: 9.7121
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #10: GFLOPs: 9.2298. Time: 0.0082 ms. Best GFLOPs: 9.7121
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #11: GFLOPs: 9.3173. Time: 0.0081 ms. Best GFLOPs: 9.7121
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #12: GFLOPs: 9.5473. Time: 0.0079 ms. Best GFLOPs: 9.7121
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #13: GFLOPs: 8.9654. Time: 0.0084 ms. Best GFLOPs: 9.7121
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #14: GFLOPs: 9.8567. Time: 0.0076 ms. Best GFLOPs: 9.8567
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #15: GFLOPs: 8.8080. Time: 0.0085 ms. Best GFLOPs: 9.8567
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #16: GFLOPs: 9.5382. Time: 0.0079 ms. Best GFLOPs: 9.8567
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #17: GFLOPs: 9.0445. Time: 0.0083 ms. Best GFLOPs: 9.8567
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #18: GFLOPs: 10.0563. Time: 0.0075 ms. Best GFLOPs: 10.0563
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #19: GFLOPs: 9.5442. Time: 0.0079 ms. Best GFLOPs: 10.0563
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #20: GFLOPs: 9.1804. Time: 0.0082 ms. Best GFLOPs: 10.0563
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #21: GFLOPs: 9.6114. Time: 0.0078 ms. Best GFLOPs: 10.0563
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #22: GFLOPs: 9.4626. Time: 0.0080 ms. Best GFLOPs: 10.0563
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #23: GFLOPs: 10.4996. Time: 0.0072 ms. Best GFLOPs: 10.4996
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #24: GFLOPs: 9.7800. Time: 0.0077 ms. Best GFLOPs: 10.4996
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #25: GFLOPs: 9.5177. Time: 0.0079 ms. Best GFLOPs: 10.4996
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #26: GFLOPs: 10.1675. Time: 0.0074 ms. Best GFLOPs: 10.4996
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #27: GFLOPs: 9.9859. Time: 0.0075 ms. Best GFLOPs: 10.4996
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #28: GFLOPs: 9.6964. Time: 0.0078 ms. Best GFLOPs: 10.4996
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #29: GFLOPs: 9.3761. Time: 0.0080 ms. Best GFLOPs: 10.4996
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #30: GFLOPs: 9.2911. Time: 0.0081 ms. Best GFLOPs: 10.4996
[15:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_copy_subtract_multiply_layout_transform"] Trial #31: GFLOPs: 9.6744. Time: 0.0078 ms. Best GFLOPs: 10.4996
[15:20:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_copy_subtract_multiply_layout_transform"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 192
Total latency (us): 577.81

[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #0: GFLOPs: 20.0918. Time: 2.1977 ms. Best GFLOPs: 20.0918
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #1: GFLOPs: 15.3338. Time: 2.8796 ms. Best GFLOPs: 20.0918
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #2: GFLOPs: 17.7530. Time: 2.4872 ms. Best GFLOPs: 20.0918
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #3: GFLOPs: 14.6714. Time: 3.0096 ms. Best GFLOPs: 20.0918
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #4: GFLOPs: 50.6841. Time: 0.8712 ms. Best GFLOPs: 50.6841
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #5: GFLOPs: 32.1260. Time: 1.3744 ms. Best GFLOPs: 50.6841
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #6: GFLOPs: 21.0248. Time: 2.1001 ms. Best GFLOPs: 50.6841
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #7: GFLOPs: 72.4252. Time: 0.6097 ms. Best GFLOPs: 72.4252
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #8: GFLOPs: 10.5495. Time: 4.1855 ms. Best GFLOPs: 72.4252
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #9: GFLOPs: 30.9658. Time: 1.4259 ms. Best GFLOPs: 72.4252
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #10: GFLOPs: 9.8354. Time: 4.4894 ms. Best GFLOPs: 72.4252
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #11: GFLOPs: 55.2682. Time: 0.7989 ms. Best GFLOPs: 72.4252
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #12: GFLOPs: 82.4462. Time: 0.5356 ms. Best GFLOPs: 82.4462
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #13: GFLOPs: 16.2283. Time: 2.7208 ms. Best GFLOPs: 82.4462
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #14: GFLOPs: 18.7782. Time: 2.3514 ms. Best GFLOPs: 82.4462
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #15: GFLOPs: 44.1514. Time: 1.0001 ms. Best GFLOPs: 82.4462
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 112, 112, 3), "float32"], placeholder_1: T.Buffer[(16, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 114, 114, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(7, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 18, 114):
                for ax4_fused in T.vectorized(3):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_fused % 7 * 16 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 4, 1, 1, 2, 4, 1):
                for i1_2_init, i2_2_init, i3_2_init, i1_3_init, i3_3_init in T.grid(4, 8, 7, 4, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_2_init * 4 + i1_3_init)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_fused * 16 + i2_1 * 8 + i2_2_init)
                        ow = T.axis.spatial(112, i3_0 * 56 + i3_1 * 14 + i3_2_init * 2 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_0)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 112, 112, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 3, 1, 4, 8, 7, 1, 1, 1, 1, 1, 4, 1, 2, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(112, i0_0_i1_0_i2_0_fused * 16 + i2_1 * 8 + i2_2)
                        ow = T.axis.spatial(112, i3_0 * 56 + i3_1 * 14 + i3_2 * 2 + i3_3)
                        oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_0, i5_0, i6_0, i7_0])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 112, 112, 3], "float32"], ["TENSOR", [16, 1, 3, 3, 3, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 112)
                        ax2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 2, 8, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 4, 7, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67, b68 = sch.get_child_blocks(b65)
l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b66)
l77 = sch.fuse(l69, l70, l71)
sch.parallel(loop=l77)
l78 = sch.fuse(l76)
sch.vectorize(loop=l78)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l103, l104, l105, l106, l107 = sch.get_loops(block=b68)
l108 = sch.fuse(l103, l104, l105)
sch.parallel(loop=l108)
l109 = sch.fuse(l107)
sch.vectorize(loop=l109)
sch.annotate(block_or_loop=l108, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l108, ann_key="pragma_unroll_explicit", ann_val=1)
b110 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b110)
b135 = sch.decompose_reduction(block=b110, loop=l119)
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #17: GFLOPs: 43.7333. Time: 1.0096 ms. Best GFLOPs: 82.4462
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #18: GFLOPs: 72.9398. Time: 0.6054 ms. Best GFLOPs: 82.4462
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #19: GFLOPs: 24.3064. Time: 1.8166 ms. Best GFLOPs: 82.4462
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #20: GFLOPs: 74.4256. Time: 0.5933 ms. Best GFLOPs: 82.4462
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #21: GFLOPs: 43.2876. Time: 1.0200 ms. Best GFLOPs: 82.4462
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #22: GFLOPs: 53.5738. Time: 0.8242 ms. Best GFLOPs: 82.4462
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #23: GFLOPs: 68.7342. Time: 0.6424 ms. Best GFLOPs: 82.4462
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #24: GFLOPs: 110.9313. Time: 0.3980 ms. Best GFLOPs: 110.9313
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #25: GFLOPs: 29.4653. Time: 1.4985 ms. Best GFLOPs: 110.9313
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #26: GFLOPs: 81.4941. Time: 0.5418 ms. Best GFLOPs: 110.9313
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #27: GFLOPs: 31.5712. Time: 1.3986 ms. Best GFLOPs: 110.9313
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #28: GFLOPs: 34.4339. Time: 1.2823 ms. Best GFLOPs: 110.9313
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #29: GFLOPs: 14.9640. Time: 2.9507 ms. Best GFLOPs: 110.9313
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #30: GFLOPs: 44.9941. Time: 0.9813 ms. Best GFLOPs: 110.9313
[15:20:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #31: GFLOPs: 46.0395. Time: 0.9591 ms. Best GFLOPs: 110.9313
[15:20:11] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 224
Total latency (us): 975.848

[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #0: GFLOPs: 0.0053. Time: 152.3233 ms. Best GFLOPs: 0.0053
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #1: GFLOPs: 7.6907. Time: 0.1044 ms. Best GFLOPs: 7.6907
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #2: GFLOPs: 0.3866. Time: 2.0768 ms. Best GFLOPs: 7.6907
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #3: GFLOPs: 5.6743. Time: 0.1415 ms. Best GFLOPs: 7.6907
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #4: GFLOPs: 2.5627. Time: 0.3133 ms. Best GFLOPs: 7.6907
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #5: GFLOPs: 4.5200. Time: 0.1776 ms. Best GFLOPs: 7.6907
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #6: GFLOPs: 6.4161. Time: 0.1251 ms. Best GFLOPs: 7.6907
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #7: GFLOPs: 0.0021. Time: 376.4479 ms. Best GFLOPs: 7.6907
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #8: GFLOPs: 7.0108. Time: 0.1145 ms. Best GFLOPs: 7.6907
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #9: GFLOPs: 0.3424. Time: 2.3445 ms. Best GFLOPs: 7.6907
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #10: GFLOPs: 0.0024. Time: 337.9459 ms. Best GFLOPs: 7.6907
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #11: GFLOPs: 8.2192. Time: 0.0977 ms. Best GFLOPs: 8.2192
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #12: GFLOPs: 0.3216. Time: 2.4962 ms. Best GFLOPs: 8.2192
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #13: GFLOPs: 4.5523. Time: 0.1764 ms. Best GFLOPs: 8.2192
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #14: GFLOPs: 14.6934. Time: 0.0546 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #15: GFLOPs: 6.6760. Time: 0.1203 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #16: GFLOPs: 0.0048. Time: 168.0513 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #17: GFLOPs: 0.0004. Time: 2131.1139 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #18: GFLOPs: 0.3537. Time: 2.2700 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #19: GFLOPs: 0.5430. Time: 1.4784 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #20: GFLOPs: 0.1036. Time: 7.7494 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #21: GFLOPs: 0.4661. Time: 1.7225 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #22: GFLOPs: 0.0050. Time: 159.5568 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #23: GFLOPs: 1.8399. Time: 0.4363 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #24: GFLOPs: 1.9302. Time: 0.4159 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #25: GFLOPs: 1.5248. Time: 0.5265 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #26: GFLOPs: 1.1261. Time: 0.7129 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #27: GFLOPs: 0.0022. Time: 358.0152 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #28: GFLOPs: 2.6098. Time: 0.3076 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #29: GFLOPs: 0.0013. Time: 597.6319 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #30: GFLOPs: 0.0011. Time: 734.3914 ms. Best GFLOPs: 14.6934
[15:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"] Trial #31: GFLOPs: 0.1321. Time: 6.0788 ms. Best GFLOPs: 14.6934
[15:20:12] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 256
Total latency (us): 1030.49

[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #0: GFLOPs: 0.5786. Time: 1.3876 ms. Best GFLOPs: 0.5786
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #1: GFLOPs: 0.6899. Time: 1.1637 ms. Best GFLOPs: 0.6899
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #2: GFLOPs: 0.4845. Time: 1.6570 ms. Best GFLOPs: 0.6899
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #3: GFLOPs: 1.3184. Time: 0.6089 ms. Best GFLOPs: 1.3184
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #4: GFLOPs: 0.8504. Time: 0.9440 ms. Best GFLOPs: 1.3184
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #5: GFLOPs: 1.6183. Time: 0.4961 ms. Best GFLOPs: 1.6183
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #6: GFLOPs: 4.3310. Time: 0.1854 ms. Best GFLOPs: 4.3310
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #7: GFLOPs: 3.3533. Time: 0.2394 ms. Best GFLOPs: 4.3310
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #8: GFLOPs: 2.4755. Time: 0.3243 ms. Best GFLOPs: 4.3310
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #9: GFLOPs: 2.3138. Time: 0.3470 ms. Best GFLOPs: 4.3310
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #10: GFLOPs: 7.4296. Time: 0.1081 ms. Best GFLOPs: 7.4296
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #11: GFLOPs: 3.1823. Time: 0.2523 ms. Best GFLOPs: 7.4296
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #12: GFLOPs: 2.7871. Time: 0.2880 ms. Best GFLOPs: 7.4296
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #13: GFLOPs: 1.9976. Time: 0.4019 ms. Best GFLOPs: 7.4296
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #14: GFLOPs: 2.3213. Time: 0.3458 ms. Best GFLOPs: 7.4296
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #15: GFLOPs: 3.2586. Time: 0.2464 ms. Best GFLOPs: 7.4296
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #16: GFLOPs: 4.1320. Time: 0.1943 ms. Best GFLOPs: 7.4296
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #17: GFLOPs: 2.5933. Time: 0.3096 ms. Best GFLOPs: 7.4296
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #18: GFLOPs: 4.3888. Time: 0.1829 ms. Best GFLOPs: 7.4296
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #19: GFLOPs: 4.2580. Time: 0.1885 ms. Best GFLOPs: 7.4296
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #20: GFLOPs: 4.3035. Time: 0.1865 ms. Best GFLOPs: 7.4296
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #21: GFLOPs: 2.4030. Time: 0.3341 ms. Best GFLOPs: 7.4296
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #22: GFLOPs: 3.5092. Time: 0.2288 ms. Best GFLOPs: 7.4296
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #23: GFLOPs: 4.1335. Time: 0.1942 ms. Best GFLOPs: 7.4296
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #24: GFLOPs: 32.0451. Time: 0.0251 ms. Best GFLOPs: 32.0451
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #25: GFLOPs: 32.2964. Time: 0.0249 ms. Best GFLOPs: 32.2964
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #26: GFLOPs: 35.5002. Time: 0.0226 ms. Best GFLOPs: 35.5002
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #27: GFLOPs: 35.3694. Time: 0.0227 ms. Best GFLOPs: 35.5002
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #28: GFLOPs: 25.0895. Time: 0.0320 ms. Best GFLOPs: 35.5002
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #29: GFLOPs: 24.5142. Time: 0.0327 ms. Best GFLOPs: 35.5002
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #30: GFLOPs: 35.7330. Time: 0.0225 ms. Best GFLOPs: 35.7330
[15:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_add_layout_transform"] Trial #31: GFLOPs: 32.3662. Time: 0.0248 ms. Best GFLOPs: 35.7330
[15:20:13] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_add_layout_transform"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 288
Total latency (us): 1052.95

[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #0: GFLOPs: 71.1019. Time: 13.0186 ms. Best GFLOPs: 71.1019
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #1: GFLOPs: 95.6781. Time: 9.6746 ms. Best GFLOPs: 95.6781
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #2: GFLOPs: 32.4972. Time: 28.4839 ms. Best GFLOPs: 95.6781
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #3: GFLOPs: 32.2268. Time: 28.7229 ms. Best GFLOPs: 95.6781
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #4: GFLOPs: 100.8388. Time: 9.1795 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #5: GFLOPs: 18.0778. Time: 51.2035 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #6: GFLOPs: 31.2270. Time: 29.6425 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #7: GFLOPs: 50.4976. Time: 18.3305 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #8: GFLOPs: 79.9021. Time: 11.5848 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #9: GFLOPs: 31.5394. Time: 29.3489 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #10: GFLOPs: 52.0910. Time: 17.7698 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #11: GFLOPs: 49.4593. Time: 18.7153 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #12: GFLOPs: 36.7457. Time: 25.1906 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #13: GFLOPs: 38.6168. Time: 23.9700 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #14: GFLOPs: 85.5156. Time: 10.8243 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #15: GFLOPs: 39.6508. Time: 23.3450 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #16: GFLOPs: 24.9259. Time: 37.1359 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #17: GFLOPs: 26.2669. Time: 35.2401 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #18: GFLOPs: 52.7080. Time: 17.5618 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #19: GFLOPs: 9.5213. Time: 97.2187 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #20: GFLOPs: 43.1965. Time: 21.4288 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #21: GFLOPs: 26.6751. Time: 34.7008 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #22: GFLOPs: 50.6316. Time: 18.2820 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #23: GFLOPs: 36.0809. Time: 25.6548 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #24: GFLOPs: 17.9869. Time: 51.4624 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #25: GFLOPs: 9.3611. Time: 98.8826 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #26: GFLOPs: 20.4251. Time: 45.3191 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #27: GFLOPs: 51.4720. Time: 17.9835 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #28: GFLOPs: 11.7385. Time: 78.8558 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #29: GFLOPs: 37.0268. Time: 24.9994 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #30: GFLOPs: 59.2324. Time: 15.6274 ms. Best GFLOPs: 100.8388
[15:20:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #31: GFLOPs: 50.0996. Time: 18.4761 ms. Best GFLOPs: 100.8388
[15:20:14] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 320
Total latency (us): 10232.4

[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #0: GFLOPs: 0.1066. Time: 7.5291 ms. Best GFLOPs: 0.1066
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #1: GFLOPs: 0.0059. Time: 136.0389 ms. Best GFLOPs: 0.1066
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #2: GFLOPs: 0.1229. Time: 6.5321 ms. Best GFLOPs: 0.1229
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #3: GFLOPs: 0.1808. Time: 4.4407 ms. Best GFLOPs: 0.1808
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #4: GFLOPs: 0.2255. Time: 3.5609 ms. Best GFLOPs: 0.2255
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #5: GFLOPs: 0.1302. Time: 6.1680 ms. Best GFLOPs: 0.2255
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #6: GFLOPs: 0.1742. Time: 4.6080 ms. Best GFLOPs: 0.2255
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #7: GFLOPs: 0.0081. Time: 98.9523 ms. Best GFLOPs: 0.2255
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #8: GFLOPs: 0.2766. Time: 2.9020 ms. Best GFLOPs: 0.2766
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #9: GFLOPs: 0.0008. Time: 1009.3290 ms. Best GFLOPs: 0.2766
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #10: GFLOPs: 0.0023. Time: 345.1127 ms. Best GFLOPs: 0.2766
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #11: GFLOPs: 0.0001. Time: 7161.7106 ms. Best GFLOPs: 0.2766
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #12: GFLOPs: 0.0071. Time: 113.1325 ms. Best GFLOPs: 0.2766
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #13: GFLOPs: 0.0033. Time: 243.8845 ms. Best GFLOPs: 0.2766
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #14: GFLOPs: 0.0013. Time: 602.3951 ms. Best GFLOPs: 0.2766
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #15: GFLOPs: 0.0618. Time: 12.9968 ms. Best GFLOPs: 0.2766
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #16: GFLOPs: 0.0004. Time: 1981.8474 ms. Best GFLOPs: 0.2766
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #17: GFLOPs: 0.2303. Time: 3.4863 ms. Best GFLOPs: 0.2766
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #18: GFLOPs: 0.2813. Time: 2.8542 ms. Best GFLOPs: 0.2813
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #19: GFLOPs: 0.4024. Time: 1.9949 ms. Best GFLOPs: 0.4024
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #20: GFLOPs: 0.0011. Time: 707.3362 ms. Best GFLOPs: 0.4024
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #21: GFLOPs: 0.1355. Time: 5.9262 ms. Best GFLOPs: 0.4024
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #22: GFLOPs: 0.1376. Time: 5.8335 ms. Best GFLOPs: 0.4024
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #23: GFLOPs: 0.1653. Time: 4.8567 ms. Best GFLOPs: 0.4024
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #24: GFLOPs: 0.0001. Time: 6304.3259 ms. Best GFLOPs: 0.4024
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #25: GFLOPs: 1.9112. Time: 0.4201 ms. Best GFLOPs: 1.9112
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #26: GFLOPs: 5.8223. Time: 0.1379 ms. Best GFLOPs: 5.8223
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #27: GFLOPs: 0.0006. Time: 1424.9331 ms. Best GFLOPs: 5.8223
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #28: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(802816,), "float32"], T_layout_trans: T.Buffer[(1, 16, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([802816], dtype="float32")
        T_prelu = T.alloc_buffer([802816], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1792, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i3 in T.serial(112):
                for ax0 in T.serial(802816):
                    for ax0_1 in T.serial(1):
                        with T.block("T_reshape"):
                            ax0_2 = T.axis.spatial(802816, ax0)
                            T.reads(placeholder[0, ax0_2 % 802816 // 50176, ax0_2 % 12544 // 112, ax0_2 % 112, ax0_2 % 50176 // 12544])
                            T.writes(T_reshape[ax0_2])
                            T_reshape[ax0_2] = T.if_then_else(0 < 1 and ax0_2 % 802816 // 12544 < 64 and ax0_2 % 12544 // 112 < 112 and ax0_2 % 112 < 112, placeholder[0, ax0_2 % 802816 // 12544 // 4, ax0_2 % 12544 // 112, ax0_2 % 112, ax0_2 % 802816 // 12544 % 4], T.float32(0), dtype="float32")
                    with T.block("T_prelu"):
                        ax0_3 = T.axis.spatial(802816, ax0)
                        T.reads(T_reshape[ax0_3], placeholder_1[ax0_3])
                        T.writes(T_prelu[ax0_3])
                        T_prelu[ax0_3] = T.Select(T.float32(0) < T_reshape[ax0_3], T_reshape[ax0_3], T_reshape[ax0_3] * placeholder_1[ax0_3])
                for ax0_4, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                    with T.block("T_reshape_1"):
                        ax0_5 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, i0_i1_i2_fused // 112 * 4 + ax1)
                        ax2_1 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        ax3_1 = T.axis.spatial(112, i3)
                        T.reads(T_prelu[(ax1_1 * 12544 + ax2_1 * 112 + ax3_1) % 802816])
                        T.writes(T_reshape_1[ax0_5, ax1_1, ax2_1, ax3_1])
                        T_reshape_1[ax0_5, ax1_1, ax2_1, ax3_1] = T_prelu[(ax1_1 * 12544 + ax2_1 * 112 + ax3_1) % 802816]
                for i4 in T.serial(4):
                    with T.block("T_layout_trans_1"):
                        ax0_6 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 112)
                        ax2 = T.axis.spatial(112, i0_i1_i2_fused % 112)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4])
                        T.reads(T_reshape_1[ax0_6, ax1 * 4 + ax4, ax2, ax3])
                        T.writes(T_layout_trans[ax0_6, ax1, ax2, ax3, ax4])
                        T_layout_trans[ax0_6, ax1, ax2, ax3, ax4] = T.if_then_else(ax0_6 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 112 and ax3 < 112, T_reshape_1[ax0_6, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_prelu", func_name="main")
b3 = sch.get_block(name="T_reshape_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=3)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)
sch.enter_postproc()
b10 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.unroll_explicit")
b11, b12, b13, b14 = sch.get_child_blocks(b10)
l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b11)
l21 = sch.fuse(l15, l16, l17)
sch.parallel(loop=l21)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l22, l23, l24 = sch.get_loops(block=b12)
sch.annotate(block_or_loop=l22, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l22, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26, l27, l28, l29, l30 = sch.get_loops(block=b13)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32, l33 = sch.get_loops(block=b14)
sch.annotate(block_or_loop=l31, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l31, ann_key="pragma_unroll_explicit", ann_val=1)
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #29: GFLOPs: 0.0241. Time: 33.3783 ms. Best GFLOPs: 5.8223
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #30: GFLOPs: 0.3614. Time: 2.2213 ms. Best GFLOPs: 5.8223
[15:20:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"] Trial #31: GFLOPs: 0.0071. Time: 112.9579 ms. Best GFLOPs: 5.8223
[15:20:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 352
Total latency (us): 10370.3

[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #0: GFLOPs: 36.9525. Time: 6.2678 ms. Best GFLOPs: 36.9525
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #1: GFLOPs: 21.4548. Time: 10.7953 ms. Best GFLOPs: 36.9525
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #2: GFLOPs: 9.1138. Time: 25.4134 ms. Best GFLOPs: 36.9525
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #3: GFLOPs: 13.8510. Time: 16.7217 ms. Best GFLOPs: 36.9525
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #4: GFLOPs: 17.8199. Time: 12.9974 ms. Best GFLOPs: 36.9525
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #5: GFLOPs: 24.3785. Time: 9.5007 ms. Best GFLOPs: 36.9525
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #6: GFLOPs: 28.1725. Time: 8.2212 ms. Best GFLOPs: 36.9525
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #7: GFLOPs: 14.8134. Time: 15.6354 ms. Best GFLOPs: 36.9525
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #8: GFLOPs: 28.1973. Time: 8.2140 ms. Best GFLOPs: 36.9525
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #9: GFLOPs: 9.5633. Time: 24.2190 ms. Best GFLOPs: 36.9525
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #10: GFLOPs: 33.1779. Time: 6.9809 ms. Best GFLOPs: 36.9525
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #11: GFLOPs: 14.6998. Time: 15.7561 ms. Best GFLOPs: 36.9525
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #12: GFLOPs: 44.1952. Time: 5.2407 ms. Best GFLOPs: 44.1952
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #13: GFLOPs: 36.5714. Time: 6.3332 ms. Best GFLOPs: 44.1952
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #14: GFLOPs: 29.0116. Time: 7.9835 ms. Best GFLOPs: 44.1952
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #15: GFLOPs: 23.3664. Time: 9.9122 ms. Best GFLOPs: 44.1952
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #16: GFLOPs: 12.8653. Time: 18.0028 ms. Best GFLOPs: 44.1952
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #17: GFLOPs: 43.6435. Time: 5.3069 ms. Best GFLOPs: 44.1952
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #18: GFLOPs: 19.3060. Time: 11.9969 ms. Best GFLOPs: 44.1952
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #19: GFLOPs: 32.9232. Time: 7.0349 ms. Best GFLOPs: 44.1952
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #20: GFLOPs: 23.1106. Time: 10.0219 ms. Best GFLOPs: 44.1952
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #21: GFLOPs: 47.7489. Time: 4.8506 ms. Best GFLOPs: 47.7489
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #22: GFLOPs: 15.4404. Time: 15.0004 ms. Best GFLOPs: 47.7489
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #23: GFLOPs: 31.0000. Time: 7.4714 ms. Best GFLOPs: 47.7489
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #24: GFLOPs: 37.4665. Time: 6.1819 ms. Best GFLOPs: 47.7489
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #25: GFLOPs: 16.3337. Time: 14.1801 ms. Best GFLOPs: 47.7489
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #26: GFLOPs: 9.7789. Time: 23.6848 ms. Best GFLOPs: 47.7489
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #27: GFLOPs: 21.2440. Time: 10.9025 ms. Best GFLOPs: 47.7489
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #28: GFLOPs: 50.4744. Time: 4.5887 ms. Best GFLOPs: 50.4744
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #29: GFLOPs: 56.5788. Time: 4.0936 ms. Best GFLOPs: 56.5788
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #30: GFLOPs: 32.7194. Time: 7.0787 ms. Best GFLOPs: 56.5788
[15:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 114, 114, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(112, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 5, 57):
                for ax4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 56 // 2 * 4 + ax2)
                        i3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 113 and 1 <= i3 and i3 < 113, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i4_0 in T.serial(4):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 4, 1):
                    for i3_2_init, i1_3_init, i2_3_init in T.grid(7, 8, 2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 56 * 8 + i1_3_init)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 2 * 2 + i2_3_init)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i3_1 * 7 + i3_2_init)
                            oc_block = T.axis.spatial(4, i4_0)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 3, 1, 1, 1, 7, 1, 8, 3, 1, 1, 8, 2, 1, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 56 * 8 + i1_3)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 2 * 2 + i2_3)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i3_1 * 7 + i3_2)
                            oc_block = T.axis.spatial(4, i4_0)
                            ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 112, 112, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 2, 28, 1):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 56 * 8 + ax1)
                        ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 2 * 2 + ax2)
                        ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[28, 1, 1, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 4, 7, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[8, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b68)
l80 = sch.fuse(l71, l72, l73, l74)
sch.parallel(loop=l80)
l81 = sch.fuse(l79)
sch.vectorize(loop=l81)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b112 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b112)
b136 = sch.decompose_reduction(block=b112, loop=l120)
[15:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 384
Total latency (us): 14463.9

[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #0: GFLOPs: 81.9407. Time: 2.8241 ms. Best GFLOPs: 81.9407
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #1: GFLOPs: 8.0579. Time: 28.7187 ms. Best GFLOPs: 81.9407
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #2: GFLOPs: 12.6916. Time: 18.2335 ms. Best GFLOPs: 81.9407
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #3: GFLOPs: 16.8074. Time: 13.7684 ms. Best GFLOPs: 81.9407
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #4: GFLOPs: 9.6425. Time: 23.9991 ms. Best GFLOPs: 81.9407
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #5: GFLOPs: 18.6382. Time: 12.4160 ms. Best GFLOPs: 81.9407
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #6: GFLOPs: 55.2176. Time: 4.1909 ms. Best GFLOPs: 81.9407
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #7: GFLOPs: 11.5268. Time: 20.0759 ms. Best GFLOPs: 81.9407
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #8: GFLOPs: 41.1604. Time: 5.6222 ms. Best GFLOPs: 81.9407
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #9: GFLOPs: 112.2208. Time: 2.0621 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #10: GFLOPs: 43.6879. Time: 5.2969 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #11: GFLOPs: 22.6045. Time: 10.2374 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #12: GFLOPs: 20.7752. Time: 11.1388 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #13: GFLOPs: 15.6620. Time: 14.7754 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #14: GFLOPs: 39.0648. Time: 5.9238 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #15: GFLOPs: 24.9688. Time: 9.2680 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #16: GFLOPs: 22.0347. Time: 10.5022 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #17: GFLOPs: 42.4068. Time: 5.4570 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #18: GFLOPs: 20.8648. Time: 11.0910 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #19: GFLOPs: 65.4479. Time: 3.5358 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #20: GFLOPs: 31.9202. Time: 7.2497 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #21: GFLOPs: 22.2528. Time: 10.3992 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #22: GFLOPs: 23.7480. Time: 9.7445 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #23: GFLOPs: 40.7840. Time: 5.6741 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #24: GFLOPs: 19.4177. Time: 11.9176 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #25: GFLOPs: 24.2317. Time: 9.5500 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #26: GFLOPs: 4.6799. Time: 49.4485 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #27: GFLOPs: 99.3274. Time: 2.3298 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #28: GFLOPs: 29.9220. Time: 7.7338 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #29: GFLOPs: 70.1331. Time: 3.2996 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #30: GFLOPs: 19.5024. Time: 11.8658 ms. Best GFLOPs: 112.2208
[15:20:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #31: GFLOPs: 48.8110. Time: 4.7410 ms. Best GFLOPs: 112.2208
[15:20:18] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 416
Total latency (us): 18588.2

[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #0: GFLOPs: 0.0001. Time: 2112.6515 ms. Best GFLOPs: 0.0001
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #1: GFLOPs: 0.0003. Time: 748.0430 ms. Best GFLOPs: 0.0003
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #2: GFLOPs: 0.0001. Time: 1728.0169 ms. Best GFLOPs: 0.0003
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #3: GFLOPs: 0.0001. Time: 2481.6461 ms. Best GFLOPs: 0.0003
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #4: GFLOPs: 0.7303. Time: 0.2748 ms. Best GFLOPs: 0.7303
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #5: GFLOPs: 1.5119. Time: 0.1328 ms. Best GFLOPs: 1.5119
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #6: GFLOPs: 1.5211. Time: 0.1320 ms. Best GFLOPs: 1.5211
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #7: GFLOPs: 11.0459. Time: 0.0182 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #8: GFLOPs: 0.0024. Time: 85.3244 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #9: GFLOPs: 0.0326. Time: 6.1659 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #10: GFLOPs: 0.0006. Time: 312.1284 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #11: GFLOPs: 0.0162. Time: 12.3619 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #12: GFLOPs: 0.0008. Time: 255.9118 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #13: GFLOPs: 0.0323. Time: 6.2211 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #14: GFLOPs: 0.0215. Time: 9.3249 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #15: GFLOPs: 0.0153. Time: 13.0879 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #16: GFLOPs: 0.0026. Time: 76.1234 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #17: GFLOPs: 0.0010. Time: 210.2818 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #18: GFLOPs: 0.0019. Time: 104.3760 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #19: GFLOPs: 0.0045. Time: 44.6507 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #20: GFLOPs: 0.0411. Time: 4.8868 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #21: GFLOPs: 0.0695. Time: 2.8887 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #22: GFLOPs: 0.0037. Time: 54.0021 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #23: GFLOPs: 0.0264. Time: 7.6120 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #24: GFLOPs: 0.0236. Time: 8.5177 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #25: GFLOPs: 0.2045. Time: 0.9815 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #26: GFLOPs: 0.1652. Time: 1.2152 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #27: GFLOPs: 0.2394. Time: 0.8385 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #28: GFLOPs: 0.5234. Time: 0.3835 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #29: GFLOPs: 0.3708. Time: 0.5413 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #30: GFLOPs: 0.0117. Time: 17.1651 ms. Best GFLOPs: 11.0459
[15:20:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"] Trial #31: GFLOPs: 0.0035. Time: 57.3154 ms. Best GFLOPs: 11.0459
[15:20:20] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 448
Total latency (us): 18624.5

[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #0: GFLOPs: 15.9262. Time: 14.5429 ms. Best GFLOPs: 15.9262
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #1: GFLOPs: 18.9549. Time: 12.2191 ms. Best GFLOPs: 18.9549
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #2: GFLOPs: 19.6476. Time: 11.7883 ms. Best GFLOPs: 19.6476
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #3: GFLOPs: 26.4335. Time: 8.7621 ms. Best GFLOPs: 26.4335
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #4: GFLOPs: 26.7861. Time: 8.6467 ms. Best GFLOPs: 26.7861
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #5: GFLOPs: 12.2861. Time: 18.8516 ms. Best GFLOPs: 26.7861
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #6: GFLOPs: 9.9291. Time: 23.3265 ms. Best GFLOPs: 26.7861
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #7: GFLOPs: 24.8065. Time: 9.3367 ms. Best GFLOPs: 26.7861
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #8: GFLOPs: 9.1616. Time: 25.2807 ms. Best GFLOPs: 26.7861
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #9: GFLOPs: 10.7582. Time: 21.5289 ms. Best GFLOPs: 26.7861
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #10: GFLOPs: 11.5855. Time: 19.9916 ms. Best GFLOPs: 26.7861
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #11: GFLOPs: 3.4743. Time: 66.6637 ms. Best GFLOPs: 26.7861
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #12: GFLOPs: 20.5444. Time: 11.2738 ms. Best GFLOPs: 26.7861
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #13: GFLOPs: 6.2877. Time: 36.8360 ms. Best GFLOPs: 26.7861
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #14: GFLOPs: 9.6525. Time: 23.9950 ms. Best GFLOPs: 26.7861
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #15: GFLOPs: 28.0928. Time: 8.2445 ms. Best GFLOPs: 28.0928
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #16: GFLOPs: 5.9894. Time: 38.6701 ms. Best GFLOPs: 28.0928
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #17: GFLOPs: 10.8585. Time: 21.3301 ms. Best GFLOPs: 28.0928
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #18: GFLOPs: 25.0426. Time: 9.2487 ms. Best GFLOPs: 28.0928
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #19: GFLOPs: 15.5386. Time: 14.9056 ms. Best GFLOPs: 28.0928
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #20: GFLOPs: 37.2189. Time: 6.2230 ms. Best GFLOPs: 37.2189
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 58, 58, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 58, 58):
                for ax4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2, i3, i4 = T.axis.remap("SSSS", [ax1, ax2, ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0 in T.grid(1, 7, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 14, 1, 1):
                    for i2_2_init, i3_2_init, i2_3_init in T.grid(2, 8, 2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_fused)
                            oh = T.axis.spatial(56, i2_1 * 4 + i2_2_init * 2 + i2_3_init)
                            ow = T.axis.spatial(56, i3_0 * 8 + i3_2_init)
                            oc_block = T.axis.spatial(4, i4_0)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 3, 1, 1, 1, 2, 8, 1, 4, 1, 3, 1, 1, 2, 1, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_fused)
                            oh = T.axis.spatial(56, i2_1 * 4 + i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(56, i3_0 * 8 + i3_2)
                            oc_block = T.axis.spatial(4, i4_0)
                            ic = T.axis.reduce(64, i5_0 * 4 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 56, 8, 1):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1 = T.axis.remap("SS", [i0_0_i1_0_fused, ax2])
                        ax3_1 = T.axis.spatial(56, i3_0 * 8 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 1, 1, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 2, 2])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 8, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b68)
l78 = sch.fuse(l71, l72)
sch.parallel(loop=l78)
l79 = sch.fuse(l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b114)
b140 = sch.decompose_reduction(block=b114, loop=l124)
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #22: GFLOPs: 17.6564. Time: 13.1178 ms. Best GFLOPs: 37.2189
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #23: GFLOPs: 34.3172. Time: 6.7492 ms. Best GFLOPs: 37.2189
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #24: GFLOPs: 12.8667. Time: 18.0009 ms. Best GFLOPs: 37.2189
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #25: GFLOPs: 10.3433. Time: 22.3925 ms. Best GFLOPs: 37.2189
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #26: GFLOPs: 28.0749. Time: 8.2498 ms. Best GFLOPs: 37.2189
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #27: GFLOPs: 12.9110. Time: 17.9392 ms. Best GFLOPs: 37.2189
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 16, 56, 56, 4), "float32"], T_add: T.Buffer[(1, 16, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 58, 58, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused in T.parallel(448, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_1, i4_1 in T.grid(1, 1):
                for i4_2_init, i1_3_init, i3_3_init in T.grid(2, 8, 28):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 8 // 4 * 8 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 32 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 4)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 32 // 16 * 28 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 16 // 8 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0 in T.grid(32, 3):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 30):
                        for ax4_fused in T.vectorized(2):
                            with T.block("data_pad"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(16, i5_0 // 2 + ax1)
                                i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 448 // 32 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 4 + i6_0 + ax2)
                                i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 32 // 16 * 28 + ax3)
                                i4 = T.axis.spatial(4, i5_0 % 2 * 2 + ax4_fused)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(data_pad[i0, i1, i2, i3, i4])
                                data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 1, 2, 2, 1, 1, 1, 8, 1, 28, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 8 // 4 * 8 + i1_3)
                            oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 32 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 4)
                            ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 32 // 16 * 28 + i3_3)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 16 // 8 * 2 + i4_2)
                            ic = T.axis.reduce(64, i5_0 * 2 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 28):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_add_1"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 8 // 4 * 8 + ax1)
                            ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused // 32 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 4)
                            ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 32 // 16 * 28 + ax3)
                            ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_fused % 16 // 8 * 2 + ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[14, 4, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 28])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b68)
l88 = sch.fuse(l71, l72, l73, l74, l75, l76, l77, l78)
sch.parallel(loop=l88)
l89 = sch.fuse(l87)
sch.vectorize(loop=l89)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b70)
l117 = sch.fuse(l116)
sch.vectorize(loop=l117)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b118 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137 = sch.get_loops(block=b118)
b138 = sch.decompose_reduction(block=b118, loop=l122)
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #29: GFLOPs: 15.8239. Time: 14.6369 ms. Best GFLOPs: 37.2189
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #30: GFLOPs: 24.8172. Time: 9.3327 ms. Best GFLOPs: 37.2189
[15:20:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"] Trial #31: GFLOPs: 13.5627. Time: 17.0772 ms. Best GFLOPs: 37.2189
[15:20:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 480
Total latency (us): 31070.5

[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #0: GFLOPs: 1.1997. Time: 0.1673 ms. Best GFLOPs: 1.1997
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #1: GFLOPs: 1.4346. Time: 0.1399 ms. Best GFLOPs: 1.4346
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #2: GFLOPs: 1.4341. Time: 0.1400 ms. Best GFLOPs: 1.4346
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #3: GFLOPs: 1.3592. Time: 0.1477 ms. Best GFLOPs: 1.4346
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #4: GFLOPs: 1.4724. Time: 0.1363 ms. Best GFLOPs: 1.4724
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #5: GFLOPs: 1.9406. Time: 0.1034 ms. Best GFLOPs: 1.9406
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #6: GFLOPs: 1.5084. Time: 0.1331 ms. Best GFLOPs: 1.9406
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #7: GFLOPs: 3.1180. Time: 0.0644 ms. Best GFLOPs: 3.1180
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #8: GFLOPs: 2.3115. Time: 0.0868 ms. Best GFLOPs: 3.1180
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #9: GFLOPs: 3.9034. Time: 0.0514 ms. Best GFLOPs: 3.9034
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #10: GFLOPs: 3.4005. Time: 0.0590 ms. Best GFLOPs: 3.9034
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #11: GFLOPs: 3.2392. Time: 0.0620 ms. Best GFLOPs: 3.9034
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #12: GFLOPs: 5.8131. Time: 0.0345 ms. Best GFLOPs: 5.8131
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #13: GFLOPs: 10.5135. Time: 0.0191 ms. Best GFLOPs: 10.5135
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #14: GFLOPs: 10.7778. Time: 0.0186 ms. Best GFLOPs: 10.7778
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #15: GFLOPs: 11.1969. Time: 0.0179 ms. Best GFLOPs: 11.1969
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #16: GFLOPs: 10.5396. Time: 0.0190 ms. Best GFLOPs: 11.1969
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #17: GFLOPs: 11.4296. Time: 0.0176 ms. Best GFLOPs: 11.4296
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #18: GFLOPs: 10.9007. Time: 0.0184 ms. Best GFLOPs: 11.4296
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #19: GFLOPs: 10.2783. Time: 0.0195 ms. Best GFLOPs: 11.4296
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #20: GFLOPs: 10.7335. Time: 0.0187 ms. Best GFLOPs: 11.4296
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #21: GFLOPs: 11.1588. Time: 0.0180 ms. Best GFLOPs: 11.4296
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #22: GFLOPs: 10.0441. Time: 0.0200 ms. Best GFLOPs: 11.4296
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #23: GFLOPs: 10.4184. Time: 0.0193 ms. Best GFLOPs: 11.4296
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #24: GFLOPs: 10.3576. Time: 0.0194 ms. Best GFLOPs: 11.4296
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #25: GFLOPs: 10.3616. Time: 0.0194 ms. Best GFLOPs: 11.4296
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #26: GFLOPs: 12.6909. Time: 0.0158 ms. Best GFLOPs: 12.6909
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #27: GFLOPs: 11.0307. Time: 0.0182 ms. Best GFLOPs: 12.6909
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #28: GFLOPs: 10.8468. Time: 0.0185 ms. Best GFLOPs: 12.6909
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #29: GFLOPs: 10.5405. Time: 0.0190 ms. Best GFLOPs: 12.6909
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #30: GFLOPs: 11.0425. Time: 0.0182 ms. Best GFLOPs: 12.6909
[15:20:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_add"] Trial #31: GFLOPs: 11.0280. Time: 0.0182 ms. Best GFLOPs: 12.6909
[15:20:22] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_add"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 512
Total latency (us): 31117.9

[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #0: GFLOPs: 38.5701. Time: 11.9995 ms. Best GFLOPs: 38.5701
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #1: GFLOPs: 28.1604. Time: 16.4353 ms. Best GFLOPs: 38.5701
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #2: GFLOPs: 31.5567. Time: 14.6664 ms. Best GFLOPs: 38.5701
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #3: GFLOPs: 14.1708. Time: 32.6604 ms. Best GFLOPs: 38.5701
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #4: GFLOPs: 46.9921. Time: 9.8490 ms. Best GFLOPs: 46.9921
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #5: GFLOPs: 22.3875. Time: 20.6733 ms. Best GFLOPs: 46.9921
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #6: GFLOPs: 25.3235. Time: 18.2764 ms. Best GFLOPs: 46.9921
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #7: GFLOPs: 37.2091. Time: 12.4385 ms. Best GFLOPs: 46.9921
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #8: GFLOPs: 53.3284. Time: 8.6787 ms. Best GFLOPs: 53.3284
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #9: GFLOPs: 17.8619. Time: 25.9112 ms. Best GFLOPs: 53.3284
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #10: GFLOPs: 33.8688. Time: 13.6652 ms. Best GFLOPs: 53.3284
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #11: GFLOPs: 9.5353. Time: 48.5378 ms. Best GFLOPs: 53.3284
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #12: GFLOPs: 34.9445. Time: 13.2445 ms. Best GFLOPs: 53.3284
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #13: GFLOPs: 19.2869. Time: 23.9968 ms. Best GFLOPs: 53.3284
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #14: GFLOPs: 24.2175. Time: 19.1111 ms. Best GFLOPs: 53.3284
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #15: GFLOPs: 35.9918. Time: 12.8591 ms. Best GFLOPs: 53.3284
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #16: GFLOPs: 27.8451. Time: 16.6214 ms. Best GFLOPs: 53.3284
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #17: GFLOPs: 20.0983. Time: 23.0280 ms. Best GFLOPs: 53.3284
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #18: GFLOPs: 36.9003. Time: 12.5425 ms. Best GFLOPs: 53.3284
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #19: GFLOPs: 21.4192. Time: 21.6079 ms. Best GFLOPs: 53.3284
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #20: GFLOPs: 49.6823. Time: 9.3157 ms. Best GFLOPs: 53.3284
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 58, 58, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 56, 56, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 30, 30):
                for ax4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 4 * 28 + ax2)
                        i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + ax3)
                        i4 = T.axis.spatial(4, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0 in T.grid(1, 8, 2, 1, 1, 1, 1):
                for i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 4, 14, 28):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_3_init)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 28 + i2_1 * 14 + i2_3_init)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 1, 2, 64, 3, 1, 1, 4, 14, 28, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 28 + i2_1 * 14 + i2_3)
                        ow = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_0])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 56, 56, 4], "float32"], ["TENSOR", [32, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 32, 28, 28):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, ax1)
                        ax2_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 28 + ax2)
                        ax3_1 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 14])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 28])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b67)
l80 = sch.fuse(l70, l71, l72, l73, l74)
sch.parallel(loop=l80)
l81 = sch.fuse(l79)
sch.vectorize(loop=l81)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b68)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b69)
l110 = sch.fuse(l109)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l104, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l104, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b111)
b134 = sch.decompose_reduction(block=b111, loop=l120)
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #22: GFLOPs: 63.6893. Time: 7.2669 ms. Best GFLOPs: 63.6893
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #23: GFLOPs: 21.6015. Time: 21.4255 ms. Best GFLOPs: 63.6893
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #24: GFLOPs: 60.7397. Time: 7.6198 ms. Best GFLOPs: 63.6893
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #25: GFLOPs: 19.2863. Time: 23.9975 ms. Best GFLOPs: 63.6893
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #26: GFLOPs: 35.6799. Time: 12.9716 ms. Best GFLOPs: 63.6893
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #27: GFLOPs: 18.0803. Time: 25.5983 ms. Best GFLOPs: 63.6893
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #28: GFLOPs: 35.4910. Time: 13.0406 ms. Best GFLOPs: 63.6893
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #29: GFLOPs: 36.0010. Time: 12.8558 ms. Best GFLOPs: 63.6893
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #30: GFLOPs: 47.6462. Time: 9.7138 ms. Best GFLOPs: 63.6893
[15:20:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #31: GFLOPs: 34.3394. Time: 13.4779 ms. Best GFLOPs: 63.6893
[15:20:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 544
Total latency (us): 38384.8

[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #0: GFLOPs: 3.0593. Time: 0.1312 ms. Best GFLOPs: 3.0593
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #1: GFLOPs: 4.8025. Time: 0.0836 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #2: GFLOPs: 3.9319. Time: 0.1021 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #3: GFLOPs: 2.2813. Time: 0.1760 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #4: GFLOPs: 3.8747. Time: 0.1036 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #5: GFLOPs: 4.0670. Time: 0.0987 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #6: GFLOPs: 3.9551. Time: 0.1015 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #7: GFLOPs: 0.0371. Time: 10.8170 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #8: GFLOPs: 0.0002. Time: 2237.9436 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #9: GFLOPs: 0.1149. Time: 3.4946 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #10: GFLOPs: 0.2255. Time: 1.7803 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #11: GFLOPs: 0.0083. Time: 48.3103 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #12: GFLOPs: 0.0021. Time: 186.9248 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #13: GFLOPs: 0.0096. Time: 41.6368 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #14: GFLOPs: 1.2202. Time: 0.3290 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #15: GFLOPs: 0.0027. Time: 151.1033 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #16: GFLOPs: 1.4254. Time: 0.2816 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #17: GFLOPs: 1.3282. Time: 0.3022 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #18: GFLOPs: 1.1923. Time: 0.3367 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #19: GFLOPs: 2.6139. Time: 0.1536 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #20: GFLOPs: 2.0017. Time: 0.2005 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #21: GFLOPs: 3.0505. Time: 0.1316 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #22: GFLOPs: 1.2992. Time: 0.3090 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #23: Error in building: LocalRunner: Timeout, killed after 30 seconds

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(401408,), "float32"], T_layout_trans: T.Buffer[(1, 32, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_prelu = T.alloc_buffer([401408], dtype="float32")
        for i0_i1_i2_fused_fused in T.parallel(1792):
            for i3 in T.serial(56):
                for ax0 in T.serial(401408):
                    with T.block("T_prelu"):
                        ax0_1 = T.axis.spatial(401408, ax0)
                        T.reads(placeholder[0, ax0_1 % 401408 // 12544, ax0_1 % 3136 // 56, ax0_1 % 56, ax0_1 % 12544 // 3136], placeholder_1[ax0_1])
                        T.writes(T_prelu[ax0_1])
                        T_prelu[ax0_1] = T.Select(T.float32(0) < T.if_then_else(0 < 1 and ax0_1 % 401408 // 3136 < 128 and ax0_1 % 3136 // 56 < 56 and ax0_1 % 56 < 56, placeholder[0, ax0_1 % 401408 // 3136 // 4, ax0_1 % 3136 // 56, ax0_1 % 56, ax0_1 % 401408 // 3136 % 4], T.float32(0), dtype="float32"), T.if_then_else(0 < 1 and ax0_1 % 401408 // 3136 < 128 and ax0_1 % 3136 // 56 < 56 and ax0_1 % 56 < 56, placeholder[0, ax0_1 % 401408 // 3136 // 4, ax0_1 % 3136 // 56, ax0_1 % 56, ax0_1 % 401408 // 3136 % 4], T.float32(0), dtype="float32"), T.if_then_else(0 < 1 and ax0_1 % 401408 // 3136 < 128 and ax0_1 % 3136 // 56 < 56 and ax0_1 % 56 < 56, placeholder[0, ax0_1 % 401408 // 3136 // 4, ax0_1 % 3136 // 56, ax0_1 % 56, ax0_1 % 401408 // 3136 % 4], T.float32(0), dtype="float32") * placeholder_1[ax0_1])
                for i4_fused in T.vectorized(4):
                    with T.block("T_layout_trans_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(32, i0_i1_i2_fused_fused // 56)
                        ax2 = T.axis.spatial(56, i0_i1_i2_fused_fused % 56)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(T_prelu[(ax1 * 12544 + ax4 * 3136 + ax2 * 56 + ax3) % 401408])
                        T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                        T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 128 and ax2 < 56 and ax3 < 56, T_prelu[((ax1 * 4 + ax4) * 3136 + ax2 * 56 + ax3) % 401408], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_prelu", func_name="main")
b3 = sch.get_block(name="T_reshape_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=-2)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=3)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)
sch.enter_postproc()
b10 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.unroll_explicit")
b11, b12 = sch.get_child_blocks(b10)
l13, l14, l15, l16, l17 = sch.get_loops(block=b11)
l18 = sch.fuse(l13, l14, l15)
sch.parallel(loop=l18)
l19, l20, l21 = sch.get_loops(block=b12)
l22 = sch.fuse(l19)
sch.parallel(loop=l22)
l23 = sch.fuse(l21)
sch.vectorize(loop=l23)
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #24: GFLOPs: 0.3497. Time: 1.1478 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #25: GFLOPs: 2.2099. Time: 0.1816 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #26: GFLOPs: 2.0712. Time: 0.1938 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #27: GFLOPs: 4.1800. Time: 0.0960 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #28: GFLOPs: 0.0001. Time: 2741.1134 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #29: GFLOPs: 0.0025. Time: 163.6699 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #30: GFLOPs: 0.0020. Time: 204.3064 ms. Best GFLOPs: 4.8025
[15:20:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"] Trial #31: GFLOPs: 0.4071. Time: 0.9861 ms. Best GFLOPs: 4.8025
[15:20:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 576
Total latency (us): 38468.4

[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 58, 58, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3 in T.grid(1, 32, 57, 29):
                    for ax4_fused in T.vectorized(4):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(58, ax2)
                            i3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 28 + ax3)
                            i4 = T.axis.spatial(4, ax4_fused)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(4, 1, 1):
                    for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(2, 7, 7, 2, 4, 2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 16 + i1_1 * 8 + i1_2_init * 4 + i1_3_init)
                            oh = T.axis.spatial(28, i2_1 * 7 + i2_2_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 14 + i3_2_init * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 3, 1, 1, 2, 7, 7, 2, 16, 1, 3, 1, 4, 1, 2, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 16 + i1_1 * 8 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(28, i2_1 * 7 + i2_2)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 14 + i3_2 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_2)
                            ic = T.axis.reduce(128, i5_0 * 16 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 56, 56, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 28, 14):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 16 + ax1)
                        ax2_1 = T.axis.spatial(28, ax2)
                        ax3_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 14 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 2, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 7, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 7, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[8, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b68)
l83 = sch.fuse(l71, l72, l73, l74, l75)
sch.parallel(loop=l83)
l84 = sch.fuse(l82)
sch.vectorize(loop=l84)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b70)
l113 = sch.fuse(l112)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l107, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l107, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b114)
b137 = sch.decompose_reduction(block=b114, loop=l121)
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #1: GFLOPs: 11.9732. Time: 19.3275 ms. Best GFLOPs: 11.9732
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #2: GFLOPs: 21.6334. Time: 10.6970 ms. Best GFLOPs: 21.6334
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #3: GFLOPs: 4.2169. Time: 54.8767 ms. Best GFLOPs: 21.6334
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #4: GFLOPs: 15.4299. Time: 14.9976 ms. Best GFLOPs: 21.6334
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #5: GFLOPs: 31.0878. Time: 7.4438 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #6: GFLOPs: 5.2386. Time: 44.1739 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #7: GFLOPs: 25.2248. Time: 9.1740 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #8: GFLOPs: 20.2279. Time: 11.4402 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #9: GFLOPs: 11.5781. Time: 19.9870 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #10: GFLOPs: 22.3490. Time: 10.3545 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #11: GFLOPs: 27.8263. Time: 8.3163 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #12: GFLOPs: 14.8763. Time: 15.5558 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #13: GFLOPs: 24.8345. Time: 9.3182 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #14: GFLOPs: 16.3202. Time: 14.1795 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #15: GFLOPs: 23.1313. Time: 10.0042 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #16: GFLOPs: 27.6199. Time: 8.3784 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #17: GFLOPs: 11.5728. Time: 19.9962 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #18: GFLOPs: 8.0731. Time: 28.6644 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #19: GFLOPs: 27.5585. Time: 8.3971 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #20: GFLOPs: 23.8215. Time: 9.7144 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #21: GFLOPs: 26.3091. Time: 8.7959 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #22: GFLOPs: 18.1778. Time: 12.7304 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #23: GFLOPs: 17.8026. Time: 12.9987 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #24: GFLOPs: 18.1891. Time: 12.7225 ms. Best GFLOPs: 31.0878
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #25: GFLOPs: 57.8571. Time: 3.9997 ms. Best GFLOPs: 57.8571
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #26: GFLOPs: 23.3162. Time: 9.9249 ms. Best GFLOPs: 57.8571
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #27: GFLOPs: 19.8421. Time: 11.6627 ms. Best GFLOPs: 57.8571
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #28: GFLOPs: 15.6413. Time: 14.7949 ms. Best GFLOPs: 57.8571
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #29: GFLOPs: 19.2868. Time: 11.9985 ms. Best GFLOPs: 57.8571
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #30: GFLOPs: 36.2703. Time: 6.3802 ms. Best GFLOPs: 57.8571
[15:20:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"] Trial #31: GFLOPs: 30.8602. Time: 7.4987 ms. Best GFLOPs: 57.8571
[15:20:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 608
Total latency (us): 42468.1

[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #0: GFLOPs: 31.7178. Time: 7.2928 ms. Best GFLOPs: 31.7178
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(14, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 32, 6, 30):
                for ax4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_fused % 7 * 4 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0 in T.grid(1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 4):
                    for i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 8, 2, 28):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_fused // 7 * 16 + i1_1 * 8 + i1_3_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_fused % 7 * 4 + i2_2_init * 2 + i2_3_init)
                            ow, oc_block = T.axis.remap("SS", [i3_3_init, i4_1])
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 8, 2, 28, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_fused // 7 * 16 + i1_1 * 8 + i1_3)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_fused % 7 * 4 + i2_2 * 2 + i2_3)
                            ow, oc_block, ic, kh, kw = T.axis.remap("SSRRR", [i3_3, i4_1, i5_0, i6_1, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 16, 4, 28):
                    for ax4_fused in T.vectorized(4):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_fused // 7 * 16 + ax1)
                            ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_fused % 7 * 4 + ax2)
                            ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 2, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 28])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b67)
l78 = sch.fuse(l70, l71, l72)
sch.parallel(loop=l78)
l79 = sch.fuse(l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b68)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b69)
l112 = sch.fuse(l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l104, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l104, ann_key="pragma_unroll_explicit", ann_val=1)
b113 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137 = sch.get_loops(block=b113)
b138 = sch.decompose_reduction(block=b113, loop=l122)
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #2: GFLOPs: 12.3770. Time: 18.6888 ms. Best GFLOPs: 31.7178
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #3: GFLOPs: 36.7979. Time: 6.2860 ms. Best GFLOPs: 36.7979
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #4: GFLOPs: 71.0382. Time: 3.2562 ms. Best GFLOPs: 71.0382
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #5: GFLOPs: 36.4013. Time: 6.3545 ms. Best GFLOPs: 71.0382
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #6: GFLOPs: 45.6754. Time: 5.0642 ms. Best GFLOPs: 71.0382
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #7: GFLOPs: 58.3120. Time: 3.9668 ms. Best GFLOPs: 71.0382
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #8: GFLOPs: 74.7189. Time: 3.0958 ms. Best GFLOPs: 74.7189
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #9: GFLOPs: 61.8125. Time: 3.7421 ms. Best GFLOPs: 74.7189
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #10: GFLOPs: 43.5162. Time: 5.3155 ms. Best GFLOPs: 74.7189
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #11: GFLOPs: 35.8951. Time: 6.4441 ms. Best GFLOPs: 74.7189
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #12: GFLOPs: 75.3362. Time: 3.0704 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #13: GFLOPs: 66.9530. Time: 3.4548 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #14: GFLOPs: 20.7186. Time: 11.1644 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #15: GFLOPs: 6.6730. Time: 34.6639 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #16: GFLOPs: 10.9547. Time: 21.1152 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #17: GFLOPs: 7.7121. Time: 29.9933 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #18: GFLOPs: 11.1965. Time: 20.6593 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #19: GFLOPs: 12.8390. Time: 18.0163 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #20: GFLOPs: 8.0213. Time: 28.8372 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #21: GFLOPs: 9.4419. Time: 24.4983 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #22: GFLOPs: 13.1413. Time: 17.6018 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #23: GFLOPs: 14.4585. Time: 15.9983 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #24: GFLOPs: 22.7142. Time: 10.1835 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #25: GFLOPs: 10.2062. Time: 22.6639 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #26: GFLOPs: 15.3085. Time: 15.1100 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #27: GFLOPs: 18.6104. Time: 12.4291 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #28: GFLOPs: 11.0575. Time: 20.9190 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #29: GFLOPs: 32.6614. Time: 7.0821 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #30: GFLOPs: 13.6967. Time: 16.8881 ms. Best GFLOPs: 75.3362
[15:20:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #31: GFLOPs: 18.3607. Time: 12.5982 ms. Best GFLOPs: 75.3362
[15:20:29] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |            N/A |          N/A |                   N/A |      0 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 640
Total latency (us): 79312.7

[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #0: GFLOPs: 0.0057. Time: 17.7008 ms. Best GFLOPs: 0.0057
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #1: GFLOPs: 1.9609. Time: 0.0512 ms. Best GFLOPs: 1.9609
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #2: GFLOPs: 0.0149. Time: 6.7471 ms. Best GFLOPs: 1.9609
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #3: GFLOPs: 2.8808. Time: 0.0348 ms. Best GFLOPs: 2.8808
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #4: GFLOPs: 0.0151. Time: 6.6479 ms. Best GFLOPs: 2.8808
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #5: GFLOPs: 2.7431. Time: 0.0366 ms. Best GFLOPs: 2.8808
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #6: GFLOPs: 0.1644. Time: 0.6104 ms. Best GFLOPs: 2.8808
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #7: GFLOPs: 0.0045. Time: 22.1572 ms. Best GFLOPs: 2.8808
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #8: GFLOPs: 2.7551. Time: 0.0364 ms. Best GFLOPs: 2.8808
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #9: GFLOPs: 2.4460. Time: 0.0410 ms. Best GFLOPs: 2.8808
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #10: GFLOPs: 2.7559. Time: 0.0364 ms. Best GFLOPs: 2.8808
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #11: GFLOPs: 0.0038. Time: 26.5160 ms. Best GFLOPs: 2.8808
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #12: GFLOPs: 0.0006. Time: 155.1136 ms. Best GFLOPs: 2.8808
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #13: GFLOPs: 2.5837. Time: 0.0388 ms. Best GFLOPs: 2.8808
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #14: GFLOPs: 0.0140. Time: 7.1911 ms. Best GFLOPs: 2.8808
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #15: GFLOPs: 2.7050. Time: 0.0371 ms. Best GFLOPs: 2.8808
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #16: GFLOPs: 4.3497. Time: 0.0231 ms. Best GFLOPs: 4.3497
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #17: GFLOPs: 3.4119. Time: 0.0294 ms. Best GFLOPs: 4.3497
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #18: GFLOPs: 3.6066. Time: 0.0278 ms. Best GFLOPs: 4.3497
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #19: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'ax3'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(100352,), "float32"], T_layout_trans: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans_1 = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_reshape = T.alloc_buffer([100352], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0_i1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 28, 28):
                for ax0_1, ax1_1, ax2_1, ax3_1 in T.grid(1, 1, 1, 1):
                    with T.block("T_layout_trans"):
                        ax0_2 = T.axis.spatial(1, 0)
                        ax1_2 = T.axis.spatial(128, i0_i1_fused * 4 + ax1)
                        ax2_2, ax3_2 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(placeholder[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4])
                        T.writes(T_layout_trans_1[ax0_2, ax1_2, ax2_2, ax3_2])
                        T_layout_trans_1[ax0_2, ax1_2, ax2_2, ax3_2] = T.if_then_else(ax0_2 < 1 and ax1_2 < 128 and ax2_2 < 28 and ax3_2 < 28, placeholder[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4], T.float32(0), dtype="float32")
                for ax0_3 in T.serial(1):
                    with T.block("T_reshape"):
                        ax0_4 = T.axis.spatial(100352, i0_i1_fused * 3136 + ax1 * 784 + ax2 * 28 + ax3)
                        T.reads(T_layout_trans_1[0, ax0_4 % 100352 // 784, ax0_4 % 784 // 28, ax0_4 % 28])
                        T.writes(T_reshape[ax0_4])
                        T_reshape[ax0_4] = T_layout_trans_1[0, ax0_4 % 100352 // 784, ax0_4 % 784 // 28, ax0_4 % 28]
                with T.block("T_reshape_1"):
                    ax0_5 = T.axis.spatial(1, 0)
                    ax1_3 = T.axis.spatial(128, i0_i1_fused * 4 + ax1)
                    ax2_3, ax3_3 = T.axis.remap("SS", [ax2, ax3])
                    T.reads(T_reshape[(ax1_3 * 784 + ax2_3 * 28 + ax3_3) % 100352], placeholder_1[(ax1_3 * 784 + ax2_3 * 28 + ax3_3) % 100352])
                    T.writes(T_reshape_1[ax0_5, ax1_3, ax2_3, ax3_3])
                    T_reshape_1[ax0_5, ax1_3, ax2_3, ax3_3] = T.Select(T.float32(0) < T_reshape[(ax1_3 * 784 + ax2_3 * 28 + ax3_3) % 100352], T_reshape[(ax1_3 * 784 + ax2_3 * 28 + ax3_3) % 100352], T_reshape[(ax1_3 * 784 + ax2_3 * 28 + ax3_3) % 100352] * placeholder_1[(ax1_3 * 784 + ax2_3 * 28 + ax3_3) % 100352])
            for i2, i3, i4 in T.grid(28, 28, 4):
                with T.block("T_layout_trans_1"):
                    ax0_6 = T.axis.spatial(1, 0)
                    ax1_4, ax2_4, ax3_4, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads(T_reshape_1[ax0_6, ax1_4 * 4 + ax4, ax2_4, ax3_4])
                    T.writes(T_layout_trans[ax0_6, ax1_4, ax2_4, ax3_4, ax4])
                    T_layout_trans[ax0_6, ax1_4, ax2_4, ax3_4, ax4] = T.if_then_else(ax0_6 < 1 and ax1_4 * 4 + ax4 < 128 and ax2_4 < 28 and ax3_4 < 28, T_reshape_1[ax0_6, ax1_4 * 4 + ax4, ax2_4, ax3_4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_prelu", func_name="main")
b3 = sch.get_block(name="T_reshape_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=1)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=-2)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b1, decision=5)
sch.compute_at(block=b1, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b0, decision=5)
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)
sch.enter_postproc()
b10 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.unroll_explicit")
b11, b12, b13, b14 = sch.get_child_blocks(b10)
l15, l16, l17, l18, l19, l20, l21, l22, l23, l24 = sch.get_loops(block=b11)
l25 = sch.fuse(l15, l16)
sch.parallel(loop=l25)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l26, l27, l28, l29, l30, l31 = sch.get_loops(block=b12)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l32, l33, l34, l35, l36 = sch.get_loops(block=b13)
sch.annotate(block_or_loop=l32, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l32, ann_key="pragma_unroll_explicit", ann_val=1)
l37, l38, l39, l40 = sch.get_loops(block=b14)
sch.annotate(block_or_loop=l37, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l37, ann_key="pragma_unroll_explicit", ann_val=1)
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #20: GFLOPs: 0.0048. Time: 20.8100 ms. Best GFLOPs: 4.3497
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #21: GFLOPs: 2.9647. Time: 0.0338 ms. Best GFLOPs: 4.3497
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #22: GFLOPs: 1.6744. Time: 0.0599 ms. Best GFLOPs: 4.3497
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #23: GFLOPs: 0.0264. Time: 3.8022 ms. Best GFLOPs: 4.3497
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #24: GFLOPs: 0.0002. Time: 449.8430 ms. Best GFLOPs: 4.3497
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #25: GFLOPs: 3.7754. Time: 0.0266 ms. Best GFLOPs: 4.3497
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #26: GFLOPs: 0.0009. Time: 107.6957 ms. Best GFLOPs: 4.3497
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #27: GFLOPs: 1.3945. Time: 0.0720 ms. Best GFLOPs: 4.3497
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #28: GFLOPs: 0.1490. Time: 0.6735 ms. Best GFLOPs: 4.3497
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #29: GFLOPs: 1.4301. Time: 0.0702 ms. Best GFLOPs: 4.3497
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #30: GFLOPs: 1.6590. Time: 0.0605 ms. Best GFLOPs: 4.3497
[15:20:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"] Trial #31: GFLOPs: 2.7337. Time: 0.0367 ms. Best GFLOPs: 4.3497
[15:20:31] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 672
Total latency (us): 79589.6

[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(960, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(30):
                for i4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(32, i0_i1_i2_fused // 30)
                        i2 = T.axis.spatial(30, i0_i1_i2_fused % 30)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3_1, i4])
                        data_pad[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i3_2_init, i1_3_init, i3_3_init in T.grid(2, 2, 8, 14):
                for i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 8 + i1_3_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i2_2_init)
                        ow = T.axis.spatial(28, i3_2_init * 14 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(16, 1, 1, 1, 1, 2, 2, 1, 8, 3, 3, 1, 8, 1, 14):
                for i4_3_fused in T.vectorized(4):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 8 + i1_3)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i2_2)
                        ow = T.axis.spatial(28, i3_2 * 14 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3_fused)
                        ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 28):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 1, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 2, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 14])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75 = sch.get_loops(block=b68)
l76 = sch.fuse(l71, l72, l73)
sch.parallel(loop=l76)
l77 = sch.fuse(l75)
sch.vectorize(loop=l77)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b69)
l104 = sch.fuse(l78, l79, l80, l81, l82, l83, l84, l85, l86, l87)
sch.parallel(loop=l104)
l105 = sch.fuse(l103)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l104, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l104, ann_key="pragma_unroll_explicit", ann_val=1)
l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b70)
l112 = sch.fuse(l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l106, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l106, ann_key="pragma_unroll_explicit", ann_val=1)
b113 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b113)
b131 = sch.decompose_reduction(block=b113, loop=l115)
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1):
                for i1_3_init, i2_3_init, i3_3_init in T.grid(16, 7, 4):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_3_init)
                            oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 7 + i2_3_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 * 4 + i3_3_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1 in T.grid(16, 3, 1, 1, 1, 1, 1, 1, 8):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 7, 6, 1):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(32, i5_0 * 2 + i5_1 // 4 + ax1)
                            i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 56 // 14 * 7 + i6_0 + ax2)
                            i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 * 4 + ax3)
                            i4 = T.axis.spatial(4, i5_1 % 4 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 3, 1, 16, 7, 4):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(32, i1_1 * 16 + i1_3)
                                oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 7 + i2_3)
                                ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 * 4 + i3_3)
                                oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused)
                                ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                                kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 32, 7, 4):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 14 * 7 + ax2)
                        ax3_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 14 // 2 * 4 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 16])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 7])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 4])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=18)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94 = sch.get_loops(block=b68)
l95 = sch.fuse(l71, l72, l73, l74, l75)
sch.parallel(loop=l95)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b69)
l118 = sch.fuse(l117)
sch.vectorize(loop=l118)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b70)
l125 = sch.fuse(l124)
sch.vectorize(loop=l125)
sch.annotate(block_or_loop=l119, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l119, ann_key="pragma_unroll_explicit", ann_val=1)
b126 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148 = sch.get_loops(block=b126)
b149 = sch.decompose_reduction(block=b126, loop=l133)
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i3_3_init in T.grid(32, 14, 2, 4, 14):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_2_init)
                    oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 14 + i2_2_init)
                    ow = T.axis.spatial(28, i3_2_init * 14 + i3_3_init)
                    oc_block = T.axis.spatial(4, i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0 in T.serial(64):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 16, 30):
                    for ax4_fused in T.vectorized(2):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(32, i5_0 // 2 + ax1)
                            i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 14 + ax2)
                            i3 = T.axis.spatial(30, ax3)
                            i4 = T.axis.spatial(4, i5_0 % 2 * 2 + ax4_fused)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 32, 14, 2, 4, 2, 3, 3, 1, 1, 1, 14, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_2)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 14 + i2_2)
                        ow = T.axis.spatial(28, i3_2 * 14 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 32, 14, 28):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 14 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 32, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 14, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 14])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b68)
l87 = sch.fuse(l71, l72, l73, l74, l75, l76, l77, l78, l79, l80)
sch.parallel(loop=l87)
l88 = sch.fuse(l86)
sch.vectorize(loop=l88)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b70)
l112 = sch.fuse(l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l106, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l106, ann_key="pragma_unroll_explicit", ann_val=1)
b113 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b113)
b131 = sch.decompose_reduction(block=b113, loop=l115)
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #3: GFLOPs: 57.7968. Time: 4.0039 ms. Best GFLOPs: 57.7968
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #4: GFLOPs: 26.9845. Time: 8.5757 ms. Best GFLOPs: 57.7968
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #5: GFLOPs: 22.5290. Time: 10.2717 ms. Best GFLOPs: 57.7968
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #6: GFLOPs: 16.0150. Time: 14.4497 ms. Best GFLOPs: 57.7968
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #7: GFLOPs: 27.4940. Time: 8.4168 ms. Best GFLOPs: 57.7968
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #8: GFLOPs: 54.7895. Time: 4.2237 ms. Best GFLOPs: 57.7968
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #9: GFLOPs: 14.5933. Time: 15.8573 ms. Best GFLOPs: 57.7968
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #10: GFLOPs: 39.8562. Time: 5.8062 ms. Best GFLOPs: 57.7968
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #11: GFLOPs: 33.3556. Time: 6.9377 ms. Best GFLOPs: 57.7968
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #12: GFLOPs: 77.1498. Time: 2.9995 ms. Best GFLOPs: 77.1498
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #13: GFLOPs: 43.9735. Time: 5.2625 ms. Best GFLOPs: 77.1498
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(128, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 14, 1, 2):
                for ax0, ax1, ax2 in T.grid(1, 32, 3):
                    for ax3_ax4_fused in T.vectorized(36):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 8 * 14 + i2_1 + ax2)
                            i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 7 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_2_init, i1_3_init in T.grid(7, 4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 16 * 4 + i1_3_init)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 8 * 14 + i2_1)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 7 + i3_2_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 3, 1, 1, 1, 1, 7, 1, 4, 1, 3, 1, 4, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 16 * 4 + i1_3)
                        oh = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 8 * 14 + i2_1)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 7 + i3_2)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_1)
                        ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 14, 7):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 16 * 4 + ax1)
                        ax2_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 16 // 8 * 14 + ax2)
                        ax3_1 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 8 // 2 * 7 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 1, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 14, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 7, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b68)
l86 = sch.fuse(l71, l72, l73, l74, l75)
sch.parallel(loop=l86)
l87 = sch.fuse(l84, l85)
sch.vectorize(loop=l87)
sch.annotate(block_or_loop=l86, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l86, ann_key="pragma_unroll_explicit", ann_val=1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b70)
l116 = sch.fuse(l115)
sch.vectorize(loop=l116)
sch.annotate(block_or_loop=l110, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l110, ann_key="pragma_unroll_explicit", ann_val=1)
b117 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b117)
b140 = sch.decompose_reduction(block=b117, loop=l124)
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #15: GFLOPs: 96.2591. Time: 2.4041 ms. Best GFLOPs: 96.2591
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(2, 7, 2, 2, 8, 4):
                for i3_3_i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 16 + i1_2_init * 8 + i1_3_init)
                        oh = T.axis.spatial(28, i2_2_init * 4 + i2_3_init)
                        ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2 * 2 + i3_2_init)
                        oc_block = T.axis.spatial(4, i4_2_init * 2 + i3_3_i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0 in T.serial(2):
                for ax0, ax1, ax2 in T.grid(1, 16, 30):
                    for ax3_ax4_fused in T.vectorized(16):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(32, i5_0 * 16 + ax1)
                            i2 = T.axis.spatial(30, ax2)
                            i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 2 * 2 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(1, 3, 1, 2, 7, 2, 2, 64, 3, 1, 1, 8, 4):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 16 + i1_2 * 8 + i1_3)
                            oh = T.axis.spatial(28, i2_2 * 4 + i2_3)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2 * 2 + i3_2)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i3_3_i4_3_fused)
                            ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 16, 28):
                for ax3_ax4_fused in T.vectorized(8):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 16 + ax1)
                        ax2_1 = T.axis.spatial(28, ax2)
                        ax3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2 * 2 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 2, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[14, 1, 2, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 64])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b68)
l87 = sch.fuse(l71, l72, l73, l74, l75, l76, l77, l78, l79, l80)
sch.parallel(loop=l87)
l88 = sch.fuse(l85, l86)
sch.vectorize(loop=l88)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b69)
l106 = sch.fuse(l104, l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b70)
l113 = sch.fuse(l111, l112)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l107, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l107, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b114)
b131 = sch.decompose_reduction(block=b114, loop=l116)
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #17: GFLOPs: 50.8796. Time: 4.5482 ms. Best GFLOPs: 96.2591
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #18: GFLOPs: 37.8341. Time: 6.1165 ms. Best GFLOPs: 96.2591
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #19: GFLOPs: 19.2094. Time: 12.0468 ms. Best GFLOPs: 96.2591
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #20: GFLOPs: 34.7475. Time: 6.6598 ms. Best GFLOPs: 96.2591
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #21: GFLOPs: 22.5863. Time: 10.2456 ms. Best GFLOPs: 96.2591
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #22: GFLOPs: 32.3319. Time: 7.1574 ms. Best GFLOPs: 96.2591
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #23: GFLOPs: 33.2648. Time: 6.9567 ms. Best GFLOPs: 96.2591
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #24: GFLOPs: 154.7633. Time: 1.4953 ms. Best GFLOPs: 154.7633
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #25: GFLOPs: 41.3493. Time: 5.5965 ms. Best GFLOPs: 154.7633
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28, 4), "float32"], T_add: T.Buffer[(1, 32, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 30, 30, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 28, 28, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1 in T.grid(1, 4):
                for ax0, ax1, ax2 in T.grid(1, 32, 30):
                    for ax3_ax4_fused in T.vectorized(64):
                        with T.block("data_pad"):
                            i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                            i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 14 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(1, 1, 2):
                    for i1_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(8, 2, 28, 14):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_2_init)
                            oh = T.axis.spatial(28, i2_3_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 14 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 3, 1, 8, 1, 1, 2, 32, 3, 1, 1, 1, 28, 14, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i1_1 * 8 + i1_2)
                            oh = T.axis.spatial(28, i2_3)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 14 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 28, 28, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 32, 28):
                for ax3_ax4_fused in T.vectorized(56):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                        ax3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 14 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 8, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 28])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 14])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[4, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b68)
l83 = sch.fuse(l71, l72, l73, l74, l75)
sch.parallel(loop=l83)
l84 = sch.fuse(l81, l82)
sch.vectorize(loop=l84)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b70)
l113 = sch.fuse(l111, l112)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l107, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l107, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b114)
b137 = sch.decompose_reduction(block=b114, loop=l121)
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #27: GFLOPs: 64.0360. Time: 3.6138 ms. Best GFLOPs: 154.7633
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #28: GFLOPs: 20.3097. Time: 11.3942 ms. Best GFLOPs: 154.7633
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #29: GFLOPs: 26.9602. Time: 8.5835 ms. Best GFLOPs: 154.7633
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #30: GFLOPs: 52.8923. Time: 4.3751 ms. Best GFLOPs: 154.7633
[15:20:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"] Trial #31: GFLOPs: 22.4384. Time: 10.3132 ms. Best GFLOPs: 154.7633
[15:20:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 704
Total latency (us): 97532.7

[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #0: GFLOPs: 11.3276. Time: 0.0089 ms. Best GFLOPs: 11.3276
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #1: GFLOPs: 12.0710. Time: 0.0083 ms. Best GFLOPs: 12.0710
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #2: GFLOPs: 11.8607. Time: 0.0085 ms. Best GFLOPs: 12.0710
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #3: GFLOPs: 11.9127. Time: 0.0084 ms. Best GFLOPs: 12.0710
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #4: GFLOPs: 12.3011. Time: 0.0082 ms. Best GFLOPs: 12.3011
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #5: GFLOPs: 12.7444. Time: 0.0079 ms. Best GFLOPs: 12.7444
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #6: GFLOPs: 13.0099. Time: 0.0077 ms. Best GFLOPs: 13.0099
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #7: GFLOPs: 13.0674. Time: 0.0077 ms. Best GFLOPs: 13.0674
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #8: GFLOPs: 13.0395. Time: 0.0077 ms. Best GFLOPs: 13.0674
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #9: GFLOPs: 12.9723. Time: 0.0077 ms. Best GFLOPs: 13.0674
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #10: GFLOPs: 13.0371. Time: 0.0077 ms. Best GFLOPs: 13.0674
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #11: GFLOPs: 13.1071. Time: 0.0077 ms. Best GFLOPs: 13.1071
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #12: GFLOPs: 12.2403. Time: 0.0082 ms. Best GFLOPs: 13.1071
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #13: GFLOPs: 13.0344. Time: 0.0077 ms. Best GFLOPs: 13.1071
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #14: GFLOPs: 13.0021. Time: 0.0077 ms. Best GFLOPs: 13.1071
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #15: GFLOPs: 13.0627. Time: 0.0077 ms. Best GFLOPs: 13.1071
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #16: GFLOPs: 12.9238. Time: 0.0078 ms. Best GFLOPs: 13.1071
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #17: GFLOPs: 12.2924. Time: 0.0082 ms. Best GFLOPs: 13.1071
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #18: GFLOPs: 13.1190. Time: 0.0076 ms. Best GFLOPs: 13.1190
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #19: GFLOPs: 13.0647. Time: 0.0077 ms. Best GFLOPs: 13.1190
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #20: GFLOPs: 13.0364. Time: 0.0077 ms. Best GFLOPs: 13.1190
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #21: GFLOPs: 13.0324. Time: 0.0077 ms. Best GFLOPs: 13.1190
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #22: GFLOPs: 12.9324. Time: 0.0078 ms. Best GFLOPs: 13.1190
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #23: GFLOPs: 11.8540. Time: 0.0085 ms. Best GFLOPs: 13.1190
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #24: GFLOPs: 12.9531. Time: 0.0077 ms. Best GFLOPs: 13.1190
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #25: GFLOPs: 12.9978. Time: 0.0077 ms. Best GFLOPs: 13.1190
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #26: GFLOPs: 12.3321. Time: 0.0081 ms. Best GFLOPs: 13.1190
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #27: GFLOPs: 13.0201. Time: 0.0077 ms. Best GFLOPs: 13.1190
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #28: GFLOPs: 13.0279. Time: 0.0077 ms. Best GFLOPs: 13.1190
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #29: GFLOPs: 9.8218. Time: 0.0102 ms. Best GFLOPs: 13.1190
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #30: GFLOPs: 13.0225. Time: 0.0077 ms. Best GFLOPs: 13.1190
[15:20:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_1"] Trial #31: GFLOPs: 13.0090. Time: 0.0077 ms. Best GFLOPs: 13.1190
[15:20:34] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_add_1"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 736
Total latency (us): 97632.2

[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #0: GFLOPs: 67.7054. Time: 6.8329 ms. Best GFLOPs: 67.7054
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #1: GFLOPs: 22.2465. Time: 20.7953 ms. Best GFLOPs: 67.7054
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #2: GFLOPs: 61.1739. Time: 7.5624 ms. Best GFLOPs: 67.7054
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #3: GFLOPs: 40.5674. Time: 11.4038 ms. Best GFLOPs: 67.7054
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #4: GFLOPs: 36.1397. Time: 12.8010 ms. Best GFLOPs: 67.7054
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #5: GFLOPs: 61.5280. Time: 7.5189 ms. Best GFLOPs: 67.7054
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #6: GFLOPs: 49.5567. Time: 9.3352 ms. Best GFLOPs: 67.7054
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #7: GFLOPs: 45.7849. Time: 10.1043 ms. Best GFLOPs: 67.7054
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #8: GFLOPs: 15.9053. Time: 29.0860 ms. Best GFLOPs: 67.7054
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #9: GFLOPs: 59.3390. Time: 7.7963 ms. Best GFLOPs: 67.7054
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #10: GFLOPs: 67.3397. Time: 6.8700 ms. Best GFLOPs: 67.7054
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #11: GFLOPs: 31.6590. Time: 14.6127 ms. Best GFLOPs: 67.7054
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #12: GFLOPs: 26.5765. Time: 17.4072 ms. Best GFLOPs: 67.7054
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #13: GFLOPs: 55.5717. Time: 8.3248 ms. Best GFLOPs: 67.7054
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #14: GFLOPs: 96.0962. Time: 4.8142 ms. Best GFLOPs: 96.0962
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #15: GFLOPs: 16.9235. Time: 27.3361 ms. Best GFLOPs: 96.0962
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #16: GFLOPs: 37.9451. Time: 12.1919 ms. Best GFLOPs: 96.0962
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #17: GFLOPs: 85.6021. Time: 5.4043 ms. Best GFLOPs: 96.0962
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #18: GFLOPs: 27.9727. Time: 16.5384 ms. Best GFLOPs: 96.0962
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #19: GFLOPs: 79.0161. Time: 5.8548 ms. Best GFLOPs: 96.0962
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #20: GFLOPs: 21.1616. Time: 21.8614 ms. Best GFLOPs: 96.0962
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #21: GFLOPs: 1.7880. Time: 258.7341 ms. Best GFLOPs: 96.0962
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #22: GFLOPs: 137.8471. Time: 3.3561 ms. Best GFLOPs: 137.8471
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #23: GFLOPs: 19.0918. Time: 24.2314 ms. Best GFLOPs: 137.8471
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #24: GFLOPs: 182.4303. Time: 2.5359 ms. Best GFLOPs: 182.4303
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #25: GFLOPs: 42.3732. Time: 10.9178 ms. Best GFLOPs: 182.4303
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #26: GFLOPs: 8.6196. Time: 53.6708 ms. Best GFLOPs: 182.4303
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #27: GFLOPs: 38.2457. Time: 12.0961 ms. Best GFLOPs: 182.4303
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #28: GFLOPs: 65.6905. Time: 7.0425 ms. Best GFLOPs: 182.4303
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #29: GFLOPs: 91.4632. Time: 5.0580 ms. Best GFLOPs: 182.4303
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #30: GFLOPs: 12.6557. Time: 36.5544 ms. Best GFLOPs: 182.4303
[15:20:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #31: GFLOPs: 16.5319. Time: 27.9836 ms. Best GFLOPs: 182.4303
[15:20:35] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 768
Total latency (us): 100168

[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #0: GFLOPs: 0.0005. Time: 406.3680 ms. Best GFLOPs: 0.0005
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #1: GFLOPs: 2.2016. Time: 0.0912 ms. Best GFLOPs: 2.2016
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #2: GFLOPs: 3.5610. Time: 0.0564 ms. Best GFLOPs: 3.5610
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #3: GFLOPs: 3.3152. Time: 0.0605 ms. Best GFLOPs: 3.5610
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #4: GFLOPs: 0.0005. Time: 400.9349 ms. Best GFLOPs: 3.5610
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #5: GFLOPs: 2.2409. Time: 0.0896 ms. Best GFLOPs: 3.5610
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #6: GFLOPs: 2.1935. Time: 0.0915 ms. Best GFLOPs: 3.5610
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #7: GFLOPs: 5.6626. Time: 0.0354 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #8: GFLOPs: 1.9696. Time: 0.1019 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #9: GFLOPs: 0.0123. Time: 16.2810 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #10: GFLOPs: 0.0032. Time: 62.8718 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #11: GFLOPs: 2.3056. Time: 0.0870 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #12: GFLOPs: 0.0865. Time: 2.3191 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #13: GFLOPs: 0.4189. Time: 0.4791 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #14: GFLOPs: 0.2571. Time: 0.7806 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #15: GFLOPs: 0.0062. Time: 32.2678 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #16: GFLOPs: 0.9603. Time: 0.2090 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #17: GFLOPs: 0.0046. Time: 44.0862 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #18: GFLOPs: 0.0076. Time: 26.3064 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #19: GFLOPs: 0.0006. Time: 362.3545 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #20: GFLOPs: 0.0016. Time: 126.3228 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #21: GFLOPs: 3.1459. Time: 0.0638 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #22: GFLOPs: 0.0011. Time: 178.0552 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #23: GFLOPs: 0.0157. Time: 12.7988 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #24: GFLOPs: 0.0075. Time: 26.5974 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #25: GFLOPs: 0.0154. Time: 12.9971 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #26: GFLOPs: 0.0204. Time: 9.8175 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #27: GFLOPs: 0.0190. Time: 10.5447 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #28: GFLOPs: 0.0173. Time: 11.6187 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #29: GFLOPs: 0.0240. Time: 8.3622 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #30: GFLOPs: 0.0003. Time: 653.3843 ms. Best GFLOPs: 5.6626
[15:20:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"] Trial #31: GFLOPs: 0.0014. Time: 143.1329 ms. Best GFLOPs: 5.6626
[15:20:37] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 800
Total latency (us): 100204

[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #0: GFLOPs: 20.1333. Time: 11.4890 ms. Best GFLOPs: 20.1333
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #1: GFLOPs: 10.7926. Time: 21.4325 ms. Best GFLOPs: 20.1333
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #2: GFLOPs: 31.1327. Time: 7.4298 ms. Best GFLOPs: 31.1327
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #3: GFLOPs: 55.2258. Time: 4.1885 ms. Best GFLOPs: 55.2258
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #4: GFLOPs: 151.3767. Time: 1.5281 ms. Best GFLOPs: 151.3767
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #5: GFLOPs: 22.6275. Time: 10.2226 ms. Best GFLOPs: 151.3767
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #6: GFLOPs: 19.6595. Time: 11.7659 ms. Best GFLOPs: 151.3767
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #7: GFLOPs: 76.4429. Time: 3.0259 ms. Best GFLOPs: 151.3767
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #8: GFLOPs: 77.0043. Time: 3.0039 ms. Best GFLOPs: 151.3767
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #9: GFLOPs: 7.7386. Time: 29.8904 ms. Best GFLOPs: 151.3767
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #10: GFLOPs: 79.2793. Time: 2.9177 ms. Best GFLOPs: 151.3767
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #11: GFLOPs: 36.8997. Time: 6.2686 ms. Best GFLOPs: 151.3767
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #12: GFLOPs: 156.2028. Time: 1.4808 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #13: GFLOPs: 56.5096. Time: 4.0933 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #14: GFLOPs: 29.7180. Time: 7.7835 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #15: GFLOPs: 19.7071. Time: 11.7375 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #16: GFLOPs: 33.2392. Time: 6.9590 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #17: GFLOPs: 10.1709. Time: 22.7424 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #18: GFLOPs: 48.6278. Time: 4.7568 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #19: GFLOPs: 20.4923. Time: 11.2877 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #20: GFLOPs: 85.8820. Time: 2.6934 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #21: GFLOPs: 49.7209. Time: 4.6522 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #22: GFLOPs: 48.3902. Time: 4.7801 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #23: GFLOPs: 50.3683. Time: 4.5924 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #24: GFLOPs: 12.3970. Time: 18.6587 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #25: GFLOPs: 65.0028. Time: 3.5585 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #26: GFLOPs: 22.3056. Time: 10.3701 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #27: GFLOPs: 45.7935. Time: 5.0512 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #28: GFLOPs: 20.0147. Time: 11.5571 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #29: GFLOPs: 34.6202. Time: 6.6814 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #30: GFLOPs: 60.4012. Time: 3.8296 ms. Best GFLOPs: 156.2028
[15:20:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"] Trial #31: GFLOPs: 3.8988. Time: 59.3294 ms. Best GFLOPs: 156.2028
[15:20:39] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 832
Total latency (us): 101684

[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #0: GFLOPs: 27.1872. Time: 8.5062 ms. Best GFLOPs: 27.1872
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #1: GFLOPs: 15.6185. Time: 14.8069 ms. Best GFLOPs: 27.1872
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #2: GFLOPs: 12.0464. Time: 19.1975 ms. Best GFLOPs: 27.1872
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #3: GFLOPs: 31.8879. Time: 7.2523 ms. Best GFLOPs: 31.8879
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #4: GFLOPs: 17.7696. Time: 13.0144 ms. Best GFLOPs: 31.8879
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #5: GFLOPs: 39.9124. Time: 5.7942 ms. Best GFLOPs: 39.9124
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #6: GFLOPs: 19.2727. Time: 11.9994 ms. Best GFLOPs: 39.9124
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #7: GFLOPs: 17.0968. Time: 13.5266 ms. Best GFLOPs: 39.9124
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #8: GFLOPs: 20.6065. Time: 11.2227 ms. Best GFLOPs: 39.9124
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #9: GFLOPs: 21.5113. Time: 10.7507 ms. Best GFLOPs: 39.9124
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #10: GFLOPs: 19.8384. Time: 11.6572 ms. Best GFLOPs: 39.9124
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #11: GFLOPs: 6.9147. Time: 33.4450 ms. Best GFLOPs: 39.9124
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #12: GFLOPs: 9.1524. Time: 25.2677 ms. Best GFLOPs: 39.9124
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #13: GFLOPs: 27.3278. Time: 8.4625 ms. Best GFLOPs: 39.9124
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #14: GFLOPs: 22.7021. Time: 10.1868 ms. Best GFLOPs: 39.9124
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #15: GFLOPs: 24.2494. Time: 9.5368 ms. Best GFLOPs: 39.9124
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #16: GFLOPs: 45.2017. Time: 5.1162 ms. Best GFLOPs: 45.2017
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #17: GFLOPs: 14.1972. Time: 16.2892 ms. Best GFLOPs: 45.2017
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #18: GFLOPs: 37.0269. Time: 6.2458 ms. Best GFLOPs: 45.2017
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #19: GFLOPs: 38.0770. Time: 6.0735 ms. Best GFLOPs: 45.2017
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #20: GFLOPs: 8.9001. Time: 25.9840 ms. Best GFLOPs: 45.2017
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #21: GFLOPs: 23.1275. Time: 9.9994 ms. Best GFLOPs: 45.2017
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #22: GFLOPs: 25.1164. Time: 9.2076 ms. Best GFLOPs: 45.2017
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #23: GFLOPs: 7.2293. Time: 31.9895 ms. Best GFLOPs: 45.2017
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #24: GFLOPs: 12.6371. Time: 18.3001 ms. Best GFLOPs: 45.2017
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #25: GFLOPs: 38.0858. Time: 6.0721 ms. Best GFLOPs: 45.2017
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #26: GFLOPs: 35.3323. Time: 6.5453 ms. Best GFLOPs: 45.2017
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #27: GFLOPs: 9.6368. Time: 23.9978 ms. Best GFLOPs: 45.2017
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #28: GFLOPs: 78.9111. Time: 2.9307 ms. Best GFLOPs: 78.9111
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #29: GFLOPs: 30.2667. Time: 7.6408 ms. Best GFLOPs: 78.9111
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #30: GFLOPs: 7.4141. Time: 31.1922 ms. Best GFLOPs: 78.9111
[15:20:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #31: GFLOPs: 9.8064. Time: 23.5827 ms. Best GFLOPs: 78.9111
[15:20:41] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |        78.9111 |    2930.6542 |            84988.9707 |     32 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |            N/A |          N/A |                   N/A |      0 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 864
Total latency (us): 186673

[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #0: GFLOPs: 1.1863. Time: 0.0423 ms. Best GFLOPs: 1.1863
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #1: GFLOPs: 1.2906. Time: 0.0389 ms. Best GFLOPs: 1.2906
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #2: GFLOPs: 1.7995. Time: 0.0279 ms. Best GFLOPs: 1.7995
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #3: GFLOPs: 0.0014. Time: 36.9209 ms. Best GFLOPs: 1.7995
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #4: GFLOPs: 1.8309. Time: 0.0274 ms. Best GFLOPs: 1.8309
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #5: GFLOPs: 1.3438. Time: 0.0373 ms. Best GFLOPs: 1.8309
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #6: GFLOPs: 0.0014. Time: 35.6018 ms. Best GFLOPs: 1.8309
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #7: GFLOPs: 2.3595. Time: 0.0213 ms. Best GFLOPs: 2.3595
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #8: GFLOPs: 1.6037. Time: 0.0313 ms. Best GFLOPs: 2.3595
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #9: GFLOPs: 0.0017. Time: 30.3030 ms. Best GFLOPs: 2.3595
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #10: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'ax1'  'ax3'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(50176,), "float32"], T_layout_trans: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans_1 = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        T_reshape = T.alloc_buffer([50176], dtype="float32")
        T_prelu = T.alloc_buffer([50176], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 14):
                for ax0_1, ax1_1, ax2_1, ax3_1 in T.grid(1, 1, 1, 1):
                    with T.block("T_layout_trans"):
                        ax0_2 = T.axis.spatial(1, 0)
                        ax1_2 = T.axis.spatial(256, i0_i1_i2_fused // 14 * 4 + ax1)
                        ax2_2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                        ax3_2 = T.axis.spatial(14, ax3)
                        T.reads(placeholder[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4])
                        T.writes(T_layout_trans_1[ax0_2, ax1_2, ax2_2, ax3_2])
                        T_layout_trans_1[ax0_2, ax1_2, ax2_2, ax3_2] = T.if_then_else(ax0_2 < 1 and ax1_2 < 256 and ax2_2 < 14 and ax3_2 < 14, placeholder[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4], T.float32(0), dtype="float32")
                for ax0_3 in T.serial(1):
                    with T.block("T_reshape"):
                        ax0_4 = T.axis.spatial(50176, i0_i1_i2_fused // 14 * 784 + ax1 * 196 + i0_i1_i2_fused % 14 * 14 + ax3)
                        T.reads(T_layout_trans_1[0, ax0_4 % 50176 // 196, ax0_4 % 196 // 14, ax0_4 % 14])
                        T.writes(T_reshape[ax0_4])
                        T_reshape[ax0_4] = T_layout_trans_1[0, ax0_4 % 50176 // 196, ax0_4 % 196 // 14, ax0_4 % 14]
                for ax0_5 in T.serial(1):
                    with T.block("T_prelu"):
                        ax0_6 = T.axis.spatial(50176, i0_i1_i2_fused // 14 * 784 + ax1 * 196 + i0_i1_i2_fused % 14 * 14 + ax3)
                        T.reads(T_reshape[ax0_6], placeholder_1[ax0_6])
                        T.writes(T_prelu[ax0_6])
                        T_prelu[ax0_6] = T.Select(T.float32(0) < T_reshape[ax0_6], T_reshape[ax0_6], T_reshape[ax0_6] * placeholder_1[ax0_6])
                with T.block("T_reshape_1"):
                    ax0_7 = T.axis.spatial(1, 0)
                    ax1_3 = T.axis.spatial(256, i0_i1_i2_fused // 14 * 4 + ax1)
                    ax2_3 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    ax3_3 = T.axis.spatial(14, ax3)
                    T.reads(T_prelu[(ax1_3 * 196 + ax2_3 * 14 + ax3_3) % 50176])
                    T.writes(T_reshape_1[ax0_7, ax1_3, ax2_3, ax3_3])
                    T_reshape_1[ax0_7, ax1_3, ax2_3, ax3_3] = T_prelu[(ax1_3 * 196 + ax2_3 * 14 + ax3_3) % 50176]
            for i3, i4 in T.grid(14, 4):
                with T.block("T_layout_trans_1"):
                    ax0_8 = T.axis.spatial(1, 0)
                    ax1_4 = T.axis.spatial(64, i0_i1_i2_fused // 14)
                    ax2_4 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    ax3_4, ax4 = T.axis.remap("SS", [i3, i4])
                    T.reads(T_reshape_1[ax0_8, ax1_4 * 4 + ax4, ax2_4, ax3_4])
                    T.writes(T_layout_trans[ax0_8, ax1_4, ax2_4, ax3_4, ax4])
                    T_layout_trans[ax0_8, ax1_4, ax2_4, ax3_4, ax4] = T.if_then_else(ax0_8 < 1 and ax1_4 * 4 + ax4 < 256 and ax2_4 < 14 and ax3_4 < 14, T_reshape_1[ax0_8, ax1_4 * 4 + ax4, ax2_4, ax3_4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_prelu", func_name="main")
b3 = sch.get_block(name="T_reshape_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=2)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=6)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b1, decision=6)
sch.compute_at(block=b1, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)
sch.enter_postproc()
b10 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.unroll_explicit")
b11, b12, b13, b14, b15 = sch.get_child_blocks(b10)
l16, l17, l18, l19, l20, l21, l22, l23, l24, l25, l26 = sch.get_loops(block=b11)
l27 = sch.fuse(l16, l17, l18)
sch.parallel(loop=l27)
sch.annotate(block_or_loop=l27, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l27, ann_key="pragma_unroll_explicit", ann_val=1)
l28, l29, l30, l31, l32, l33 = sch.get_loops(block=b12)
sch.annotate(block_or_loop=l28, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l28, ann_key="pragma_unroll_explicit", ann_val=1)
l34, l35, l36, l37, l38, l39 = sch.get_loops(block=b13)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l40, l41, l42, l43, l44 = sch.get_loops(block=b14)
sch.annotate(block_or_loop=l40, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l40, ann_key="pragma_unroll_explicit", ann_val=1)
l45, l46, l47 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l45, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l45, ann_key="pragma_unroll_explicit", ann_val=1)
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #11: GFLOPs: 2.1400. Time: 0.0234 ms. Best GFLOPs: 2.3595
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #12: GFLOPs: 1.2753. Time: 0.0393 ms. Best GFLOPs: 2.3595
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #13: GFLOPs: 2.0544. Time: 0.0244 ms. Best GFLOPs: 2.3595
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #14: GFLOPs: 2.6529. Time: 0.0189 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #15: GFLOPs: 2.3772. Time: 0.0211 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #16: GFLOPs: 0.0003. Time: 188.3093 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #17: GFLOPs: 2.0861. Time: 0.0241 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #18: GFLOPs: 1.4992. Time: 0.0335 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #19: GFLOPs: 0.0007. Time: 67.8266 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #20: GFLOPs: 2.1274. Time: 0.0236 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #21: GFLOPs: 0.0007. Time: 71.3519 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #22: GFLOPs: 0.0011. Time: 47.2282 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #23: GFLOPs: 1.3297. Time: 0.0377 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #24: GFLOPs: 1.4062. Time: 0.0357 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #25: GFLOPs: 0.0048. Time: 10.5489 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #26: GFLOPs: 1.9447. Time: 0.0258 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #27: GFLOPs: 0.0151. Time: 3.3225 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #28: GFLOPs: 2.0766. Time: 0.0242 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #29: GFLOPs: 2.3542. Time: 0.0213 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #30: GFLOPs: 2.6430. Time: 0.0190 ms. Best GFLOPs: 2.6529
[15:20:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"] Trial #31: GFLOPs: 0.0038. Time: 13.2078 ms. Best GFLOPs: 2.6529
[15:20:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |        78.9111 |    2930.6542 |            84988.9707 |     32 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |         2.6529 |      18.9137 |              548.4970 |     32 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 896
Total latency (us): 187222

[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #0: GFLOPs: 18.2814. Time: 12.6528 ms. Best GFLOPs: 18.2814
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #1: GFLOPs: 27.9341. Time: 8.2806 ms. Best GFLOPs: 27.9341
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #2: GFLOPs: 91.7610. Time: 2.5208 ms. Best GFLOPs: 91.7610
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #3: GFLOPs: 25.6862. Time: 9.0053 ms. Best GFLOPs: 91.7610
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #4: GFLOPs: 30.0918. Time: 7.6869 ms. Best GFLOPs: 91.7610
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #5: GFLOPs: 59.7486. Time: 3.8714 ms. Best GFLOPs: 91.7610
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #6: GFLOPs: 13.3905. Time: 17.2742 ms. Best GFLOPs: 91.7610
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #7: GFLOPs: 30.7936. Time: 7.5117 ms. Best GFLOPs: 91.7610
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #8: GFLOPs: 43.7686. Time: 5.2849 ms. Best GFLOPs: 91.7610
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #9: GFLOPs: 4.9769. Time: 46.4773 ms. Best GFLOPs: 91.7610
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #10: GFLOPs: 11.0623. Time: 20.9099 ms. Best GFLOPs: 91.7610
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #11: GFLOPs: 71.5704. Time: 3.2319 ms. Best GFLOPs: 91.7610
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 64, 14, 14, 4), "float32"], T_add: T.Buffer[(1, 64, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 16, 16, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 64, 9):
                for ax3_ax4_fused in T.vectorized(12):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + ax2)
                        i3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 14 * 7 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i4_1 in T.serial(2):
                for i1_2_init, i2_2_init, i1_3_init in T.grid(4, 7, 16):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_2_init * 16 + i1_3_init)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + i2_2_init)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 14 * 7 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i2_3_i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(128, 3, 1, 1, 4, 7, 1, 1, 2, 1, 3, 1, 16):
                    for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_2 * 16 + i1_3)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + i2_2)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 14 * 7 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i2_3_i3_3_i4_3_fused)
                            ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 14, 14, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2 in T.grid(1, 64, 7):
                    for ax3_ax4_fused in T.vectorized(2):
                        with T.block("T_add_1"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(64, ax1)
                            ax2_1 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + ax2)
                            ax3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 14 * 7 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 7)
                            ax4 = T.axis.spatial(4, i4_1 * 2 + ax3_ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 4, 16])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 7, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[128, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b68)
l85 = sch.fuse(l71, l72, l73, l74, l75, l76, l77, l78, l79)
sch.parallel(loop=l85)
l86 = sch.fuse(l83, l84)
sch.vectorize(loop=l86)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b69)
l105 = sch.fuse(l102, l103, l104)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b70)
l113 = sch.fuse(l111, l112)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l106, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l106, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b114)
b131 = sch.decompose_reduction(block=b114, loop=l117)
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #13: GFLOPs: 7.0872. Time: 32.6380 ms. Best GFLOPs: 91.7610
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #14: GFLOPs: 30.2589. Time: 7.6444 ms. Best GFLOPs: 91.7610
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #15: GFLOPs: 31.6000. Time: 7.3200 ms. Best GFLOPs: 91.7610
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #16: GFLOPs: 140.1687. Time: 1.6502 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #17: GFLOPs: 21.2891. Time: 10.8653 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #18: GFLOPs: 8.1929. Time: 28.2332 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #19: GFLOPs: 23.4191. Time: 9.8771 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #20: GFLOPs: 13.9517. Time: 16.5794 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #21: GFLOPs: 31.4309. Time: 7.3594 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #22: GFLOPs: 68.5198. Time: 3.3758 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #23: GFLOPs: 21.5206. Time: 10.7484 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #24: GFLOPs: 3.9433. Time: 58.6597 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #25: GFLOPs: 48.3067. Time: 4.7884 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #26: GFLOPs: 123.4338. Time: 1.8740 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #27: GFLOPs: 22.2330. Time: 10.4040 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #28: GFLOPs: 19.2776. Time: 11.9990 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #29: GFLOPs: 52.1054. Time: 4.4393 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #30: GFLOPs: 20.9932. Time: 11.0184 ms. Best GFLOPs: 140.1687
[15:20:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"] Trial #31: GFLOPs: 83.6599. Time: 2.7649 ms. Best GFLOPs: 140.1687
[15:20:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |        78.9111 |    2930.6542 |            84988.9707 |     32 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |         2.6529 |      18.9137 |              548.4970 |     32 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |       140.1687 |    1650.2350 |            47856.8157 |     32 |            
 29 |                                                        fused_add_2 |     50176 |     30 |            N/A |          N/A |                   N/A |      0 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 928
Total latency (us): 235079

[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #0: GFLOPs: 6.3955. Time: 0.0078 ms. Best GFLOPs: 6.3955
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #1: GFLOPs: 6.7639. Time: 0.0074 ms. Best GFLOPs: 6.7639
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #2: GFLOPs: 6.3956. Time: 0.0078 ms. Best GFLOPs: 6.7639
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #3: GFLOPs: 6.7853. Time: 0.0074 ms. Best GFLOPs: 6.7853
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #4: GFLOPs: 6.3545. Time: 0.0079 ms. Best GFLOPs: 6.7853
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #5: GFLOPs: 6.7594. Time: 0.0074 ms. Best GFLOPs: 6.7853
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #6: GFLOPs: 6.7611. Time: 0.0074 ms. Best GFLOPs: 6.7853
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #7: GFLOPs: 5.9434. Time: 0.0084 ms. Best GFLOPs: 6.7853
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #8: GFLOPs: 6.7972. Time: 0.0074 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #9: GFLOPs: 6.7937. Time: 0.0074 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #10: GFLOPs: 6.6891. Time: 0.0075 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #11: GFLOPs: 6.7917. Time: 0.0074 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #12: GFLOPs: 6.7507. Time: 0.0074 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #13: GFLOPs: 6.7807. Time: 0.0074 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #14: GFLOPs: 6.7214. Time: 0.0075 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #15: GFLOPs: 6.6763. Time: 0.0075 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #16: GFLOPs: 5.9454. Time: 0.0084 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #17: GFLOPs: 6.3055. Time: 0.0080 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #18: GFLOPs: 6.7236. Time: 0.0075 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #19: GFLOPs: 6.6507. Time: 0.0075 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #20: GFLOPs: 6.6952. Time: 0.0075 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #21: GFLOPs: 6.7456. Time: 0.0074 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #22: GFLOPs: 6.7160. Time: 0.0075 ms. Best GFLOPs: 6.7972
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #23: GFLOPs: 6.8068. Time: 0.0074 ms. Best GFLOPs: 6.8068
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #24: GFLOPs: 6.7924. Time: 0.0074 ms. Best GFLOPs: 6.8068
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #25: GFLOPs: 6.7633. Time: 0.0074 ms. Best GFLOPs: 6.8068
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #26: GFLOPs: 6.7954. Time: 0.0074 ms. Best GFLOPs: 6.8068
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #27: GFLOPs: 6.7685. Time: 0.0074 ms. Best GFLOPs: 6.8068
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #28: GFLOPs: 6.7184. Time: 0.0075 ms. Best GFLOPs: 6.8068
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #29: GFLOPs: 6.7941. Time: 0.0074 ms. Best GFLOPs: 6.8068
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #30: GFLOPs: 6.8155. Time: 0.0074 ms. Best GFLOPs: 6.8155
[15:20:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_add_2"] Trial #31: GFLOPs: 5.8358. Time: 0.0086 ms. Best GFLOPs: 6.8155
[15:20:46] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #29: "fused_add_2"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |        78.9111 |    2930.6542 |            84988.9707 |     32 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |         2.6529 |      18.9137 |              548.4970 |     32 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |       140.1687 |    1650.2350 |            47856.8157 |     32 |            
 29 |                                                        fused_add_2 |     50176 |     30 |         6.8155 |       7.3621 |              220.8620 |     32 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 960
Total latency (us): 235300

[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #0: GFLOPs: 37.5745. Time: 12.3095 ms. Best GFLOPs: 37.5745
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #1: GFLOPs: 23.3650. Time: 19.7955 ms. Best GFLOPs: 37.5745
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #2: GFLOPs: 32.2908. Time: 14.3237 ms. Best GFLOPs: 37.5745
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #3: GFLOPs: 157.6358. Time: 2.9341 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #4: GFLOPs: 17.0237. Time: 27.1693 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #5: GFLOPs: 54.0067. Time: 8.5642 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #6: GFLOPs: 7.5418. Time: 61.3282 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #7: GFLOPs: 10.2040. Time: 45.3277 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #8: GFLOPs: 11.3399. Time: 40.7873 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #9: GFLOPs: 34.9239. Time: 13.2437 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #10: GFLOPs: 133.6209. Time: 3.4615 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #11: GFLOPs: 3.4455. Time: 134.2408 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #12: GFLOPs: 15.2711. Time: 30.2875 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #13: GFLOPs: 7.9123. Time: 58.4562 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #14: GFLOPs: 6.9412. Time: 66.6339 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #15: GFLOPs: 8.6538. Time: 53.4473 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #16: GFLOPs: 30.0307. Time: 15.4017 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #17: GFLOPs: 47.3164. Time: 9.7751 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #18: GFLOPs: 1.7969. Time: 257.3951 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #19: GFLOPs: 26.3203. Time: 17.5728 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #20: GFLOPs: 23.1569. Time: 19.9735 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #21: GFLOPs: 6.5857. Time: 70.2315 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #22: GFLOPs: 32.3709. Time: 14.2882 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #23: GFLOPs: 59.5424. Time: 7.7679 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #24: GFLOPs: 22.8328. Time: 20.2569 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #25: GFLOPs: 17.6010. Time: 26.2782 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #26: GFLOPs: 61.1778. Time: 7.5603 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #27: GFLOPs: 17.1277. Time: 27.0043 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #28: GFLOPs: 8.9410. Time: 51.7305 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #29: GFLOPs: 17.6002. Time: 26.2794 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #30: GFLOPs: 30.9937. Time: 14.9231 ms. Best GFLOPs: 157.6358
[15:20:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #31: GFLOPs: 4.9364. Time: 93.6958 ms. Best GFLOPs: 157.6358
[15:20:49] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |        78.9111 |    2930.6542 |            84988.9707 |     32 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |         2.6529 |      18.9137 |              548.4970 |     32 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |       140.1687 |    1650.2350 |            47856.8157 |     32 |            
 29 |                                                        fused_add_2 |     50176 |     30 |         6.8155 |       7.3621 |              220.8620 |     32 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |       157.6358 |    2934.1194 |             2934.1194 |     32 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 992
Total latency (us): 238234

[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #0: GFLOPs: 1.3282. Time: 0.0756 ms. Best GFLOPs: 1.3282
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #1: GFLOPs: 0.0026. Time: 38.1846 ms. Best GFLOPs: 1.3282
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #2: GFLOPs: 0.0014. Time: 74.1080 ms. Best GFLOPs: 1.3282
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #3: GFLOPs: 0.0234. Time: 4.2928 ms. Best GFLOPs: 1.3282
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #4: GFLOPs: 0.0046. Time: 21.9173 ms. Best GFLOPs: 1.3282
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #5: GFLOPs: 0.0006. Time: 175.1531 ms. Best GFLOPs: 1.3282
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #6: GFLOPs: 1.4388. Time: 0.0697 ms. Best GFLOPs: 1.4388
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #7: GFLOPs: 1.3205. Time: 0.0760 ms. Best GFLOPs: 1.4388
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #8: GFLOPs: 1.1003. Time: 0.0912 ms. Best GFLOPs: 1.4388
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #9: GFLOPs: 1.9057. Time: 0.0527 ms. Best GFLOPs: 1.9057
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #10: GFLOPs: 0.0006. Time: 160.9780 ms. Best GFLOPs: 1.9057
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #11: GFLOPs: 1.1112. Time: 0.0903 ms. Best GFLOPs: 1.9057
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #12: GFLOPs: 1.4618. Time: 0.0686 ms. Best GFLOPs: 1.9057
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #13: GFLOPs: 0.0057. Time: 17.6347 ms. Best GFLOPs: 1.9057
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #14: GFLOPs: 1.4067. Time: 0.0713 ms. Best GFLOPs: 1.9057
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #15: GFLOPs: 0.0081. Time: 12.4440 ms. Best GFLOPs: 1.9057
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #16: GFLOPs: 0.0004. Time: 246.0455 ms. Best GFLOPs: 1.9057
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #17: GFLOPs: 0.7099. Time: 0.1414 ms. Best GFLOPs: 1.9057
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #18: GFLOPs: 0.9570. Time: 0.1049 ms. Best GFLOPs: 1.9057
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #19: GFLOPs: 0.0028. Time: 35.9341 ms. Best GFLOPs: 1.9057
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #20: GFLOPs: 2.7220. Time: 0.0369 ms. Best GFLOPs: 2.7220
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #21: GFLOPs: 3.0070. Time: 0.0334 ms. Best GFLOPs: 3.0070
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #22: GFLOPs: 0.2325. Time: 0.4316 ms. Best GFLOPs: 3.0070
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #23: GFLOPs: 0.0004. Time: 275.4510 ms. Best GFLOPs: 3.0070
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #24: GFLOPs: 1.5875. Time: 0.0632 ms. Best GFLOPs: 3.0070
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #25: GFLOPs: 1.2223. Time: 0.0821 ms. Best GFLOPs: 3.0070
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #26: GFLOPs: 0.0001. Time: 1161.4176 ms. Best GFLOPs: 3.0070
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #27: GFLOPs: 0.9537. Time: 0.1052 ms. Best GFLOPs: 3.0070
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #28: GFLOPs: 2.7839. Time: 0.0360 ms. Best GFLOPs: 3.0070
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #29: GFLOPs: 2.7865. Time: 0.0360 ms. Best GFLOPs: 3.0070
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #30: GFLOPs: 0.4218. Time: 0.2379 ms. Best GFLOPs: 3.0070
[15:20:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"] Trial #31: GFLOPs: 0.0011. Time: 87.3918 ms. Best GFLOPs: 3.0070
[15:20:51] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |        78.9111 |    2930.6542 |            84988.9707 |     32 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |         2.6529 |      18.9137 |              548.4970 |     32 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |       140.1687 |    1650.2350 |            47856.8157 |     32 |            
 29 |                                                        fused_add_2 |     50176 |     30 |         6.8155 |       7.3621 |              220.8620 |     32 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |       157.6358 |    2934.1194 |             2934.1194 |     32 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |         3.0070 |      33.3730 |               33.3730 |     32 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1024
Total latency (us): 238267

[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #0: GFLOPs: 6.6800. Time: 34.6198 ms. Best GFLOPs: 6.6800
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #1: GFLOPs: 5.0536. Time: 45.7612 ms. Best GFLOPs: 6.6800
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #2: GFLOPs: 11.8132. Time: 19.5764 ms. Best GFLOPs: 11.8132
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #3: GFLOPs: 19.0790. Time: 12.1212 ms. Best GFLOPs: 19.0790
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #4: GFLOPs: 5.0438. Time: 45.8504 ms. Best GFLOPs: 19.0790
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #5: GFLOPs: 15.5278. Time: 14.8934 ms. Best GFLOPs: 19.0790
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 16, 16, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 32, 1, 1, 1):
                for i1_2_init, i2_2_init, i3_2_init in T.grid(2, 7, 7):
                    for i0_3_i1_3_i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 64 + i1_1 * 2 + i1_2_init)
                            oh, ow, oc_block = T.axis.remap("SSS", [i2_2_init, i3_2_init, i0_3_i1_3_i2_3_i3_3_i4_3_fused_init])
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0 in T.grid(16, 3):
                    for ax0, ax1, ax2 in T.grid(1, 8, 13):
                        for ax3_ax4_fused in T.vectorized(60):
                            with T.block("data_pad"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(128, i5_0 * 8 + ax1)
                                i2 = T.axis.spatial(16, i6_0 + ax2)
                                i3 = T.axis.spatial(16, ax3_ax4_fused // 4)
                                i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(data_pad[i0, i1, i2, i3, i4])
                                data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1 in T.grid(1, 1, 2, 7, 7, 1, 32, 1, 3):
                        for i0_3_i1_3_i2_3_i3_3_i4_3_fused in T.vectorized(4):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 64 + i1_1 * 2 + i1_2)
                                oh, ow, oc_block = T.axis.remap("SSS", [i2_2, i3_2, i0_3_i1_3_i2_3_i3_3_i4_3_fused])
                                ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                                kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 64, 7):
                for ax3_ax4_fused in T.vectorized(28):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 64 + ax1)
                        ax2_1 = T.axis.spatial(7, ax2)
                        ax3 = T.axis.spatial(7, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 32, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b68)
l88 = sch.fuse(l71, l72, l73, l74, l75)
sch.parallel(loop=l88)
l89 = sch.fuse(l86, l87)
sch.vectorize(loop=l89)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b69)
l112 = sch.fuse(l107, l108, l109, l110, l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b70)
l119 = sch.fuse(l117, l118)
sch.vectorize(loop=l119)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
b120 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
b139 = sch.decompose_reduction(block=b120, loop=l127)
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #7: GFLOPs: 3.6543. Time: 63.2849 ms. Best GFLOPs: 19.0790
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #8: GFLOPs: 4.1002. Time: 56.4029 ms. Best GFLOPs: 19.0790
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #9: GFLOPs: 5.7217. Time: 40.4183 ms. Best GFLOPs: 19.0790
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #10: GFLOPs: 20.3271. Time: 11.3770 ms. Best GFLOPs: 20.3271
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #11: GFLOPs: 4.2063. Time: 54.9803 ms. Best GFLOPs: 20.3271
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #12: GFLOPs: 1.2108. Time: 191.0014 ms. Best GFLOPs: 20.3271
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #13: GFLOPs: 32.0508. Time: 7.2155 ms. Best GFLOPs: 32.0508
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #14: GFLOPs: 38.9278. Time: 5.9408 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #15: GFLOPs: 31.3276. Time: 7.3820 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #16: GFLOPs: 36.2225. Time: 6.3845 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #17: GFLOPs: 28.1278. Time: 8.2218 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #18: GFLOPs: 33.8677. Time: 6.8284 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 16, 16, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(2048, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(64):
                with T.block("data_pad"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(128, i0_i1_i2_fused // 16)
                    i2 = T.axis.spatial(16, i0_i1_i2_fused % 16)
                    i3 = T.axis.spatial(16, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                    T.writes(data_pad[i0, i1, i2, i3, i4])
                    data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(56, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i1_3_init, i3_3_init in T.grid(2, 32, 7):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28 * 64 + i1_2_init * 32 + i1_3_init)
                    oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 4)
                    ow = T.axis.spatial(7, i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 3, 3, 1, 2, 1, 1, 1, 64, 1, 1, 1, 32, 1, 7, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28 * 64 + i1_2 * 32 + i1_3)
                    oh = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 4)
                    ow = T.axis.spatial(7, i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    ic = T.axis.reduce(512, i5_0 * 64 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 14, 14, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 1, 7, 1):
                with T.block("T_add_1"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 28 * 64 + ax1)
                    ax2_1 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 28 // 4)
                    ax3_1 = T.axis.spatial(7, ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 32])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[8, 64])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75 = sch.get_loops(block=b68)
l76 = sch.fuse(l71, l72, l73)
sch.parallel(loop=l76)
l77 = sch.fuse(l74, l75)
sch.vectorize(loop=l77)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b69)
l104 = sch.fuse(l78, l79, l80, l81, l82, l83, l84, l85, l86, l87)
sch.parallel(loop=l104)
sch.annotate(block_or_loop=l104, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l104, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
b129 = sch.decompose_reduction(block=b111, loop=l113)
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #20: GFLOPs: 3.0288. Time: 76.3532 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #21: GFLOPs: 3.5578. Time: 65.0004 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #22: GFLOPs: 26.9110. Time: 8.5936 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #23: GFLOPs: 13.0801. Time: 17.6804 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #24: GFLOPs: 23.1429. Time: 9.9928 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #25: GFLOPs: 18.4406. Time: 12.5409 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #26: GFLOPs: 12.3024. Time: 18.7981 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #27: GFLOPs: 22.8662. Time: 10.1137 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #28: GFLOPs: 23.0244. Time: 10.0442 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #29: GFLOPs: 6.3132. Time: 36.6313 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #30: GFLOPs: 6.6744. Time: 34.6492 ms. Best GFLOPs: 38.9278
[15:20:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"] Trial #31: GFLOPs: 18.7759. Time: 12.3169 ms. Best GFLOPs: 38.9278
[15:20:53] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |        78.9111 |    2930.6542 |            84988.9707 |     32 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |         2.6529 |      18.9137 |              548.4970 |     32 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |       140.1687 |    1650.2350 |            47856.8157 |     32 |            
 29 |                                                        fused_add_2 |     50176 |     30 |         6.8155 |       7.3621 |              220.8620 |     32 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |       157.6358 |    2934.1194 |             2934.1194 |     32 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |         3.0070 |      33.3730 |               33.3730 |     32 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |        38.9278 |    5940.7738 |             5940.7738 |     32 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1056
Total latency (us): 244208

[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #0: GFLOPs: 20.6410. Time: 11.2040 ms. Best GFLOPs: 20.6410
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #1: GFLOPs: 57.6956. Time: 4.0083 ms. Best GFLOPs: 57.6956
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #2: GFLOPs: 30.2229. Time: 7.6519 ms. Best GFLOPs: 57.6956
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #3: GFLOPs: 40.8168. Time: 5.6658 ms. Best GFLOPs: 57.6956
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #4: GFLOPs: 54.1467. Time: 4.2710 ms. Best GFLOPs: 57.6956
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #5: GFLOPs: 64.1149. Time: 3.6070 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #6: GFLOPs: 18.8207. Time: 12.2876 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #7: GFLOPs: 20.6270. Time: 11.2116 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #8: GFLOPs: 48.3133. Time: 4.7867 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #9: GFLOPs: 25.3333. Time: 9.1287 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #10: GFLOPs: 12.6769. Time: 18.2428 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #11: GFLOPs: 12.4881. Time: 18.5185 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #12: GFLOPs: 53.1501. Time: 4.3511 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #13: GFLOPs: 6.0428. Time: 38.2706 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #14: GFLOPs: 35.6343. Time: 6.4899 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #15: GFLOPs: 6.6121. Time: 34.9754 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #16: GFLOPs: 18.8682. Time: 12.2567 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #17: GFLOPs: 4.7755. Time: 48.4262 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #18: GFLOPs: 36.7192. Time: 6.2981 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 9, 9, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(14, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i4_2_init, i1_3_init in T.grid(4, 7, 2, 16):
                for i2_3_i3_3_i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 7 * 64 + i1_2_init * 16 + i1_3_init)
                        oh = T.axis.spatial(7, i2_2_init)
                        ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7)
                        oc_block = T.axis.spatial(4, i4_2_init * 2 + i2_3_i3_3_i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0 in T.serial(32):
                for ax0, ax1, ax2 in T.grid(1, 4, 9):
                    for ax3_ax4_fused in T.vectorized(12):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(128, i5_0 * 4 + ax1)
                            i2 = T.axis.spatial(9, ax2)
                            i3 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(1, 1, 1, 4, 7, 1, 2, 16, 3, 3, 1, 16):
                    for i2_3_i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 7 * 64 + i1_2 * 16 + i1_3)
                            oh = T.axis.spatial(7, i2_2)
                            ow = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i2_3_i3_3_i4_3_fused)
                            ic = T.axis.reduce(512, i5_0 * 16 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 64, 7):
                for ax3_ax4_fused in T.vectorized(4):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 7 * 64 + ax1)
                        ax2_1 = T.axis.spatial(7, ax2)
                        ax3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 7)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4], placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 16])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b68)
l87 = sch.fuse(l71, l72, l73, l74, l75, l76, l77, l78, l79, l80)
sch.parallel(loop=l87)
l88 = sch.fuse(l85, l86)
sch.vectorize(loop=l88)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b69)
l106 = sch.fuse(l103, l104, l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b70)
l113 = sch.fuse(l111, l112)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l107, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l107, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b114)
b130 = sch.decompose_reduction(block=b114, loop=l116)
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #20: GFLOPs: 18.2791. Time: 12.6517 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #21: GFLOPs: 48.4553. Time: 4.7727 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #22: GFLOPs: 17.6592. Time: 13.0958 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #23: GFLOPs: 26.4621. Time: 8.7393 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #24: GFLOPs: 39.1190. Time: 5.9117 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 7, 7, 4), "float32"], T_add: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 9, 9, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 7, 7, 4], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 128, 9):
                for ax3_ax4_fused in T.vectorized(36):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2 = T.axis.remap("SS", [ax1, ax2])
                        i3 = T.axis.spatial(9, ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(7, 7, 2, 1, 1, 1, 1, 2):
                for i1_2_init, i1_3_init in T.grid(16, 2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_fused * 32 + i1_2_init * 2 + i1_3_init)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_0])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 3, 1, 16, 1, 1, 1, 64, 3, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_fused * 32 + i1_2 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_0])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(512, i5_0 * 64 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 7, 7, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 1, 1, 1):
                    with T.block("T_add_1"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(128, i0_0_i1_0_fused * 32 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [i2_0, i3_0])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 16, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[8, 64])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b68)
l78 = sch.fuse(l71, l72)
sch.parallel(loop=l78)
l79 = sch.fuse(l76, l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b119 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144 = sch.get_loops(block=b119)
b145 = sch.decompose_reduction(block=b119, loop=l129)
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #26: GFLOPs: 33.2617. Time: 6.9528 ms. Best GFLOPs: 64.1149
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #27: GFLOPs: 68.0878. Time: 3.3965 ms. Best GFLOPs: 68.0878
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #28: GFLOPs: 16.5902. Time: 13.9396 ms. Best GFLOPs: 68.0878
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #29: GFLOPs: 35.5196. Time: 6.5108 ms. Best GFLOPs: 68.0878
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #30: GFLOPs: 17.0419. Time: 13.5702 ms. Best GFLOPs: 68.0878
[15:20:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"] Trial #31: GFLOPs: 42.3944. Time: 5.4550 ms. Best GFLOPs: 68.0878
[15:20:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |        78.9111 |    2930.6542 |            84988.9707 |     32 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |         2.6529 |      18.9137 |              548.4970 |     32 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |       140.1687 |    1650.2350 |            47856.8157 |     32 |            
 29 |                                                        fused_add_2 |     50176 |     30 |         6.8155 |       7.3621 |              220.8620 |     32 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |       157.6358 |    2934.1194 |             2934.1194 |     32 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |         3.0070 |      33.3730 |               33.3730 |     32 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |        38.9278 |    5940.7738 |             5940.7738 |     32 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |        68.0878 |    3396.5166 |             3396.5166 |     32 |            
 34 |                                                        fused_add_3 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1088
Total latency (us): 247604

[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #0: GFLOPs: 1.7518. Time: 0.0143 ms. Best GFLOPs: 1.7518
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #1: GFLOPs: 2.0930. Time: 0.0120 ms. Best GFLOPs: 2.0930
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #2: GFLOPs: 2.0271. Time: 0.0124 ms. Best GFLOPs: 2.0930
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #3: GFLOPs: 1.9162. Time: 0.0131 ms. Best GFLOPs: 2.0930
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #4: GFLOPs: 1.8480. Time: 0.0136 ms. Best GFLOPs: 2.0930
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #5: GFLOPs: 1.9243. Time: 0.0130 ms. Best GFLOPs: 2.0930
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #6: GFLOPs: 1.9163. Time: 0.0131 ms. Best GFLOPs: 2.0930
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #7: GFLOPs: 1.8289. Time: 0.0137 ms. Best GFLOPs: 2.0930
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #8: GFLOPs: 2.0606. Time: 0.0122 ms. Best GFLOPs: 2.0930
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #9: GFLOPs: 1.9839. Time: 0.0126 ms. Best GFLOPs: 2.0930
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #10: GFLOPs: 1.9889. Time: 0.0126 ms. Best GFLOPs: 2.0930
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #11: GFLOPs: 1.9199. Time: 0.0131 ms. Best GFLOPs: 2.0930
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #12: GFLOPs: 1.9502. Time: 0.0129 ms. Best GFLOPs: 2.0930
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #13: GFLOPs: 2.0560. Time: 0.0122 ms. Best GFLOPs: 2.0930
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #14: GFLOPs: 1.9178. Time: 0.0131 ms. Best GFLOPs: 2.0930
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #15: GFLOPs: 2.1747. Time: 0.0115 ms. Best GFLOPs: 2.1747
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #16: GFLOPs: 1.8768. Time: 0.0134 ms. Best GFLOPs: 2.1747
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #17: GFLOPs: 1.8667. Time: 0.0134 ms. Best GFLOPs: 2.1747
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #18: GFLOPs: 1.6520. Time: 0.0152 ms. Best GFLOPs: 2.1747
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #19: GFLOPs: 1.6113. Time: 0.0156 ms. Best GFLOPs: 2.1747
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #20: GFLOPs: 3.3352. Time: 0.0075 ms. Best GFLOPs: 3.3352
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #21: GFLOPs: 3.4675. Time: 0.0072 ms. Best GFLOPs: 3.4675
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #22: GFLOPs: 3.0871. Time: 0.0081 ms. Best GFLOPs: 3.4675
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #23: GFLOPs: 2.1777. Time: 0.0115 ms. Best GFLOPs: 3.4675
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #24: GFLOPs: 1.8302. Time: 0.0137 ms. Best GFLOPs: 3.4675
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #25: GFLOPs: 1.9808. Time: 0.0127 ms. Best GFLOPs: 3.4675
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #26: GFLOPs: 2.0663. Time: 0.0121 ms. Best GFLOPs: 3.4675
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #27: GFLOPs: 2.0525. Time: 0.0122 ms. Best GFLOPs: 3.4675
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #28: GFLOPs: 2.0301. Time: 0.0124 ms. Best GFLOPs: 3.4675
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #29: GFLOPs: 1.8377. Time: 0.0137 ms. Best GFLOPs: 3.4675
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #30: GFLOPs: 1.9260. Time: 0.0130 ms. Best GFLOPs: 3.4675
[15:20:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_add_3"] Trial #31: GFLOPs: 2.1179. Time: 0.0118 ms. Best GFLOPs: 3.4675
[15:20:58] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #34: "fused_add_3"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |        78.9111 |    2930.6542 |            84988.9707 |     32 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |         2.6529 |      18.9137 |              548.4970 |     32 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |       140.1687 |    1650.2350 |            47856.8157 |     32 |            
 29 |                                                        fused_add_2 |     50176 |     30 |         6.8155 |       7.3621 |              220.8620 |     32 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |       157.6358 |    2934.1194 |             2934.1194 |     32 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |         3.0070 |      33.3730 |               33.3730 |     32 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |        38.9278 |    5940.7738 |             5940.7738 |     32 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |        68.0878 |    3396.5166 |             3396.5166 |     32 |            
 34 |                                                        fused_add_3 |     25088 |      2 |         3.4675 |       7.2353 |               14.4706 |     32 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1120
Total latency (us): 247619

[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #0: GFLOPs: 33.7570. Time: 6.8500 ms. Best GFLOPs: 33.7570
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #1: GFLOPs: 29.0678. Time: 7.9551 ms. Best GFLOPs: 33.7570
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #2: GFLOPs: 42.1916. Time: 5.4806 ms. Best GFLOPs: 42.1916
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #3: GFLOPs: 30.5657. Time: 7.5652 ms. Best GFLOPs: 42.1916
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #4: GFLOPs: 71.0396. Time: 3.2550 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #5: GFLOPs: 31.2952. Time: 7.3889 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #6: GFLOPs: 4.0977. Time: 56.4307 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #7: GFLOPs: 16.9629. Time: 13.6318 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #8: GFLOPs: 20.2080. Time: 11.4428 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #9: GFLOPs: 42.0243. Time: 5.5024 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #10: GFLOPs: 40.1678. Time: 5.7568 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #11: GFLOPs: 36.8554. Time: 6.2741 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #12: GFLOPs: 47.0154. Time: 4.9183 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #13: GFLOPs: 54.2808. Time: 4.2600 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #14: GFLOPs: 20.1901. Time: 11.4530 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #15: GFLOPs: 13.4557. Time: 17.1851 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #16: GFLOPs: 39.3406. Time: 5.8778 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #17: GFLOPs: 38.4088. Time: 6.0204 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #18: GFLOPs: 28.3274. Time: 8.1630 ms. Best GFLOPs: 71.0396
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #19: GFLOPs: 95.2916. Time: 2.4266 ms. Best GFLOPs: 95.2916
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #20: GFLOPs: 26.8586. Time: 8.6094 ms. Best GFLOPs: 95.2916
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #21: GFLOPs: 42.1987. Time: 5.4797 ms. Best GFLOPs: 95.2916
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #22: GFLOPs: 31.1839. Time: 7.4152 ms. Best GFLOPs: 95.2916
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #23: GFLOPs: 16.5353. Time: 13.9844 ms. Best GFLOPs: 95.2916
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #24: GFLOPs: 0.7592. Time: 304.5945 ms. Best GFLOPs: 95.2916
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #25: GFLOPs: 58.0025. Time: 3.9867 ms. Best GFLOPs: 95.2916
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #26: GFLOPs: 18.5607. Time: 12.4583 ms. Best GFLOPs: 95.2916
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #27: GFLOPs: 8.5245. Time: 27.1261 ms. Best GFLOPs: 95.2916
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #28: GFLOPs: 52.8681. Time: 4.3738 ms. Best GFLOPs: 95.2916
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #29: GFLOPs: 24.7863. Time: 9.3292 ms. Best GFLOPs: 95.2916
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #30: GFLOPs: 6.7654. Time: 34.1791 ms. Best GFLOPs: 95.2916
[15:20:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #31: GFLOPs: 61.3326. Time: 3.7702 ms. Best GFLOPs: 95.2916
[15:21:01] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |        78.9111 |    2930.6542 |            84988.9707 |     32 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |         2.6529 |      18.9137 |              548.4970 |     32 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |       140.1687 |    1650.2350 |            47856.8157 |     32 |            
 29 |                                                        fused_add_2 |     50176 |     30 |         6.8155 |       7.3621 |              220.8620 |     32 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |       157.6358 |    2934.1194 |             2934.1194 |     32 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |         3.0070 |      33.3730 |               33.3730 |     32 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |        38.9278 |    5940.7738 |             5940.7738 |     32 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |        68.0878 |    3396.5166 |             3396.5166 |     32 |            
 34 |                                                        fused_add_3 |     25088 |      2 |         3.4675 |       7.2353 |               14.4706 |     32 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |        95.2916 |    2426.6155 |             4853.2309 |     32 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |            N/A |          N/A |                   N/A |      0 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1152
Total latency (us): 252472

[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #0: GFLOPs: 1.6556. Time: 0.0152 ms. Best GFLOPs: 1.6556
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #1: GFLOPs: 1.1827. Time: 0.0212 ms. Best GFLOPs: 1.6556
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #2: GFLOPs: 1.2234. Time: 0.0205 ms. Best GFLOPs: 1.6556
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #3: GFLOPs: 0.8483. Time: 0.0296 ms. Best GFLOPs: 1.6556
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #4: GFLOPs: 0.5035. Time: 0.0498 ms. Best GFLOPs: 1.6556
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #5: GFLOPs: 0.0017. Time: 14.6774 ms. Best GFLOPs: 1.6556
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #6: GFLOPs: 0.8588. Time: 0.0292 ms. Best GFLOPs: 1.6556
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #7: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'ax0'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(25088,), "float32"], T_layout_trans: T.Buffer[(1, 128, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans_1 = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        T_reshape = T.alloc_buffer([25088], dtype="float32")
        T_prelu = T.alloc_buffer([25088], dtype="float32")
        for i0_i1_i2_fused in T.parallel(896, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0 in T.serial(154):
                for ax0_1, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("T_layout_trans"):
                        ax0_2 = T.axis.spatial(1, ax0_1)
                        ax1_1 = T.axis.spatial(512, i0_i1_i2_fused % 896 // 7 * 4 + (i0_i1_i2_fused % 7 * 7 + ax0) // 49 + ax1)
                        ax2_1 = T.axis.spatial(7, (i0_i1_i2_fused % 7 * 7 + ax0) % 49 // 7 + ax2)
                        ax3_1 = T.axis.spatial(7, ax0 % 7 + ax3)
                        T.reads(placeholder[ax0_2, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                        T.writes(T_layout_trans_1[ax0_2, ax1_1, ax2_1, ax3_1])
                        T_layout_trans_1[ax0_2, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_2 < 1 and ax1_1 < 512 and ax2_1 < 7 and ax3_1 < 7, placeholder[ax0_2, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], T.float32(0), dtype="float32")
                with T.block("T_reshape"):
                    ax0_3 = T.axis.spatial(25088, i0_i1_i2_fused % 896 // 7 * 196 + i0_i1_i2_fused % 7 * 7 + ax0)
                    T.reads(T_layout_trans_1[0, ax0_3 % 25088 // 49, ax0_3 % 49 // 7, ax0_3 % 7])
                    T.writes(T_reshape[ax0_3])
                    T_reshape[ax0_3] = T_layout_trans_1[0, ax0_3 % 25088 // 49, ax0_3 % 49 // 7, ax0_3 % 7]
            for i3, i4 in T.grid(7, 4):
                for ax0_4 in T.serial(1):
                    with T.block("T_prelu"):
                        ax0_5 = T.axis.spatial(25088, i0_i1_i2_fused // 7 * 196 + i4 * 49 + i0_i1_i2_fused % 7 * 7 + i3)
                        T.reads(T_reshape[ax0_5], placeholder_1[ax0_5])
                        T.writes(T_prelu[ax0_5])
                        T_prelu[ax0_5] = T.Select(T.float32(0) < T_reshape[ax0_5], T_reshape[ax0_5], T_reshape[ax0_5] * placeholder_1[ax0_5])
                with T.block("T_layout_trans_1"):
                    ax0_6 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(128, i0_i1_i2_fused // 7)
                    ax2 = T.axis.spatial(7, i0_i1_i2_fused % 7)
                    ax3, ax4 = T.axis.remap("SS", [i3, i4])
                    T.reads(T_prelu[(ax1 * 196 + ax4 * 49 + ax2 * 7 + ax3) % 25088])
                    T.writes(T_layout_trans[ax0_6, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0_6, ax1, ax2, ax3, ax4] = T.if_then_else(ax0_6 < 1 and ax1 * 4 + ax4 < 512 and ax2 < 7 and ax3 < 7, T_prelu[((ax1 * 4 + ax4) * 49 + ax2 * 7 + ax3) % 25088], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_reshape", func_name="main")
b2 = sch.get_block(name="T_prelu", func_name="main")
b3 = sch.get_block(name="T_reshape_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=-2)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=4)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b1, decision=2)
sch.compute_at(block=b1, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l9, preserve_unit_loops=True)
sch.enter_postproc()
b10 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b10, ann_key="meta_schedule.unroll_explicit")
b11, b12, b13, b14 = sch.get_child_blocks(b10)
l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b11)
l23 = sch.fuse(l15, l16, l17)
sch.parallel(loop=l23)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
l24, l25 = sch.get_loops(block=b12)
sch.annotate(block_or_loop=l24, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l24, ann_key="pragma_unroll_explicit", ann_val=1)
l26, l27, l28, l29 = sch.get_loops(block=b13)
sch.annotate(block_or_loop=l26, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l26, ann_key="pragma_unroll_explicit", ann_val=1)
l30, l31, l32 = sch.get_loops(block=b14)
sch.annotate(block_or_loop=l30, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l30, ann_key="pragma_unroll_explicit", ann_val=1)
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #8: GFLOPs: 1.2328. Time: 0.0204 ms. Best GFLOPs: 1.6556
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #9: GFLOPs: 0.0028. Time: 8.9515 ms. Best GFLOPs: 1.6556
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #10: GFLOPs: 1.9133. Time: 0.0131 ms. Best GFLOPs: 1.9133
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #11: GFLOPs: 0.6962. Time: 0.0360 ms. Best GFLOPs: 1.9133
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #12: GFLOPs: 0.0110. Time: 2.2903 ms. Best GFLOPs: 1.9133
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #13: GFLOPs: 0.5402. Time: 0.0464 ms. Best GFLOPs: 1.9133
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #14: GFLOPs: 0.0021. Time: 11.9489 ms. Best GFLOPs: 1.9133
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #15: GFLOPs: 2.2683. Time: 0.0111 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #16: GFLOPs: 0.5985. Time: 0.0419 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #17: GFLOPs: 1.6121. Time: 0.0156 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #18: GFLOPs: 0.4788. Time: 0.0524 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #19: GFLOPs: 0.5159. Time: 0.0486 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #20: GFLOPs: 0.0002. Time: 100.7633 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #21: GFLOPs: 0.0847. Time: 0.2961 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #22: GFLOPs: 0.0104. Time: 2.4020 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #23: GFLOPs: 1.0155. Time: 0.0247 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #24: GFLOPs: 0.6991. Time: 0.0359 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #25: GFLOPs: 0.7085. Time: 0.0354 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #26: GFLOPs: 0.5877. Time: 0.0427 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #27: GFLOPs: 0.0097. Time: 2.5852 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #28: GFLOPs: 0.8402. Time: 0.0299 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #29: GFLOPs: 1.3753. Time: 0.0182 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #30: GFLOPs: 0.6791. Time: 0.0369 ms. Best GFLOPs: 2.2683
[15:21:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"] Trial #31: GFLOPs: 1.2249. Time: 0.0205 ms. Best GFLOPs: 2.2683
[15:21:03] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |        78.9111 |    2930.6542 |            84988.9707 |     32 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |         2.6529 |      18.9137 |              548.4970 |     32 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |       140.1687 |    1650.2350 |            47856.8157 |     32 |            
 29 |                                                        fused_add_2 |     50176 |     30 |         6.8155 |       7.3621 |              220.8620 |     32 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |       157.6358 |    2934.1194 |             2934.1194 |     32 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |         3.0070 |      33.3730 |               33.3730 |     32 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |        38.9278 |    5940.7738 |             5940.7738 |     32 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |        68.0878 |    3396.5166 |             3396.5166 |     32 |            
 34 |                                                        fused_add_3 |     25088 |      2 |         3.4675 |       7.2353 |               14.4706 |     32 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |        95.2916 |    2426.6155 |             4853.2309 |     32 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |         2.2683 |      11.0601 |               22.1201 |     32 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1184
Total latency (us): 252494

[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #0: GFLOPs: 4.0828. Time: 56.6546 ms. Best GFLOPs: 4.0828
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #1: GFLOPs: 36.6398. Time: 6.3131 ms. Best GFLOPs: 36.6398
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #2: GFLOPs: 11.3653. Time: 20.3523 ms. Best GFLOPs: 36.6398
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #3: GFLOPs: 5.0145. Time: 46.1282 ms. Best GFLOPs: 36.6398
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #4: GFLOPs: 22.3692. Time: 10.3406 ms. Best GFLOPs: 36.6398
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #5: GFLOPs: 6.8327. Time: 33.8536 ms. Best GFLOPs: 36.6398
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #6: GFLOPs: 47.3407. Time: 4.8861 ms. Best GFLOPs: 47.3407
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #7: GFLOPs: 20.6666. Time: 11.1925 ms. Best GFLOPs: 47.3407
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #8: GFLOPs: 19.6671. Time: 11.7614 ms. Best GFLOPs: 47.3407
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #9: GFLOPs: 52.6556. Time: 4.3929 ms. Best GFLOPs: 52.6556
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #10: GFLOPs: 15.6590. Time: 14.7718 ms. Best GFLOPs: 52.6556
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #11: GFLOPs: 67.3606. Time: 3.4339 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #12: GFLOPs: 21.3208. Time: 10.8491 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #13: GFLOPs: 0.8834. Time: 261.8421 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #14: GFLOPs: 22.8171. Time: 10.1376 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #15: GFLOPs: 2.5658. Time: 90.1526 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #16: GFLOPs: 24.1620. Time: 9.5734 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #17: GFLOPs: 37.6886. Time: 6.1374 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #18: GFLOPs: 56.0135. Time: 4.1296 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #19: GFLOPs: 17.7198. Time: 13.0539 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #20: GFLOPs: 3.2049. Time: 72.1746 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #21: GFLOPs: 47.4117. Time: 4.8788 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #22: GFLOPs: 15.7697. Time: 14.6681 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #23: GFLOPs: 12.9140. Time: 17.9117 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #24: GFLOPs: 34.5548. Time: 6.6940 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #25: GFLOPs: 13.5860. Time: 17.0257 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #26: GFLOPs: 18.1700. Time: 12.7304 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #27: GFLOPs: 5.2325. Time: 44.2069 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #28: GFLOPs: 23.2860. Time: 9.9335 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #29: GFLOPs: 4.6707. Time: 49.5239 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #30: GFLOPs: 50.7300. Time: 4.5597 ms. Best GFLOPs: 67.3606
[15:21:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"] Trial #31: GFLOPs: 15.0875. Time: 15.3313 ms. Best GFLOPs: 67.3606
[15:21:06] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |        78.9111 |    2930.6542 |            84988.9707 |     32 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |         2.6529 |      18.9137 |              548.4970 |     32 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |       140.1687 |    1650.2350 |            47856.8157 |     32 |            
 29 |                                                        fused_add_2 |     50176 |     30 |         6.8155 |       7.3621 |              220.8620 |     32 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |       157.6358 |    2934.1194 |             2934.1194 |     32 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |         3.0070 |      33.3730 |               33.3730 |     32 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |        38.9278 |    5940.7738 |             5940.7738 |     32 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |        68.0878 |    3396.5166 |             3396.5166 |     32 |            
 34 |                                                        fused_add_3 |     25088 |      2 |         3.4675 |       7.2353 |               14.4706 |     32 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |        95.2916 |    2426.6155 |             4853.2309 |     32 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |         2.2683 |      11.0601 |               22.1201 |     32 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |        67.3606 |    3433.9253 |             3433.9253 |     32 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1216
Total latency (us): 255928

[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #0: GFLOPs: 0.0000. Time: 0.0236 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #1: GFLOPs: 0.0000. Time: 0.0226 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #2: GFLOPs: 0.0000. Time: 0.0265 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #3: GFLOPs: 0.0000. Time: 0.0283 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #4: GFLOPs: 0.0000. Time: 0.0284 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #5: GFLOPs: 0.0000. Time: 0.0224 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #6: GFLOPs: 0.0000. Time: 0.0224 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #7: GFLOPs: 0.0000. Time: 0.0225 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #8: GFLOPs: 0.0000. Time: 0.0233 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #9: GFLOPs: 0.0000. Time: 0.0233 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #10: GFLOPs: 0.0000. Time: 0.0235 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #11: GFLOPs: 0.0000. Time: 0.0228 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #12: GFLOPs: 0.0000. Time: 0.0230 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #13: GFLOPs: 0.0000. Time: 0.0231 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #14: GFLOPs: 0.0000. Time: 0.0260 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #15: GFLOPs: 0.0000. Time: 0.0215 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #16: GFLOPs: 0.0000. Time: 0.0262 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #17: GFLOPs: 0.0000. Time: 0.0215 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #18: GFLOPs: 0.0000. Time: 0.0229 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #19: GFLOPs: 0.0000. Time: 0.0231 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #20: GFLOPs: 0.0000. Time: 0.0227 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #21: GFLOPs: 0.0000. Time: 0.0331 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #22: GFLOPs: 0.0000. Time: 0.0273 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #23: GFLOPs: 0.0000. Time: 0.0155 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #24: GFLOPs: 0.0000. Time: 0.0169 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #25: GFLOPs: 0.0000. Time: 0.0156 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #26: GFLOPs: 0.0000. Time: 0.0237 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #27: GFLOPs: 0.0000. Time: 0.0238 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #28: GFLOPs: 0.0000. Time: 0.0219 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #29: GFLOPs: 0.0000. Time: 0.0238 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #30: GFLOPs: 0.0000. Time: 0.0261 ms. Best GFLOPs: 0.0000
[15:21:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_layout_transform_nn_batch_flatten"] Trial #31: GFLOPs: 0.0000. Time: 0.0192 ms. Best GFLOPs: 0.0000
[15:21:08] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #38: "fused_layout_transform_nn_batch_flatten"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |        78.9111 |    2930.6542 |            84988.9707 |     32 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |         2.6529 |      18.9137 |              548.4970 |     32 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |       140.1687 |    1650.2350 |            47856.8157 |     32 |            
 29 |                                                        fused_add_2 |     50176 |     30 |         6.8155 |       7.3621 |              220.8620 |     32 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |       157.6358 |    2934.1194 |             2934.1194 |     32 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |         3.0070 |      33.3730 |               33.3730 |     32 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |        38.9278 |    5940.7738 |             5940.7738 |     32 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |        68.0878 |    3396.5166 |             3396.5166 |     32 |            
 34 |                                                        fused_add_3 |     25088 |      2 |         3.4675 |       7.2353 |               14.4706 |     32 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |        95.2916 |    2426.6155 |             4853.2309 |     32 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |         2.2683 |      11.0601 |               22.1201 |     32 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |        67.3606 |    3433.9253 |             3433.9253 |     32 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |         0.0001 |      15.4667 |               15.4667 |     32 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1248
Total latency (us): 255943

[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #0: GFLOPs: 22.1673. Time: 1.1589 ms. Best GFLOPs: 22.1673
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #1: GFLOPs: 23.5619. Time: 1.0903 ms. Best GFLOPs: 23.5619
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #2: GFLOPs: 12.5984. Time: 2.0392 ms. Best GFLOPs: 23.5619
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #3: GFLOPs: 5.3274. Time: 4.8224 ms. Best GFLOPs: 23.5619
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #4: GFLOPs: 14.2647. Time: 1.8010 ms. Best GFLOPs: 23.5619
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #5: GFLOPs: 52.3838. Time: 0.4904 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #6: GFLOPs: 37.7493. Time: 0.6806 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #7: GFLOPs: 5.3975. Time: 4.7597 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #8: GFLOPs: 30.9345. Time: 0.8305 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #9: GFLOPs: 38.3333. Time: 0.6702 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #10: GFLOPs: 6.0856. Time: 4.2215 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #11: GFLOPs: 4.3296. Time: 5.9337 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #12: GFLOPs: 5.8347. Time: 4.4031 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #13: GFLOPs: 33.1972. Time: 0.7739 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #14: GFLOPs: 30.3514. Time: 0.8464 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #15: GFLOPs: 11.3337. Time: 2.2668 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #16: GFLOPs: 33.1401. Time: 0.7752 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #17: GFLOPs: 2.7248. Time: 9.4283 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #18: GFLOPs: 9.6014. Time: 2.6757 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #19: GFLOPs: 13.7174. Time: 1.8729 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #20: GFLOPs: 25.2907. Time: 1.0158 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #21: GFLOPs: 5.2708. Time: 4.8741 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #22: GFLOPs: 5.1972. Time: 4.9432 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #23: GFLOPs: 5.0900. Time: 5.0473 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #24: GFLOPs: 26.8114. Time: 0.9582 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #25: GFLOPs: 28.5362. Time: 0.9003 ms. Best GFLOPs: 52.3838
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #26: GFLOPs: 62.7725. Time: 0.4093 ms. Best GFLOPs: 62.7725
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #27: GFLOPs: 52.5730. Time: 0.4887 ms. Best GFLOPs: 62.7725
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #28: GFLOPs: 3.4788. Time: 7.3850 ms. Best GFLOPs: 62.7725
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #29: GFLOPs: 54.9651. Time: 0.4674 ms. Best GFLOPs: 62.7725
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #30: GFLOPs: 11.5289. Time: 2.2284 ms. Best GFLOPs: 62.7725
[15:21:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_dense_add"] Trial #31: GFLOPs: 6.3395. Time: 4.0525 ms. Best GFLOPs: 62.7725
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #39: "fused_nn_dense_add"
 ID |                                                               Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                  fused_nn_contrib_conv2d_NCHWc_add |  12870144 |      1 |       119.9741 |     107.2744 |              107.2744 |     32 |            
  1 |                                fused_nn_contrib_conv2d_NCHWc_add_1 |  12895232 |      1 |        71.1961 |     181.1228 |              181.1228 |     32 |            
  2 |                                fused_nn_contrib_conv2d_NCHWc_add_2 |  12945408 |      1 |       177.7715 |      72.8205 |               72.8205 |     32 |            
  3 |                                             fused_layout_transform |         1 |      1 |         0.0000 |      21.1520 |               21.1520 |     32 |            
  4 |                                fused_nn_contrib_conv2d_NCHWc_add_3 |  25890816 |      1 |       137.5179 |     188.2723 |              188.2723 |     32 |            
  5 |                      fused_copy_subtract_multiply_layout_transform |     75264 |      1 |        10.4996 |       7.1683 |                7.1683 |     32 |            
  6 |                                fused_nn_contrib_conv2d_NCHWc_add_4 |  44154880 |      1 |       110.9313 |     398.0381 |              398.0381 |     32 |            
  7 |                    fused_layout_transform_reshape_nn_prelu_reshape |    802816 |      1 |        14.6934 |      54.6378 |               54.6378 |     32 |            
  8 |                                         fused_add_layout_transform |    802816 |      1 |        35.7330 |      22.4671 |               22.4671 |     32 |            
  9 |                                fused_nn_contrib_conv2d_NCHWc_add_5 | 925646848 |      1 |       100.8388 |    9179.4751 |             9179.4751 |     32 |            
 10 |   fused_layout_transform_reshape_nn_prelu_reshape_layout_transform |    802816 |      1 |         5.8223 |     137.8858 |              137.8858 |     32 |            
 11 |                              fused_nn_contrib_conv2d_NCHWc_add_add | 231612416 |      1 |        56.5788 |    4093.6234 |             4093.6234 |     32 |            
 12 |                                fused_nn_contrib_conv2d_NCHWc_add_6 | 231411712 |      2 |       112.2208 |    2062.1111 |             4124.2223 |     32 |            
 13 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1 |    200704 |      2 |        11.0459 |      18.1700 |               36.3400 |     32 |            
 14 |                            fused_nn_contrib_conv2d_NCHWc_add_add_1 | 231612416 |      2 |        37.2189 |    6222.9821 |            12445.9641 |     32 |            
 15 |                                                          fused_add |    200704 |      3 |        12.6909 |      15.8147 |               47.4442 |     32 |            
 16 |                                fused_nn_contrib_conv2d_NCHWc_add_7 | 462823424 |      1 |        63.6893 |    7266.8938 |             7266.8938 |     32 |            
 17 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2 |    401408 |      1 |         4.8025 |      83.5829 |               83.5829 |     32 |            
 18 |                            fused_nn_contrib_conv2d_NCHWc_add_add_2 | 231411712 |      1 |        57.8571 |    3999.7146 |             3999.7146 |     32 |            
 19 |                                fused_nn_contrib_conv2d_NCHWc_add_8 | 231311360 |     12 |        75.3362 |    3070.3875 |            36844.6500 |     32 |            
 20 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3 |    100352 |     12 |         4.3497 |      23.0708 |              276.8494 |     32 |            
 21 |                            fused_nn_contrib_conv2d_NCHWc_add_add_3 | 231411712 |     12 |       154.7633 |    1495.2621 |            17943.1451 |     32 |            
 22 |                                                        fused_add_1 |    100352 |     13 |        13.1190 |       7.6493 |               99.4415 |     32 |            
 23 |                                fused_nn_contrib_conv2d_NCHWc_add_9 | 462622720 |      1 |       182.4303 |    2535.8873 |             2535.8873 |     32 |            
 24 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4 |    200704 |      1 |         5.6626 |      35.4438 |               35.4438 |     32 |            
 25 |                            fused_nn_contrib_conv2d_NCHWc_add_add_4 | 231311360 |      1 |       156.2028 |    1480.8404 |             1480.8404 |     32 |            
 26 |                               fused_nn_contrib_conv2d_NCHWc_add_10 | 231261184 |     29 |        78.9111 |    2930.6542 |            84988.9707 |     32 |            
 27 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5 |     50176 |     29 |         2.6529 |      18.9137 |              548.4970 |     32 |            
 28 |                            fused_nn_contrib_conv2d_NCHWc_add_add_5 | 231311360 |     29 |       140.1687 |    1650.2350 |            47856.8157 |     32 |            
 29 |                                                        fused_add_2 |     50176 |     30 |         6.8155 |       7.3621 |              220.8620 |     32 |            
 30 |                               fused_nn_contrib_conv2d_NCHWc_add_11 | 462522368 |      1 |       157.6358 |    2934.1194 |             2934.1194 |     32 |            
 31 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6 |    100352 |      1 |         3.0070 |      33.3730 |               33.3730 |     32 |            
 32 |                            fused_nn_contrib_conv2d_NCHWc_add_add_6 | 231261184 |      1 |        38.9278 |    5940.7738 |             5940.7738 |     32 |            
 33 |                            fused_nn_contrib_conv2d_NCHWc_add_add_7 | 231261184 |      1 |        68.0878 |    3396.5166 |             3396.5166 |     32 |            
 34 |                                                        fused_add_3 |     25088 |      2 |         3.4675 |       7.2353 |               14.4706 |     32 |            
 35 |                               fused_nn_contrib_conv2d_NCHWc_add_12 | 231236096 |      2 |        95.2916 |    2426.6155 |             4853.2309 |     32 |            
 36 | fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7 |     25088 |      2 |         2.2683 |      11.0601 |               22.1201 |     32 |            
 37 |                 fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add | 231311360 |      1 |        67.3606 |    3433.9253 |             3433.9253 |     32 |            
 38 |                            fused_layout_transform_nn_batch_flatten |         1 |      1 |         0.0001 |      15.4667 |               15.4667 |     32 |            
 39 |                                                 fused_nn_dense_add |  25690624 |      1 |        62.7725 |     409.2653 |              409.2653 |     32 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1280
Total latency (us): 256353

[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #26: "fused_nn_contrib_conv2d_NCHWc_add_10"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #26 has finished. Remaining task(s): 39
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #28: "fused_nn_contrib_conv2d_NCHWc_add_add_5"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #28 has finished. Remaining task(s): 38
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #19: "fused_nn_contrib_conv2d_NCHWc_add_8"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #19 has finished. Remaining task(s): 37
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #21: "fused_nn_contrib_conv2d_NCHWc_add_add_3"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #21 has finished. Remaining task(s): 36
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #14: "fused_nn_contrib_conv2d_NCHWc_add_add_1"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #14 has finished. Remaining task(s): 35
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_nn_contrib_conv2d_NCHWc_add_5"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #9 has finished. Remaining task(s): 34
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #16: "fused_nn_contrib_conv2d_NCHWc_add_7"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #16 has finished. Remaining task(s): 33
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #32: "fused_nn_contrib_conv2d_NCHWc_add_add_6"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #32 has finished. Remaining task(s): 32
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #35: "fused_nn_contrib_conv2d_NCHWc_add_12"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #35 has finished. Remaining task(s): 31
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_nn_contrib_conv2d_NCHWc_add_6"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #12 has finished. Remaining task(s): 30
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_nn_contrib_conv2d_NCHWc_add_add"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #11 has finished. Remaining task(s): 29
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #18: "fused_nn_contrib_conv2d_NCHWc_add_add_2"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #18 has finished. Remaining task(s): 28
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #37: "fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #37 has finished. Remaining task(s): 27
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #33: "fused_nn_contrib_conv2d_NCHWc_add_add_7"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #33 has finished. Remaining task(s): 26
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #30: "fused_nn_contrib_conv2d_NCHWc_add_11"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #30 has finished. Remaining task(s): 25
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #23: "fused_nn_contrib_conv2d_NCHWc_add_9"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #23 has finished. Remaining task(s): 24
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #25: "fused_nn_contrib_conv2d_NCHWc_add_add_4"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #25 has finished. Remaining task(s): 23
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #27: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #27 has finished. Remaining task(s): 22
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #39: "fused_nn_dense_add"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #39 has finished. Remaining task(s): 21
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_nn_contrib_conv2d_NCHWc_add_4"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #6 has finished. Remaining task(s): 20
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #20: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #20 has finished. Remaining task(s): 19
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #29: "fused_add_2"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #29 has finished. Remaining task(s): 18
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_nn_contrib_conv2d_NCHWc_add_3"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #4 has finished. Remaining task(s): 17
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_nn_contrib_conv2d_NCHWc_add_1"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #1 has finished. Remaining task(s): 16
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #10 has finished. Remaining task(s): 15
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_nn_contrib_conv2d_NCHWc_add"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #0 has finished. Remaining task(s): 14
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #22: "fused_add_1"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #22 has finished. Remaining task(s): 13
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #17: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_2"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #17 has finished. Remaining task(s): 12
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_nn_contrib_conv2d_NCHWc_add_2"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #2 has finished. Remaining task(s): 11
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_layout_transform_reshape_nn_prelu_reshape"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #7 has finished. Remaining task(s): 10
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #15: "fused_add"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #15 has finished. Remaining task(s): 9
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #13 has finished. Remaining task(s): 8
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #24: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_4"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #24 has finished. Remaining task(s): 7
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #31: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_6"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #31 has finished. Remaining task(s): 6
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_add_layout_transform"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #8 has finished. Remaining task(s): 5
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #36: "fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #36 has finished. Remaining task(s): 4
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_layout_transform"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #3 has finished. Remaining task(s): 3
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #38: "fused_layout_transform_nn_batch_flatten"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #38 has finished. Remaining task(s): 2
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #34: "fused_add_3"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #34 has finished. Remaining task(s): 1
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_copy_subtract_multiply_layout_transform"
[15:21:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #5 has finished. Remaining task(s): 0
[15:23:51] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_add_layout_transform_1
[15:23:51] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_3
[15:23:51] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_1
[15:23:51] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_1
[15:23:51] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_1
[15:23:51] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_add_layout_transform_2
[15:23:51] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_add_layout_transform_3
[15:23:53] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_2
[15:23:53] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_5
[15:23:53] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_3
[15:23:54] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_add_layout_transform_4
[15:23:54] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_6
[15:23:54] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_3
[15:23:54] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_4
[15:23:54] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_3
[15:23:54] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_add_layout_transform_5
[15:23:54] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_add_layout_transform_6
[15:23:57] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_5
[15:23:57] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_8
[15:23:57] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_6
[15:23:59] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_add_layout_transform_7
[15:23:59] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_9
[15:23:59] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_5
[15:23:59] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_7
[15:23:59] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_5
[15:24:00] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_add_layout_transform_8
[15:24:00] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_add_layout_transform_9
[15:24:00] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_8
[15:24:00] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_11
[15:24:00] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_9
[15:24:01] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_add_layout_transform_10
[15:24:01] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_12
[15:24:01] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_reshape_nn_prelu_reshape_layout_transform_7
[15:24:01] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_10
[15:24:01] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_7
[15:24:01] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_add_layout_transform_11
[15:24:01] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_NCHWc_add_add_multiply_add
[15:24:01] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Warning: Cannot find workload: tvmgen_default_fused_layout_transform_nn_batch_flatten
[[-1.02558315e-01 -1.15963124e-01  8.11154917e-02  1.38195038e-01
   5.07063381e-02  1.81593299e-01 -2.58568972e-01 -8.69039372e-02
   1.67426705e-01 -1.56778991e-01  6.34564981e-02  6.24744222e-02
  -4.95257437e-01  1.06318131e-01 -1.12201929e-01 -1.12367965e-01
  -1.60388246e-01  5.20263314e-02  8.92400295e-02  1.17977962e-01
   2.67485410e-01  1.92458972e-01  7.64203742e-02 -1.14088058e-01
   4.62729074e-02 -1.18353754e-01  1.97693139e-01  6.89868405e-02
   2.62872100e-01  2.23111868e-01 -4.94682491e-02 -2.68154800e-01
  -2.00967193e-01  1.31003797e-01  8.32321495e-02 -1.15344539e-01
   1.59170836e-01  1.80473387e-01 -1.05326086e-01 -2.00597748e-01
  -1.31752089e-01 -1.78094268e-01 -4.25730087e-02  1.48578472e-02
   6.68882877e-02  5.65896034e-01  3.88344496e-01 -3.19792569e-01
   1.75185412e-01  4.73998785e-02 -5.28097786e-02 -2.08028764e-01
  -2.14016855e-01 -4.86913919e-02 -2.37506419e-01  1.13782987e-01
   9.74895060e-02  1.97467525e-02  1.67186677e-01  2.25037515e-01
   6.56373203e-02 -1.36174321e-01 -1.25796720e-01  2.58053988e-01
   2.11941212e-01  2.65023708e-01 -7.41114989e-02  1.08068533e-01
   3.96260619e-02 -7.44093060e-02  3.85659277e-01 -3.93695310e-02
   4.21891809e-01  3.59670848e-01 -1.33057028e-01 -4.37048217e-03
   5.00235222e-02  2.48111725e-01 -1.56610325e-01 -3.08823228e-01
   2.28794634e-01  2.57436603e-01 -8.82297158e-02  5.34421280e-02
  -4.01847601e-01  3.09545584e-02 -7.77864233e-02 -1.82430044e-01
   1.05304107e-01 -3.01263571e-01  6.88065141e-02 -1.05405495e-01
   8.96051340e-03  1.18104115e-01 -1.93724781e-01 -3.21590453e-01
   2.00895414e-01 -5.28561771e-02  2.36203521e-01  8.23755488e-02
  -2.47819811e-01  2.46571988e-01 -2.77583867e-01  7.16389567e-02
   2.73349226e-01 -1.04065999e-01  2.02156126e-01  3.01267862e-01
  -2.12267712e-01 -1.08643144e-01  7.50522316e-02 -3.08074564e-01
  -6.97488338e-02  2.68334776e-01 -7.70673677e-02 -2.21151754e-01
  -1.25284612e-01 -1.79436237e-01  1.31612092e-01  3.23001176e-01
  -4.60190289e-02 -2.13200506e-02  8.81996155e-02 -4.06771973e-02
   8.55758861e-02  6.96297660e-02 -4.50410753e-01  2.04333544e-01
  -6.87356070e-02  1.58890262e-01  4.38197702e-02  4.73927036e-02
   3.43460351e-01 -1.66045025e-01  2.27002427e-01 -2.27797851e-01
  -4.34226580e-02  2.72271037e-01  3.04355472e-01  1.90898523e-01
   3.76754522e-01  1.11831568e-01 -1.87527567e-01 -3.47633585e-02
   1.96201831e-01 -2.32014731e-01  2.18137711e-01  1.49689227e-01
   4.11520720e-01  2.89945662e-01 -1.37498498e-01  1.70824394e-01
   9.03387740e-02 -4.59694475e-01 -4.23429638e-01  4.53238236e-03
   1.97979152e-01 -7.31761456e-02 -1.52294427e-01  1.76185682e-01
  -3.45808297e-01  1.49122439e-03  8.72401986e-03  1.48955435e-01
   1.97086841e-01 -2.17184052e-01  2.47449577e-01 -2.49248356e-01
  -2.92208016e-01 -5.54395728e-02  2.45651975e-01 -4.45837006e-02
   7.23628104e-02  2.26250783e-01  1.23790748e-01 -2.56692916e-02
  -1.46670416e-01  2.61656314e-01 -6.54286668e-02  3.08562014e-02
  -2.21971422e-01  2.31430784e-01  1.18786484e-01  1.59713417e-01
  -1.96886390e-01  6.92951754e-02  1.21761441e-01 -2.97311038e-01
   1.30890146e-01 -2.84387529e-01 -1.98228836e-01  2.56482214e-01
  -1.03676528e-01  2.85366535e-01 -1.36665061e-01  4.35779184e-01
  -1.77614987e-01 -3.17052096e-01 -3.89721155e-01 -1.28567070e-01
   1.24767110e-01  3.39350671e-01 -2.23527297e-01  8.80868733e-03
  -2.05879360e-02  1.15691964e-02  2.13429838e-01 -7.89078325e-02
   1.86508238e-01 -2.37193376e-01  6.63956776e-02  6.07317239e-02
  -4.28998023e-02  6.16505370e-02  1.77070901e-01  3.04475904e-01
   5.57997376e-02  7.81845152e-02  3.33435535e-02  1.31309018e-01
  -1.66599378e-02 -1.89662576e-01  4.83053625e-01 -2.49324724e-01
   8.50963667e-02  9.78039578e-03 -8.64186883e-02 -2.63979226e-01
  -1.50477260e-01  2.25097582e-01 -9.70235243e-02 -1.81989059e-01
  -8.55201408e-02  1.16401106e-01  1.16452929e-02 -3.03942710e-01
   1.13443829e-01  4.46644157e-01  1.58805594e-01  1.84399232e-01
  -3.38214636e-02  1.59929544e-02  1.97138369e-01 -1.19114943e-01
  -5.68937026e-02  3.87161314e-01 -9.18615609e-02 -8.99555236e-02
   2.92818956e-02 -4.11272317e-01 -1.41308755e-01  8.72275233e-02
  -2.91907322e-02 -1.24276415e-01  2.65378326e-01 -1.40366957e-01
   1.12157322e-01  1.25294819e-01 -1.87734827e-01  3.57191637e-02
   2.42448896e-01 -1.29154790e-02 -1.11931905e-01 -1.95553198e-01
   1.99487522e-01  1.03449756e-02 -1.16469838e-01 -3.95540595e-01
  -1.43941939e-01  3.16099614e-01  3.80032718e-01  6.41699731e-01
   5.60299866e-02  1.05956137e-01 -3.22022140e-02 -6.32554665e-02
  -7.59380609e-02  5.76742738e-02  5.43563738e-02  2.74143443e-02
   3.26542296e-02 -3.08551222e-01  2.29127616e-01 -2.99653351e-01
   6.26529902e-02 -5.54595888e-02  1.79517046e-02 -2.93287009e-01
   1.42539293e-01  1.23959601e-01  2.63431311e-01 -2.35297129e-01
   2.68431846e-02 -1.25510246e-01  3.26595694e-01  1.41862288e-01
  -1.95545793e-01 -2.61852890e-02 -2.54326463e-01 -9.53933597e-03
   1.75090343e-01  1.83903947e-01 -2.84097582e-01  1.61246732e-01
   7.89409950e-02 -2.32535787e-02 -1.85600072e-01  1.18728362e-01
  -5.53887635e-02  4.70203757e-02  3.23000044e-01  1.59083068e-01
   7.04306066e-02 -2.53624439e-01  1.15006700e-01  1.15303859e-01
   8.24894905e-02  6.84612170e-02 -5.05614616e-02  3.30097109e-01
  -6.99519217e-02 -6.32169545e-02 -6.57227486e-02  1.43972650e-01
  -2.20108420e-01 -3.81714962e-02 -4.69708778e-02 -3.13003242e-01
  -1.68224573e-02  8.54169577e-03 -4.47189510e-01  4.15773004e-01
  -7.13163987e-02 -1.30358770e-01  5.32332435e-02 -1.39634058e-01
   2.76472062e-01 -3.50679755e-01  3.92158106e-02  1.96976051e-01
  -1.46471307e-01 -1.40748203e-01  4.39338595e-01  1.37445092e-01
   4.83149290e-03  1.61102474e-01  7.66560659e-02  1.07971624e-01
  -4.01329100e-01  1.04189761e-01  8.06784481e-02  2.61283904e-01
   3.15773427e-01  1.06826842e-01 -9.22580361e-02 -9.28241834e-02
   1.61626786e-01  3.22724938e-01  3.12202781e-01  1.25058085e-01
   2.96779156e-01 -6.22111447e-02 -4.50780615e-02 -1.52727827e-01
  -1.66879058e-01 -4.25716251e-01 -5.13967216e-01 -7.09131733e-02
  -1.57352790e-01  2.64308542e-01 -6.43549934e-02  2.45586142e-01
   5.53805800e-03 -3.96530390e-01 -6.60579205e-02 -1.71789557e-01
  -2.02870920e-01  1.38778135e-01 -1.40654087e-01 -1.61540866e-01
  -2.47231722e-01 -1.77000865e-01 -3.79807614e-02  1.46115005e-01
   2.81638831e-01  5.05855866e-02  1.89387888e-01 -4.19348478e-02
   1.77120008e-02  3.09086174e-01  2.83886969e-01 -1.97232887e-01
  -2.24250615e-01 -1.44351989e-01  1.10491533e-02 -2.11723953e-01
   1.86161950e-01 -1.52091697e-01  5.07200137e-03 -6.47522569e-01
   1.58263773e-01  1.68056265e-01 -1.01126119e-01  1.92598566e-01
  -1.20769173e-01  5.11552870e-01 -5.73481061e-02  1.19081005e-01
  -2.70942390e-01  1.26619309e-01 -4.87152815e-01  6.63137902e-03
   1.82156220e-01 -3.34296584e-01  1.04581952e-01 -1.90655693e-01
   2.43233174e-01  3.32145303e-01 -3.38094085e-02 -1.22545071e-01
  -1.03113919e-01 -4.14505571e-01  1.33934796e-01 -1.31049842e-01
   2.49005422e-01 -1.37431294e-01 -4.51115251e-01 -1.36422530e-01
  -1.71651468e-01  7.26715177e-02  1.03758983e-02  1.53382510e-01
  -1.06941313e-01  5.81847541e-02 -1.41288444e-01 -3.98059115e-02
   6.88302875e-01 -6.33062199e-02 -2.67154455e-01  4.29916382e-01
   7.50815794e-02  3.25048894e-01  4.62963656e-02 -1.65960081e-02
  -1.72879755e-01 -1.34530917e-01  2.85338610e-01  1.27314448e-01
  -3.70602794e-02  3.32104534e-01  8.30151886e-03 -3.17632914e-01
  -3.12668443e-01 -9.08051729e-02 -1.28798932e-02 -2.24409297e-01
   1.35432929e-05  1.45861536e-01  2.40635648e-01  1.90352760e-02
  -3.63874495e-01 -6.41611442e-02 -1.12812236e-01 -1.49074541e-02
   1.20970562e-01 -2.62552649e-01 -4.55731720e-01  3.45782489e-02
   1.97742373e-01 -5.32308698e-01 -1.41752250e-02 -1.41117424e-01
  -2.32384697e-01  2.60792404e-01  2.12274149e-01 -2.80655771e-01
   1.03540160e-02  6.25573918e-02 -5.03798127e-02  2.62256742e-01
  -4.18344975e-01  1.44999698e-01 -2.69870818e-01  1.95422038e-01
   2.03410715e-01  1.71754852e-01 -9.59306061e-02 -2.06528604e-03
   1.70090243e-01  1.06467560e-01  1.94702223e-01  1.51461720e-01
  -2.97687232e-01 -3.48597884e-01  1.85940281e-01  6.87513202e-02
   2.57622302e-02 -2.61668503e-01  1.00660145e-01 -5.10569930e-01
   2.96705924e-02 -1.58335209e-01 -6.87223524e-02 -1.73334181e-02
  -1.88274577e-01 -1.58287227e-01 -3.60218175e-02  1.75248712e-01
  -1.51607677e-01  3.42230767e-01  2.37410411e-01  3.97241533e-01]]
[[-1.02558412e-01 -1.15963377e-01  8.11157078e-02  1.38194650e-01
   5.07068112e-02  1.81594208e-01 -2.58569717e-01 -8.69031623e-02
   1.67425558e-01 -1.56778783e-01  6.34559616e-02  6.24745898e-02
  -4.95255232e-01  1.06318161e-01 -1.12201855e-01 -1.12367019e-01
  -1.60385907e-01  5.20275906e-02  8.92405063e-02  1.17978632e-01
   2.67484903e-01  1.92457795e-01  7.64206573e-02 -1.14086822e-01
   4.62736227e-02 -1.18354440e-01  1.97693363e-01  6.89860657e-02
   2.62872487e-01  2.23110691e-01 -4.94683757e-02 -2.68155783e-01
  -2.00965628e-01  1.31003633e-01  8.32323208e-02 -1.15344293e-01
   1.59171119e-01  1.80472746e-01 -1.05325282e-01 -2.00597510e-01
  -1.31751060e-01 -1.78094253e-01 -4.25740890e-02  1.48573881e-02
   6.68883249e-02  5.65896630e-01  3.88344228e-01 -3.19794148e-01
   1.75185516e-01  4.73997220e-02 -5.28096966e-02 -2.08028764e-01
  -2.14016810e-01 -4.86914665e-02 -2.37506121e-01  1.13782689e-01
   9.74898338e-02  1.97462179e-02  1.67186469e-01  2.25036293e-01
   6.56367317e-02 -1.36173218e-01 -1.25797376e-01  2.58053035e-01
   2.11941734e-01  2.65024066e-01 -7.41113275e-02  1.08067699e-01
   3.96254398e-02 -7.44102597e-02  3.85657489e-01 -3.93693447e-02
   4.21892285e-01  3.59671086e-01 -1.33056730e-01 -4.37084353e-03
   5.00225537e-02  2.48111218e-01 -1.56609461e-01 -3.08820844e-01
   2.28793576e-01  2.57435799e-01 -8.82287398e-02  5.34418486e-02
  -4.01848912e-01  3.09529230e-02 -7.77869150e-02 -1.82429656e-01
   1.05305418e-01 -3.01263064e-01  6.88069761e-02 -1.05404988e-01
   8.95997323e-03  1.18103549e-01 -1.93724319e-01 -3.21590960e-01
   2.00896487e-01 -5.28568998e-02  2.36203313e-01  8.23764428e-02
  -2.47818679e-01  2.46571362e-01 -2.77583361e-01  7.16388300e-02
   2.73348868e-01 -1.04065433e-01  2.02155828e-01  3.01267564e-01
  -2.12266758e-01 -1.08642712e-01  7.50525445e-02 -3.08073848e-01
  -6.97504580e-02  2.68335044e-01 -7.70680383e-02 -2.21151933e-01
  -1.25285357e-01 -1.79435596e-01  1.31612673e-01  3.23001564e-01
  -4.60192747e-02 -2.13191323e-02  8.81991908e-02 -4.06768546e-02
   8.55757892e-02  6.96286261e-02 -4.50411081e-01  2.04332590e-01
  -6.87346458e-02  1.58889502e-01  4.38200384e-02  4.73935455e-02
   3.43459994e-01 -1.66045010e-01  2.27001801e-01 -2.27798104e-01
  -4.34224829e-02  2.72272080e-01  3.04355741e-01  1.90899760e-01
   3.76755059e-01  1.11831293e-01 -1.87526762e-01 -3.47630791e-02
   1.96202010e-01 -2.32015610e-01  2.18138009e-01  1.49689764e-01
   4.11520600e-01  2.89945394e-01 -1.37499005e-01  1.70823812e-01
   9.03382525e-02 -4.59694743e-01 -4.23430562e-01  4.53403825e-03
   1.97977692e-01 -7.31761158e-02 -1.52294666e-01  1.76185623e-01
  -3.45808327e-01  1.49112009e-03  8.72449949e-03  1.48954988e-01
   1.97088405e-01 -2.17184097e-01  2.47450799e-01 -2.49247521e-01
  -2.92206973e-01 -5.54392077e-02  2.45652035e-01 -4.45845686e-02
   7.23623037e-02  2.26251528e-01  1.23790771e-01 -2.56689619e-02
  -1.46669999e-01  2.61655569e-01 -6.54276609e-02  3.08558363e-02
  -2.21970975e-01  2.31429860e-01  1.18787870e-01  1.59712136e-01
  -1.96887746e-01  6.92955554e-02  1.21759705e-01 -2.97310919e-01
   1.30888775e-01 -2.84387410e-01 -1.98228985e-01  2.56482899e-01
  -1.03676081e-01  2.85369068e-01 -1.36664584e-01  4.35779780e-01
  -1.77614480e-01 -3.17053080e-01 -3.89721394e-01 -1.28567487e-01
   1.24768935e-01  3.39352816e-01 -2.23527342e-01  8.80857836e-03
  -2.05876194e-02  1.15691517e-02  2.13429406e-01 -7.89076686e-02
   1.86509147e-01 -2.37194091e-01  6.63962737e-02  6.07324392e-02
  -4.28983122e-02  6.16517216e-02  1.77071750e-01  3.04475665e-01
   5.57994694e-02  7.81846195e-02  3.33435275e-02  1.31308407e-01
  -1.66599527e-02 -1.89662561e-01  4.83053327e-01 -2.49324709e-01
   8.50969777e-02  9.78156179e-03 -8.64166766e-02 -2.63980031e-01
  -1.50477350e-01  2.25097939e-01 -9.70239714e-02 -1.81987971e-01
  -8.55195299e-02  1.16401464e-01  1.16437431e-02 -3.03944528e-01
   1.13443635e-01  4.46644306e-01  1.58805341e-01  1.84399426e-01
  -3.38219777e-02  1.59932598e-02  1.97138399e-01 -1.19114786e-01
  -5.68943806e-02  3.87160987e-01 -9.18615609e-02 -8.99579227e-02
   2.92810500e-02 -4.11272675e-01 -1.41308606e-01  8.72259289e-02
  -2.91908812e-02 -1.24275550e-01  2.65378624e-01 -1.40366599e-01
   1.12156332e-01  1.25293627e-01 -1.87733918e-01  3.57197151e-02
   2.42448509e-01 -1.29169151e-02 -1.11932099e-01 -1.95553854e-01
   1.99488506e-01  1.03441188e-02 -1.16469748e-01 -3.95540893e-01
  -1.43941984e-01  3.16100895e-01  3.80033016e-01  6.41698599e-01
   5.60294352e-02  1.05956979e-01 -3.22029926e-02 -6.32566586e-02
  -7.59391412e-02  5.76737672e-02  5.43568879e-02  2.74133664e-02
   3.26551646e-02 -3.08551699e-01  2.29127765e-01 -2.99654782e-01
   6.26543239e-02 -5.54587543e-02  1.79524571e-02 -2.93286264e-01
   1.42538428e-01  1.23959169e-01  2.63431221e-01 -2.35297516e-01
   2.68431697e-02 -1.25511169e-01  3.26596588e-01  1.41861349e-01
  -1.95545927e-01 -2.61842366e-02 -2.54325718e-01 -9.53882467e-03
   1.75089791e-01  1.83903813e-01 -2.84096658e-01  1.61246836e-01
   7.89417252e-02 -2.32536197e-02 -1.85600922e-01  1.18728630e-01
  -5.53871170e-02  4.70207632e-02  3.22999924e-01  1.59083784e-01
   7.04310536e-02 -2.53625095e-01  1.15006737e-01  1.15305066e-01
   8.24901313e-02  6.84601739e-02 -5.05620651e-02  3.30097437e-01
  -6.99522346e-02 -6.32168874e-02 -6.57225400e-02  1.43972099e-01
  -2.20107362e-01 -3.81732024e-02 -4.69713137e-02 -3.13002229e-01
  -1.68227274e-02  8.54171813e-03 -4.47189689e-01  4.15773422e-01
  -7.13162869e-02 -1.30357429e-01  5.32325059e-02 -1.39634922e-01
   2.76470959e-01 -3.50679189e-01  3.92149538e-02  1.96975186e-01
  -1.46470547e-01 -1.40746489e-01  4.39337581e-01  1.37444541e-01
   4.83154226e-03  1.61102921e-01  7.66571909e-02  1.07971184e-01
  -4.01330292e-01  1.04190312e-01  8.06778669e-02  2.61285305e-01
   3.15773934e-01  1.06827781e-01 -9.22567695e-02 -9.28246677e-02
   1.61626443e-01  3.22723925e-01  3.12202990e-01  1.25058427e-01
   2.96778530e-01 -6.22101016e-02 -4.50789668e-02 -1.52728155e-01
  -1.66878343e-01 -4.25715387e-01 -5.13967276e-01 -7.09134415e-02
  -1.57353476e-01  2.64308035e-01 -6.43544570e-02  2.45585933e-01
   5.53776650e-03 -3.96530420e-01 -6.60573393e-02 -1.71789661e-01
  -2.02870041e-01  1.38777047e-01 -1.40655428e-01 -1.61541075e-01
  -2.47231662e-01 -1.77000791e-01 -3.79807465e-02  1.46114454e-01
   2.81637251e-01  5.05856834e-02  1.89387828e-01 -4.19345237e-02
   1.77118704e-02  3.09086293e-01  2.83885509e-01 -1.97232589e-01
  -2.24251643e-01 -1.44351870e-01  1.10491514e-02 -2.11723864e-01
   1.86162665e-01 -1.52090758e-01  5.07244281e-03 -6.47522926e-01
   1.58263788e-01  1.68056160e-01 -1.01125441e-01  1.92598671e-01
  -1.20768040e-01  5.11551082e-01 -5.73470034e-02  1.19080231e-01
  -2.70942479e-01  1.26620635e-01 -4.87155139e-01  6.63242163e-03
   1.82156891e-01 -3.34297478e-01  1.04582444e-01 -1.90655962e-01
   2.43234500e-01  3.32144618e-01 -3.38090360e-02 -1.22544795e-01
  -1.03115097e-01 -4.14506078e-01  1.33935213e-01 -1.31048650e-01
   2.49006584e-01 -1.37431517e-01 -4.51115757e-01 -1.36423096e-01
  -1.71651512e-01  7.26709738e-02  1.03756823e-02  1.53382152e-01
  -1.06940985e-01  5.81846833e-02 -1.41287714e-01 -3.98047715e-02
   6.88301027e-01 -6.33062571e-02 -2.67154694e-01  4.29916084e-01
   7.50825033e-02  3.25048327e-01  4.62977290e-02 -1.65956691e-02
  -1.72879070e-01 -1.34530395e-01  2.85338491e-01  1.27314746e-01
  -3.70599143e-02  3.32105160e-01  8.30128044e-03 -3.17633778e-01
  -3.12668383e-01 -9.08041224e-02 -1.28797591e-02 -2.24407718e-01
   1.26622617e-05  1.45862028e-01  2.40636259e-01  1.90337803e-02
  -3.63874435e-01 -6.41601682e-02 -1.12812400e-01 -1.49068031e-02
   1.20971173e-01 -2.62553364e-01 -4.55732375e-01  3.45788077e-02
   1.97742432e-01 -5.32307863e-01 -1.41757559e-02 -1.41117260e-01
  -2.32384011e-01  2.60792851e-01  2.12275311e-01 -2.80656397e-01
   1.03538139e-02  6.25573471e-02 -5.03797568e-02  2.62256652e-01
  -4.18345809e-01  1.45000875e-01 -2.69869626e-01  1.95421934e-01
   2.03409508e-01  1.71753883e-01 -9.59293395e-02 -2.06529256e-03
   1.70090660e-01  1.06467396e-01  1.94702834e-01  1.51462540e-01
  -2.97687680e-01 -3.48597258e-01  1.85941130e-01  6.87517971e-02
   2.57623568e-02 -2.61669397e-01  1.00659981e-01 -5.10570168e-01
   2.96705514e-02 -1.58334702e-01 -6.87218234e-02 -1.73330083e-02
  -1.88273817e-01 -1.58287749e-01 -3.60228457e-02  1.75248653e-01
  -1.51608407e-01  3.42230469e-01  2.37408742e-01  3.97241771e-01]]
