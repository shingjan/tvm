nohup: ignoring input
Starting to build with relay.
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
[19:02:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #0: "fused_layout_transform"
[19:02:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[19:02:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:02:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[19:02:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[19:02:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(24, 1, 11, 11, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 54, 54, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 24, 54, 54, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 24, 54, 54, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 24, 54, 54, 4, 3, 11, 11):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 3, oh * 4 + kh, ow * 4 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [24, 1, 11, 11, 3, 4], "float32"], [4, 4], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 4 + kh, ow * 4 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 54, 54, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 54, 54, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[19:02:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:02:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(24, 1, 11, 11, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 54, 54, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 24, 54, 54, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 18, 1, 1, 6, 2, 1, 1, 3, 1, 11, 1, 1, 3, 1, 4, 1, 11, 1, 1, 1, 9, 3, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(24, i1_0 * 6 + i1_1)
                    oh = T.axis.spatial(54, i2_1 * 27 + i2_2 * 9 + i2_3)
                    ow = T.axis.spatial(54, i3_0 * 3 + i3_3)
                    oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_2, i5_0, i6_1, i7_0])
                    T.reads(placeholder[n, ic // 3, oh * 4 + kh, ow * 4 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [24, 1, 11, 11, 3, 4], "float32"], [4, 4], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 4 + kh, ow * 4 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 24, 54, 54, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 6, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 3, 9])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[18, 1, 1, 3])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 11])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[11, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:02:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(24, 1, 11, 11, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 54, 54, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 24, 54, 54, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 18, 1, 1, 6, 2, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 11, 1, 1, 3, 1, 4, 1, 11, 1, 1, 1, 9, 3, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i1_0 * 6 + i1_1)
                        oh = T.axis.spatial(54, i2_1 * 27 + i2_2 * 9 + i2_3)
                        ow = T.axis.spatial(54, i3_0 * 3 + i3_3)
                        oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_2, i5_0, i6_1, i7_0])
                        T.reads(placeholder[n, ic // 3, oh * 4 + kh, ow * 4 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [24, 1, 11, 11, 3, 4], "float32"], [4, 4], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 4 + kh, ow * 4 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 27, 3, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(24, i1_0 * 6 + i1_1 + ax1)
                        ax2_1 = T.axis.spatial(54, i2_1 * 27 + ax2)
                        ax3_1 = T.axis.spatial(54, i3_0 * 3 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 6, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 3, 9])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[18, 1, 1, 3])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 11])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[11, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[19:02:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(24, 1, 11, 11, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 24, 54, 54, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 24, 54, 54, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 18, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 6, 2, 1, 1, 3, 1, 11, 1, 1, 3, 1, 4, 1, 11, 1, 1, 1, 9, 3, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(24, i1_0 * 6 + i1_1)
                        oh = T.axis.spatial(54, i2_1 * 27 + i2_2 * 9 + i2_3)
                        ow = T.axis.spatial(54, i3_0 * 3 + i3_3)
                        oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_2, i5_0, i6_1, i7_0])
                        T.reads(placeholder[n, ic // 3, oh * 4 + kh, ow * 4 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [24, 1, 11, 11, 3, 4], "float32"], [4, 4], [0, 0, 0, 0], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 3, oh * 4 + kh, ow * 4 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 54, 3, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(24, i1_0 * 6 + ax1)
                        ax2_1 = T.axis.spatial(54, ax2)
                        ax3_1 = T.axis.spatial(54, i3_0 * 3 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 6, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 3, 9])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[18, 1, 1, 3])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 11])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[11, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[19:02:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #2: "fused_nn_max_pool2d"
[19:02:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 54, 54, 4), "float32"], tensor: T.Buffer[(1, 24, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 24, 56, 56, 4):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(pad_temp[ax0, ax1, ax2, ax3, ax4])
                pad_temp[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax2 < 54 and ax3 < 54, placeholder[ax0, ax1, ax2, ax3, ax4], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 24, 27, 27, 4, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

[19:02:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:02:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 54, 54, 4), "float32"], tensor: T.Buffer[(1, 24, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            tensor_rf = T.alloc_buffer([1, 24, 27, 27, 4, 1], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 24, 27, 27, 4, 1, 9):
                with T.block("tensor_rf"):
                    vi5_i6_fused_0 = T.axis.spatial(1, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    rv0 = T.axis.reduce(3, i5_i6_fused_1 // 3)
                    rv1 = T.axis.reduce(3, i5_i6_fused_1 % 3)
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    with T.init():
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.float32(-3.4028234663852886e+38)
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], T.if_then_else(ax2 * 2 + rv0 < 54 and ax3 * 2 + rv1 < 54, placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4], T.float32(-3.4028234663852886e+38), dtype="float32"))
            for i0, i1, i2, i3, i4, i5_i6_fused_0 in T.grid(1, 24, 27, 27, 4, 1):
                with T.block("tensor"):
                    vi5_i6_fused_0 = T.axis.reduce(1, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 9])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
l17 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
[19:02:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 54, 54, 4), "float32"], tensor: T.Buffer[(1, 24, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 24, 27, 27, 4, 9], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 24, 27, 27, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 3, 3, 1):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(24, i1 + ax1)
                        ax2_1 = T.axis.spatial(56, i2 * 2 + ax2)
                        ax3_1 = T.axis.spatial(56, i3 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4 + ax4)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(ax2_1 < 54 and ax3_1 < 54, placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 9):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_1 = T.axis.spatial(9, i5_i6_fused_1)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                        T.reads(pad_temp[ax0, ax1, ax2 * 2 + vi5_i6_fused_1 // 3, ax3 * 2 + vi5_i6_fused_1 % 3, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        with T.init():
                            tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(-3.4028234663852886e+38)
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], pad_temp[ax0, ax1, ax2 * 2 + vi5_i6_fused_1 // 3, ax3 * 2 + vi5_i6_fused_1 % 3, ax4])
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 24, 27, 27, 4, 1, 9):
                with T.block("tensor"):
                    vi5_i6_fused_1 = T.axis.reduce(9, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 9])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l14, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
l17 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
[19:02:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 54, 54, 4), "float32"], tensor: T.Buffer[(1, 24, 27, 27, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 24, 56, 56, 4], dtype="float32")
            for i0, i1 in T.grid(1, 24):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 55, 55, 4):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(24, i1 + ax1)
                        ax2_1 = T.axis.spatial(56, ax2)
                        ax3_1 = T.axis.spatial(56, ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(ax2_1 < 54 and ax3_1 < 54, placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i2, i3, i4, i5, i6 in T.grid(27, 27, 4, 3, 3):
                    with T.block("tensor"):
                        ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                        T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                        T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                        with T.init():
                            tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[19:02:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #3: "fused_layout_transform_1"
[19:02:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 27, 27, 4), "float32"], T_layout_trans: T.Buffer[(1, 96, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 96, 27, 27):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 96 and ax2 < 27 and ax3 < 27, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

[19:02:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:02:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 27, 27, 4), "float32"], T_layout_trans: T.Buffer[(1, 96, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3 in T.grid(1, 96, 27, 27):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 96 and ax2 < 27 and ax3 < 27, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[19:02:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #4: "fused_nn_lrn"
[19:02:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 27, 27), "float32"], T_divide: T.Buffer[(1, 96, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_data = T.alloc_buffer([1, 100, 27, 27], dtype="float32")
        tensor = T.alloc_buffer([1, 96, 27, 27], dtype="float32")
        tensor_1 = T.alloc_buffer([1, 96, 27, 27], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 100, 27, 27):
            with T.block("pad_data"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 - 2, ax2, ax3])
                T.writes(pad_data[ax0, ax1, ax2, ax3])
                pad_data[ax0, ax1, ax2, ax3] = T.if_then_else(2 <= ax1 and ax1 < 98, placeholder[ax0, ax1 - 2, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 27, 27, 5):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rxs = T.axis.remap("SSSSR", [i0, i1, i2, i3, i4])
                T.reads(pad_data[ax0, ax1 + rxs, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor[ax0, ax1, ax2, ax3] = tensor[ax0, ax1, ax2, ax3] + pad_data[ax0, ax1 + rxs, ax2, ax3] * pad_data[ax0, ax1 + rxs, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 96, 27, 27):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor[ax0, ax1, ax2, ax3])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                tensor_1[ax0, ax1, ax2, ax3] = T.pow(T.float32(1) + T.float32(9.9999997473787516e-05) * tensor[ax0, ax1, ax2, ax3] * T.float32(0.20000000000000001), T.float32(0.75), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 96, 27, 27):
            with T.block("T_divide"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], tensor_1[ax0, ax1, ax2, ax3])
                T.writes(T_divide[ax0, ax1, ax2, ax3])
                T_divide[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] / tensor_1[ax0, ax1, ax2, ax3]
    

[19:02:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:02:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 27, 27), "float32"], T_divide: T.Buffer[(1, 96, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            pad_data = T.alloc_buffer([1, 100, 27, 27], dtype="float32")
            tensor = T.alloc_buffer([1, 96, 27, 27], dtype="float32")
            for i0, i1 in T.grid(1, 96):
                for ax0, ax1, ax2, ax3 in T.grid(1, 5, 27, 27):
                    with T.block("pad_data"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(100, i1 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(placeholder[ax0_1, ax1_1 - 2, ax2_1, ax3_1])
                        T.writes(pad_data[ax0_1, ax1_1, ax2_1, ax3_1])
                        pad_data[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(2 <= ax1_1 and ax1_1 < 98, placeholder[ax0_1, ax1_1 - 2, ax2_1, ax3_1], T.float32(0), dtype="float32")
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 27, 27, 5):
                    with T.block("tensor"):
                        ax0_2 = T.axis.spatial(1, ax0)
                        ax1_2 = T.axis.spatial(96, i1 + ax1)
                        ax2_2, ax3_2, rxs = T.axis.remap("SSR", [ax2, ax3, ax4])
                        T.reads(pad_data[ax0_2, ax1_2 + rxs, ax2_2, ax3_2])
                        T.writes(tensor[ax0_2, ax1_2, ax2_2, ax3_2])
                        with T.init():
                            tensor[ax0_2, ax1_2, ax2_2, ax3_2] = T.float32(0)
                        tensor[ax0_2, ax1_2, ax2_2, ax3_2] = tensor[ax0_2, ax1_2, ax2_2, ax3_2] + pad_data[ax0_2, ax1_2 + rxs, ax2_2, ax3_2] * pad_data[ax0_2, ax1_2 + rxs, ax2_2, ax3_2]
                for i2, i3 in T.grid(27, 27):
                    with T.block("T_divide"):
                        ax0_3, ax1_3, ax2_3, ax3_3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(placeholder[ax0_3, ax1_3, ax2_3, ax3_3], tensor[ax0_3, ax1_3, ax2_3, ax3_3])
                        T.writes(T_divide[ax0_3, ax1_3, ax2_3, ax3_3])
                        T_divide[ax0_3, ax1_3, ax2_3, ax3_3] = placeholder[ax0_3, ax1_3, ax2_3, ax3_3] / T.pow(T.float32(1) + T.float32(9.9999997473787516e-05) * tensor[ax0_3, ax1_3, ax2_3, ax3_3] * T.float32(0.20000000000000001), T.float32(0.75), dtype="float32")
    

b0 = sch.get_block(name="pad_data", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="tensor_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l6, preserve_unit_loops=True)
[19:02:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #5: "fused_nn_conv2d_add_nn_relu"
[19:02:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 27, 27), "float32"], placeholder_1: T.Buffer[(256, 48, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 96, 31, 31], dtype="float32")
        data_vec = T.alloc_buffer([2, 1, 12, 31, 4, 31], dtype="float32")
        kernel_vec = T.alloc_buffer([2, 32, 12, 5, 5, 4, 4], dtype="float32")
        conv = T.alloc_buffer([2, 1, 32, 27, 27, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 256, 27, 27], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 27, 27], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 96, 31, 31):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1])
                data_pad[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(2 <= i2_1 and i2_1 < 29 and 2 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(2, 1, 12, 31, 4, 31):
            with T.block("data_vec"):
                g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                T.reads(data_pad[n, g * 48 + C * 4 + c, h, w])
                T.writes(data_vec[g, n, C, h, c, w])
                data_vec[g, n, C, h, c, w] = data_pad[n, g * 48 + C * 4 + c, h, w]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(2, 32, 12, 5, 5, 4, 4):
            with T.block("kernel_vec"):
                g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(2, 1, 32, 27, 27, 4, 48, 5, 5):
            with T.block("conv"):
                g, n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3 in T.grid(1, 256, 27, 27):
            with T.block("output_unpack"):
                n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv[c // 128, n, c % 128 // 4, h, w, c % 4])
                T.writes(output_unpack[n, c, h, w])
                T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 96, 27, 27], "float32"], ["TENSOR", [256, 48, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], 2, "float32"]})
                output_unpack[n, c, h, w] = conv[c // 128, n, c % 128 // 4, h, w, c % 4]
        for i0, i1, i2, i3 in T.grid(1, 256, 27, 27):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 27, 27):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[19:03:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:03:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 27, 27), "float32"], placeholder_1: T.Buffer[(256, 48, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([2, 1, 12, 31, 4, 31], dtype="float32")
            conv = T.alloc_buffer([2, 1, 32, 27, 27, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 256, 27, 27], dtype="float32")
            conv_global = T.alloc_buffer([2, 1, 32, 27, 27, 4], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(2, 1, 12, 31, 4, 31):
                with T.block("data_vec"):
                    g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[n, g * 48 + C * 4 + c, h - 2, w - 2])
                    T.writes(data_vec[g, n, C, h, c, w])
                    data_vec[g, n, C, h, c, w] = T.if_then_else(2 <= h and h < 29 and 2 <= w and w < 29, placeholder[n, g * 48 + C * 4 + c, h - 2, w - 2], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_1 in T.grid(1, 1, 1, 1, 1, 2, 1, 1, 16, 1, 1, 1):
                for i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(3, 5, 5, 1, 1, 1, 1, 27, 1, 16, 1, 1, 2, 1, 2, 27, 1, 2):
                    with T.block("conv"):
                        g = T.axis.spatial(2, i0_3)
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i2_1 * 2 + i2_3)
                        oh, ow = T.axis.remap("SS", [i3_3, i4_2])
                        oc_block = T.axis.spatial(4, i5_0 * 2 + i5_3)
                        ic = T.axis.reduce(48, i6_0 * 16 + i6_1)
                        kh, kw = T.axis.remap("RR", [i7_0, i8_0])
                        T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], placeholder_1[g * 128 + oc_chunk * 4 + oc_block, ic, kh, kw])
                        T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * placeholder_1[g * 128 + oc_chunk * 4 + oc_block, ic // 4 * 4 + ic % 4, kh, kw]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 2, 27, 27, 2):
                    with T.block("conv_global"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(32, i2_1 * 2 + ax2)
                        v3, v4 = T.axis.remap("SS", [ax3, ax4])
                        v5 = T.axis.spatial(4, i5_0 * 2 + ax5)
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 256, 27, 27):
                with T.block("output_unpack"):
                    n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(conv[c // 128, n, c % 128 // 4, h, w, c % 4])
                    T.writes(output_unpack[n, c, h, w])
                    T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 96, 27, 27], "float32"], ["TENSOR", [256, 48, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], 2, "float32"]})
                    output_unpack[n, c, h, w] = conv[c // 128, n, c % 128 // 4, h, w, c % 4]
            for i0, i1, i2, i3 in T.grid(1, 256, 27, 27):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="data_vec", func_name="main")
b2 = sch.get_block(name="kernel_vec", func_name="main")
b3 = sch.get_block(name="conv", func_name="main")
b4 = sch.get_block(name="output_unpack", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b3)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 2])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 16, 1, 2])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 1, 27])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 27, 1])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[3, 16])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[5, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[5, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b3, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l61, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b4, decision=-1)
sch.compute_at(block=b4, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b2, decision=-2)
sch.compute_at(block=b2, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
[19:03:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 27, 27), "float32"], placeholder_1: T.Buffer[(256, 48, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 96, 31, 31], dtype="float32")
            kernel_vec = T.alloc_buffer([2, 32, 12, 5, 5, 4, 4], dtype="float32")
            conv = T.alloc_buffer([2, 1, 32, 27, 27, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 256, 27, 27], dtype="float32")
            conv_global = T.alloc_buffer([2, 1, 32, 27, 27, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0 in T.grid(1, 1, 1, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 32, 12, 5, 5, 4, 2):
                    with T.block("kernel_vec"):
                        g, out_channel, in_channel, h, w, ci = T.axis.remap("SSSSSS", [ax0, ax1, ax2, ax3, ax4, ax5])
                        co = T.axis.spatial(4, i5_0 * 2 + ax6)
                        T.reads(placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0, i7_0 in T.grid(1, 1, 16, 1, 1, 1, 3, 5):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 64, 27, 31):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(96, i6_0 * 16 + ax1)
                            i2 = T.axis.spatial(31, i7_0 + ax2)
                            i3 = T.axis.spatial(31, ax3)
                            T.reads(placeholder[i0, i1, i2 - 2, i3 - 2])
                            T.writes(data_pad[i0, i1, i2, i3])
                            data_pad[i0, i1, i2, i3] = T.if_then_else(2 <= i2 and i2 < 29 and 2 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 2, i3 - 2], T.float32(0), dtype="float32")
                    for i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(5, 1, 1, 1, 1, 27, 1, 16, 1, 1, 2, 1, 2, 27, 1, 2):
                        with T.block("conv"):
                            g = T.axis.spatial(2, i0_3)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i2_1 * 2 + i2_3)
                            oh, ow = T.axis.remap("SS", [i3_3, i4_2])
                            oc_block = T.axis.spatial(4, i5_0 * 2 + i5_3)
                            ic = T.axis.reduce(48, i6_0 * 16 + i6_1)
                            kh, kw = T.axis.remap("RR", [i7_0, i8_0])
                            T.reads(data_pad[n, g * 48 + ic, oh + kh, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_pad[n, g * 48 + ic // 4 * 4 + ic % 4, oh + kh, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 32, 27, 27, 2):
                    with T.block("conv_global"):
                        v0, v1, v2, v3, v4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        v5 = T.axis.spatial(4, i5_0 * 2 + ax5)
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 256, 27, 27):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(256, i1 + ax1)
                        h = T.axis.spatial(27, i2 + ax2)
                        w = T.axis.spatial(27, i3 + ax3)
                        T.reads(conv[c // 128, n, c % 128 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 96, 27, 27], "float32"], ["TENSOR", [256, 48, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], 2, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 128, n, c % 128 // 4, h, w, c % 4]
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="data_vec", func_name="main")
b2 = sch.get_block(name="kernel_vec", func_name="main")
b3 = sch.get_block(name="conv", func_name="main")
b4 = sch.get_block(name="output_unpack", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b3)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 2])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 16, 1, 2])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 1, 27])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 27, 1])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[3, 16])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[5, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[5, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b3, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l60, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b4, decision=3)
sch.compute_at(block=b4, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b2, decision=5)
sch.compute_at(block=b2, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=13)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
[19:03:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 27, 27), "float32"], placeholder_1: T.Buffer[(256, 48, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 96, 31, 31], dtype="float32")
            kernel_vec = T.alloc_buffer([2, 32, 12, 5, 5, 4, 4], dtype="float32")
            conv = T.alloc_buffer([2, 1, 32, 27, 27, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 256, 27, 27], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0 in T.grid(1, 1, 1, 1, 1, 2, 1, 1, 16, 1, 1, 1, 3):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 2, 4, 5, 5, 4, 2):
                    with T.block("kernel_vec"):
                        g = T.axis.spatial(2, ax0)
                        out_channel = T.axis.spatial(32, i2_1 * 2 + ax1)
                        in_channel = T.axis.spatial(12, i6_0 * 4 + ax2)
                        h, w, ci = T.axis.remap("SSS", [ax3, ax4, ax5])
                        co = T.axis.spatial(4, i5_0 * 2 + ax6)
                        T.reads(placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                for i7_0 in T.serial(5):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 64, 27, 31):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(96, i6_0 * 16 + ax1)
                            i2 = T.axis.spatial(31, i7_0 + ax2)
                            i3 = T.axis.spatial(31, ax3)
                            T.reads(placeholder[i0, i1, i2 - 2, i3 - 2])
                            T.writes(data_pad[i0, i1, i2, i3])
                            data_pad[i0, i1, i2, i3] = T.if_then_else(2 <= i2 and i2 < 29 and 2 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 2, i3 - 2], T.float32(0), dtype="float32")
                    for i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(5, 1, 1, 1, 1, 27, 1, 16, 1, 1, 2, 1, 2, 27, 1, 2):
                        with T.block("conv"):
                            g = T.axis.spatial(2, i0_3)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i2_1 * 2 + i2_3)
                            oh, ow = T.axis.remap("SS", [i3_3, i4_2])
                            oc_block = T.axis.spatial(4, i5_0 * 2 + i5_3)
                            ic = T.axis.reduce(48, i6_0 * 16 + i6_1)
                            kh, kw = T.axis.remap("RR", [i7_0, i8_0])
                            T.reads(data_pad[n, g * 48 + ic, oh + kh, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_pad[n, g * 48 + ic // 4 * 4 + ic % 4, oh + kh, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1 in T.grid(1, 256):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 27, 27):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(256, i1 + ax1)
                        h, w = T.axis.remap("SS", [ax2, ax3])
                        T.reads(conv[c // 128, n, c % 128 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 96, 27, 27], "float32"], ["TENSOR", [256, 48, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], 2, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 128, n, c % 128 // 4, h, w, c % 4]
                for i2, i3 in T.grid(27, 27):
                    with T.block("T_relu"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="data_vec", func_name="main")
b2 = sch.get_block(name="kernel_vec", func_name="main")
b3 = sch.get_block(name="conv", func_name="main")
b4 = sch.get_block(name="output_unpack", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b3)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 2])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 16, 1, 2])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 1, 27])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 27, 1])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[3, 16])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[5, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[5, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b4, decision=1)
sch.compute_at(block=b4, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b2, decision=12)
sch.compute_at(block=b2, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=13)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
[19:03:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #6: "fused_nn_max_pool2d_1"
[19:03:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], tensor: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3])
                T.writes(pad_temp[ax0, ax1, ax2, ax3])
                pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(ax2 < 27 and ax3 < 27, placeholder[ax0, ax1, ax2, ax3], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 13, 13, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[19:03:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:03:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], tensor: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 256, 13, 13, 9], dtype="float32")
            for i0, i1, i2, i3, i4_i5_fused_0 in T.grid(1, 256, 13, 13, 9):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1 + ax1)
                        ax2_1 = T.axis.spatial(28, i2 * 2 + i4_i5_fused_0 // 3 + ax2)
                        ax3_1 = T.axis.spatial(28, i3 * 2 + i4_i5_fused_0 % 3 + ax3)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax2_1 < 27 and ax3_1 < 27, placeholder[ax0_1, ax1_1, ax2_1, ax3_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i4_i5_fused_1 in T.serial(1):
                    with T.block("tensor_rf"):
                        vi4_i5_fused_0 = T.axis.spatial(9, i4_i5_fused_0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3 = T.axis.remap("SSS", [i1, i2, i3])
                        T.reads(pad_temp[ax0, ax1, ax2 * 2 + vi4_i5_fused_0 // 3, ax3 * 2 + vi4_i5_fused_0 % 3])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_0])
                        with T.init():
                            tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_0] = T.float32(-3.4028234663852886e+38)
                        tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_0] = T.max(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_0], pad_temp[ax0, ax1, ax2 * 2 + vi4_i5_fused_0 // 3, ax3 * 2 + vi4_i5_fused_0 % 3])
            for i0, i1, i2, i3, i4_i5_fused_0, i4_i5_fused_1 in T.grid(1, 256, 13, 13, 9, 1):
                with T.block("tensor"):
                    vi4_i5_fused_0 = T.axis.reduce(9, i4_i5_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3 = T.axis.remap("SSS", [i1, i2, i3])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_0])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b1)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[9, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
l16 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l16, preserve_unit_loops=True)
[19:03:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], tensor: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            tensor_rf = T.alloc_buffer([1, 256, 13, 13, 1], dtype="float32")
            for i0, i1, i2, i3, i4_i5_fused_0, i4_i5_fused_1 in T.grid(1, 256, 13, 13, 9, 1):
                with T.block("tensor_rf"):
                    vi4_i5_fused_1 = T.axis.spatial(1, i4_i5_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3 = T.axis.remap("SSS", [i1, i2, i3])
                    rv0 = T.axis.reduce(3, i4_i5_fused_0 // 3)
                    rv1 = T.axis.reduce(3, i4_i5_fused_0 % 3)
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1])
                    with T.init():
                        tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1] = T.float32(-3.4028234663852886e+38)
                    tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1], T.if_then_else(ax2 * 2 + rv0 < 27 and ax3 * 2 + rv1 < 27, placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1], T.float32(-3.4028234663852886e+38), dtype="float32"))
            for i0, i1, i2, i3, i4_i5_fused_1 in T.grid(1, 256, 13, 13, 1):
                with T.block("tensor"):
                    vi4_i5_fused_1 = T.axis.reduce(1, i4_i5_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3 = T.axis.remap("SSS", [i1, i2, i3])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b1)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[9, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
l16 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l16, preserve_unit_loops=True)
[19:03:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], tensor: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 13, 13, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], T.if_then_else(ax2 * 2 + rv0 < 27 and ax3 * 2 + rv1 < 27, placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1], T.float32(-3.4028234663852886e+38), dtype="float32"))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[19:03:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #7: "fused_nn_lrn_1"
[19:03:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], T_divide: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_data = T.alloc_buffer([1, 260, 13, 13], dtype="float32")
        tensor = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
        tensor_1 = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 260, 13, 13):
            with T.block("pad_data"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 - 2, ax2, ax3])
                T.writes(pad_data[ax0, ax1, ax2, ax3])
                pad_data[ax0, ax1, ax2, ax3] = T.if_then_else(2 <= ax1 and ax1 < 258, placeholder[ax0, ax1 - 2, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 5):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rxs = T.axis.remap("SSSSR", [i0, i1, i2, i3, i4])
                T.reads(pad_data[ax0, ax1 + rxs, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor[ax0, ax1, ax2, ax3] = tensor[ax0, ax1, ax2, ax3] + pad_data[ax0, ax1 + rxs, ax2, ax3] * pad_data[ax0, ax1 + rxs, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor[ax0, ax1, ax2, ax3])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                tensor_1[ax0, ax1, ax2, ax3] = T.pow(T.float32(1) + T.float32(9.9999997473787516e-05) * tensor[ax0, ax1, ax2, ax3] * T.float32(0.20000000000000001), T.float32(0.75), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("T_divide"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], tensor_1[ax0, ax1, ax2, ax3])
                T.writes(T_divide[ax0, ax1, ax2, ax3])
                T_divide[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] / tensor_1[ax0, ax1, ax2, ax3]
    

[19:03:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:03:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], T_divide: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            pad_data = T.alloc_buffer([1, 260, 13, 13], dtype="float32")
            tensor = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
            for i0, i1 in T.grid(1, 256):
                for ax0, ax1, ax2, ax3 in T.grid(1, 5, 13, 13):
                    with T.block("pad_data"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(260, i1 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(placeholder[ax0_1, ax1_1 - 2, ax2_1, ax3_1])
                        T.writes(pad_data[ax0_1, ax1_1, ax2_1, ax3_1])
                        pad_data[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(2 <= ax1_1 and ax1_1 < 258, placeholder[ax0_1, ax1_1 - 2, ax2_1, ax3_1], T.float32(0), dtype="float32")
                for i2, i3 in T.grid(13, 13):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 5):
                        with T.block("tensor"):
                            ax0_2 = T.axis.spatial(1, ax0)
                            ax1_2 = T.axis.spatial(256, i1 + ax1)
                            ax2_2 = T.axis.spatial(13, i2 + ax2)
                            ax3_2 = T.axis.spatial(13, i3 + ax3)
                            rxs = T.axis.reduce(5, ax4)
                            T.reads(pad_data[ax0_2, ax1_2 + rxs, ax2_2, ax3_2])
                            T.writes(tensor[ax0_2, ax1_2, ax2_2, ax3_2])
                            with T.init():
                                tensor[ax0_2, ax1_2, ax2_2, ax3_2] = T.float32(0)
                            tensor[ax0_2, ax1_2, ax2_2, ax3_2] = tensor[ax0_2, ax1_2, ax2_2, ax3_2] + pad_data[ax0_2, ax1_2 + rxs, ax2_2, ax3_2] * pad_data[ax0_2, ax1_2 + rxs, ax2_2, ax3_2]
                    with T.block("T_divide"):
                        ax0_3, ax1_3, ax2_3, ax3_3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(placeholder[ax0_3, ax1_3, ax2_3, ax3_3], tensor[ax0_3, ax1_3, ax2_3, ax3_3])
                        T.writes(T_divide[ax0_3, ax1_3, ax2_3, ax3_3])
                        T_divide[ax0_3, ax1_3, ax2_3, ax3_3] = placeholder[ax0_3, ax1_3, ax2_3, ax3_3] / T.pow(T.float32(1) + T.float32(9.9999997473787516e-05) * tensor[ax0_3, ax1_3, ax2_3, ax3_3] * T.float32(0.20000000000000001), T.float32(0.75), dtype="float32")
    

b0 = sch.get_block(name="pad_data", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="tensor_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b1, decision=3)
sch.compute_at(block=b1, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l6, preserve_unit_loops=True)
[19:03:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #8: "fused_layout_transform_2"
[19:03:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], T_layout_trans: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 13, 13, 4):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 13 and ax3 < 13, placeholder[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[19:03:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:03:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], T_layout_trans: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 13, 13, 4):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1 * 4 + ax4, ax2, ax3])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 13 and ax3 < 13, placeholder[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[19:03:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[19:03:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(96, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 96, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 96, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 15, 15, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 96, 13, 13, 4, 256, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 13, 13, 4], "float32"], ["TENSOR", [96, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 13, 13, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[19:03:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:03:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(96, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 15, 15, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 96, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 13, 1, 1, 1, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 15, 3, 4):
                    with T.block("data_pad"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(15, i3_0 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 3, 1, 1, 6, 1, 1, 2, 16, 1, 3, 1, 8, 13, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(96, i1_0 * 48 + i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_0])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0 * 16 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 13, 13, 4], "float32"], ["TENSOR", [96, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 96, 13, 13, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 6, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:03:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(96, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 15, 15, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 96, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0 in T.grid(1, 2, 1, 13):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 15, 3, 4):
                    with T.block("data_pad"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(15, i3_0 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 2):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 3, 1, 1, 6, 1, 1, 2, 16, 1, 3, 1, 8, 13, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(96, i1_0 * 48 + i1_2 * 8 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_3, i3_0])
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic = T.axis.reduce(256, i5_0 * 16 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 13, 13, 4], "float32"], ["TENSOR", [96, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 48, 13, 1, 2):
                        with T.block("T_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(96, i1_0 * 48 + ax1)
                            ax2_1 = T.axis.spatial(13, ax2)
                            ax3_1 = T.axis.spatial(13, i3_0 + ax3)
                            ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 6, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[19:03:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(96, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 96, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 13, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 16, 3, 1, 1, 6, 1, 1, 2, 16, 1, 3, 1, 8, 13, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(96, i1_0 * 48 + i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_0])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0 * 16 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 13, 13, 4], "float32"], ["TENSOR", [96, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 14 and 1 <= ow + kw and ow + kw < 14, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 48, 13, 1, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(96, i1_0 * 48 + ax1)
                        ax2_1 = T.axis.spatial(13, ax2)
                        ax3_1 = T.axis.spatial(13, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 6, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[16, 16])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[19:03:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #10: "fused_layout_transform_3"
[19:03:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 13, 13, 4), "float32"], T_layout_trans: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 384 and ax2 < 13 and ax3 < 13, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

[19:03:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:03:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 13, 13, 4), "float32"], T_layout_trans: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 384 and ax2 < 13 and ax3 < 13, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[19:03:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #11: "fused_nn_conv2d_add_nn_relu_1"
[19:03:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(384, 192, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 384, 15, 15], dtype="float32")
        data_vec = T.alloc_buffer([2, 1, 48, 15, 4, 15], dtype="float32")
        kernel_vec = T.alloc_buffer([2, 48, 48, 3, 3, 4, 4], dtype="float32")
        conv = T.alloc_buffer([2, 1, 48, 13, 13, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 384, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 384, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 15, 15):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1])
                data_pad[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(2, 1, 48, 15, 4, 15):
            with T.block("data_vec"):
                g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                T.reads(data_pad[n, g * 192 + C * 4 + c, h, w])
                T.writes(data_vec[g, n, C, h, c, w])
                data_vec[g, n, C, h, c, w] = data_pad[n, g * 192 + C * 4 + c, h, w]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(2, 48, 48, 3, 3, 4, 4):
            with T.block("kernel_vec"):
                g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder_1[g * 192 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 192 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(2, 1, 48, 13, 13, 4, 192, 3, 3):
            with T.block("conv"):
                g, n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
            with T.block("output_unpack"):
                n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv[c // 192, n, c % 192 // 4, h, w, c % 4])
                T.writes(output_unpack[n, c, h, w])
                T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [384, 192, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], 2, "float32"]})
                output_unpack[n, c, h, w] = conv[c // 192, n, c % 192 // 4, h, w, c % 4]
        for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[19:03:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:03:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(384, 192, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 384, 15, 15], dtype="float32")
            data_vec = T.alloc_buffer([2, 1, 48, 15, 4, 15], dtype="float32")
            kernel_vec = T.alloc_buffer([2, 48, 48, 3, 3, 4, 4], dtype="float32")
            conv = T.alloc_buffer([2, 1, 48, 13, 13, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 384, 13, 13], dtype="float32")
            conv_global = T.alloc_buffer([2, 1, 48, 13, 13, 4], dtype="float32")
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(2, 48, 48, 3, 3, 4, 4):
                with T.block("kernel_vec"):
                    g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder_1[g * 192 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                    T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                    kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 192 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 6):
                for ax0 in T.serial(2):
                    for ax0_1, ax1, ax2, ax3 in T.grid(1, 192, 15, 15):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0_1)
                            i1 = T.axis.spatial(384, ax0 * 192 + ax1)
                            i2, i3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1])
                            T.writes(data_pad[i0, i1, i2, i3])
                            data_pad[i0, i1, i2, i3] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1], T.float32(0), dtype="float32")
                    for ax1, ax2, ax3, ax4, ax5 in T.grid(1, 48, 15, 4, 15):
                        with T.block("data_vec"):
                            g, n, C, h, c, w = T.axis.remap("SSSSSS", [ax0, ax1, ax2, ax3, ax4, ax5])
                            T.reads(data_pad[n, g * 192 + C * 4 + c, h, w])
                            T.writes(data_vec[g, n, C, h, c, w])
                            data_vec[g, n, C, h, c, w] = data_pad[n, g * 192 + C * 4 + c, h, w]
                for i3_1, i4_1, i5_1 in T.grid(1, 13, 2):
                    for i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(96, 3, 1, 2, 1, 8, 1, 1, 2, 2, 1, 3, 1, 1, 1, 13, 1, 1):
                        with T.block("conv"):
                            g = T.axis.spatial(2, i0_2)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(48, i2_1 * 8 + i2_2)
                            oh, ow = T.axis.remap("SS", [i3_3, i4_1])
                            oc_block = T.axis.spatial(4, i5_1 * 2 + i5_2)
                            ic = T.axis.reduce(192, i6_0 * 2 + i6_1)
                            kh, kw = T.axis.remap("RR", [i7_0, i8_1])
                            T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 8, 13, 1, 2):
                        with T.block("conv_global"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(48, i2_1 * 8 + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            v4 = T.axis.spatial(13, i4_1 + ax4)
                            v5 = T.axis.spatial(4, i5_1 * 2 + ax5)
                            T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                            T.writes(conv[v0, v1, v2, v3, v4, v5])
                            conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(384, i1 + ax1)
                        h = T.axis.spatial(13, i2 + ax2)
                        w = T.axis.spatial(13, i3 + ax3)
                        T.reads(conv[c // 192, n, c % 192 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [384, 192, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], 2, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 192, n, c % 192 // 4, h, w, c % 4]
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="data_vec", func_name="main")
b2 = sch.get_block(name="kernel_vec", func_name="main")
b3 = sch.get_block(name="conv", func_name="main")
b4 = sch.get_block(name="output_unpack", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b3)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 6, 8, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[96, 2])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[3, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 3])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b3, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l61, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b4, decision=3)
sch.compute_at(block=b4, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=8)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
[19:03:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(384, 192, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 384, 15, 15], dtype="float32")
            data_vec = T.alloc_buffer([2, 1, 48, 15, 4, 15], dtype="float32")
            kernel_vec = T.alloc_buffer([2, 48, 48, 3, 3, 4, 4], dtype="float32")
            conv = T.alloc_buffer([2, 1, 48, 13, 13, 4], dtype="float32")
            conv_global = T.alloc_buffer([2, 1, 48, 13, 13, 4], dtype="float32")
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(2, 48, 48, 3, 3, 4, 4):
                with T.block("kernel_vec"):
                    g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder_1[g * 192 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                    T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                    kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 192 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0 in T.grid(1, 1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0 in T.grid(1, 1, 6, 1, 13, 2, 96):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 194, 15, 3):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(384, i6_0 * 2 + ax1)
                            i2 = T.axis.spatial(15, ax2)
                            i3 = T.axis.spatial(15, i4_1 + ax3)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1])
                            T.writes(data_pad[i0, i1, i2, i3])
                            data_pad[i0, i1, i2, i3] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1], T.float32(0), dtype="float32")
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 1, 15, 2, 3):
                        with T.block("data_vec"):
                            g, n = T.axis.remap("SS", [ax0, ax1])
                            C = T.axis.spatial(48, i6_0 // 2 + ax2)
                            h = T.axis.spatial(15, ax3)
                            c = T.axis.spatial(4, i6_0 % 2 * 2 + ax4)
                            w = T.axis.spatial(15, i4_1 + ax5)
                            T.reads(data_pad[n, g * 192 + C * 4 + c, h, w])
                            T.writes(data_vec[g, n, C, h, c, w])
                            data_vec[g, n, C, h, c, w] = data_pad[n, g * 192 + C * 4 + c, h, w]
                    for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(3, 1, 2, 1, 8, 1, 1, 2, 2, 1, 3, 1, 1, 1, 13, 1, 1):
                        with T.block("conv"):
                            g = T.axis.spatial(2, i0_2)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(48, i2_1 * 8 + i2_2)
                            oh, ow = T.axis.remap("SS", [i3_3, i4_1])
                            oc_block = T.axis.spatial(4, i5_1 * 2 + i5_2)
                            ic = T.axis.reduce(192, i6_0 * 2 + i6_1)
                            kh, kw = T.axis.remap("RR", [i7_0, i8_1])
                            T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 48, 13, 13, 4):
                    with T.block("conv_global"):
                        v0, v1, v2, v3, v4, v5 = T.axis.remap("SSSSSS", [ax0, ax1, ax2, ax3, ax4, ax5])
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(conv[ax1 // 192, ax0, ax1 % 192 // 4, ax2, ax3, ax1 % 4], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(conv[ax1 // 192, ax0, ax1 % 192 // 4, ax2, ax3, ax1 % 4] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="data_vec", func_name="main")
b2 = sch.get_block(name="kernel_vec", func_name="main")
b3 = sch.get_block(name="conv", func_name="main")
b4 = sch.get_block(name="output_unpack", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b3)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 6, 8, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[96, 2])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[3, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 3])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b3, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l60, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b4, decision=-2)
sch.compute_at(block=b4, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=12)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
[19:03:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(384, 192, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 384, 15, 15], dtype="float32")
            data_vec = T.alloc_buffer([2, 1, 48, 15, 4, 15], dtype="float32")
            kernel_vec = T.alloc_buffer([2, 48, 48, 3, 3, 4, 4], dtype="float32")
            conv = T.alloc_buffer([2, 1, 48, 13, 13, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 384, 13, 13], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 6):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 8, 48, 3, 3, 4, 4):
                    with T.block("kernel_vec"):
                        g = T.axis.spatial(2, ax0)
                        out_channel = T.axis.spatial(48, i2_1 * 8 + ax1)
                        in_channel, h, w, ci, co = T.axis.remap("SSSSS", [ax2, ax3, ax4, ax5, ax6])
                        T.reads(placeholder_1[g * 192 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 192 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                for i3_1, i4_1 in T.grid(1, 13):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 384, 15, 3):
                        with T.block("data_pad"):
                            i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                            i3 = T.axis.spatial(15, i4_1 + ax3)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1])
                            T.writes(data_pad[i0, i1, i2, i3])
                            data_pad[i0, i1, i2, i3] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1], T.float32(0), dtype="float32")
                    for i5_1, i6_0 in T.grid(2, 96):
                        for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 1, 15, 2, 3):
                            with T.block("data_vec"):
                                g, n = T.axis.remap("SS", [ax0, ax1])
                                C = T.axis.spatial(48, i6_0 // 2 + ax2)
                                h = T.axis.spatial(15, ax3)
                                c = T.axis.spatial(4, i6_0 % 2 * 2 + ax4)
                                w = T.axis.spatial(15, i4_1 + ax5)
                                T.reads(data_pad[n, g * 192 + C * 4 + c, h, w])
                                T.writes(data_vec[g, n, C, h, c, w])
                                data_vec[g, n, C, h, c, w] = data_pad[n, g * 192 + C * 4 + c, h, w]
                        for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(3, 1, 2, 1, 8, 1, 1, 2, 2, 1, 3, 1, 1, 1, 13, 1, 1):
                            with T.block("conv"):
                                g = T.axis.spatial(2, i0_2)
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(48, i2_1 * 8 + i2_2)
                                oh, ow = T.axis.remap("SS", [i3_3, i4_1])
                                oc_block = T.axis.spatial(4, i5_1 * 2 + i5_2)
                                ic = T.axis.reduce(192, i6_0 * 2 + i6_1)
                                kh, kw = T.axis.remap("RR", [i7_0, i8_1])
                                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                                with T.init():
                                    conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                                conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(384, i1 + ax1)
                        h = T.axis.spatial(13, i2 + ax2)
                        w = T.axis.spatial(13, i3 + ax3)
                        T.reads(conv[c // 192, n, c % 192 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [384, 192, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], 2, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 192, n, c % 192 // 4, h, w, c % 4]
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="data_vec", func_name="main")
b2 = sch.get_block(name="kernel_vec", func_name="main")
b3 = sch.get_block(name="conv", func_name="main")
b4 = sch.get_block(name="output_unpack", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b3)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 6, 8, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[96, 2])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[3, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 3])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b4, decision=3)
sch.compute_at(block=b4, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b2, decision=8)
sch.compute_at(block=b2, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=12)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
[19:03:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #12: "fused_nn_conv2d_add_nn_relu_2"
[19:03:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 192, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 384, 15, 15], dtype="float32")
        data_vec = T.alloc_buffer([2, 1, 48, 15, 4, 15], dtype="float32")
        kernel_vec = T.alloc_buffer([2, 32, 48, 3, 3, 4, 4], dtype="float32")
        conv = T.alloc_buffer([2, 1, 32, 13, 13, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 15, 15):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1])
                data_pad[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(2, 1, 48, 15, 4, 15):
            with T.block("data_vec"):
                g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                T.reads(data_pad[n, g * 192 + C * 4 + c, h, w])
                T.writes(data_vec[g, n, C, h, c, w])
                data_vec[g, n, C, h, c, w] = data_pad[n, g * 192 + C * 4 + c, h, w]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(2, 32, 48, 3, 3, 4, 4):
            with T.block("kernel_vec"):
                g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(2, 1, 32, 13, 13, 4, 192, 3, 3):
            with T.block("conv"):
                g, n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("output_unpack"):
                n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv[c // 128, n, c % 128 // 4, h, w, c % 4])
                T.writes(output_unpack[n, c, h, w])
                T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [256, 192, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], 2, "float32"]})
                output_unpack[n, c, h, w] = conv[c // 128, n, c % 128 // 4, h, w, c % 4]
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 192, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 384, 15, 15], dtype="float32")
            data_vec = T.alloc_buffer([2, 1, 48, 15, 4, 15], dtype="float32")
            kernel_vec = T.alloc_buffer([2, 32, 48, 3, 3, 4, 4], dtype="float32")
            conv = T.alloc_buffer([2, 1, 32, 13, 13, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
            conv_global = T.alloc_buffer([2, 1, 32, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1 in T.grid(1, 1, 8, 1, 1, 1, 1, 1, 2):
                for ax0, ax1, ax2, ax3 in T.grid(1, 384, 15, 15):
                    with T.block("data_pad"):
                        i0, i1, i2, i3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1])
                        T.writes(data_pad[i0, i1, i2, i3])
                        data_pad[i0, i1, i2, i3] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1], T.float32(0), dtype="float32")
                for i3_1, i4_1, i5_1 in T.grid(13, 13, 1):
                    for i6_0 in T.serial(192):
                        for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 2, 1, 3, 3, 1, 4):
                            with T.block("kernel_vec"):
                                g = T.axis.spatial(2, ax0)
                                out_channel = T.axis.spatial(32, i2_0 * 4 + i2_1 * 2 + ax1)
                                in_channel = T.axis.spatial(48, i6_0 // 4 + ax2)
                                h, w = T.axis.remap("SS", [ax3, ax4])
                                ci = T.axis.spatial(4, i6_0 % 4 + ax5)
                                co = T.axis.spatial(4, ax6)
                                T.reads(placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                        for i7_0 in T.serial(3):
                            for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 1, 1, 1, 3):
                                with T.block("data_vec"):
                                    g, n = T.axis.remap("SS", [ax0, ax1])
                                    C = T.axis.spatial(48, i6_0 // 4 + ax2)
                                    h = T.axis.spatial(15, i3_1 + i7_0 + ax3)
                                    c = T.axis.spatial(4, i6_0 % 4 + ax4)
                                    w = T.axis.spatial(15, i4_1 + ax5)
                                    T.reads(data_pad[n, g * 192 + C * 4 + c, h, w])
                                    T.writes(data_vec[g, n, C, h, c, w])
                                    data_vec[g, n, C, h, c, w] = data_pad[n, g * 192 + C * 4 + c, h, w]
                            for i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(3, 2, 1, 2, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                                with T.block("conv"):
                                    g = T.axis.spatial(2, i0_2)
                                    n = T.axis.spatial(1, 0)
                                    oc_chunk = T.axis.spatial(32, i2_0 * 4 + i2_1 * 2 + i2_2)
                                    oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSRRR", [i3_1, i4_1, i5_2, i6_0, i7_0, i8_0])
                                    T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                    T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                                    with T.init():
                                        conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                                    conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 2, 1, 1, 4):
                        with T.block("conv_global"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(32, i2_0 * 4 + i2_1 * 2 + ax2)
                            v3 = T.axis.spatial(13, i3_1 + ax3)
                            v4 = T.axis.spatial(13, i4_1 + ax4)
                            v5 = T.axis.spatial(4, ax5)
                            T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                            T.writes(conv[v0, v1, v2, v3, v4, v5])
                            conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
                with T.block("output_unpack"):
                    n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(conv[c // 128, n, c % 128 // 4, h, w, c % 4])
                    T.writes(output_unpack[n, c, h, w])
                    T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [256, 192, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], 2, "float32"]})
                    output_unpack[n, c, h, w] = conv[c // 128, n, c % 128 // 4, h, w, c % 4]
            for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="data_vec", func_name="main")
b2 = sch.get_block(name="kernel_vec", func_name="main")
b3 = sch.get_block(name="conv", func_name="main")
b4 = sch.get_block(name="output_unpack", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b3)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[8, 2, 2, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[192, 1])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[3, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[3, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b3, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l61, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b4, decision=-1)
sch.compute_at(block=b4, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b2, decision=12)
sch.compute_at(block=b2, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=13)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 192, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([2, 1, 48, 15, 4, 15], dtype="float32")
            kernel_vec = T.alloc_buffer([2, 32, 48, 3, 3, 4, 4], dtype="float32")
            conv = T.alloc_buffer([2, 1, 32, 13, 13, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
            conv_global = T.alloc_buffer([2, 1, 32, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0 in T.grid(1, 1, 8, 1, 1, 1):
                for i0_1, i1_1, i2_1 in T.grid(1, 1, 2):
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 48, 15, 4, 15):
                        with T.block("data_vec"):
                            g, n, C, h, c, w = T.axis.remap("SSSSSS", [ax0, ax1, ax2, ax3, ax4, ax5])
                            T.reads(placeholder[n, g * 192 + C * 4 + c, h - 1, w - 1])
                            T.writes(data_vec[g, n, C, h, c, w])
                            data_vec[g, n, C, h, c, w] = T.if_then_else(1 <= h and h < 14 and 1 <= w and w < 14, placeholder[n, g * 192 + C * 4 + c, h - 1, w - 1], T.float32(0), dtype="float32")
                    for i3_1, i4_1, i5_1, i6_0, i7_0, i8_0 in T.grid(13, 13, 1, 192, 3, 3):
                        for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 2, 1, 1, 1, 1, 4):
                            with T.block("kernel_vec"):
                                g = T.axis.spatial(2, ax0)
                                out_channel = T.axis.spatial(32, i2_0 * 4 + i2_1 * 2 + ax1)
                                in_channel = T.axis.spatial(48, i6_0 // 4 + ax2)
                                h = T.axis.spatial(3, i7_0 + ax3)
                                w = T.axis.spatial(3, i8_0 + ax4)
                                ci = T.axis.spatial(4, i6_0 % 4 + ax5)
                                co = T.axis.spatial(4, ax6)
                                T.reads(placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                        for i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(2, 1, 2, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv"):
                                g = T.axis.spatial(2, i0_2)
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(32, i2_0 * 4 + i2_1 * 2 + i2_2)
                                oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSRRR", [i3_1, i4_1, i5_2, i6_0, i7_0, i8_0])
                                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                                with T.init():
                                    conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 4, 13, 13, 4):
                    with T.block("conv_global"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(32, i2_0 * 4 + ax2)
                        v3, v4, v5 = T.axis.remap("SSS", [ax3, ax4, ax5])
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1 in T.grid(1, 256):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 13, 13):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(256, i1 + ax1)
                        h, w = T.axis.remap("SS", [ax2, ax3])
                        T.reads(conv[c // 128, n, c % 128 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [256, 192, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], 2, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 128, n, c % 128 // 4, h, w, c % 4]
                for i2, i3 in T.grid(13, 13):
                    with T.block("T_relu"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="data_vec", func_name="main")
b2 = sch.get_block(name="kernel_vec", func_name="main")
b3 = sch.get_block(name="conv", func_name="main")
b4 = sch.get_block(name="output_unpack", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b3)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[8, 2, 2, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[192, 1])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[3, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[3, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b3, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l60, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b4, decision=1)
sch.compute_at(block=b4, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b2, decision=14)
sch.compute_at(block=b2, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=8)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 192, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 384, 15, 15], dtype="float32")
            kernel_vec = T.alloc_buffer([2, 32, 48, 3, 3, 4, 4], dtype="float32")
            conv = T.alloc_buffer([2, 1, 32, 13, 13, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 384, 15, 15):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1_1, i1_1_1, i2_1_1 in T.grid(1, 1, 8, 1, 1, 1, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 2, 48, 3, 3, 4, 4):
                    with T.block("kernel_vec"):
                        g = T.axis.spatial(2, ax0)
                        out_channel = T.axis.spatial(32, i2_0 * 4 + i2_1_1 * 2 + ax1)
                        in_channel, h, w, ci, co = T.axis.remap("SSSSS", [ax2, ax3, ax4, ax5, ax6])
                        T.reads(placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                for i3_1_1, i4_1, i5_1, i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(13, 13, 1, 192, 3, 3, 2, 1, 2, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv"):
                        g = T.axis.spatial(2, i0_2)
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i2_0 * 4 + i2_1_1 * 2 + i2_2)
                        oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSRRR", [i3_1_1, i4_1, i5_2, i6_0, i7_0, i8_0])
                        T.reads(data_pad[n, g * 192 + ic, oh + kh, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_pad[n, g * 192 + ic // 4 * 4 + ic % 4, oh + kh, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2 in T.grid(1, 256, 13):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 13):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(256, i1 + ax1)
                        h = T.axis.spatial(13, i2 + ax2)
                        w = T.axis.spatial(13, ax3)
                        T.reads(conv[c // 128, n, c % 128 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [256, 192, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], 2, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 128, n, c % 128 // 4, h, w, c % 4]
                for i3 in T.serial(13):
                    with T.block("T_relu"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="data_vec", func_name="main")
b2 = sch.get_block(name="kernel_vec", func_name="main")
b3 = sch.get_block(name="conv", func_name="main")
b4 = sch.get_block(name="output_unpack", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b3)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[8, 2, 2, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[192, 1])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[3, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[3, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b4, decision=2)
sch.compute_at(block=b4, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b2, decision=8)
sch.compute_at(block=b2, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #13: "fused_nn_max_pool2d_2"
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], tensor: T.Buffer[(1, 256, 6, 6), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3])
                T.writes(pad_temp[ax0, ax1, ax2, ax3])
                pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(ax2 < 13 and ax3 < 13, placeholder[ax0, ax1, ax2, ax3], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 6, 6, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], tensor: T.Buffer[(1, 256, 6, 6), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 256, 6, 6, 1], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
                with T.block("pad_temp"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3])
                    T.writes(pad_temp[ax0, ax1, ax2, ax3])
                    pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(ax2 < 13 and ax3 < 13, placeholder[ax0, ax1, ax2, ax3], T.float32(-3.4028234663852886e+38), dtype="float32")
            for i0, i1, i2, i3, i4_i5_fused_0, i4_i5_fused_1 in T.grid(1, 256, 6, 6, 1, 9):
                with T.block("tensor_rf"):
                    vi4_i5_fused_0 = T.axis.spatial(1, i4_i5_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3 = T.axis.remap("SSS", [i1, i2, i3])
                    rv0 = T.axis.reduce(3, i4_i5_fused_1 // 3)
                    rv1 = T.axis.reduce(3, i4_i5_fused_1 % 3)
                    T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_0])
                    with T.init():
                        tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_0] = T.float32(-3.4028234663852886e+38)
                    tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_0] = T.max(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_0], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
            for i0, i1, i2, i3, i4_i5_fused_0 in T.grid(1, 256, 6, 6, 1):
                with T.block("tensor"):
                    vi4_i5_fused_0 = T.axis.reduce(1, i4_i5_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3 = T.axis.remap("SSS", [i1, i2, i3])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_0])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b1)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 9])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
l16 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l16, preserve_unit_loops=True)
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], tensor: T.Buffer[(1, 256, 6, 6), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 256, 6, 6, 9], dtype="float32")
            for i0, i1 in T.grid(1, 256):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 13, 13):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1 + ax1)
                        ax2_1 = T.axis.spatial(14, ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax2_1 < 13 and ax3_1 < 13, placeholder[ax0_1, ax1_1, ax2_1, ax3_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i2, i3, i4_i5_fused_0, i4_i5_fused_1 in T.grid(6, 6, 1, 9):
                    with T.block("tensor_rf"):
                        vi4_i5_fused_1 = T.axis.spatial(9, i4_i5_fused_1)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3 = T.axis.remap("SSS", [i1, i2, i3])
                        T.reads(pad_temp[ax0, ax1, ax2 * 2 + vi4_i5_fused_1 // 3, ax3 * 2 + vi4_i5_fused_1 % 3])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1])
                        with T.init():
                            tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1] = T.float32(-3.4028234663852886e+38)
                        tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1], pad_temp[ax0, ax1, ax2 * 2 + vi4_i5_fused_1 // 3, ax3 * 2 + vi4_i5_fused_1 % 3])
            for i0, i1, i2, i3, i4_i5_fused_0, i4_i5_fused_1 in T.grid(1, 256, 6, 6, 1, 9):
                with T.block("tensor"):
                    vi4_i5_fused_1 = T.axis.reduce(9, i4_i5_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3 = T.axis.remap("SSS", [i1, i2, i3])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b1)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 9])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
l16 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l16, preserve_unit_loops=True)
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], tensor: T.Buffer[(1, 256, 6, 6), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 256, 6, 6):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 3, 3):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1 + ax1)
                        ax2_1 = T.axis.spatial(14, i2 * 2 + ax2)
                        ax3_1 = T.axis.spatial(14, i3 * 2 + ax3)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax2_1 < 13 and ax3_1 < 13, placeholder[ax0_1, ax1_1, ax2_1, ax3_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i4, i5 in T.grid(3, 3):
                    with T.block("tensor"):
                        ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                        T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                        T.writes(tensor[ax0, ax1, ax2, ax3])
                        with T.init():
                            tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                        tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #14: "fused_reshape"
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 6, 6), "float32"], T_reshape: T.Buffer[(1, 9216), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(1, 9216):
            with T.block("T_reshape"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[0, ax1 % 9216 // 36, ax1 % 36 // 6, ax1 % 6])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = placeholder[0, ax1 % 9216 // 36, ax1 % 36 // 6, ax1 % 6]
    

[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 6, 6), "float32"], T_reshape: T.Buffer[(1, 9216), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1 in T.grid(1, 9216):
                with T.block("T_reshape"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[0, ax1 % 9216 // 36, ax1 % 36 // 6, ax1 % 6])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = placeholder[0, ax1 % 9216 // 36, ax1 % 36 // 6, ax1 % 6]
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #15: "fused_nn_dense_add_nn_relu"
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 9216), "float32"], placeholder_1: T.Buffer[(4096, 9216), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 4096], dtype="float32")
        T_add = T.alloc_buffer([1, 4096], dtype="float32")
        for i0, i1, i2 in T.grid(1, 4096, 9216):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0, i1 in T.grid(1, 4096):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
        for i0, i1 in T.grid(1, 4096):
            with T.block("T_relu"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_add[ax0, ax1])
                T.writes(T_relu[ax0, ax1])
                T_relu[ax0, ax1] = T.max(T_add[ax0, ax1], T.float32(0))
    

[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 9216), "float32"], placeholder_1: T.Buffer[(4096, 9216), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 4096], dtype="float32")
            for i0_0, i1_0, i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 1, 1, 1, 768, 1, 256, 12, 1, 16):
                with T.block("T_matmul_NT"):
                    i = T.axis.spatial(1, 0)
                    j = T.axis.spatial(4096, i1_2 * 16 + i1_3)
                    k = T.axis.reduce(9216, i2_0 * 12 + i2_1)
                    T.reads(placeholder[i, k], placeholder_1[j, k])
                    T.writes(T_matmul_NT[i, j])
                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        T_matmul_NT[i, j] = T.float32(0)
                    T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
            for i0, i1 in T.grid(1, 4096):
                with T.block("T_relu"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                    T.writes(T_relu[ax0, ax1])
                    T_relu[ax0, ax1] = T.max(T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1], T.float32(0))
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l10, l11, l12, l13 = sch.split(loop=l3, factors=[v6, v7, v8, v9])
v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 256, 16])
l18, l19, l20, l21 = sch.split(loop=l4, factors=[v14, v15, v16, v17])
v22, v23 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[768, 12])
l24, l25 = sch.split(loop=l5, factors=[v22, v23])
sch.reorder(l10, l18, l11, l19, l24, l12, l20, l25, l13, l21)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 9216), "float32"], placeholder_1: T.Buffer[(4096, 9216), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 4096], dtype="float32")
            for i0_0, i1_0, i0_1, i1_1 in T.grid(1, 1, 1, 1):
                for i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(768, 1, 256, 12, 1, 16):
                    with T.block("T_matmul_NT"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(4096, i1_2 * 16 + i1_3)
                        k = T.axis.reduce(9216, i2_0 * 12 + i2_1)
                        T.reads(placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
                for ax0, ax1 in T.grid(1, 4096):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                        T.writes(T_relu[ax0_1, ax1_1])
                        T_relu[ax0_1, ax1_1] = T.max(T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1], T.float32(0))
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l10, l11, l12, l13 = sch.split(loop=l3, factors=[v6, v7, v8, v9])
v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 256, 16])
l18, l19, l20, l21 = sch.split(loop=l4, factors=[v14, v15, v16, v17])
v22, v23 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[768, 12])
l24, l25 = sch.split(loop=l5, factors=[v22, v23])
sch.reorder(l10, l18, l11, l19, l24, l12, l20, l25, l13, l21)
b26, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b26, loop=l19, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v27 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v27)
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 9216), "float32"], placeholder_1: T.Buffer[(4096, 9216), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 4096], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 1):
                for i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 1, 768, 1, 256, 12, 1, 16):
                    with T.block("T_matmul_NT"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(4096, i1_2 * 16 + i1_3)
                        k = T.axis.reduce(9216, i2_0 * 12 + i2_1)
                        T.reads(placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
                for ax0, ax1 in T.grid(1, 4096):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                        T.writes(T_relu[ax0_1, ax1_1])
                        T_relu[ax0_1, ax1_1] = T.max(T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1], T.float32(0))
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l10, l11, l12, l13 = sch.split(loop=l3, factors=[v6, v7, v8, v9])
v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 256, 16])
l18, l19, l20, l21 = sch.split(loop=l4, factors=[v14, v15, v16, v17])
v22, v23 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[768, 12])
l24, l25 = sch.split(loop=l5, factors=[v22, v23])
sch.reorder(l10, l18, l11, l19, l24, l12, l20, l25, l13, l21)
b26, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b26, loop=l18, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v27 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v27)
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #16: "fused_nn_dense_add_nn_relu_1"
[19:03:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4096), "float32"], placeholder_1: T.Buffer[(4096, 4096), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 4096], dtype="float32")
        T_add = T.alloc_buffer([1, 4096], dtype="float32")
        for i0, i1, i2 in T.grid(1, 4096, 4096):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0, i1 in T.grid(1, 4096):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
        for i0, i1 in T.grid(1, 4096):
            with T.block("T_relu"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_add[ax0, ax1])
                T.writes(T_relu[ax0, ax1])
                T_relu[ax0, ax1] = T.max(T_add[ax0, ax1], T.float32(0))
    

[19:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4096), "float32"], placeholder_1: T.Buffer[(4096, 4096), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 4096], dtype="float32")
            for i0_0, i1_0, i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 32, 1, 2, 512, 1, 8, 8, 1, 8):
                with T.block("T_matmul_NT"):
                    i = T.axis.spatial(1, 0)
                    j = T.axis.spatial(4096, i1_0 * 128 + i1_1 * 64 + i1_2 * 8 + i1_3)
                    k = T.axis.reduce(4096, i2_0 * 8 + i2_1)
                    T.reads(placeholder[i, k], placeholder_1[j, k])
                    T.writes(T_matmul_NT[i, j])
                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        T_matmul_NT[i, j] = T.float32(0)
                    T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
            for i0, i1 in T.grid(1, 4096):
                with T.block("T_relu"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                    T.writes(T_relu[ax0, ax1])
                    T_relu[ax0, ax1] = T.max(T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1], T.float32(0))
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l10, l11, l12, l13 = sch.split(loop=l3, factors=[v6, v7, v8, v9])
v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[32, 2, 8, 8])
l18, l19, l20, l21 = sch.split(loop=l4, factors=[v14, v15, v16, v17])
v22, v23 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[512, 8])
l24, l25 = sch.split(loop=l5, factors=[v22, v23])
sch.reorder(l10, l18, l11, l19, l24, l12, l20, l25, l13, l21)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[19:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4096), "float32"], placeholder_1: T.Buffer[(4096, 4096), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 4096], dtype="float32")
            for i0_0, i1_0, i0_1, i1_1 in T.grid(1, 32, 1, 2):
                for i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(512, 1, 8, 8, 1, 8):
                    with T.block("T_matmul_NT"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(4096, i1_0 * 128 + i1_1 * 64 + i1_2 * 8 + i1_3)
                        k = T.axis.reduce(4096, i2_0 * 8 + i2_1)
                        T.reads(placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
                for ax0, ax1 in T.grid(1, 64):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(4096, i1_0 * 128 + i1_1 * 64 + ax1)
                        T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                        T.writes(T_relu[ax0_1, ax1_1])
                        T_relu[ax0_1, ax1_1] = T.max(T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1], T.float32(0))
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l10, l11, l12, l13 = sch.split(loop=l3, factors=[v6, v7, v8, v9])
v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[32, 2, 8, 8])
l18, l19, l20, l21 = sch.split(loop=l4, factors=[v14, v15, v16, v17])
v22, v23 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[512, 8])
l24, l25 = sch.split(loop=l5, factors=[v22, v23])
sch.reorder(l10, l18, l11, l19, l24, l12, l20, l25, l13, l21)
b26, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b26, loop=l19, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v27 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v27)
[19:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4096), "float32"], placeholder_1: T.Buffer[(4096, 4096), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 4096], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 32):
                for i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 2, 512, 1, 8, 8, 1, 8):
                    with T.block("T_matmul_NT"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(4096, i1_0 * 128 + i1_1 * 64 + i1_2 * 8 + i1_3)
                        k = T.axis.reduce(4096, i2_0 * 8 + i2_1)
                        T.reads(placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
                for ax0, ax1 in T.grid(1, 128):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(4096, i1_0 * 128 + ax1)
                        T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                        T.writes(T_relu[ax0_1, ax1_1])
                        T_relu[ax0_1, ax1_1] = T.max(T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1], T.float32(0))
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l10, l11, l12, l13 = sch.split(loop=l3, factors=[v6, v7, v8, v9])
v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[32, 2, 8, 8])
l18, l19, l20, l21 = sch.split(loop=l4, factors=[v14, v15, v16, v17])
v22, v23 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[512, 8])
l24, l25 = sch.split(loop=l5, factors=[v22, v23])
sch.reorder(l10, l18, l11, l19, l24, l12, l20, l25, l13, l21)
b26, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b26, loop=l18, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v27 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v27)
[19:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #17: "fused_nn_dense_add"
[19:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4096), "float32"], placeholder_1: T.Buffer[(200, 4096), "float32"], placeholder_2: T.Buffer[(1, 200), "float32"], T_add: T.Buffer[(1, 200), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 200], dtype="float32")
        for i0, i1, i2 in T.grid(1, 200, 4096):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0, i1 in T.grid(1, 200):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

[19:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4096), "float32"], placeholder_1: T.Buffer[(200, 4096), "float32"], placeholder_2: T.Buffer[(1, 200), "float32"], T_add: T.Buffer[(1, 200), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 200], dtype="float32")
            for i0_0, i1_0, i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 50, 1, 2, 128, 1, 1, 32, 1, 2):
                with T.block("T_matmul_NT"):
                    i = T.axis.spatial(1, 0)
                    j = T.axis.spatial(200, i1_0 * 4 + i1_1 * 2 + i1_3)
                    k = T.axis.reduce(4096, i2_0 * 32 + i2_1)
                    T.reads(placeholder[i, k], placeholder_1[j, k])
                    T.writes(T_matmul_NT[i, j])
                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        T_matmul_NT[i, j] = T.float32(0)
                    T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
            for i0, i1 in T.grid(1, 200):
                with T.block("T_add"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                    T.writes(T_add[ax0, ax1])
                    T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[50, 2, 1, 2])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[128, 32])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v25 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v25)
[19:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4096), "float32"], placeholder_1: T.Buffer[(200, 4096), "float32"], placeholder_2: T.Buffer[(1, 200), "float32"], T_add: T.Buffer[(1, 200), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 200], dtype="float32")
            for i0_0, i1_0, i0_1, i1_1 in T.grid(1, 50, 1, 2):
                for i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(128, 1, 1, 32, 1, 2):
                    with T.block("T_matmul_NT"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(200, i1_0 * 4 + i1_1 * 2 + i1_3)
                        k = T.axis.reduce(4096, i2_0 * 32 + i2_1)
                        T.reads(placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
                for ax0, ax1 in T.grid(1, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(200, i1_0 * 4 + i1_1 * 2 + ax1)
                        T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                        T.writes(T_add[ax0_1, ax1_1])
                        T_add[ax0_1, ax1_1] = T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[50, 2, 1, 2])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[128, 32])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
b25, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b25, loop=l18, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[19:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4096), "float32"], placeholder_1: T.Buffer[(200, 4096), "float32"], placeholder_2: T.Buffer[(1, 200), "float32"], T_add: T.Buffer[(1, 200), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 200], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 50):
                for i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 2, 128, 1, 1, 32, 1, 2):
                    with T.block("T_matmul_NT"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(200, i1_0 * 4 + i1_1 * 2 + i1_3)
                        k = T.axis.reduce(4096, i2_0 * 32 + i2_1)
                        T.reads(placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
                for ax0, ax1 in T.grid(1, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(200, i1_0 * 4 + ax1)
                        T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                        T.writes(T_add[ax0_1, ax1_1])
                        T_add[ax0_1, ax1_1] = T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[50, 2, 1, 2])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[128, 32])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
b25, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b25, loop=l17, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[19:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                fused_nn_lrn |    979776 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[19:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_layout_transform"
[19:03:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:03:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:03:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[19:03:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:03:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:03:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_nn_max_pool2d"
[19:03:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:04:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:04:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_layout_transform_1"
[19:04:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:04:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_nn_lrn"
[19:04:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:04:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:05:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_nn_conv2d_add_nn_relu"
[19:05:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:05:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:05:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_nn_max_pool2d_1"
[19:05:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:05:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:06:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_nn_lrn_1"
[19:06:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:06:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:06:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_layout_transform_2"
[19:06:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[19:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:07:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:07:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_layout_transform_3"
[19:07:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:07:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:07:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_nn_conv2d_add_nn_relu_1"
[19:07:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:08:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:08:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_nn_conv2d_add_nn_relu_2"
[19:08:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:08:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:09:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_nn_max_pool2d_2"
[19:09:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:09:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:09:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #14: "fused_reshape"
[19:09:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:09:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:09:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #15: "fused_nn_dense_add_nn_relu"
[19:09:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:09:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:11:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #16: "fused_nn_dense_add_nn_relu_1"
[19:11:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:11:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:12:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #17: "fused_nn_dense_add"
[19:12:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:12:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #0: GFLOPs: 0.0000. Time: 0.0435 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #1: GFLOPs: 0.0000. Time: 0.0336 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #2: GFLOPs: 0.0000. Time: 0.0268 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #3: GFLOPs: 0.0000. Time: 0.0436 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #4: GFLOPs: 0.0000. Time: 0.0336 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #5: GFLOPs: 0.0000. Time: 0.0415 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #6: GFLOPs: 0.0000. Time: 0.0477 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #7: GFLOPs: 0.0000. Time: 0.0362 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #8: GFLOPs: 0.0000. Time: 0.0713 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #9: GFLOPs: 0.0000. Time: 0.0694 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #10: GFLOPs: 0.0000. Time: 0.0522 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #11: GFLOPs: 0.0000. Time: 0.0372 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #12: GFLOPs: 0.0000. Time: 0.0275 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #13: GFLOPs: 0.0000. Time: 0.0442 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #14: GFLOPs: 0.0000. Time: 0.0729 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #15: GFLOPs: 0.0000. Time: 0.0242 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #16: GFLOPs: 0.0000. Time: 0.0361 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #17: GFLOPs: 0.0000. Time: 0.0263 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #18: GFLOPs: 0.0000. Time: 0.0400 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #19: GFLOPs: 0.0000. Time: 0.0261 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #20: GFLOPs: 0.0000. Time: 0.0277 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #21: GFLOPs: 0.0000. Time: 0.0633 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #22: GFLOPs: 0.0000. Time: 0.0379 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #23: GFLOPs: 0.0000. Time: 0.0261 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #24: GFLOPs: 0.0000. Time: 0.0325 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #25: GFLOPs: 0.0000. Time: 0.0366 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #26: GFLOPs: 0.0000. Time: 0.0191 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #27: GFLOPs: 0.0000. Time: 0.0261 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #28: GFLOPs: 0.0000. Time: 0.0358 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #29: GFLOPs: 0.0000. Time: 0.0209 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #30: GFLOPs: 0.0000. Time: 0.0259 ms. Best GFLOPs: 0.0000
[19:12:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_layout_transform"] Trial #31: GFLOPs: 0.0000. Time: 0.0272 ms. Best GFLOPs: 0.0000
/home/yj/anaconda3/lib/python3.9/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
[19:12:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_layout_transform"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                fused_nn_lrn |    979776 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 19.1389

[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #0: GFLOPs: 46.3471. Time: 4.3971 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #1: GFLOPs: 27.4153. Time: 7.4336 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #2: GFLOPs: 26.8175. Time: 7.5993 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #3: GFLOPs: 33.3724. Time: 6.1066 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #4: GFLOPs: 31.6500. Time: 6.4390 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #5: GFLOPs: 38.2654. Time: 5.3258 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #6: GFLOPs: 26.3125. Time: 7.7451 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #7: GFLOPs: 13.3143. Time: 15.3063 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #8: GFLOPs: 29.9060. Time: 6.8145 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #9: GFLOPs: 20.1771. Time: 10.1002 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #10: GFLOPs: 27.0067. Time: 7.5460 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #11: GFLOPs: 20.0165. Time: 10.1813 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #12: GFLOPs: 20.6223. Time: 9.8822 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #13: GFLOPs: 11.3044. Time: 18.0278 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #14: GFLOPs: 34.7359. Time: 5.8669 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #15: GFLOPs: 10.1359. Time: 20.1061 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #16: GFLOPs: 28.4111. Time: 7.1730 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #17: GFLOPs: 5.5514. Time: 36.7100 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #18: GFLOPs: 21.3459. Time: 9.5472 ms. Best GFLOPs: 46.3471
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #19: GFLOPs: 62.9491. Time: 3.2374 ms. Best GFLOPs: 62.9491
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #20: GFLOPs: 35.9708. Time: 5.6655 ms. Best GFLOPs: 62.9491
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #21: GFLOPs: 41.9545. Time: 4.8575 ms. Best GFLOPs: 62.9491
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #22: GFLOPs: 35.4735. Time: 5.7450 ms. Best GFLOPs: 62.9491
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #23: GFLOPs: 17.4109. Time: 11.7049 ms. Best GFLOPs: 62.9491
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #24: GFLOPs: 7.0286. Time: 28.9950 ms. Best GFLOPs: 62.9491
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #25: GFLOPs: 22.2116. Time: 9.1751 ms. Best GFLOPs: 62.9491
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #26: GFLOPs: 20.0200. Time: 10.1795 ms. Best GFLOPs: 62.9491
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #27: GFLOPs: 30.2936. Time: 6.7273 ms. Best GFLOPs: 62.9491
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #28: GFLOPs: 12.3143. Time: 16.5494 ms. Best GFLOPs: 62.9491
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #29: GFLOPs: 25.1906. Time: 8.0900 ms. Best GFLOPs: 62.9491
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #30: GFLOPs: 22.8391. Time: 8.9230 ms. Best GFLOPs: 62.9491
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"] Trial #31: GFLOPs: 30.8447. Time: 6.6071 ms. Best GFLOPs: 62.9491
[19:12:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                fused_nn_lrn |    979776 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 64
Total latency (us): 3256.57

[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #0: GFLOPs: 4.9409. Time: 0.1275 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #1: GFLOPs: 2.9308. Time: 0.2149 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #2: GFLOPs: 4.1620. Time: 0.1513 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #3: GFLOPs: 3.6909. Time: 0.1707 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #4: GFLOPs: 2.8648. Time: 0.2199 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #5: GFLOPs: 1.7410. Time: 0.3618 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #6: GFLOPs: 1.5541. Time: 0.4053 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #7: GFLOPs: 1.2812. Time: 0.4916 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #8: GFLOPs: 1.7174. Time: 0.3667 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #9: GFLOPs: 4.4353. Time: 0.1420 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #10: GFLOPs: 4.4751. Time: 0.1407 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #11: GFLOPs: 2.8879. Time: 0.2181 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #12: GFLOPs: 2.8764. Time: 0.2190 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #13: GFLOPs: 0.2980. Time: 2.1139 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #14: GFLOPs: 2.8236. Time: 0.2231 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #15: GFLOPs: 4.1204. Time: 0.1529 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #16: GFLOPs: 2.0116. Time: 0.3131 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #17: GFLOPs: 1.1188. Time: 0.5630 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #18: GFLOPs: 2.8487. Time: 0.2211 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #19: GFLOPs: 2.3432. Time: 0.2688 ms. Best GFLOPs: 4.9409
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #20: GFLOPs: 5.4412. Time: 0.1158 ms. Best GFLOPs: 5.4412
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #21: GFLOPs: 4.5812. Time: 0.1375 ms. Best GFLOPs: 5.4412
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #22: GFLOPs: 4.8933. Time: 0.1287 ms. Best GFLOPs: 5.4412
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #23: GFLOPs: 5.3165. Time: 0.1185 ms. Best GFLOPs: 5.4412
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #24: GFLOPs: 4.6619. Time: 0.1351 ms. Best GFLOPs: 5.4412
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #25: GFLOPs: 4.6844. Time: 0.1345 ms. Best GFLOPs: 5.4412
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #26: GFLOPs: 4.8218. Time: 0.1306 ms. Best GFLOPs: 5.4412
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #27: GFLOPs: 3.1980. Time: 0.1970 ms. Best GFLOPs: 5.4412
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #28: GFLOPs: 2.8344. Time: 0.2222 ms. Best GFLOPs: 5.4412
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #29: GFLOPs: 2.6226. Time: 0.2402 ms. Best GFLOPs: 5.4412
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #30: GFLOPs: 2.9093. Time: 0.2165 ms. Best GFLOPs: 5.4412
[19:12:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_max_pool2d"] Trial #31: GFLOPs: 2.5368. Time: 0.2483 ms. Best GFLOPs: 5.4412
[19:12:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_max_pool2d"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                fused_nn_lrn |    979776 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 96
Total latency (us): 3372.33

[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #0: GFLOPs: 0.0000. Time: 0.0396 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #1: GFLOPs: 0.0000. Time: 0.0470 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #2: GFLOPs: 0.0000. Time: 0.0478 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #3: GFLOPs: 0.0000. Time: 0.0361 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #4: GFLOPs: 0.0000. Time: 0.0646 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #5: GFLOPs: 0.0000. Time: 0.0760 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #6: GFLOPs: 0.0000. Time: 0.0439 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #7: GFLOPs: 0.0000. Time: 0.0368 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #8: GFLOPs: 0.0000. Time: 0.0256 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #9: GFLOPs: 0.0000. Time: 0.0267 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #10: GFLOPs: 0.0000. Time: 0.0316 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #11: GFLOPs: 0.0000. Time: 0.0413 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #12: GFLOPs: 0.0000. Time: 0.0575 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #13: GFLOPs: 0.0000. Time: 0.0409 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #14: GFLOPs: 0.0000. Time: 0.0419 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #15: GFLOPs: 0.0000. Time: 0.0498 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #16: GFLOPs: 0.0000. Time: 0.0259 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #17: GFLOPs: 0.0000. Time: 0.0325 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #18: GFLOPs: 0.0000. Time: 0.0396 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #19: GFLOPs: 0.0000. Time: 0.0356 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #20: GFLOPs: 0.0000. Time: 0.0351 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #21: GFLOPs: 0.0000. Time: 0.0437 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #22: GFLOPs: 0.0000. Time: 0.0303 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #23: GFLOPs: 0.0000. Time: 0.0361 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #24: GFLOPs: 0.0000. Time: 0.0290 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #25: GFLOPs: 0.0000. Time: 0.0314 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #26: GFLOPs: 0.0000. Time: 0.0529 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #27: GFLOPs: 0.0000. Time: 0.0501 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #28: GFLOPs: 0.0000. Time: 0.0703 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #29: GFLOPs: 0.0000. Time: 0.0744 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #30: GFLOPs: 0.0000. Time: 0.0716 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_layout_transform_1"] Trial #31: GFLOPs: 0.0000. Time: 0.0680 ms. Best GFLOPs: 0.0000
[19:12:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_layout_transform_1"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 128
Total latency (us): 3397.97

[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #0: GFLOPs: 4.5353. Time: 0.2160 ms. Best GFLOPs: 4.5353
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #1: GFLOPs: 4.9336. Time: 0.1986 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #2: GFLOPs: 4.4202. Time: 0.2217 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #3: GFLOPs: 4.4598. Time: 0.2197 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #4: GFLOPs: 4.7654. Time: 0.2056 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #5: GFLOPs: 4.8521. Time: 0.2019 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #6: GFLOPs: 4.8030. Time: 0.2040 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #7: GFLOPs: 3.6430. Time: 0.2689 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #8: GFLOPs: 4.3694. Time: 0.2242 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #9: GFLOPs: 4.1566. Time: 0.2357 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #10: GFLOPs: 1.3744. Time: 0.7129 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #11: GFLOPs: 1.4356. Time: 0.6825 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #12: GFLOPs: 1.9098. Time: 0.5130 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #13: GFLOPs: 3.1907. Time: 0.3071 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #14: GFLOPs: 3.1645. Time: 0.3096 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #15: GFLOPs: 2.9009. Time: 0.3377 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #16: GFLOPs: 3.2250. Time: 0.3038 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #17: GFLOPs: 1.0760. Time: 0.9106 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #18: GFLOPs: 1.5365. Time: 0.6377 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #19: GFLOPs: 1.3406. Time: 0.7309 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #20: GFLOPs: 1.3409. Time: 0.7307 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #21: GFLOPs: 1.1806. Time: 0.8299 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #22: GFLOPs: 1.2017. Time: 0.8153 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #23: GFLOPs: 1.1968. Time: 0.8186 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #24: GFLOPs: 0.9857. Time: 0.9940 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #25: GFLOPs: 1.3928. Time: 0.7035 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #26: GFLOPs: 1.5692. Time: 0.6244 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #27: GFLOPs: 4.4081. Time: 0.2223 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #28: GFLOPs: 4.7386. Time: 0.2068 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #29: GFLOPs: 4.6757. Time: 0.2095 ms. Best GFLOPs: 4.9336
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #30: GFLOPs: 5.0368. Time: 0.1945 ms. Best GFLOPs: 5.0368
[19:12:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_lrn"] Trial #31: GFLOPs: 4.9702. Time: 0.1971 ms. Best GFLOPs: 5.0368
[19:12:57] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_lrn"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |         5.0368 |     194.5248 |              194.5248 |     32 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 160
Total latency (us): 3592.5

[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #0: GFLOPs: 30.9828. Time: 14.4684 ms. Best GFLOPs: 30.9828
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #1: GFLOPs: 22.4504. Time: 19.9672 ms. Best GFLOPs: 30.9828
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #2: GFLOPs: 21.2738. Time: 21.0715 ms. Best GFLOPs: 30.9828
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #3: GFLOPs: 15.2766. Time: 29.3436 ms. Best GFLOPs: 30.9828
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #4: GFLOPs: 21.4795. Time: 20.8697 ms. Best GFLOPs: 30.9828
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #5: GFLOPs: 19.3246. Time: 23.1969 ms. Best GFLOPs: 30.9828
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #6: GFLOPs: 14.7300. Time: 30.4324 ms. Best GFLOPs: 30.9828
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #7: GFLOPs: 11.5968. Time: 38.6545 ms. Best GFLOPs: 30.9828
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #8: GFLOPs: 18.6672. Time: 24.0139 ms. Best GFLOPs: 30.9828
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #9: GFLOPs: 9.0881. Time: 49.3251 ms. Best GFLOPs: 30.9828
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #10: GFLOPs: 10.8457. Time: 41.3318 ms. Best GFLOPs: 30.9828
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #11: GFLOPs: 42.5134. Time: 10.5442 ms. Best GFLOPs: 42.5134
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #12: GFLOPs: 28.0193. Time: 15.9986 ms. Best GFLOPs: 42.5134
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 27, 27), "float32"], placeholder_1: T.Buffer[(256, 48, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 96, 31, 31], dtype="float32")
        data_vec = T.alloc_buffer([2, 1, 12, 31, 4, 31], dtype="float32")
        kernel_vec = T.alloc_buffer([2, 32, 12, 5, 5, 4, 4], dtype="float32")
        conv = T.alloc_buffer([2, 1, 32, 27, 27, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 256, 27, 27], dtype="float32")
        conv_global = T.alloc_buffer([2, 1, 32, 27, 27, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(768, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3, i4, i5, i6 in T.grid(5, 5, 4, 4):
                with T.block("kernel_vec"):
                    g = T.axis.spatial(2, i0_i1_i2_fused // 384)
                    out_channel = T.axis.spatial(32, i0_i1_i2_fused % 384 // 12)
                    in_channel = T.axis.spatial(12, i0_i1_i2_fused % 12)
                    h, w, ci, co = T.axis.remap("SSSS", [i3, i4, i5, i6])
                    T.reads(placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                    T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                    kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
        for i0_0_i1_0_i2_0_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 96, 31):
                for ax3_fused in T.vectorized(31):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2, i3 = T.axis.remap("SSS", [ax1, ax2, ax3_fused])
                        T.reads(placeholder[i0, i1, i2 - 2, i3 - 2])
                        T.writes(data_pad[i0, i1, i2, i3])
                        data_pad[i0, i1, i2, i3] = T.if_then_else(2 <= i2 and i2 < 29 and 2 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 2, i3 - 2], T.float32(0), dtype="float32")
            for i3_0, i4_0, i5_0 in T.grid(1, 9, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 3, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(2, 1, 12, 13, 4):
                        for ax5_fused in T.vectorized(5):
                            with T.block("data_vec"):
                                g, n, C = T.axis.remap("SSS", [ax0, ax1, ax2])
                                h = T.axis.spatial(31, i3_1 * 9 + ax3)
                                c = T.axis.spatial(4, ax4)
                                w = T.axis.spatial(31, i4_0 * 3 + i4_1 + ax5_fused)
                                T.reads(data_pad[n, g * 48 + C * 4 + c, h, w])
                                T.writes(data_vec[g, n, C, h, c, w])
                                data_vec[g, n, C, h, c, w] = data_pad[n, g * 48 + C * 4 + c, h, w]
                    for i5_1, i6_0 in T.grid(2, 1):
                        for i0_2_init, i3_2_init, i5_2_init in T.grid(2, 9, 2):
                            with T.block("conv_init"):
                                g = T.axis.spatial(2, i0_2_init)
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_fused)
                                oh = T.axis.spatial(27, i3_1 * 9 + i3_2_init)
                                ow = T.axis.spatial(27, i4_0 * 3 + i4_1)
                                oc_block = T.axis.spatial(4, i5_1 * 2 + i5_2_init)
                                T.reads()
                                T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(5, 1, 2, 1, 1, 9, 1, 2, 48, 1, 5, 1, 1, 1, 1, 1, 1):
                            with T.block("conv_update"):
                                g = T.axis.spatial(2, i0_2)
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_fused)
                                oh = T.axis.spatial(27, i3_1 * 9 + i3_2)
                                ow = T.axis.spatial(27, i4_0 * 3 + i4_1)
                                oc_block = T.axis.spatial(4, i5_1 * 2 + i5_2)
                                ic, kh, kw = T.axis.remap("RRR", [i6_1, i7_0, i8_1])
                                T.reads(conv_global[g, n, oc_chunk, oh, ow, oc_block], data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(2, 1, 1, 27):
                    for ax4_ax5_fused in T.vectorized(12):
                        with T.block("conv_global"):
                            v0 = T.axis.spatial(2, ax0)
                            v1 = T.axis.spatial(1, 0)
                            v2, v3 = T.axis.remap("SS", [i0_0_i1_0_i2_0_fused, ax3])
                            v4 = T.axis.spatial(27, i4_0 * 3 + ax4_ax5_fused // 4)
                            v5 = T.axis.spatial(4, ax4_ax5_fused % 4)
                            T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                            T.writes(conv[v0, v1, v2, v3, v4, v5])
                            conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(27):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 27):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, 0)
                        c, h, w = T.axis.remap("SSS", [i0_i1_fused, i2, ax3])
                        T.reads(conv[c // 128, n, c % 128 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 96, 27, 27], "float32"], ["TENSOR", [256, 48, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], 2, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 128, n, c % 128 // 4, h, w, c % 4]
                for i3_fused in T.vectorized(27):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3 = T.axis.remap("SSS", [i0_i1_fused, i2, i3_fused])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="data_vec", func_name="main")
b2 = sch.get_block(name="kernel_vec", func_name="main")
b3 = sch.get_block(name="conv", func_name="main")
b4 = sch.get_block(name="output_unpack", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b3)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[32, 1, 1, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 3, 9, 1])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[9, 3, 1, 1])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[1, 48])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[5, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 5])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b3, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l60, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b4, decision=2)
sch.compute_at(block=b4, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=10)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
sch.enter_postproc()
b82 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.unroll_explicit")
b83, b84, b85, b86, b87, b88, b89 = sch.get_child_blocks(b82)
l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b83)
l97 = sch.fuse(l90, l91, l92)
sch.parallel(loop=l97)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b84)
l105 = sch.fuse(l98, l99, l100)
sch.parallel(loop=l105)
l106 = sch.fuse(l104)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b85)
l122 = sch.fuse(l121)
sch.vectorize(loop=l122)
sch.annotate(block_or_loop=l107, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l107, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b86)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b87)
l161 = sch.fuse(l159, l160)
sch.vectorize(loop=l161)
sch.annotate(block_or_loop=l151, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l151, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b88)
l169 = sch.fuse(l162, l163)
sch.parallel(loop=l169)
sch.annotate(block_or_loop=l169, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l169, ann_key="pragma_unroll_explicit", ann_val=1)
l170, l171, l172 = sch.get_loops(block=b89)
l173 = sch.fuse(l172)
sch.vectorize(loop=l173)
sch.annotate(block_or_loop=l170, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l170, ann_key="pragma_unroll_explicit", ann_val=1)
b174 = sch.get_block(name="conv", func_name="main")
l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191, l192, l193, l194, l195, l196, l197, l198, l199, l200, l201, l202 = sch.get_loops(block=b174)
b203 = sch.decompose_reduction(block=b174, loop=l186)
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 27, 27), "float32"], placeholder_1: T.Buffer[(256, 48, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 96, 31, 31], dtype="float32")
        data_vec = T.alloc_buffer([2, 1, 12, 31, 4, 31], dtype="float32")
        kernel_vec = T.alloc_buffer([2, 32, 12, 5, 5, 4, 4], dtype="float32")
        conv = T.alloc_buffer([2, 1, 32, 27, 27, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 256, 27, 27], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_fused in T.parallel(96, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 48, 31):
                for ax3_fused in T.vectorized(13):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_fused % 32 // 16 * 48 + ax1)
                        i2 = T.axis.spatial(31, ax2)
                        i3 = T.axis.spatial(31, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_fused % 96 // 32 * 9 + ax3_fused)
                        T.reads(placeholder[i0, i1, i2 - 2, i3 - 2])
                        T.writes(data_pad[i0, i1, i2, i3])
                        data_pad[i0, i1, i2, i3] = T.if_then_else(2 <= i2 and i2 < 29 and 2 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 2, i3 - 2], T.float32(0), dtype="float32")
            for i3_1, i4_1, i5_1 in T.grid(3, 1, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 2, 12, 5, 5, 4, 2):
                    with T.block("kernel_vec"):
                        g = T.axis.spatial(2, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_fused % 32 // 16)
                        out_channel = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_fused % 16 * 2 + ax1)
                        in_channel, h, w, ci = T.axis.remap("SSSS", [ax2, ax3, ax4, ax5])
                        co = T.axis.spatial(4, i5_1 * 2 + ax6)
                        T.reads(placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                for i3_2_init, i4_2_init, i5_2_init, i2_3_init, i4_3_init in T.grid(9, 3, 2, 2, 3):
                    with T.block("conv_init"):
                        g = T.axis.spatial(2, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_fused % 32 // 16)
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_fused % 16 * 2 + i2_3_init)
                        oh = T.axis.spatial(27, i3_1 * 9 + i3_2_init)
                        ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_fused // 32 * 9 + i4_2_init * 3 + i4_3_init)
                        oc_block = T.axis.spatial(4, i5_1 * 2 + i5_2_init)
                        T.reads()
                        T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i6_0 in T.serial(6):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 2, 13, 4):
                        for ax5_fused in T.vectorized(13):
                            with T.block("data_vec"):
                                g = T.axis.spatial(2, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_fused % 32 // 16 + ax0)
                                n = T.axis.spatial(1, ax1)
                                C = T.axis.spatial(12, i6_0 * 2 + ax2)
                                h = T.axis.spatial(31, i3_1 * 9 + ax3)
                                c = T.axis.spatial(4, ax4)
                                w = T.axis.spatial(31, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_fused % 96 // 32 * 9 + ax5_fused)
                                T.reads(data_pad[n, g * 48 + C * 4 + c, h, w])
                                T.writes(data_vec[g, n, C, h, c, w])
                                data_vec[g, n, C, h, c, w] = data_pad[n, g * 48 + C * 4 + c, h, w]
                    for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(5, 1, 1, 1, 1, 9, 3, 2, 8, 1, 5, 1, 1, 2, 1, 3, 1):
                        with T.block("conv_update"):
                            g = T.axis.spatial(2, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_fused % 32 // 16)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_fused % 16 * 2 + i2_3)
                            oh = T.axis.spatial(27, i3_1 * 9 + i3_2)
                            ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_fused // 32 * 9 + i4_2 * 3 + i4_3)
                            oc_block = T.axis.spatial(4, i5_1 * 2 + i5_2)
                            ic = T.axis.reduce(48, i6_0 * 8 + i6_1)
                            kh, kw = T.axis.remap("RR", [i7_0, i8_1])
                            T.reads(conv[g, n, oc_chunk, oh, ow, oc_block], data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 27, 27):
                with T.block("output_unpack"):
                    n = T.axis.spatial(1, 0)
                    c, h, w = T.axis.remap("SSS", [i0_i1_fused, ax2, ax3])
                    T.reads(conv[c // 128, n, c % 128 // 4, h, w, c % 4])
                    T.writes(output_unpack[n, c, h, w])
                    T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 96, 27, 27], "float32"], ["TENSOR", [256, 48, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], 2, "float32"]})
                    output_unpack[n, c, h, w] = conv[c // 128, n, c % 128 // 4, h, w, c % 4]
            for i2 in T.serial(27):
                for i3_fused in T.vectorized(27):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3 = T.axis.remap("SSS", [i0_i1_fused, i2, i3_fused])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="data_vec", func_name="main")
b2 = sch.get_block(name="kernel_vec", func_name="main")
b3 = sch.get_block(name="conv", func_name="main")
b4 = sch.get_block(name="output_unpack", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b3)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 16, 1, 2])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 3, 9, 1])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[3, 1, 3, 3])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[6, 8])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[5, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 5])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b4, decision=1)
sch.compute_at(block=b4, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b2, decision=11)
sch.compute_at(block=b2, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=12)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
sch.enter_postproc()
b81 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b81, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b81, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b81, ann_key="meta_schedule.unroll_explicit")
b82, b83, b84, b85, b86, b87 = sch.get_child_blocks(b81)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b82)
l101 = sch.fuse(l88, l89, l90, l91, l92, l93, l94, l95, l96)
sch.parallel(loop=l101)
l102 = sch.fuse(l100)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l101, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l101, ann_key="pragma_unroll_explicit", ann_val=1)
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b83)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b84)
l125 = sch.fuse(l124)
sch.vectorize(loop=l125)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b85)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153 = sch.get_loops(block=b86)
l154 = sch.fuse(l148, l149)
sch.parallel(loop=l154)
sch.annotate(block_or_loop=l154, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l154, ann_key="pragma_unroll_explicit", ann_val=1)
l155, l156, l157 = sch.get_loops(block=b87)
l158 = sch.fuse(l157)
sch.vectorize(loop=l158)
sch.annotate(block_or_loop=l155, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l155, ann_key="pragma_unroll_explicit", ann_val=1)
b159 = sch.get_block(name="conv", func_name="main")
l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b159)
b182 = sch.decompose_reduction(block=b159, loop=l164)
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #15: GFLOPs: 33.2097. Time: 13.4982 ms. Best GFLOPs: 42.5134
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #16: GFLOPs: 21.4883. Time: 20.8612 ms. Best GFLOPs: 42.5134
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #17: GFLOPs: 68.7139. Time: 6.5237 ms. Best GFLOPs: 68.7139
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #18: GFLOPs: 43.7740. Time: 10.2406 ms. Best GFLOPs: 68.7139
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #19: GFLOPs: 30.6345. Time: 14.6329 ms. Best GFLOPs: 68.7139
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 27, 27), "float32"], placeholder_1: T.Buffer[(256, 48, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 96, 31, 31], dtype="float32")
        data_vec = T.alloc_buffer([2, 1, 12, 31, 4, 31], dtype="float32")
        kernel_vec = T.alloc_buffer([2, 32, 12, 5, 5, 4, 4], dtype="float32")
        conv = T.alloc_buffer([2, 1, 32, 27, 27, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 256, 27, 27], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(324, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4_1, i5_1 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 12, 7, 4, 7):
                    for ax0_1, ax1_1, ax2_1, ax3_1 in T.grid(1, 1, 1, 1):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0_1)
                            i1 = T.axis.spatial(96, ax0 * 48 + ax2 * 4 + ax4 + ax1_1)
                            i2 = T.axis.spatial(31, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused % 9 * 3 + ax3 + ax2_1)
                            i3 = T.axis.spatial(31, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused % 162 // 18 * 3 + ax5 + ax3_1)
                            T.reads(placeholder[i0, i1, i2 - 2, i3 - 2])
                            T.writes(data_pad[i0, i1, i2, i3])
                            data_pad[i0, i1, i2, i3] = T.if_then_else(2 <= i2 and i2 < 29 and 2 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 2, i3 - 2], T.float32(0), dtype="float32")
                    with T.block("data_vec"):
                        g, n, C = T.axis.remap("SSS", [ax0, ax1, ax2])
                        h = T.axis.spatial(31, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused % 9 * 3 + ax3)
                        c = T.axis.spatial(4, ax4)
                        w = T.axis.spatial(31, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused % 162 // 18 * 3 + ax5)
                        T.reads(data_pad[n, g * 48 + C * 4 + c, h, w])
                        T.writes(data_vec[g, n, C, h, c, w])
                        data_vec[g, n, C, h, c, w] = data_pad[n, g * 48 + C * 4 + c, h, w]
                for i0_2_init, i3_2_init, i5_2_init, i2_3_init, i4_3_init in T.grid(2, 3, 2, 8, 3):
                    with T.block("conv_init"):
                        g = T.axis.spatial(2, i0_2_init)
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused // 162 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused % 18 // 9 * 8 + i2_3_init)
                        oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused % 9 * 3 + i3_2_init)
                        ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused % 162 // 18 * 3 + i4_3_init)
                        oc_block = T.axis.spatial(4, i5_1 * 2 + i5_2_init)
                        T.reads()
                        T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i6_0 in T.serial(48):
                    for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 8, 1, 5, 5, 1, 2):
                        with T.block("kernel_vec"):
                            g = T.axis.spatial(2, ax0)
                            out_channel = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused // 162 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused % 18 // 9 * 8 + ax1)
                            in_channel = T.axis.spatial(12, i6_0 // 4)
                            h, w = T.axis.remap("SS", [ax3, ax4])
                            ci = T.axis.spatial(4, i6_0 % 4)
                            co = T.axis.spatial(4, i5_1 * 2 + ax6)
                            T.reads(placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                            T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                            kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 128 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                    for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(5, 5, 2, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 8, 1, 3, 1):
                        with T.block("conv_update"):
                            g = T.axis.spatial(2, i0_2)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused // 162 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused % 18 // 9 * 8 + i2_3)
                            oh = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused % 9 * 3 + i3_2)
                            ow = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_i0_1_i1_1_i2_1_i3_1_fused % 162 // 18 * 3 + i4_3)
                            oc_block = T.axis.spatial(4, i5_1 * 2 + i5_2)
                            ic, kh, kw = T.axis.remap("RRR", [i6_0, i7_0, i8_0])
                            T.reads(conv[g, n, oc_chunk, oh, ow, oc_block], data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(27):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 27):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, 0)
                        c, h, w = T.axis.remap("SSS", [i0_i1_fused, i2, ax3])
                        T.reads(conv[c // 128, n, c % 128 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 96, 27, 27], "float32"], ["TENSOR", [256, 48, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], 2, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 128, n, c % 128 // 4, h, w, c % 4]
                for i3_fused in T.vectorized(27):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3 = T.axis.remap("SSS", [i0_i1_fused, i2, i3_fused])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="data_vec", func_name="main")
b2 = sch.get_block(name="kernel_vec", func_name="main")
b3 = sch.get_block(name="conv", func_name="main")
b4 = sch.get_block(name="output_unpack", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b3)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[2, 2, 1, 8])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 9, 3, 1])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[9, 1, 1, 3])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[48, 1])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[5, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[5, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b4, decision=2)
sch.compute_at(block=b4, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b2, decision=12)
sch.compute_at(block=b2, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=11)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=17)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
sch.enter_postproc()
b81 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b81, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b81, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b81, ann_key="meta_schedule.unroll_explicit")
b82, b83, b84, b85, b86, b87 = sch.get_child_blocks(b81)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b82)
l110 = sch.fuse(l88, l89, l90, l91, l92, l93, l94, l95, l96, l97)
sch.parallel(loop=l110)
sch.annotate(block_or_loop=l110, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l110, ann_key="pragma_unroll_explicit", ann_val=1)
l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b83)
sch.annotate(block_or_loop=l111, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l111, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b84)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151 = sch.get_loops(block=b85)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b86)
l159 = sch.fuse(l152, l153)
sch.parallel(loop=l159)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162 = sch.get_loops(block=b87)
l163 = sch.fuse(l162)
sch.vectorize(loop=l163)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b164 = sch.get_block(name="conv", func_name="main")
l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185 = sch.get_loops(block=b164)
b186 = sch.decompose_reduction(block=b164, loop=l168)
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #21: GFLOPs: 5.8392. Time: 76.7689 ms. Best GFLOPs: 68.7139
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #22: GFLOPs: 35.5012. Time: 12.6269 ms. Best GFLOPs: 68.7139
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #23: GFLOPs: 6.5878. Time: 68.0458 ms. Best GFLOPs: 68.7139
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #24: GFLOPs: 17.0797. Time: 26.2458 ms. Best GFLOPs: 68.7139
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #25: GFLOPs: 11.5949. Time: 38.6609 ms. Best GFLOPs: 68.7139
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #26: GFLOPs: 4.6059. Time: 97.3256 ms. Best GFLOPs: 68.7139
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #27: GFLOPs: 13.6079. Time: 32.9419 ms. Best GFLOPs: 68.7139
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #28: GFLOPs: 20.7558. Time: 21.5974 ms. Best GFLOPs: 68.7139
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #29: GFLOPs: 5.3372. Time: 83.9903 ms. Best GFLOPs: 68.7139
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #30: GFLOPs: 41.3112. Time: 10.8511 ms. Best GFLOPs: 68.7139
[19:12:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu"] Trial #31: GFLOPs: 15.3604. Time: 29.1835 ms. Best GFLOPs: 68.7139
[19:12:58] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_conv2d_add_nn_relu"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |         5.0368 |     194.5248 |              194.5248 |     32 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |        68.7139 |    6523.7298 |             6523.7298 |     32 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 192
Total latency (us): 10116.2

[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #0: GFLOPs: 9.8247. Time: 0.0396 ms. Best GFLOPs: 9.8247
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #1: GFLOPs: 1.9223. Time: 0.2026 ms. Best GFLOPs: 9.8247
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #2: GFLOPs: 2.3555. Time: 0.1653 ms. Best GFLOPs: 9.8247
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #3: GFLOPs: 1.2612. Time: 0.3087 ms. Best GFLOPs: 9.8247
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #4: GFLOPs: 2.3021. Time: 0.1691 ms. Best GFLOPs: 9.8247
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #5: GFLOPs: 4.6755. Time: 0.0833 ms. Best GFLOPs: 9.8247
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #6: GFLOPs: 4.3233. Time: 0.0901 ms. Best GFLOPs: 9.8247
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #7: GFLOPs: 3.9453. Time: 0.0987 ms. Best GFLOPs: 9.8247
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #8: GFLOPs: 3.6491. Time: 0.1067 ms. Best GFLOPs: 9.8247
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #9: GFLOPs: 2.3875. Time: 0.1631 ms. Best GFLOPs: 9.8247
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #10: GFLOPs: 4.0374. Time: 0.0964 ms. Best GFLOPs: 9.8247
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #11: GFLOPs: 5.4168. Time: 0.0719 ms. Best GFLOPs: 9.8247
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #12: GFLOPs: 6.3065. Time: 0.0617 ms. Best GFLOPs: 9.8247
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #13: GFLOPs: 10.7422. Time: 0.0362 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #14: GFLOPs: 4.1080. Time: 0.0948 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #15: GFLOPs: 4.7669. Time: 0.0817 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #16: GFLOPs: 3.6143. Time: 0.1077 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #17: GFLOPs: 4.2284. Time: 0.0921 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #18: GFLOPs: 9.0711. Time: 0.0429 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #19: GFLOPs: 4.3413. Time: 0.0897 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #20: GFLOPs: 4.8968. Time: 0.0795 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #21: GFLOPs: 3.3854. Time: 0.1150 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #22: GFLOPs: 4.8607. Time: 0.0801 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #23: GFLOPs: 4.2318. Time: 0.0920 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #24: GFLOPs: 6.5804. Time: 0.0592 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #25: GFLOPs: 5.8017. Time: 0.0671 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #26: GFLOPs: 4.2464. Time: 0.0917 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #27: GFLOPs: 7.7999. Time: 0.0499 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #28: GFLOPs: 5.1826. Time: 0.0751 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #29: GFLOPs: 4.9881. Time: 0.0781 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #30: GFLOPs: 2.8076. Time: 0.1387 ms. Best GFLOPs: 10.7422
[19:12:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_max_pool2d_1"] Trial #31: GFLOPs: 3.8509. Time: 0.1011 ms. Best GFLOPs: 10.7422
[19:13:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_max_pool2d_1"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |         5.0368 |     194.5248 |              194.5248 |     32 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |        68.7139 |    6523.7298 |             6523.7298 |     32 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |        10.7422 |      36.2474 |               36.2474 |     32 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 224
Total latency (us): 10152.5

[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #0: GFLOPs: 3.8839. Time: 0.1560 ms. Best GFLOPs: 3.8839
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #1: GFLOPs: 4.2106. Time: 0.1438 ms. Best GFLOPs: 4.2106
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #2: GFLOPs: 3.7928. Time: 0.1597 ms. Best GFLOPs: 4.2106
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #3: GFLOPs: 3.6114. Time: 0.1677 ms. Best GFLOPs: 4.2106
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #4: GFLOPs: 4.7187. Time: 0.1284 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #5: GFLOPs: 2.7446. Time: 0.2207 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #6: GFLOPs: 3.5718. Time: 0.1696 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #7: GFLOPs: 3.8358. Time: 0.1579 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #8: GFLOPs: 4.0548. Time: 0.1494 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #9: GFLOPs: 3.9275. Time: 0.1542 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #10: GFLOPs: 4.2831. Time: 0.1414 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #11: GFLOPs: 3.1813. Time: 0.1904 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #12: GFLOPs: 3.2836. Time: 0.1845 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #13: GFLOPs: 3.4245. Time: 0.1769 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #14: GFLOPs: 2.6979. Time: 0.2245 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #15: GFLOPs: 3.1748. Time: 0.1908 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #16: GFLOPs: 3.1801. Time: 0.1905 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #17: GFLOPs: 0.8067. Time: 0.7508 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #18: GFLOPs: 0.7253. Time: 0.8351 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #19: GFLOPs: 4.2685. Time: 0.1419 ms. Best GFLOPs: 4.7187
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #20: GFLOPs: 5.3322. Time: 0.1136 ms. Best GFLOPs: 5.3322
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #21: GFLOPs: 5.1128. Time: 0.1185 ms. Best GFLOPs: 5.3322
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #22: GFLOPs: 5.1565. Time: 0.1175 ms. Best GFLOPs: 5.3322
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #23: GFLOPs: 4.6987. Time: 0.1289 ms. Best GFLOPs: 5.3322
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #24: GFLOPs: 4.8610. Time: 0.1246 ms. Best GFLOPs: 5.3322
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #25: GFLOPs: 5.1690. Time: 0.1172 ms. Best GFLOPs: 5.3322
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #26: GFLOPs: 3.1117. Time: 0.1947 ms. Best GFLOPs: 5.3322
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #27: GFLOPs: 4.4926. Time: 0.1348 ms. Best GFLOPs: 5.3322
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #28: GFLOPs: 4.3410. Time: 0.1395 ms. Best GFLOPs: 5.3322
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #29: GFLOPs: 4.8786. Time: 0.1242 ms. Best GFLOPs: 5.3322
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #30: GFLOPs: 5.7021. Time: 0.1062 ms. Best GFLOPs: 5.7021
[19:13:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_lrn_1"] Trial #31: GFLOPs: 4.8933. Time: 0.1238 ms. Best GFLOPs: 5.7021
[19:13:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_lrn_1"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |         5.0368 |     194.5248 |              194.5248 |     32 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |        68.7139 |    6523.7298 |             6523.7298 |     32 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |        10.7422 |      36.2474 |               36.2474 |     32 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |         5.7021 |     106.2228 |              106.2228 |     32 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 256
Total latency (us): 10258.7

[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #0: GFLOPs: 0.0000. Time: 0.0178 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #1: GFLOPs: 0.0000. Time: 0.0193 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #2: GFLOPs: 0.0000. Time: 0.0190 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #3: GFLOPs: 0.0000. Time: 0.0208 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #4: GFLOPs: 0.0000. Time: 0.0158 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #5: GFLOPs: 0.0000. Time: 0.0231 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #6: GFLOPs: 0.0000. Time: 0.0228 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #7: GFLOPs: 0.0000. Time: 0.0239 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #8: GFLOPs: 0.0000. Time: 0.0224 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #9: GFLOPs: 0.0000. Time: 0.0233 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #10: GFLOPs: 0.0000. Time: 0.0169 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #11: GFLOPs: 0.0000. Time: 0.0181 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #12: GFLOPs: 0.0000. Time: 0.0186 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #13: GFLOPs: 0.0000. Time: 0.0203 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #14: GFLOPs: 0.0000. Time: 0.0174 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #15: GFLOPs: 0.0000. Time: 0.0182 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #16: GFLOPs: 0.0000. Time: 0.0168 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #17: GFLOPs: 0.0000. Time: 0.0179 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #18: GFLOPs: 0.0000. Time: 0.0198 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #19: GFLOPs: 0.0000. Time: 0.0170 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #20: GFLOPs: 0.0000. Time: 0.0215 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #21: GFLOPs: 0.0000. Time: 0.0202 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #22: GFLOPs: 0.0000. Time: 0.0182 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #23: GFLOPs: 0.0000. Time: 0.0202 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #24: GFLOPs: 0.0000. Time: 0.0237 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #25: GFLOPs: 0.0000. Time: 0.0551 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #26: GFLOPs: 0.0000. Time: 0.1610 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #27: GFLOPs: 0.0000. Time: 0.1436 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #28: GFLOPs: 0.0000. Time: 0.1351 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #29: GFLOPs: 0.0000. Time: 0.1717 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #30: GFLOPs: 0.0000. Time: 0.0737 ms. Best GFLOPs: 0.0000
[19:13:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #31: GFLOPs: 0.0000. Time: 0.0519 ms. Best GFLOPs: 0.0000
[19:13:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_layout_transform_2"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |         5.0368 |     194.5248 |              194.5248 |     32 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |        68.7139 |    6523.7298 |             6523.7298 |     32 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |        10.7422 |      36.2474 |               36.2474 |     32 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |         5.7021 |     106.2228 |              106.2228 |     32 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |         0.0001 |      15.8475 |               15.8475 |     32 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 288
Total latency (us): 10274.5

[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #0: GFLOPs: 12.2287. Time: 24.4647 ms. Best GFLOPs: 12.2287
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #1: GFLOPs: 8.8784. Time: 33.6964 ms. Best GFLOPs: 12.2287
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #2: GFLOPs: 23.3407. Time: 12.8175 ms. Best GFLOPs: 23.3407
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #3: GFLOPs: 12.9145. Time: 23.1656 ms. Best GFLOPs: 23.3407
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #4: GFLOPs: 22.2448. Time: 13.4490 ms. Best GFLOPs: 23.3407
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #5: GFLOPs: 12.4568. Time: 24.0167 ms. Best GFLOPs: 23.3407
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #6: GFLOPs: 51.2250. Time: 5.8403 ms. Best GFLOPs: 51.2250
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #7: GFLOPs: 19.3033. Time: 15.4984 ms. Best GFLOPs: 51.2250
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #8: GFLOPs: 42.1076. Time: 7.1049 ms. Best GFLOPs: 51.2250
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #9: GFLOPs: 20.7325. Time: 14.4300 ms. Best GFLOPs: 51.2250
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #10: GFLOPs: 8.5955. Time: 34.8053 ms. Best GFLOPs: 51.2250
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #11: GFLOPs: 19.1056. Time: 15.6588 ms. Best GFLOPs: 51.2250
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #12: GFLOPs: 18.6871. Time: 16.0094 ms. Best GFLOPs: 51.2250
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #13: GFLOPs: 11.5844. Time: 25.8253 ms. Best GFLOPs: 51.2250
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #14: GFLOPs: 11.8714. Time: 25.2010 ms. Best GFLOPs: 51.2250
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #15: GFLOPs: 6.7583. Time: 44.2671 ms. Best GFLOPs: 51.2250
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(96, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 96, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 96, 13, 13, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(960, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(60):
                with T.block("data_pad"):
                    i0 = T.axis.spatial(1, 0)
                    i1 = T.axis.spatial(64, i0_i1_i2_fused // 15)
                    i2 = T.axis.spatial(15, i0_i1_i2_fused % 15)
                    i3 = T.axis.spatial(15, i3_i4_fused // 4)
                    i4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                    T.writes(data_pad[i0, i1, i2, i3, i4])
                    data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(128, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_3_init, i3_3_init in T.grid(3, 13, 13):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 3 + i1_2_init)
                    oh, ow = T.axis.remap("SS", [i2_3_init, i3_3_init])
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 13, 13, 4], "float32"], ["TENSOR", [96, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 3, 1, 1, 1, 8, 3, 3, 1, 1, 13, 13, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 3 + i1_2)
                    oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 13, 13, 4], "float32"], ["TENSOR", [96, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1248, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(96, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[32, 1, 3, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74 = sch.get_loops(block=b67)
l75 = sch.fuse(l70, l71, l72)
sch.parallel(loop=l75)
l76 = sch.fuse(l73, l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b68)
l103 = sch.fuse(l77, l78, l79, l80, l81, l82, l83, l84, l85, l86)
sch.parallel(loop=l103)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108 = sch.get_loops(block=b69)
l109 = sch.fuse(l104, l105, l106)
sch.parallel(loop=l109)
l110 = sch.fuse(l107, l108)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
b129 = sch.decompose_reduction(block=b111, loop=l113)
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #17: GFLOPs: 7.2764. Time: 41.1153 ms. Best GFLOPs: 51.2250
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #18: GFLOPs: 56.7876. Time: 5.2682 ms. Best GFLOPs: 56.7876
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #19: GFLOPs: 4.5776. Time: 65.3547 ms. Best GFLOPs: 56.7876
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #20: GFLOPs: 23.9963. Time: 12.4674 ms. Best GFLOPs: 56.7876
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #21: GFLOPs: 0.7580. Time: 394.6751 ms. Best GFLOPs: 56.7876
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #22: GFLOPs: 6.9039. Time: 43.3339 ms. Best GFLOPs: 56.7876
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #23: GFLOPs: 34.9057. Time: 8.5708 ms. Best GFLOPs: 56.7876
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #24: GFLOPs: 23.0090. Time: 13.0024 ms. Best GFLOPs: 56.7876
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #25: GFLOPs: 4.4738. Time: 66.8719 ms. Best GFLOPs: 56.7876
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #26: GFLOPs: 9.0677. Time: 32.9931 ms. Best GFLOPs: 56.7876
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #27: GFLOPs: 7.1545. Time: 41.8160 ms. Best GFLOPs: 56.7876
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #28: GFLOPs: 8.5375. Time: 35.0420 ms. Best GFLOPs: 56.7876
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #29: GFLOPs: 13.8326. Time: 21.6279 ms. Best GFLOPs: 56.7876
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #30: GFLOPs: 2.5036. Time: 119.4984 ms. Best GFLOPs: 56.7876
[19:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"] Trial #31: GFLOPs: 39.9911. Time: 7.4809 ms. Best GFLOPs: 56.7876
[19:13:08] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |         5.0368 |     194.5248 |              194.5248 |     32 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |        68.7139 |    6523.7298 |             6523.7298 |     32 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |        10.7422 |      36.2474 |               36.2474 |     32 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |         5.7021 |     106.2228 |              106.2228 |     32 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |         0.0001 |      15.8475 |               15.8475 |     32 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |        56.7876 |    5268.2420 |             5268.2420 |     32 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 320
Total latency (us): 15542.8

[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #0: GFLOPs: 0.0000. Time: 0.0687 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #1: GFLOPs: 0.0000. Time: 0.0711 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #2: GFLOPs: 0.0000. Time: 0.0605 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #3: GFLOPs: 0.0000. Time: 0.0437 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #4: GFLOPs: 0.0000. Time: 0.0585 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #5: GFLOPs: 0.0000. Time: 0.0428 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #6: GFLOPs: 0.0000. Time: 0.0269 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #7: GFLOPs: 0.0000. Time: 0.0204 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #8: GFLOPs: 0.0000. Time: 0.0296 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #9: GFLOPs: 0.0000. Time: 0.0362 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #10: GFLOPs: 0.0000. Time: 0.0287 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #11: GFLOPs: 0.0000. Time: 0.0316 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #12: GFLOPs: 0.0000. Time: 0.0316 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #13: GFLOPs: 0.0000. Time: 0.0293 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #14: GFLOPs: 0.0000. Time: 0.0290 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #15: GFLOPs: 0.0000. Time: 0.0325 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #16: GFLOPs: 0.0000. Time: 0.0141 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #17: GFLOPs: 0.0000. Time: 0.0316 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #18: GFLOPs: 0.0000. Time: 0.0318 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #19: GFLOPs: 0.0000. Time: 0.0222 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #20: GFLOPs: 0.0000. Time: 0.0201 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #21: GFLOPs: 0.0000. Time: 0.0316 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #22: GFLOPs: 0.0000. Time: 0.0292 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #23: GFLOPs: 0.0000. Time: 0.0267 ms. Best GFLOPs: 0.0000
[19:13:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #24: GFLOPs: 0.0000. Time: 0.3944 ms. Best GFLOPs: 0.0000
[19:13:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #25: GFLOPs: 0.0000. Time: 0.1849 ms. Best GFLOPs: 0.0000
[19:13:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #26: GFLOPs: 0.0000. Time: 0.2689 ms. Best GFLOPs: 0.0000
[19:13:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #27: GFLOPs: 0.0000. Time: 0.2350 ms. Best GFLOPs: 0.0000
[19:13:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #28: GFLOPs: 0.0000. Time: 0.2288 ms. Best GFLOPs: 0.0000
[19:13:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #29: GFLOPs: 0.0000. Time: 0.1454 ms. Best GFLOPs: 0.0000
[19:13:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #30: GFLOPs: 0.0000. Time: 0.0876 ms. Best GFLOPs: 0.0000
[19:13:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_layout_transform_3"] Trial #31: GFLOPs: 0.0000. Time: 0.0806 ms. Best GFLOPs: 0.0000
[19:13:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_layout_transform_3"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |         5.0368 |     194.5248 |              194.5248 |     32 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |        68.7139 |    6523.7298 |             6523.7298 |     32 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |        10.7422 |      36.2474 |               36.2474 |     32 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |         5.7021 |     106.2228 |              106.2228 |     32 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |         0.0001 |      15.8475 |               15.8475 |     32 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |        56.7876 |    5268.2420 |             5268.2420 |     32 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |         0.0001 |      14.0736 |               14.0736 |     32 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 352
Total latency (us): 15556.9

[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #0: GFLOPs: 2.9859. Time: 75.1559 ms. Best GFLOPs: 2.9859
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #1: GFLOPs: 8.5979. Time: 26.1005 ms. Best GFLOPs: 8.5979
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #2: GFLOPs: 16.6952. Time: 13.4416 ms. Best GFLOPs: 16.6952
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #3: GFLOPs: 23.6286. Time: 9.4974 ms. Best GFLOPs: 23.6286
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #4: GFLOPs: 10.9607. Time: 20.4740 ms. Best GFLOPs: 23.6286
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #5: GFLOPs: 5.4654. Time: 41.0604 ms. Best GFLOPs: 23.6286
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #6: GFLOPs: 7.3029. Time: 30.7290 ms. Best GFLOPs: 23.6286
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #7: GFLOPs: 8.5392. Time: 26.2801 ms. Best GFLOPs: 23.6286
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #8: GFLOPs: 7.8992. Time: 28.4093 ms. Best GFLOPs: 23.6286
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #9: GFLOPs: 13.4348. Time: 16.7037 ms. Best GFLOPs: 23.6286
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #10: GFLOPs: 37.5488. Time: 5.9765 ms. Best GFLOPs: 37.5488
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #11: GFLOPs: 47.5677. Time: 4.7177 ms. Best GFLOPs: 47.5677
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #12: GFLOPs: 18.3246. Time: 12.2464 ms. Best GFLOPs: 47.5677
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #13: GFLOPs: 2.0814. Time: 107.8168 ms. Best GFLOPs: 47.5677
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #14: GFLOPs: 4.8109. Time: 46.6458 ms. Best GFLOPs: 47.5677
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #15: GFLOPs: 30.3917. Time: 7.3839 ms. Best GFLOPs: 47.5677
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #16: GFLOPs: 22.5922. Time: 9.9331 ms. Best GFLOPs: 47.5677
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #17: GFLOPs: 85.0727. Time: 2.6379 ms. Best GFLOPs: 85.0727
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #18: GFLOPs: 11.7561. Time: 19.0888 ms. Best GFLOPs: 85.0727
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(384, 192, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 384, 15, 15], dtype="float32")
        data_vec = T.alloc_buffer([2, 1, 48, 15, 4, 15], dtype="float32")
        conv = T.alloc_buffer([2, 1, 48, 13, 13, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 384, 13, 13], dtype="float32")
        conv_global = T.alloc_buffer([2, 1, 48, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 384, 15):
                for ax3_fused in T.vectorized(15):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2, i3 = T.axis.remap("SSS", [ax1, ax2, ax3_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1])
                        T.writes(data_pad[i0, i1, i2, i3])
                        data_pad[i0, i1, i2, i3] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1 in T.grid(1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(2, 1, 48, 15, 4):
                    for ax5_fused in T.vectorized(15):
                        with T.block("data_vec"):
                            g = T.axis.spatial(2, ax0)
                            n = T.axis.spatial(1, 0)
                            C, h, c, w = T.axis.remap("SSSS", [ax2, ax3, ax4, ax5_fused])
                            T.reads(data_pad[n, g * 192 + C * 4 + c, h, w])
                            T.writes(data_vec[g, n, C, h, c, w])
                            data_vec[g, n, C, h, c, w] = data_pad[n, g * 192 + C * 4 + c, h, w]
                for i3_1, i4_1, i5_1 in T.grid(1, 1, 1):
                    for i0_2_init, i2_2_init, i2_3_init, i3_3_init, i4_3_init, i5_3_init in T.grid(2, 6, 2, 13, 13, 2):
                        with T.block("conv_init"):
                            g = T.axis.spatial(2, i0_2_init)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(48, i2_1 * 12 + i2_2_init * 2 + i2_3_init)
                            oh, ow = T.axis.remap("SS", [i3_3_init, i4_3_init])
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_fused * 2 + i5_3_init)
                            T.reads()
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(16, 1, 1, 2, 1, 6, 1, 1, 1, 12, 3, 3, 1, 1, 2, 13, 13, 2):
                        with T.block("conv_update"):
                            g = T.axis.spatial(2, i0_2)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(48, i2_1 * 12 + i2_2 * 2 + i2_3)
                            oh, ow = T.axis.remap("SS", [i3_3, i4_3])
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_fused * 2 + i5_3)
                            ic = T.axis.reduce(192, i6_0 * 12 + i6_1)
                            kh, kw = T.axis.remap("RR", [i7_1, i8_1])
                            T.reads(conv_global[g, n, oc_chunk, oh, ow, oc_block], data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], placeholder_1[g * 192 + oc_chunk * 4 + oc_block, ic, kh, kw])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * placeholder_1[g * 192 + oc_chunk * 4 + oc_block, ic // 4 * 4 + ic % 4, kh, kw]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(2, 1, 48, 13, 13):
                for ax5_fused in T.vectorized(2):
                    with T.block("conv_global"):
                        v0 = T.axis.spatial(2, ax0)
                        v1 = T.axis.spatial(1, 0)
                        v2, v3, v4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        v5 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_fused * 2 + ax5_fused)
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
        for i0_i1_fused in T.parallel(384, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3 in T.grid(13, 13):
                with T.block("output_unpack"):
                    n = T.axis.spatial(1, 0)
                    c, h, w = T.axis.remap("SSS", [i0_i1_fused, i2, i3])
                    T.reads(conv[c // 192, n, c % 192 // 4, h, w, c % 4])
                    T.writes(output_unpack[n, c, h, w])
                    T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [384, 192, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], 2, "float32"]})
                    output_unpack[n, c, h, w] = conv[c // 192, n, c % 192 // 4, h, w, c % 4]
        for i0_i1_fused in T.parallel(384, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(13):
                for i3_fused in T.vectorized(13):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3 = T.axis.remap("SSS", [i0_i1_fused, i2, i3_fused])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="data_vec", func_name="main")
b2 = sch.get_block(name="kernel_vec", func_name="main")
b3 = sch.get_block(name="conv", func_name="main")
b4 = sch.get_block(name="output_unpack", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b3)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 4, 6, 2])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[16, 12])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 3])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 3])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b3, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l60, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b4, decision=-1)
sch.compute_at(block=b4, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b2, decision=-2)
sch.compute_at(block=b2, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=8)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=5)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
sch.enter_postproc()
b82 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.unroll_explicit")
b83, b84, b85, b86, b87, b88 = sch.get_child_blocks(b82)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b83)
l99 = sch.fuse(l89, l90, l91, l92, l93, l94)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b84)
l111 = sch.fuse(l110)
sch.vectorize(loop=l111)
sch.annotate(block_or_loop=l101, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l101, ann_key="pragma_unroll_explicit", ann_val=1)
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b85)
sch.annotate(block_or_loop=l112, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l112, ann_key="pragma_unroll_explicit", ann_val=1)
l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b86)
l144 = sch.fuse(l143)
sch.vectorize(loop=l144)
sch.annotate(block_or_loop=l137, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l137, ann_key="pragma_unroll_explicit", ann_val=1)
l145, l146, l147, l148 = sch.get_loops(block=b87)
l149 = sch.fuse(l145, l146)
sch.parallel(loop=l149)
sch.annotate(block_or_loop=l149, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l149, ann_key="pragma_unroll_explicit", ann_val=1)
l150, l151, l152, l153 = sch.get_loops(block=b88)
l154 = sch.fuse(l150, l151)
sch.parallel(loop=l154)
l155 = sch.fuse(l153)
sch.vectorize(loop=l155)
sch.annotate(block_or_loop=l154, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l154, ann_key="pragma_unroll_explicit", ann_val=1)
b156 = sch.get_block(name="conv", func_name="main")
l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181 = sch.get_loops(block=b156)
b182 = sch.decompose_reduction(block=b156, loop=l164)
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #20: GFLOPs: 4.1900. Time: 53.5582 ms. Best GFLOPs: 85.0727
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #21: GFLOPs: 5.7812. Time: 38.8176 ms. Best GFLOPs: 85.0727
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #22: GFLOPs: 11.1892. Time: 20.0560 ms. Best GFLOPs: 85.0727
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #23: GFLOPs: 17.7318. Time: 12.6558 ms. Best GFLOPs: 85.0727
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #24: GFLOPs: 12.9225. Time: 17.3658 ms. Best GFLOPs: 85.0727
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #25: GFLOPs: 6.4128. Time: 34.9939 ms. Best GFLOPs: 85.0727
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #26: GFLOPs: 11.4310. Time: 19.6317 ms. Best GFLOPs: 85.0727
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #27: GFLOPs: 23.1663. Time: 9.6869 ms. Best GFLOPs: 85.0727
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #28: GFLOPs: 4.3618. Time: 51.4487 ms. Best GFLOPs: 85.0727
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #29: GFLOPs: 5.2091. Time: 43.0804 ms. Best GFLOPs: 85.0727
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #30: GFLOPs: 3.5866. Time: 62.5694 ms. Best GFLOPs: 85.0727
[19:13:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_1"] Trial #31: GFLOPs: 6.4214. Time: 34.9475 ms. Best GFLOPs: 85.0727
[19:13:13] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_conv2d_add_nn_relu_1"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |         5.0368 |     194.5248 |              194.5248 |     32 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |        68.7139 |    6523.7298 |             6523.7298 |     32 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |        10.7422 |      36.2474 |               36.2474 |     32 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |         5.7021 |     106.2228 |              106.2228 |     32 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |         0.0001 |      15.8475 |               15.8475 |     32 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |        56.7876 |    5268.2420 |             5268.2420 |     32 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |         0.0001 |      14.0736 |               14.0736 |     32 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |        85.0727 |    2637.8649 |             2637.8649 |     32 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 384
Total latency (us): 18194.7

[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #0: GFLOPs: 3.3740. Time: 44.3406 ms. Best GFLOPs: 3.3740
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #1: GFLOPs: 7.4082. Time: 20.1947 ms. Best GFLOPs: 7.4082
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #2: GFLOPs: 15.8578. Time: 9.4343 ms. Best GFLOPs: 15.8578
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #3: GFLOPs: 6.0438. Time: 24.7538 ms. Best GFLOPs: 15.8578
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #4: GFLOPs: 5.1789. Time: 28.8879 ms. Best GFLOPs: 15.8578
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #5: GFLOPs: 16.6034. Time: 9.0106 ms. Best GFLOPs: 16.6034
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #6: GFLOPs: 3.5898. Time: 41.6759 ms. Best GFLOPs: 16.6034
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #7: GFLOPs: 5.7857. Time: 25.8579 ms. Best GFLOPs: 16.6034
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #8: GFLOPs: 2.0855. Time: 71.7356 ms. Best GFLOPs: 16.6034
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #9: GFLOPs: 7.4425. Time: 20.1018 ms. Best GFLOPs: 16.6034
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #10: GFLOPs: 7.3667. Time: 20.3086 ms. Best GFLOPs: 16.6034
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #11: GFLOPs: 13.2471. Time: 11.2935 ms. Best GFLOPs: 16.6034
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #12: GFLOPs: 5.5742. Time: 26.8394 ms. Best GFLOPs: 16.6034
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #13: GFLOPs: 27.4027. Time: 5.4596 ms. Best GFLOPs: 27.4027
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #14: GFLOPs: 14.5854. Time: 10.2573 ms. Best GFLOPs: 27.4027
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #15: GFLOPs: 5.0900. Time: 29.3924 ms. Best GFLOPs: 27.4027
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #16: GFLOPs: 8.6711. Time: 17.2534 ms. Best GFLOPs: 27.4027
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #17: GFLOPs: 10.3196. Time: 14.4974 ms. Best GFLOPs: 27.4027
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #18: GFLOPs: 20.1563. Time: 7.4223 ms. Best GFLOPs: 27.4027
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #19: GFLOPs: 4.3159. Time: 34.6644 ms. Best GFLOPs: 27.4027
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #20: GFLOPs: 11.0068. Time: 13.5922 ms. Best GFLOPs: 27.4027
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #21: GFLOPs: 18.7011. Time: 7.9999 ms. Best GFLOPs: 27.4027
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #22: GFLOPs: 13.1763. Time: 11.3542 ms. Best GFLOPs: 27.4027
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #23: GFLOPs: 27.6876. Time: 5.4034 ms. Best GFLOPs: 27.6876
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #24: GFLOPs: 25.2960. Time: 5.9143 ms. Best GFLOPs: 27.6876
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #25: GFLOPs: 26.6558. Time: 5.6126 ms. Best GFLOPs: 27.6876
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #26: GFLOPs: 47.1753. Time: 3.1713 ms. Best GFLOPs: 47.1753
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #27: GFLOPs: 14.1853. Time: 10.5466 ms. Best GFLOPs: 47.1753
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #28: GFLOPs: 4.4298. Time: 33.7728 ms. Best GFLOPs: 47.1753
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #29: GFLOPs: 11.7430. Time: 12.7401 ms. Best GFLOPs: 47.1753
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #30: GFLOPs: 21.9566. Time: 6.8138 ms. Best GFLOPs: 47.1753
[19:13:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_nn_relu_2"] Trial #31: GFLOPs: 18.2611. Time: 8.1926 ms. Best GFLOPs: 47.1753
[19:13:16] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_conv2d_add_nn_relu_2"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |         5.0368 |     194.5248 |              194.5248 |     32 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |        68.7139 |    6523.7298 |             6523.7298 |     32 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |        10.7422 |      36.2474 |               36.2474 |     32 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |         5.7021 |     106.2228 |              106.2228 |     32 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |         0.0001 |      15.8475 |               15.8475 |     32 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |        56.7876 |    5268.2420 |             5268.2420 |     32 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |         0.0001 |      14.0736 |               14.0736 |     32 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |        85.0727 |    2637.8649 |             2637.8649 |     32 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |        47.1753 |    3171.2979 |             3171.2979 |     32 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 416
Total latency (us): 21366

[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #0: GFLOPs: 2.2160. Time: 0.0374 ms. Best GFLOPs: 2.2160
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #1: GFLOPs: 2.1103. Time: 0.0393 ms. Best GFLOPs: 2.2160
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #2: GFLOPs: 1.8648. Time: 0.0445 ms. Best GFLOPs: 2.2160
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #3: GFLOPs: 1.4625. Time: 0.0567 ms. Best GFLOPs: 2.2160
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #4: GFLOPs: 2.7469. Time: 0.0302 ms. Best GFLOPs: 2.7469
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #5: GFLOPs: 0.9472. Time: 0.0876 ms. Best GFLOPs: 2.7469
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #6: GFLOPs: 1.7401. Time: 0.0477 ms. Best GFLOPs: 2.7469
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #7: GFLOPs: 1.2079. Time: 0.0687 ms. Best GFLOPs: 2.7469
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #8: GFLOPs: 1.4514. Time: 0.0571 ms. Best GFLOPs: 2.7469
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #9: GFLOPs: 1.8813. Time: 0.0441 ms. Best GFLOPs: 2.7469
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #10: GFLOPs: 2.1449. Time: 0.0387 ms. Best GFLOPs: 2.7469
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #11: GFLOPs: 1.3005. Time: 0.0638 ms. Best GFLOPs: 2.7469
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #12: GFLOPs: 0.6551. Time: 0.1266 ms. Best GFLOPs: 2.7469
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #13: GFLOPs: 0.2626. Time: 0.3159 ms. Best GFLOPs: 2.7469
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #14: GFLOPs: 1.0304. Time: 0.0805 ms. Best GFLOPs: 2.7469
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #15: GFLOPs: 2.8574. Time: 0.0290 ms. Best GFLOPs: 2.8574
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #16: GFLOPs: 1.0724. Time: 0.0773 ms. Best GFLOPs: 2.8574
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #17: GFLOPs: 1.6405. Time: 0.0506 ms. Best GFLOPs: 2.8574
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #18: GFLOPs: 1.6229. Time: 0.0511 ms. Best GFLOPs: 2.8574
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #19: GFLOPs: 1.4066. Time: 0.0590 ms. Best GFLOPs: 2.8574
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #20: GFLOPs: 1.4528. Time: 0.0571 ms. Best GFLOPs: 2.8574
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #21: GFLOPs: 3.0212. Time: 0.0275 ms. Best GFLOPs: 3.0212
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #22: GFLOPs: 2.2137. Time: 0.0375 ms. Best GFLOPs: 3.0212
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #23: GFLOPs: 1.4560. Time: 0.0570 ms. Best GFLOPs: 3.0212
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #24: GFLOPs: 1.3268. Time: 0.0625 ms. Best GFLOPs: 3.0212
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #25: GFLOPs: 0.9282. Time: 0.0894 ms. Best GFLOPs: 3.0212
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #26: GFLOPs: 1.5290. Time: 0.0542 ms. Best GFLOPs: 3.0212
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #27: GFLOPs: 2.6667. Time: 0.0311 ms. Best GFLOPs: 3.0212
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #28: GFLOPs: 6.2568. Time: 0.0133 ms. Best GFLOPs: 6.2568
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #29: GFLOPs: 3.3343. Time: 0.0249 ms. Best GFLOPs: 6.2568
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #30: GFLOPs: 1.8346. Time: 0.0452 ms. Best GFLOPs: 6.2568
[19:13:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_max_pool2d_2"] Trial #31: GFLOPs: 2.4984. Time: 0.0332 ms. Best GFLOPs: 6.2568
[19:13:20] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_nn_max_pool2d_2"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |         5.0368 |     194.5248 |              194.5248 |     32 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |        68.7139 |    6523.7298 |             6523.7298 |     32 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |        10.7422 |      36.2474 |               36.2474 |     32 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |         5.7021 |     106.2228 |              106.2228 |     32 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |         0.0001 |      15.8475 |               15.8475 |     32 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |        56.7876 |    5268.2420 |             5268.2420 |     32 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |         0.0001 |      14.0736 |               14.0736 |     32 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |        85.0727 |    2637.8649 |             2637.8649 |     32 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |        47.1753 |    3171.2979 |             3171.2979 |     32 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |         6.2568 |      13.2565 |               13.2565 |     32 |            
 14 |                               fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 448
Total latency (us): 21379.3

[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #0: GFLOPs: 0.0000. Time: 0.0158 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #1: GFLOPs: 0.0000. Time: 0.0121 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #2: GFLOPs: 0.0000. Time: 0.0112 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #3: GFLOPs: 0.0000. Time: 0.0091 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #4: GFLOPs: 0.0000. Time: 0.0134 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #5: GFLOPs: 0.0000. Time: 0.0124 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #6: GFLOPs: 0.0000. Time: 0.0143 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #7: GFLOPs: 0.0000. Time: 0.0148 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #8: GFLOPs: 0.0000. Time: 0.0131 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #9: GFLOPs: 0.0000. Time: 0.0136 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #10: GFLOPs: 0.0000. Time: 0.0113 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #11: GFLOPs: 0.0000. Time: 0.0126 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #12: GFLOPs: 0.0000. Time: 0.0132 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #13: GFLOPs: 0.0000. Time: 0.0143 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #14: GFLOPs: 0.0000. Time: 0.0109 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #15: GFLOPs: 0.0000. Time: 0.0140 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #16: GFLOPs: 0.0000. Time: 0.0143 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #17: GFLOPs: 0.0000. Time: 0.0120 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #18: GFLOPs: 0.0000. Time: 0.0162 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #19: GFLOPs: 0.0000. Time: 1.1754 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #20: GFLOPs: 0.0000. Time: 0.1434 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #21: GFLOPs: 0.0000. Time: 0.2235 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #22: GFLOPs: 0.0000. Time: 0.1313 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #23: GFLOPs: 0.0000. Time: 0.0197 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #24: GFLOPs: 0.0000. Time: 0.0232 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #25: GFLOPs: 0.0000. Time: 0.0245 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #26: GFLOPs: 0.0000. Time: 0.0235 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #27: GFLOPs: 0.0000. Time: 0.0235 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #28: GFLOPs: 0.0000. Time: 0.0239 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #29: GFLOPs: 0.0000. Time: 0.0175 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #30: GFLOPs: 0.0000. Time: 0.0229 ms. Best GFLOPs: 0.0000
[19:13:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_reshape"] Trial #31: GFLOPs: 0.0000. Time: 0.0148 ms. Best GFLOPs: 0.0000
[19:13:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_reshape"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |         5.0368 |     194.5248 |              194.5248 |     32 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |        68.7139 |    6523.7298 |             6523.7298 |     32 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |        10.7422 |      36.2474 |               36.2474 |     32 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |         5.7021 |     106.2228 |              106.2228 |     32 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |         0.0001 |      15.8475 |               15.8475 |     32 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |        56.7876 |    5268.2420 |             5268.2420 |     32 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |         0.0001 |      14.0736 |               14.0736 |     32 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |        85.0727 |    2637.8649 |             2637.8649 |     32 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |        47.1753 |    3171.2979 |             3171.2979 |     32 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |         6.2568 |      13.2565 |               13.2565 |     32 |            
 14 |                               fused_reshape |         1 |      1 |         0.0001 |       9.0699 |                9.0699 |     32 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 480
Total latency (us): 21388.3

[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #0: GFLOPs: 5.5951. Time: 13.4950 ms. Best GFLOPs: 5.5951
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #1: GFLOPs: 3.4015. Time: 22.1979 ms. Best GFLOPs: 5.5951
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #2: GFLOPs: 8.9220. Time: 8.4629 ms. Best GFLOPs: 8.9220
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #3: GFLOPs: 5.6766. Time: 13.3012 ms. Best GFLOPs: 8.9220
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #4: GFLOPs: 2.6971. Time: 27.9949 ms. Best GFLOPs: 8.9220
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #5: GFLOPs: 3.1034. Time: 24.3301 ms. Best GFLOPs: 8.9220
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #6: GFLOPs: 10.5684. Time: 7.1445 ms. Best GFLOPs: 10.5684
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #7: GFLOPs: 2.1755. Time: 34.7074 ms. Best GFLOPs: 10.5684
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #8: GFLOPs: 10.3281. Time: 7.3107 ms. Best GFLOPs: 10.5684
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #9: GFLOPs: 8.8493. Time: 8.5324 ms. Best GFLOPs: 10.5684
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #10: GFLOPs: 13.7185. Time: 5.5039 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #11: GFLOPs: 6.8027. Time: 11.0994 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #12: GFLOPs: 1.6755. Time: 45.0649 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #13: GFLOPs: 1.0346. Time: 72.9799 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #14: GFLOPs: 7.5913. Time: 9.9464 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #15: GFLOPs: 5.3445. Time: 14.1278 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #16: GFLOPs: 2.1362. Time: 35.3455 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #17: GFLOPs: 11.8612. Time: 6.3657 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #18: GFLOPs: 7.0238. Time: 10.7499 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #19: GFLOPs: 10.8207. Time: 6.9779 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #20: GFLOPs: 8.6590. Time: 8.7199 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #21: GFLOPs: 12.0464. Time: 6.2679 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #22: GFLOPs: 2.8817. Time: 26.2015 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #23: GFLOPs: 4.6449. Time: 16.2555 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #24: GFLOPs: 7.1060. Time: 10.6256 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #25: GFLOPs: 2.3599. Time: 31.9951 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #26: GFLOPs: 8.8420. Time: 8.5394 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #27: GFLOPs: 1.5837. Time: 47.6761 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #28: GFLOPs: 8.4018. Time: 8.9868 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #29: GFLOPs: 7.7631. Time: 9.7262 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #30: GFLOPs: 9.4928. Time: 7.9540 ms. Best GFLOPs: 13.7185
[19:13:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_dense_add_nn_relu"] Trial #31: GFLOPs: 3.6213. Time: 20.8503 ms. Best GFLOPs: 13.7185
[19:13:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_dense_add_nn_relu"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |         5.0368 |     194.5248 |              194.5248 |     32 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |        68.7139 |    6523.7298 |             6523.7298 |     32 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |        10.7422 |      36.2474 |               36.2474 |     32 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |         5.7021 |     106.2228 |              106.2228 |     32 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |         0.0001 |      15.8475 |               15.8475 |     32 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |        56.7876 |    5268.2420 |             5268.2420 |     32 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |         0.0001 |      14.0736 |               14.0736 |     32 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |        85.0727 |    2637.8649 |             2637.8649 |     32 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |        47.1753 |    3171.2979 |             3171.2979 |     32 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |         6.2568 |      13.2565 |               13.2565 |     32 |            
 14 |                               fused_reshape |         1 |      1 |         0.0001 |       9.0699 |                9.0699 |     32 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |        13.7185 |    5503.9308 |             5503.9308 |     32 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 512
Total latency (us): 26892.3

[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #0: GFLOPs: 7.7611. Time: 4.3245 ms. Best GFLOPs: 7.7611
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #1: GFLOPs: 1.9271. Time: 17.4158 ms. Best GFLOPs: 7.7611
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #2: GFLOPs: 4.9240. Time: 6.8162 ms. Best GFLOPs: 7.7611
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #3: GFLOPs: 2.4767. Time: 13.5513 ms. Best GFLOPs: 7.7611
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #4: GFLOPs: 3.6289. Time: 9.2488 ms. Best GFLOPs: 7.7611
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #5: GFLOPs: 1.9369. Time: 17.3277 ms. Best GFLOPs: 7.7611
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #6: GFLOPs: 2.4563. Time: 13.6639 ms. Best GFLOPs: 7.7611
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #7: GFLOPs: 2.1762. Time: 15.4225 ms. Best GFLOPs: 7.7611
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #8: GFLOPs: 4.5336. Time: 7.4032 ms. Best GFLOPs: 7.7611
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #9: GFLOPs: 0.9644. Time: 34.8012 ms. Best GFLOPs: 7.7611
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #10: GFLOPs: 4.3092. Time: 7.7886 ms. Best GFLOPs: 7.7611
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #11: GFLOPs: 1.6139. Time: 20.7961 ms. Best GFLOPs: 7.7611
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #12: GFLOPs: 9.0142. Time: 3.7233 ms. Best GFLOPs: 9.0142
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #13: GFLOPs: 4.1356. Time: 8.1155 ms. Best GFLOPs: 9.0142
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #14: GFLOPs: 4.8836. Time: 6.8725 ms. Best GFLOPs: 9.0142
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #15: GFLOPs: 3.0927. Time: 10.8521 ms. Best GFLOPs: 9.0142
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #16: GFLOPs: 5.3957. Time: 6.2202 ms. Best GFLOPs: 9.0142
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #17: GFLOPs: 1.2214. Time: 27.4798 ms. Best GFLOPs: 9.0142
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #18: GFLOPs: 7.9899. Time: 4.2006 ms. Best GFLOPs: 9.0142
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #19: GFLOPs: 2.2524. Time: 14.9005 ms. Best GFLOPs: 9.0142
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #20: GFLOPs: 17.7201. Time: 1.8940 ms. Best GFLOPs: 17.7201
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #21: GFLOPs: 6.3810. Time: 5.2597 ms. Best GFLOPs: 17.7201
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #22: GFLOPs: 0.2238. Time: 149.9847 ms. Best GFLOPs: 17.7201
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #23: GFLOPs: 5.2651. Time: 6.3746 ms. Best GFLOPs: 17.7201
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #24: GFLOPs: 2.6115. Time: 12.8518 ms. Best GFLOPs: 17.7201
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4096), "float32"], placeholder_1: T.Buffer[(4096, 4096), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 4096], dtype="float32")
        for i0_0_i1_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1 in T.grid(1, 8):
                for i1_3_init in T.serial(64):
                    with T.block("T_matmul_NT_init"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(4096, i0_0_i1_0_fused * 512 + i1_1 * 64 + i1_3_init)
                        T.reads()
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                        T_matmul_NT[i, j] = T.float32(0)
                for i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(512, 1, 1, 8, 1, 64):
                    with T.block("T_matmul_NT_update"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(4096, i0_0_i1_0_fused * 512 + i1_1 * 64 + i1_3)
                        k = T.axis.reduce(4096, i2_0 * 8 + i2_1)
                        T.reads(T_matmul_NT[i, j], placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
            for ax0, ax1 in T.grid(1, 512):
                with T.block("T_relu"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(4096, i0_0_i1_0_fused * 512 + ax1)
                    T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                    T.writes(T_relu[ax0_1, ax1_1])
                    T_relu[ax0_1, ax1_1] = T.max(T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1], T.float32(0))
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l10, l11, l12, l13 = sch.split(loop=l3, factors=[v6, v7, v8, v9])
v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 8, 1, 64])
l18, l19, l20, l21 = sch.split(loop=l4, factors=[v14, v15, v16, v17])
v22, v23 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[512, 8])
l24, l25 = sch.split(loop=l5, factors=[v22, v23])
sch.reorder(l10, l18, l11, l19, l24, l12, l20, l25, l13, l21)
b26, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b26, loop=l18, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v27 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v27)
sch.enter_postproc()
b28 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b28, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b28, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b28, ann_key="meta_schedule.unroll_explicit")
b29, b30 = sch.get_child_blocks(b28)
l31, l32, l33, l34, l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b29)
l41 = sch.fuse(l31, l32)
sch.parallel(loop=l41)
sch.annotate(block_or_loop=l41, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l41, ann_key="pragma_unroll_explicit", ann_val=1)
l42, l43, l44 = sch.get_loops(block=b30)
sch.annotate(block_or_loop=l42, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l42, ann_key="pragma_unroll_explicit", ann_val=1)
b45 = sch.get_block(name="T_matmul_NT", func_name="main")
l46, l47, l48, l49, l50, l51, l52, l53, l54 = sch.get_loops(block=b45)
b55 = sch.decompose_reduction(block=b45, loop=l49)
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #26: GFLOPs: 4.7805. Time: 7.0208 ms. Best GFLOPs: 17.7201
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #27: GFLOPs: 1.8159. Time: 18.4826 ms. Best GFLOPs: 17.7201
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #28: GFLOPs: 2.9025. Time: 11.5634 ms. Best GFLOPs: 17.7201
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #29: GFLOPs: 7.2865. Time: 4.6061 ms. Best GFLOPs: 17.7201
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #30: GFLOPs: 2.0065. Time: 16.7265 ms. Best GFLOPs: 17.7201
[19:13:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_add_nn_relu_1"] Trial #31: GFLOPs: 0.9067. Time: 37.0157 ms. Best GFLOPs: 17.7201
[19:13:26] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_dense_add_nn_relu_1"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |         5.0368 |     194.5248 |              194.5248 |     32 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |        68.7139 |    6523.7298 |             6523.7298 |     32 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |        10.7422 |      36.2474 |               36.2474 |     32 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |         5.7021 |     106.2228 |              106.2228 |     32 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |         0.0001 |      15.8475 |               15.8475 |     32 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |        56.7876 |    5268.2420 |             5268.2420 |     32 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |         0.0001 |      14.0736 |               14.0736 |     32 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |        85.0727 |    2637.8649 |             2637.8649 |     32 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |        47.1753 |    3171.2979 |             3171.2979 |     32 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |         6.2568 |      13.2565 |               13.2565 |     32 |            
 14 |                               fused_reshape |         1 |      1 |         0.0001 |       9.0699 |                9.0699 |     32 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |        13.7185 |    5503.9308 |             5503.9308 |     32 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |        17.7201 |    1894.0424 |             1894.0424 |     32 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 544
Total latency (us): 28786.3

[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #0: GFLOPs: 2.3489. Time: 0.6976 ms. Best GFLOPs: 2.3489
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #1: GFLOPs: 0.6893. Time: 2.3771 ms. Best GFLOPs: 2.3489
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #2: GFLOPs: 2.2941. Time: 0.7143 ms. Best GFLOPs: 2.3489
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #3: GFLOPs: 1.9552. Time: 0.8381 ms. Best GFLOPs: 2.3489
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #4: GFLOPs: 0.6226. Time: 2.6320 ms. Best GFLOPs: 2.3489
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #5: GFLOPs: 0.8524. Time: 1.9223 ms. Best GFLOPs: 2.3489
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #6: GFLOPs: 0.9007. Time: 1.8192 ms. Best GFLOPs: 2.3489
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #7: GFLOPs: 0.7819. Time: 2.0958 ms. Best GFLOPs: 2.3489
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #8: GFLOPs: 4.3254. Time: 0.3788 ms. Best GFLOPs: 4.3254
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #9: GFLOPs: 0.3494. Time: 4.6892 ms. Best GFLOPs: 4.3254
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #10: GFLOPs: 5.0060. Time: 0.3273 ms. Best GFLOPs: 5.0060
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #11: GFLOPs: 4.7795. Time: 0.3428 ms. Best GFLOPs: 5.0060
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #12: GFLOPs: 4.5829. Time: 0.3575 ms. Best GFLOPs: 5.0060
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #13: GFLOPs: 1.2618. Time: 1.2986 ms. Best GFLOPs: 5.0060
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #14: GFLOPs: 6.3301. Time: 0.2589 ms. Best GFLOPs: 6.3301
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #15: GFLOPs: 0.4945. Time: 3.3136 ms. Best GFLOPs: 6.3301
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #16: GFLOPs: 9.6794. Time: 0.1693 ms. Best GFLOPs: 9.6794
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #17: GFLOPs: 1.4524. Time: 1.1282 ms. Best GFLOPs: 9.6794
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #18: GFLOPs: 11.2404. Time: 0.1458 ms. Best GFLOPs: 11.2404
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #19: GFLOPs: 0.7033. Time: 2.3299 ms. Best GFLOPs: 11.2404
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #20: GFLOPs: 1.0787. Time: 1.5191 ms. Best GFLOPs: 11.2404
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #21: GFLOPs: 3.1896. Time: 0.5137 ms. Best GFLOPs: 11.2404
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #22: GFLOPs: 4.7693. Time: 0.3436 ms. Best GFLOPs: 11.2404
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #23: GFLOPs: 10.5132. Time: 0.1559 ms. Best GFLOPs: 11.2404
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #24: GFLOPs: 13.9137. Time: 0.1178 ms. Best GFLOPs: 13.9137
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #25: GFLOPs: 14.7700. Time: 0.1109 ms. Best GFLOPs: 14.7700
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #26: GFLOPs: 5.1741. Time: 0.3167 ms. Best GFLOPs: 14.7700
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #27: GFLOPs: 12.5626. Time: 0.1304 ms. Best GFLOPs: 14.7700
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #28: GFLOPs: 13.3506. Time: 0.1227 ms. Best GFLOPs: 14.7700
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #29: GFLOPs: 12.3051. Time: 0.1332 ms. Best GFLOPs: 14.7700
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #30: GFLOPs: 13.7293. Time: 0.1194 ms. Best GFLOPs: 14.7700
[19:13:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_dense_add"] Trial #31: GFLOPs: 6.2628. Time: 0.2616 ms. Best GFLOPs: 14.7700
[19:13:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #17: "fused_nn_dense_add"
 ID |                                        Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                      fused_layout_transform |         1 |      1 |         0.0001 |      19.1389 |               19.1389 |     32 |            
  1 |   fused_nn_contrib_conv2d_NCHWc_add_nn_relu | 203793408 |      1 |        62.9491 |    3237.4336 |             3237.4336 |     32 |            
  2 |                         fused_nn_max_pool2d |    629856 |      1 |         5.4412 |     115.7567 |              115.7567 |     32 |            
  3 |                    fused_layout_transform_1 |         1 |      1 |         0.0000 |      25.6416 |               25.6416 |     32 |            
  4 |                                fused_nn_lrn |    979776 |      1 |         5.0368 |     194.5248 |              194.5248 |     32 |            
  5 |                 fused_nn_conv2d_add_nn_relu | 448270848 |      1 |        68.7139 |    6523.7298 |             6523.7298 |     32 |            
  6 |                       fused_nn_max_pool2d_1 |    389376 |      1 |        10.7422 |      36.2474 |               36.2474 |     32 |            
  7 |                              fused_nn_lrn_1 |    605696 |      1 |         5.7021 |     106.2228 |              106.2228 |     32 |            
  8 |                    fused_layout_transform_2 |         1 |      1 |         0.0001 |      15.8475 |               15.8475 |     32 |            
  9 | fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1 | 299170560 |      1 |        56.7876 |    5268.2420 |             5268.2420 |     32 |            
 10 |                    fused_layout_transform_3 |         1 |      1 |         0.0001 |      14.0736 |               14.0736 |     32 |            
 11 |               fused_nn_conv2d_add_nn_relu_1 | 224410368 |      1 |        85.0727 |    2637.8649 |             2637.8649 |     32 |            
 12 |               fused_nn_conv2d_add_nn_relu_2 | 149606912 |      1 |        47.1753 |    3171.2979 |             3171.2979 |     32 |            
 13 |                       fused_nn_max_pool2d_2 |     82944 |      1 |         6.2568 |      13.2565 |               13.2565 |     32 |            
 14 |                               fused_reshape |         1 |      1 |         0.0001 |       9.0699 |                9.0699 |     32 |            
 15 |                  fused_nn_dense_add_nn_relu |  75505664 |      1 |        13.7185 |    5503.9308 |             5503.9308 |     32 |            
 16 |                fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |        17.7201 |    1894.0424 |             1894.0424 |     32 |            
 17 |                          fused_nn_dense_add |   1638600 |      1 |        14.7700 |     110.9408 |              110.9408 |     32 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 576
Total latency (us): 28897.3

[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_nn_conv2d_add_nn_relu"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #5 has finished. Remaining task(s): 17
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #15: "fused_nn_dense_add_nn_relu"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #15 has finished. Remaining task(s): 16
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_1"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #9 has finished. Remaining task(s): 15
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #1 has finished. Remaining task(s): 14
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_nn_conv2d_add_nn_relu_2"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #12 has finished. Remaining task(s): 13
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_nn_conv2d_add_nn_relu_1"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #11 has finished. Remaining task(s): 12
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #16: "fused_nn_dense_add_nn_relu_1"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #16 has finished. Remaining task(s): 11
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_nn_lrn"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #4 has finished. Remaining task(s): 10
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_nn_max_pool2d"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #2 has finished. Remaining task(s): 9
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #17: "fused_nn_dense_add"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #17 has finished. Remaining task(s): 8
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_nn_lrn_1"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #7 has finished. Remaining task(s): 7
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_nn_max_pool2d_1"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #6 has finished. Remaining task(s): 6
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_layout_transform_1"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #3 has finished. Remaining task(s): 5
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_layout_transform"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #0 has finished. Remaining task(s): 4
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_layout_transform_2"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #8 has finished. Remaining task(s): 3
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_layout_transform_3"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #10 has finished. Remaining task(s): 2
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_nn_max_pool2d_2"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #13 has finished. Remaining task(s): 1
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #14: "fused_reshape"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #14 has finished. Remaining task(s): 0
[[-1.0245826 -1.0679985 -1.2266252 -1.0116462 -1.0223234 -1.2654979
  -1.0114285 -1.5005814 -1.4736326 -0.9995859 -1.0362138 -1.0155962
  -1.0416201 -1.0246532 -1.0135362 -1.0206407 -1.0154053 -1.0000266
  -1.0596446 -1.362435  -1.7981586 -1.0341055 -1.0786169 -1.027573
  -1.0115907 -1.721767  -1.0160991 -1.1640528 -1.0210532 -1.0623167
  -1.0345714 -1.6056011 -1.015458  -1.7572845 -1.0098894 -1.231657
  -1.017937  -1.0236406 -1.0476304 -1.0276726 -1.6193912 -1.5930095
  -1.0901426 -1.0372727 -1.0391059 -1.663974  -1.2613317 -1.4698259
  -1.023799  -0.9934474 -1.0138636 -1.0292428 -1.0451585 -1.012347
  -1.0189176 -1.4401059 -1.7011749 -1.33722   -1.2646111 -1.6635549
  -1.0470194 -1.0480344 -1.4507825 -1.046677  -1.0155323 -1.0327183
  -1.1866719 -1.0162795 -1.0295868 -1.0854087 -1.0236616 -1.6600728
  -1.2664309 -1.4841795 -1.023712  -1.0237173 -1.0131778 -1.4398992
  -1.0459021 -1.1247709 -1.0144047 -1.2096632 -1.4373392 -1.2022951
  -1.0255797 -1.4219714 -1.0039996 -1.0277395 -1.0244364 -1.5828145
  -1.0202312 -1.0547528 -1.0118504 -1.0103844 -1.5001384 -0.9981401
  -1.6019638 -1.007854  -1.3577874 -1.0416945 -1.045501  -1.0310696
  -1.3220137 -1.0222334 -1.5568525 -1.0066996 -1.015503  -1.0395607
  -1.0193641 -1.6058182 -1.0180193 -1.2414031 -1.2784325 -1.013007
  -1.0294187 -1.02344   -1.0230159 -1.0339296 -1.0263019 -1.0424259
  -1.0448684 -1.4390912 -1.0046316 -1.0641818 -1.2654629 -1.0333371
  -1.0036689 -1.1709589 -1.1495209 -1.0297585 -1.2034849 -1.020696
  -1.0138552 -1.6048203 -1.1787777 -1.0189469 -1.3364528 -1.0151783
  -1.2088859 -1.0181752 -1.4087029 -1.0191661 -1.0028914 -1.5463345
  -1.6921824 -1.0283024 -1.0119932 -1.0375811 -1.0280417 -1.202429
  -1.0305458 -1.5053264 -1.0014036 -1.0486016 -1.0253522 -1.0234282
  -1.5338669 -1.3040063 -1.7124774 -1.0163494 -1.4099641 -1.1878939
  -1.0133338 -1.198331  -1.2092052 -1.3482672 -1.0460564 -1.018763
  -1.2279396 -1.0107467 -1.0543184 -1.01872   -1.0090942 -1.0140321
  -1.0398207 -1.0897802 -1.2143366 -1.5205137 -1.2077643 -1.4078741
  -1.0131913 -1.4611276 -1.2662741 -1.0320007 -1.1088471 -1.1461574
  -1.0367904 -1.6355501 -1.0637128 -1.0116646 -1.034723  -1.0303341
  -1.0230225 -1.4347198 -1.0015848 -1.027051  -1.2106155 -1.4230932
  -1.0318786 -1.0062934]]
[[-1.0245826 -1.0679985 -1.2266252 -1.0116462 -1.0223234 -1.2654979
  -1.0114285 -1.5005815 -1.4736326 -0.999586  -1.0362136 -1.0155962
  -1.0416201 -1.0246532 -1.0135362 -1.0206407 -1.0154053 -1.0000266
  -1.0596446 -1.362435  -1.7981586 -1.0341055 -1.0786169 -1.027573
  -1.0115907 -1.721767  -1.0160991 -1.1640528 -1.0210532 -1.0623167
  -1.0345714 -1.6056011 -1.015458  -1.7572845 -1.0098894 -1.231657
  -1.017937  -1.0236406 -1.0476304 -1.0276726 -1.6193912 -1.5930095
  -1.0901426 -1.0372727 -1.0391059 -1.663974  -1.2613317 -1.4698259
  -1.023799  -0.9934474 -1.0138636 -1.0292428 -1.0451585 -1.012347
  -1.0189176 -1.4401059 -1.7011749 -1.33722   -1.2646111 -1.6635549
  -1.0470194 -1.0480344 -1.4507824 -1.046677  -1.0155323 -1.0327182
  -1.1866719 -1.0162796 -1.0295868 -1.0854087 -1.0236616 -1.6600728
  -1.2664309 -1.4841795 -1.023712  -1.0237174 -1.0131778 -1.4398992
  -1.0459021 -1.1247709 -1.0144047 -1.2096632 -1.4373392 -1.2022951
  -1.0255797 -1.4219714 -1.0039996 -1.0277395 -1.0244364 -1.5828145
  -1.0202312 -1.0547528 -1.0118504 -1.0103844 -1.5001384 -0.9981401
  -1.6019638 -1.007854  -1.3577874 -1.0416946 -1.045501  -1.0310696
  -1.3220137 -1.0222334 -1.5568526 -1.0066996 -1.0155029 -1.0395607
  -1.0193641 -1.6058182 -1.0180193 -1.2414031 -1.2784325 -1.013007
  -1.0294187 -1.02344   -1.0230159 -1.0339296 -1.026302  -1.0424259
  -1.0448684 -1.4390913 -1.0046316 -1.0641818 -1.2654629 -1.0333371
  -1.0036689 -1.1709589 -1.1495209 -1.0297585 -1.2034849 -1.020696
  -1.0138552 -1.6048203 -1.1787777 -1.018947  -1.3364528 -1.0151783
  -1.2088859 -1.0181752 -1.4087029 -1.0191661 -1.0028914 -1.5463345
  -1.6921824 -1.0283024 -1.0119932 -1.0375811 -1.0280417 -1.202429
  -1.030546  -1.5053264 -1.0014036 -1.0486017 -1.0253522 -1.0234282
  -1.5338669 -1.3040063 -1.7124774 -1.0163494 -1.4099641 -1.1878939
  -1.0133338 -1.198331  -1.2092052 -1.3482672 -1.0460564 -1.0187631
  -1.2279396 -1.0107467 -1.0543184 -1.01872   -1.0090942 -1.0140321
  -1.0398207 -1.0897802 -1.2143366 -1.5205137 -1.2077643 -1.4078741
  -1.0131913 -1.4611276 -1.2662741 -1.0320007 -1.1088471 -1.1461574
  -1.0367904 -1.6355503 -1.0637128 -1.0116646 -1.034723  -1.0303341
  -1.0230225 -1.4347198 -1.0015848 -1.0270511 -1.2106155 -1.4230932
  -1.0318786 -1.0062934]]
