nohup: ignoring input
[14:43:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_multiply_add_multiply_add_nn_relu"
[14:43:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 992, 1, 1), "float32"], T_relu: T.Buffer[(1, 992, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 992, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 992, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 992, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 992, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 992, 1, 1), "float32"], T_relu: T.Buffer[(1, 992, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:43:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_nn_conv2d_add_nn_relu"
[14:43:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 992, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 992, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 992, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 992, 7, 7], "float32"], ["TENSOR", [128, 992, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 992, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 992, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 992, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(31, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1568):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(992, i4_0 * 32 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(4096):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 32)
                                    v1 = T.axis.spatial(992, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 2, 1, 7, 4, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    xx = T.axis.spatial(7, i3_3)
                                    rc = T.axis.reduce(992, i4_0 * 32 + i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 992, 7, 7], "float32"], ["TENSOR", [128, 992, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 4, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[31, 8, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_multiply_add_multiply_add_nn_relu_1"
[14:43:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:43:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_conv2d_add_nn_relu_1"
[14:43:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 960, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [128, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 960, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(735):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(960, i4_0 * 15 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1920):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 15)
                                    v1 = T.axis.spatial(960, i4_0 * 15 + ax0_ax1_ax2_ax3_fused % 15)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 1, 1, 15, 1, 1, 1, 1, 1, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3)
                                    yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    xx = T.axis.spatial(7, i3_4)
                                    rc = T.axis.reduce(960, i4_0 * 15 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [128, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 2, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 15])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_multiply_add_multiply_add_nn_relu_2"
[14:43:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 928, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 928, 1, 1), "float32"], T_relu: T.Buffer[(1, 928, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 928, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 928, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 928, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 928, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 928, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 928, 1, 1), "float32"], T_relu: T.Buffer[(1, 928, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:43:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_conv2d_add_nn_relu_2"
[14:43:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 928, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 928, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 928, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 928, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 928, 7, 7], "float32"], ["TENSOR", [128, 928, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 928, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 928, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 928, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 928, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(196, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(464):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(928, i4_0 * 464 + ax0_ax1_ax2_ax3_fused)
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(14848):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 32 + ax0_ax1_ax2_ax3_fused // 464)
                                    v1 = T.axis.spatial(928, i4_0 * 464 + ax0_ax1_ax2_ax3_fused % 464)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(116, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                    yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7)
                                    xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    rc = T.axis.reduce(928, i4_0 * 464 + i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 928, 7, 7], "float32"], ["TENSOR", [128, 928, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                                v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 + ax2)
                                v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 116, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_multiply_add_multiply_add_nn_relu_3"
[14:43:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 896, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 896, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 896, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 896, 1, 1), "float32"], T_relu: T.Buffer[(1, 896, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 896, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 896, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 896, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 896, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 896, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 896, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 896, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 896, 1, 1), "float32"], T_relu: T.Buffer[(1, 896, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:43:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_nn_conv2d_add_nn_relu_3"
[14:43:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 896, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 896, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 896, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 896, 7, 7], "float32"], ["TENSOR", [128, 896, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 896, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 896, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 896, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(112, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(21952):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(896, i4_0 * 448 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(57344):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 448)
                                    v1 = T.axis.spatial(896, i4_0 * 448 + ax0_ax1_ax2_ax3_fused % 448)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(112, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    rc = T.axis.reduce(896, i4_0 * 448 + i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 896, 7, 7], "float32"], ["TENSOR", [128, 896, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 2, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 112, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #8: "fused_multiply_add_multiply_add_nn_relu_4"
[14:43:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 864, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 864, 1, 1), "float32"], T_relu: T.Buffer[(1, 864, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 864, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 864, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 864, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 864, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 864, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 864, 1, 1), "float32"], T_relu: T.Buffer[(1, 864, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:43:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #9: "fused_nn_conv2d_add_nn_relu_4"
[14:43:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 864, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 864, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 864, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 864, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 864, 7, 7], "float32"], ["TENSOR", [128, 864, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 864, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 864, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 864, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 864, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(18, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(2352):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(864, i4_0 * 48 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(384):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 8 + ax0_ax1_ax2_ax3_fused // 48)
                                    v1 = T.axis.spatial(864, i4_0 * 48 + ax0_ax1_ax2_ax3_fused % 48)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(12, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 1, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_3)
                                    yy = T.axis.spatial(7, i2_4)
                                    xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    rc = T.axis.reduce(864, i4_0 * 48 + i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 864, 7, 7], "float32"], ["TENSOR", [128, 864, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + ax1)
                                v2 = T.axis.spatial(7, ax2)
                                v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 2, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[18, 12, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #10: "fused_multiply_add_multiply_add_nn_relu_5"
[14:43:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 832, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 832, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 832, 1, 1), "float32"], T_relu: T.Buffer[(1, 832, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 832, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 832, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 832, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 832, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 832, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 832, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 832, 1, 1), "float32"], T_relu: T.Buffer[(1, 832, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:43:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #11: "fused_nn_conv2d_add_nn_relu_5"
[14:43:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 832, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 832, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 7, 7], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 832, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 832, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(416, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(98):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(832, i4_0 * 2 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(16):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 8 + ax0_ax1_ax2_ax3_fused // 2)
                                    v1 = T.axis.spatial(832, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_3)
                                    yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    rc = T.axis.reduce(832, i4_0 * 2 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 7, 7], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + ax1)
                                v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused // 7 + ax2)
                                v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 2, 1, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[416, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #12: "fused_multiply_add_multiply_add_nn_relu_6"
[14:43:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 800, 1, 1), "float32"], T_relu: T.Buffer[(1, 800, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 800, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 800, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 800, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 800, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 800, 1, 1), "float32"], T_relu: T.Buffer[(1, 800, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:43:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #13: "fused_nn_conv2d_add_nn_relu_6"
[14:43:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 800, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 800, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 800, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 800, 7, 7], "float32"], ["TENSOR", [128, 800, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 800, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 800, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 800, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(39200):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(800, ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(102400):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 800)
                                    v1 = T.axis.spatial(800, ax0_ax1_ax2_ax3_fused % 800)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 8, 7, 1, 50, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3)
                                    yy = T.axis.spatial(7, i2_3)
                                    xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    rc = T.axis.reduce(800, i4_1 * 50 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 800, 7, 7], "float32"], ["TENSOR", [128, 800, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + ax1)
                                v2 = T.axis.spatial(7, ax2)
                                v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 16, 50])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #14: "fused_multiply_add_multiply_add_nn_relu_7"
[14:43:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 768, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 768, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 768, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 768, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 768, 1, 1), "float32"], T_relu: T.Buffer[(1, 768, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 768, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 768, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 768, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 768, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 768, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 768, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 768, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 768, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 768, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 768, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 768, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 768, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 768, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 768, 1, 1), "float32"], T_relu: T.Buffer[(1, 768, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 768, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:43:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #15: "fused_nn_conv2d_add_nn_relu_7"
[14:43:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 768, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 768, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 768, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 768, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 768, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 768, 7, 7], "float32"], ["TENSOR", [128, 768, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 768, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 768, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 768, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 768, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(256, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(147):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(768, i4_0 * 3 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(192):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(768, i4_0 * 3 + ax0_ax1_ax2_ax3_fused % 3)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 7, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                    yy, xx = T.axis.remap("SS", [i2_4, i3_4])
                                    rc = T.axis.reduce(768, i4_0 * 3 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 768, 7, 7], "float32"], ["TENSOR", [128, 768, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                                v2, v3 = T.axis.remap("SS", [ax2, ax3])
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 2, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #16: "fused_multiply_add_multiply_add_nn_relu_8"
[14:43:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 736, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 736, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 736, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 736, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 736, 1, 1), "float32"], T_relu: T.Buffer[(1, 736, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 736, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 736, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 736, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 736, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 736, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 736, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 736, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 736, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 736, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 736, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 736, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 736, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 736, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 736, 1, 1), "float32"], T_relu: T.Buffer[(1, 736, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 736, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:43:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #17: "fused_nn_conv2d_add_nn_relu_8"
[14:43:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 736, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 736, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 736, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 736, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 736, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 736, 7, 7], "float32"], ["TENSOR", [128, 736, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 736, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 736, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 736, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 736, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(16, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(92, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(56):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(736, i4_0 * 8 + ax0_ax1_ax2_ax3_fused // 7)
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(256):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + ax0_ax1_ax2_ax3_fused // 8)
                                    v1 = T.axis.spatial(736, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                    yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                                    rc = T.axis.reduce(736, i4_0 * 8 + i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 736, 7, 7], "float32"], ["TENSOR", [128, 736, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                                v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[92, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #18: "fused_multiply_add_multiply_add_nn_relu_9"
[14:43:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 704, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 704, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 704, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 704, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 704, 1, 1), "float32"], T_relu: T.Buffer[(1, 704, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 704, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 704, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 704, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 704, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 704, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 704, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 704, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 704, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 704, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 704, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 704, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 704, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 704, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 704, 1, 1), "float32"], T_relu: T.Buffer[(1, 704, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 704, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:43:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #19: "fused_nn_conv2d_add_multiply_add_nn_relu"
[14:43:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 704, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 704, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 704, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_multiply = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 704, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 704, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 704, 7, 7], "float32"], ["TENSOR", [128, 704, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 704, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 704, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 704, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 704, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(7, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(352, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(14):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(704, i4_0 * 2 + ax0_ax1_ax2_ax3_fused // 7)
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(32):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + ax0_ax1_ax2_ax3_fused // 2)
                                    v1 = T.axis.spatial(704, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 1, 1, 2, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i1_3)
                                    yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused)
                                    rc = T.axis.reduce(704, i4_0 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 704, 7, 7], "float32"], ["TENSOR", [128, 704, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + ax1)
                                v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0], placeholder_4[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max((conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * placeholder_3[v0, v1, 0, 0] + placeholder_4[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_multiply", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l19, l20, l21, l22, l23 = sch.split(loop=l7, factors=[v14, v15, v16, v17, v18])
v24, v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[8, 2, 1, 8, 1])
l29, l30, l31, l32, l33 = sch.split(loop=l8, factors=[v24, v25, v26, v27, v28])
v34, v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l39, l40, l41, l42, l43 = sch.split(loop=l9, factors=[v34, v35, v36, v37, v38])
v44, v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l10, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l49, l50, l51, l52, l53 = sch.split(loop=l10, factors=[v44, v45, v46, v47, v48])
v54, v55, v56 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[352, 1, 2])
l57, l58, l59 = sch.split(loop=l11, factors=[v54, v55, v56])
v60, v61, v62 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l63, l64, l65 = sch.split(loop=l12, factors=[v60, v61, v62])
v66, v67, v68 = sch.sample_perfect_tile(loop=l13, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l69, l70, l71 = sch.split(loop=l13, factors=[v66, v67, v68])
sch.reorder(l19, l29, l39, l49, l20, l30, l40, l50, l21, l31, l41, l51, l57, l63, l69, l58, l64, l70, l22, l32, l42, l52, l59, l65, l71, l23, l33, l43, l53)
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="vthread.x")
l74 = sch.fuse(l21, l31, l41, l51)
sch.bind(loop=l74, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b75 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b75, loop=l74, preserve_unit_loops=True)
b76 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b76, loop=l69, preserve_unit_loops=True)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b76)
l87 = sch.fuse(l83, l84, l85, l86)
v88 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch", ann_val=v88)
b89 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b89, loop=l69, preserve_unit_loops=True)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b89)
l100 = sch.fuse(l96, l97, l98, l99)
v101 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
[14:43:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #20: "fused_multiply_add_multiply_add_nn_relu_10"
[14:43:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 672, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 672, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 672, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 672, 1, 1), "float32"], T_relu: T.Buffer[(1, 672, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 672, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 672, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 672, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 672, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 672, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 672, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 672, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 672, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 672, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 672, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 672, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 672, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 672, 1, 1), "float32"], T_relu: T.Buffer[(1, 672, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 672, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:43:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #21: "fused_nn_conv2d_add_nn_relu_9"
[14:43:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 672, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 672, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 672, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 672, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 672, 7, 7], "float32"], ["TENSOR", [128, 672, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 672, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 672, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 672, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(6, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(784):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(672, i4_0 * 112 + ax0_ax1_ax2_ax3_fused // 7)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(7168):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + ax0_ax1_ax2_ax3_fused // 112)
                                    v1 = T.axis.spatial(672, i4_0 * 112 + ax0_ax1_ax2_ax3_fused % 112)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 8, 1, 1, 56, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3)
                                    yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                                    xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    rc = T.axis.reduce(672, i4_0 * 112 + i4_1 * 56 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 672, 7, 7], "float32"], ["TENSOR", [128, 672, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax2)
                                v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 8, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[6, 2, 56])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #22: "fused_multiply_add_multiply_add_nn_relu_11"
[14:43:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 640, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 640, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 640, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 640, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 640, 1, 1), "float32"], T_relu: T.Buffer[(1, 640, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 640, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 640, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 640, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 640, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 640, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 640, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 640, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 640, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 640, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 640, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 640, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 640, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 640, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 640, 1, 1), "float32"], T_relu: T.Buffer[(1, 640, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 640, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:43:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #23: "fused_nn_conv2d_add_nn_relu_10"
[14:43:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 640, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 640, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 640, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 640, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 640, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 640, 7, 7], "float32"], ["TENSOR", [128, 640, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 640, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 640, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 640, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 640, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1120):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(640, i4_0 * 160 + ax0_ax1_ax2_ax3_fused // 7)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(2560):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + ax0_ax1_ax2_ax3_fused // 160)
                                    v1 = T.axis.spatial(640, i4_0 * 160 + ax0_ax1_ax2_ax3_fused % 160)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 1, 40, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    rc = T.axis.reduce(640, i4_0 * 160 + i4_1 * 40 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 640, 7, 7], "float32"], ["TENSOR", [128, 640, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                                v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 4, 40])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #24: "fused_multiply_add_multiply_add_nn_relu_12"
[14:43:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 608, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 608, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 608, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 608, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 608, 1, 1), "float32"], T_relu: T.Buffer[(1, 608, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 608, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 608, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 608, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 608, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 608, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 608, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 608, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 608, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 608, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 608, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 608, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 608, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 608, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 608, 1, 1), "float32"], T_relu: T.Buffer[(1, 608, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 608, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:43:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #25: "fused_nn_conv2d_add_nn_relu_11"
[14:43:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 608, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 608, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 608, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 608, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 608, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 608, 7, 7], "float32"], ["TENSOR", [128, 608, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 608, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 608, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 608, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 608, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3724):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(608, i4_0 * 76 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(9728):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 76)
                                    v1 = T.axis.spatial(608, i4_0 * 76 + ax0_ax1_ax2_ax3_fused % 76)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 38, 1, 1, 1, 2, 1, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    xx = T.axis.spatial(7, i3_4)
                                    rc = T.axis.reduce(608, i4_0 * 76 + i4_1 * 38 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 608, 7, 7], "float32"], ["TENSOR", [128, 608, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 38])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #26: "fused_multiply_add_multiply_add_nn_relu_13"
[14:43:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 576, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 576, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 576, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 576, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 576, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 576, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 576, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 576, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 576, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 576, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 576, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 576, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:43:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #27: "fused_nn_conv2d_add_nn_relu_12"
[14:43:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 576, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 576, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 576, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 7, 7], "float32"], ["TENSOR", [128, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 576, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 576, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3528):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(576, i4_0 * 72 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(9216):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 72)
                                    v1 = T.axis.spatial(576, i4_0 * 72 + ax0_ax1_ax2_ax3_fused % 72)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(9, 1, 1, 1, 2, 1, 1, 8, 1, 1, 1, 1, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3)
                                    yy = T.axis.spatial(7, i2_4)
                                    xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    rc = T.axis.reduce(576, i4_0 * 72 + i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 7, 7], "float32"], ["TENSOR", [128, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                                v2 = T.axis.spatial(7, ax2)
                                v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 9, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #28: "fused_multiply_add_nn_relu"
[14:43:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:43:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #29: "fused_nn_conv2d_add_nn_relu_13"
[14:43:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 544, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 544, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [128, 544, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 544, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 544, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(68, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(56):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(544, i4_0 * 8 + ax0_ax1_ax2_ax3_fused // 7)
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(64):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + ax0_ax1_ax2_ax3_fused // 8)
                                    v1 = T.axis.spatial(544, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4)
                                    yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                                    rc = T.axis.reduce(544, i4_0 * 8 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [128, 544, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                                v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 2, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[68, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #30: "fused_multiply_add_nn_relu_1"
[14:43:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:43:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #31: "fused_nn_conv2d_add_nn_relu_14"
[14:43:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 512, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [128, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:43:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:43:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 512, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(28):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 4 + ax0_ax1_ax2_ax3_fused // 7)
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(64):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + ax0_ax1_ax2_ax3_fused // 4)
                                    v1 = T.axis.spatial(512, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 7, 4, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    xx = T.axis.spatial(7, i3_3)
                                    rc = T.axis.reduce(512, i4_0 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 7, 7], "float32"], ["TENSOR", [128, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + ax1)
                                v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 4, 1, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:43:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #32: "fused_nn_conv2d"
[14:43:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7), "float32"], placeholder_1: T.Buffer[(32, 128, 3, 3), "float32"], conv2d_nchw: T.Buffer[(1, 32, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 9, 9], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 9, 9):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 7, 7, 128, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 7, 7], "float32"], ["TENSOR", [32, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
    

[14:44:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 7, 7), "float32"], placeholder_1: T.Buffer[(32, 128, 3, 3), "float32"], conv2d_nchw: T.Buffer[(1, 32, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 9, 9], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 128, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1728):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 64 + ax0_ax1_ax2_ax3_fused % 1728 // 27)
                                    v2 = T.axis.spatial(9, ax0_ax1_ax2_ax3_fused % 27 // 3)
                                    v3 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1152):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + ax0_ax1_ax2_ax3_fused // 576)
                                    v1 = T.axis.spatial(128, i4_0 * 64 + ax0_ax1_ax2_ax3_fused % 576 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 3, 1, 1, 1, 7, 1, 8, 1, 3, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused)
                                    yy = T.axis.spatial(7, i2_3)
                                    xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    rc = T.axis.reduce(128, i4_0 * 64 + i4_1 * 8 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 7, 7], "float32"], ["TENSOR", [32, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                                v2 = T.axis.spatial(7, ax2)
                                v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                                T.writes(conv2d_nchw[v0, v1, v2, v3])
                                conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[16, 1, 2, 1, 1])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[2, 8, 8])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58])
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
sch.compute_inline(block=b0)
v98 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v98)
[14:44:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #33: "fused_multiply_add_multiply_add_nn_relu_14"
[14:44:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 992, 1, 1), "float32"], T_relu: T.Buffer[(1, 992, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 992, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 992, 14, 14], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 992, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 992, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 992, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 992, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 992, 14, 14):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 992, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 992, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 992, 1, 1), "float32"], T_relu: T.Buffer[(1, 992, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 992, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:44:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #34: "fused_nn_conv2d_add_nn_relu_15"
[14:44:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 992, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 992, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 992, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 992, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 992, 14, 14], "float32"], ["TENSOR", [128, 992, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 992, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 992, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 992, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(28, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(48608):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(992, i4_0 * 248 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(3968):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + ax0_ax1_ax2_ax3_fused // 248)
                                    v1 = T.axis.spatial(992, i4_0 * 248 + ax0_ax1_ax2_ax3_fused % 248)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 2, 1, 7, 31, 1, 1, 1, 4, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                                    xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i3_3)
                                    rc = T.axis.reduce(992, i4_0 * 248 + i4_1 * 31 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 992, 14, 14], "float32"], ["TENSOR", [128, 992, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 + ax2)
                                v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 2, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 8, 31])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #35: "fused_multiply_add_multiply_add_nn_relu_15"
[14:44:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 960, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 960, 14, 14], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 960, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 960, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 960, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 14, 14):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 960, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:44:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #36: "fused_nn_conv2d_add_nn_relu_16"
[14:44:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 960, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 960, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 960, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 14, 14], "float32"], ["TENSOR", [128, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 960, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 960, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(80, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(2352):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(960, i4_0 * 12 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1536):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 12)
                                    v1 = T.axis.spatial(960, i4_0 * 12 + ax0_ax1_ax2_ax3_fused % 12)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 8, 1, 1, 3, 1, 1, 1, 4, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 14 * 32 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused * 7 + i2_4)
                                    xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14)
                                    rc = T.axis.reduce(960, i4_0 * 12 + i4_1 * 3 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 14, 14], "float32"], ["TENSOR", [128, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 14 * 32 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused * 7 + ax2)
                                v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 8, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[80, 4, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #37: "fused_multiply_add_multiply_add_nn_relu_16"
[14:44:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 928, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 928, 1, 1), "float32"], T_relu: T.Buffer[(1, 928, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 928, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 928, 14, 14], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 928, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 928, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 928, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 928, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 928, 14, 14):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 928, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 928, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 928, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 928, 1, 1), "float32"], T_relu: T.Buffer[(1, 928, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 928, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:44:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #38: "fused_nn_conv2d_add_nn_relu_17"
[14:44:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 928, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 928, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 928, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 928, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 928, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 928, 14, 14], "float32"], ["TENSOR", [128, 928, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 928, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 928, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 928, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 928, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(90944):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(928, ax0_ax1_ax2_ax3_fused // 98)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 98 // 7)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(118784):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 928)
                                    v1 = T.axis.spatial(928, ax0_ax1_ax2_ax3_fused % 928)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(464, 1, 1, 1, 32, 2, 1, 2, 1, 1, 1, 1, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i1_3)
                                    yy = T.axis.spatial(14, i2_3 * 7 + i2_4)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    rc = T.axis.reduce(928, i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 928, 14, 14], "float32"], ["TENSOR", [128, 928, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 14, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + ax1)
                                v2 = T.axis.spatial(14, ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 32, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 464, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #39: "fused_multiply_add_nn_relu_2"
[14:44:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 896, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 896, 1, 1), "float32"], T_relu: T.Buffer[(1, 896, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 896, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 896, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 896, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 896, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 896, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 896, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 896, 1, 1), "float32"], T_relu: T.Buffer[(1, 896, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 896, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:44:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #40: "fused_nn_conv2d_add_nn_relu_18"
[14:44:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 896, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 896, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 896, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 896, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 896, 14, 14], "float32"], ["TENSOR", [128, 896, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 896, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 896, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 896, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(5488):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(896, i4_0 * 28 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(3584):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 28)
                                    v1 = T.axis.spatial(896, i4_0 * 28 + ax0_ax1_ax2_ax3_fused % 28)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(14, 1, 1, 1, 32, 14, 1, 2, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i1_3)
                                    yy, xx = T.axis.remap("SS", [i2_3, i0_2_i1_2_i2_2_i3_2_fused])
                                    rc = T.axis.reduce(896, i4_0 * 28 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 896, 14, 14], "float32"], ["TENSOR", [128, 896, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 14, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + ax1)
                                v2 = T.axis.spatial(14, ax2)
                                v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 32, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 14, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 14, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #41: "fused_multiply_add_multiply_add_nn_relu_17"
[14:44:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 864, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 864, 1, 1), "float32"], T_relu: T.Buffer[(1, 864, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 864, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 864, 14, 14], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 864, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 864, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 864, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 864, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 864, 14, 14):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 864, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 864, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 864, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 864, 1, 1), "float32"], T_relu: T.Buffer[(1, 864, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 864, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:44:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #42: "fused_nn_conv2d_add_nn_relu_19"
[14:44:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 864, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 864, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 864, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 864, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 864, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 864, 14, 14], "float32"], ["TENSOR", [128, 864, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 864, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 864, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 864, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 864, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(256, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(54, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3136):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(864, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(2048):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(864, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 7, 1, 4, 1, 1, 1, 1, 1, 14):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2)
                                    yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i2_3)
                                    xx = T.axis.spatial(14, i3_4)
                                    rc = T.axis.reduce(864, i4_0 * 16 + i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 864, 14, 14], "float32"], ["TENSOR", [128, 864, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 14):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax2)
                                v3 = T.axis.spatial(14, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 128, 1, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[54, 4, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #43: "fused_multiply_add_nn_relu_3"
[14:44:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 832, 1, 1), "float32"], T_relu: T.Buffer[(1, 832, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 832, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 832, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 832, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 832, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 832, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 832, 1, 1), "float32"], T_relu: T.Buffer[(1, 832, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 832, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:44:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #44: "fused_nn_conv2d_add_nn_relu_20"
[14:44:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 832, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 832, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 832, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 14, 14], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 832, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 832, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(16, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(81536):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(832, i4_0 * 416 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(53248):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 416)
                                    v1 = T.axis.spatial(832, i4_0 * 416 + ax0_ax1_ax2_ax3_fused % 416)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(416, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i2_4)
                                    xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i3_3)
                                    rc = T.axis.reduce(832, i4_0 * 416 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 14, 14], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax2)
                                v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 416, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #45: "fused_multiply_add_multiply_add_nn_relu_18"
[14:44:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 800, 1, 1), "float32"], T_relu: T.Buffer[(1, 800, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 800, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 800, 14, 14], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 800, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 800, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 800, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 800, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 800, 14, 14):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 800, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 800, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 800, 1, 1), "float32"], T_relu: T.Buffer[(1, 800, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 800, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:44:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #46: "fused_nn_conv2d_add_nn_relu_21"
[14:44:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 800, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 800, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 800, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 800, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 800, 14, 14], "float32"], ["TENSOR", [128, 800, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 800, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 800, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 800, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(156800):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(800, ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(25600):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 32 + ax0_ax1_ax2_ax3_fused // 800)
                                    v1 = T.axis.spatial(800, ax0_ax1_ax2_ax3_fused % 800)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 1, 2, 1, 50, 1, 1, 1, 1, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    yy = T.axis.spatial(14, i2_3 * 7 + i2_4)
                                    xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    rc = T.axis.reduce(800, i4_1 * 50 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 800, 14, 14], "float32"], ["TENSOR", [128, 800, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                                v2 = T.axis.spatial(14, ax2)
                                v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 16, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 16, 50])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #47: "fused_multiply_add_nn_relu_4"
[14:44:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 768, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 768, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 768, 1, 1), "float32"], T_relu: T.Buffer[(1, 768, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 768, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 768, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 768, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 768, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 768, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 768, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 768, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 768, 1, 1), "float32"], T_relu: T.Buffer[(1, 768, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 768, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:44:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #48: "fused_nn_conv2d_add_nn_relu_22"
[14:44:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 768, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 768, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 768, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 768, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 768, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 768, 14, 14], "float32"], ["TENSOR", [128, 768, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 768, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 768, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 768, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 768, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(49, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(256, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(48, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1568):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(768, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 98)
                                    v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + ax0_ax1_ax2_ax3_fused % 98 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(2048):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(768, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_1_i1_1_i2_1_i3_1_fused // 7)
                                    xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    rc = T.axis.reduce(768, i4_0 * 16 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 768, 14, 14], "float32"], ["TENSOR", [128, 768, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                                v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_1_i1_1_i2_1_i3_1_fused // 7 + ax2)
                                v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 128, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[48, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #49: "fused_multiply_add_multiply_add_nn_relu_19"
[14:44:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 736, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 736, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 736, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 736, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 736, 1, 1), "float32"], T_relu: T.Buffer[(1, 736, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 736, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 736, 14, 14], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 736, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 736, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 736, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 736, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 736, 14, 14):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 736, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 736, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 736, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 736, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 736, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 736, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 736, 1, 1), "float32"], T_relu: T.Buffer[(1, 736, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 736, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:44:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #50: "fused_nn_conv2d_add_nn_relu_23"
[14:44:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 736, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 736, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 736, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 736, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 736, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 736, 14, 14], "float32"], ["TENSOR", [128, 736, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 736, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 736, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 736, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 736, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(46, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1568):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(736, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 98)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 98 // 7)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1024):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(736, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 64, 1, 1, 2, 1, 1, 1, 1, 1, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i1_3)
                                    yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i3_4)
                                    rc = T.axis.reduce(736, i4_0 * 16 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 736, 14, 14], "float32"], ["TENSOR", [128, 736, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 1, 64, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[46, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #51: "fused_multiply_add_multiply_add_nn_relu_20"
[14:44:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 704, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 704, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 704, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 704, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 704, 1, 1), "float32"], T_relu: T.Buffer[(1, 704, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 704, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 704, 14, 14], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 704, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 704, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 704, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 704, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 704, 14, 14):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 704, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 704, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 704, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 704, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 704, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 704, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 704, 1, 1), "float32"], T_relu: T.Buffer[(1, 704, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 704, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:44:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #52: "fused_nn_conv2d_add_nn_relu_24"
[14:44:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 704, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 704, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 704, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 704, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 704, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 704, 14, 14], "float32"], ["TENSOR", [128, 704, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 704, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 704, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 704, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 704, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(128, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(28, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(88, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(784):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(704, i4_0 * 8 + ax0_ax1_ax2_ax3_fused // 98)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 98 // 7)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(16):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + ax0_ax1_ax2_ax3_fused // 8)
                                    v1 = T.axis.spatial(704, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 7, 4, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                    yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i3_3)
                                    rc = T.axis.reduce(704, i4_0 * 8 + i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 704, 14, 14], "float32"], ["TENSOR", [128, 704, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[64, 1, 2, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[88, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #53: "fused_multiply_add_nn_relu_5"
[14:44:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 672, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 672, 1, 1), "float32"], T_relu: T.Buffer[(1, 672, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 672, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 672, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 672, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 672, 1, 1), "float32"], T_relu: T.Buffer[(1, 672, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:44:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #54: "fused_nn_conv2d_add_nn_relu_25"
[14:44:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 672, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 672, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 672, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 672, 14, 14], "float32"], ["TENSOR", [128, 672, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 672, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 672, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 672, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(448, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(336, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(196):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(672, i4_0 * 2 + ax0_ax1_ax2_ax3_fused // 98)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 98 // 7)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(256):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 2)
                                    v1 = T.axis.spatial(672, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 4, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 14 * 4 + i1_4)
                                    yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + i2_4)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    rc = T.axis.reduce(672, i4_0 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 672, 14, 14], "float32"], ["TENSOR", [128, 672, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 14 * 4 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 32, 1, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[336, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #55: "fused_multiply_add_multiply_add_nn_relu_21"
[14:44:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 640, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 640, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 640, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 640, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 640, 1, 1), "float32"], T_relu: T.Buffer[(1, 640, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 640, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 640, 14, 14], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 640, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 640, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 640, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 640, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 640, 14, 14):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 640, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 640, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 640, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 640, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 640, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 640, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 640, 1, 1), "float32"], T_relu: T.Buffer[(1, 640, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 640, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:44:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #56: "fused_nn_conv2d_add_nn_relu_26"
[14:44:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 640, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 640, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 640, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 640, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 640, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 640, 14, 14], "float32"], ["TENSOR", [128, 640, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 640, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 640, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 640, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 640, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1120):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(640, i4_0 * 80 + ax0_ax1_ax2_ax3_fused // 14)
                                    v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 7 * 7 + ax0_ax1_ax2_ax3_fused % 14 // 2)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(10240):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 80)
                                    v1 = T.axis.spatial(640, i4_0 * 80 + ax0_ax1_ax2_ax3_fused % 80)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 32, 7, 1, 5, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i1_3)
                                    yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 7 * 7 + i2_3)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                    rc = T.axis.reduce(640, i4_0 * 80 + i4_1 * 5 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 640, 14, 14], "float32"], ["TENSOR", [128, 640, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + ax1)
                                v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 7 * 7 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 32, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 16, 5])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #57: "fused_multiply_add_multiply_add_nn_relu_22"
[14:44:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 608, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 608, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 608, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 608, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 608, 1, 1), "float32"], T_relu: T.Buffer[(1, 608, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 608, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 608, 14, 14], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 608, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 608, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 608, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 608, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 608, 14, 14):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 608, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 608, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 608, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 608, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 608, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 608, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 608, 1, 1), "float32"], T_relu: T.Buffer[(1, 608, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 608, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:44:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #58: "fused_nn_conv2d_add_nn_relu_27"
[14:44:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 608, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 608, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 608, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 608, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 608, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 608, 14, 14], "float32"], ["TENSOR", [128, 608, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 608, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 608, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 608, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 608, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(17024):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(608, ax0_ax1_ax2_ax3_fused // 28)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 28 // 2)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(77824):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 608)
                                    v1 = T.axis.spatial(608, ax0_ax1_ax2_ax3_fused % 608)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(608, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 4, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + i3_3)
                                    rc = T.axis.reduce(608, i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 608, 14, 14], "float32"], ["TENSOR", [128, 608, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 4, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 608, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #59: "fused_multiply_add_nn_relu_6"
[14:44:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 576, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 576, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 576, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 576, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 576, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 576, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:44:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #60: "fused_nn_conv2d_add_nn_relu_28"
[14:44:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 576, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 576, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 576, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [128, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 576, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(28224):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(576, i4_0 * 144 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(9216):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + ax0_ax1_ax2_ax3_fused // 144)
                                    v1 = T.axis.spatial(576, i4_0 * 144 + ax0_ax1_ax2_ax3_fused % 144)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(18, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 14, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                    yy = T.axis.spatial(14, i2_4)
                                    xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                                    rc = T.axis.reduce(576, i4_0 * 144 + i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [128, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                                v2 = T.axis.spatial(14, ax2)
                                v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 18, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #61: "fused_multiply_add_multiply_add_nn_relu_23"
[14:44:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 544, 1, 1), "float32"], T_relu: T.Buffer[(1, 544, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 544, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 544, 14, 14], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 544, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 544, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 544, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 544, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 544, 14, 14):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 544, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 544, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 544, 1, 1), "float32"], T_relu: T.Buffer[(1, 544, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 544, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:44:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #62: "fused_nn_conv2d_add_nn_relu_29"
[14:44:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 544, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 544, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 544, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 544, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 544, 14, 14], "float32"], ["TENSOR", [128, 544, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 544, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 544, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 544, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(15232):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(544, ax0_ax1_ax2_ax3_fused // 28)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 28 // 2)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(34816):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + ax0_ax1_ax2_ax3_fused // 544)
                                    v1 = T.axis.spatial(544, ax0_ax1_ax2_ax3_fused % 544)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(272, 1, 1, 1, 4, 7, 1, 2, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i2_3)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    rc = T.axis.reduce(544, i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 544, 14, 14], "float32"], ["TENSOR", [128, 544, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 4, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 272, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #63: "fused_multiply_add_nn_relu_7"
[14:44:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:44:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #64: "fused_nn_conv2d_add_nn_relu_30"
[14:44:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 512, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [128, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 512, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 512, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(6272):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 64 + ax0_ax1_ax2_ax3_fused // 98)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 98 // 7)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(8192):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 64)
                                    v1 = T.axis.spatial(512, i4_0 * 64 + ax0_ax1_ax2_ax3_fused % 64)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 1, 1, 1, 16, 14, 1, 2, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(14, i2_3)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    rc = T.axis.reduce(512, i4_0 * 64 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 14, 14], "float32"], ["TENSOR", [128, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 14, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + ax1)
                                v2 = T.axis.spatial(14, ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 16, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 14, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 32, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #65: "fused_multiply_add_nn_relu_8"
[14:44:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 480, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 480, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 480, 1, 1), "float32"], T_relu: T.Buffer[(1, 480, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 480, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 480, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 480, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 480, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 480, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 480, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 480, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 480, 1, 1), "float32"], T_relu: T.Buffer[(1, 480, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 480, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:44:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #66: "fused_nn_conv2d_add_nn_relu_31"
[14:44:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 480, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 480, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 480, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 480, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 480, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 480, 14, 14], "float32"], ["TENSOR", [128, 480, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 480, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 480, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 480, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 480, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(20, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(336):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(480, i4_0 * 24 + ax0_ax1_ax2_ax3_fused // 14)
                                    v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + ax0_ax1_ax2_ax3_fused % 14 // 7)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(3072):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 24)
                                    v1 = T.axis.spatial(480, i4_0 * 24 + ax0_ax1_ax2_ax3_fused % 24)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                    yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i0_1_i1_1_i2_1_i3_1_fused // 7)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    rc = T.axis.reduce(480, i4_0 * 24 + i4_1 * 6 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 480, 14, 14], "float32"], ["TENSOR", [128, 480, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                                v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i0_1_i1_1_i2_1_i3_1_fused // 7 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 64, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[20, 4, 6])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #67: "fused_multiply_add_multiply_add_nn_relu_24"
[14:44:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 448, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 448, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 448, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 448, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 448, 1, 1), "float32"], T_relu: T.Buffer[(1, 448, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 448, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 448, 14, 14], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 448, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 448, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 448, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 448, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 448, 14, 14):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 448, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 448, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 448, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 448, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 448, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 448, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 448, 1, 1), "float32"], T_relu: T.Buffer[(1, 448, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 448, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:44:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #68: "fused_nn_conv2d_add_nn_relu_32"
[14:44:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 448, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 448, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 448, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 448, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 448, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 448, 14, 14], "float32"], ["TENSOR", [128, 448, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 448, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 448, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 448, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 448, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(5488):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(448, i4_0 * 28 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(3584):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 28)
                                    v1 = T.axis.spatial(448, i4_0 * 28 + ax0_ax1_ax2_ax3_fused % 28)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(28, 1, 1, 1, 32, 1, 1, 1, 1, 1, 1, 2, 2, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 64 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i2_4)
                                    xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                    rc = T.axis.reduce(448, i4_0 * 28 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 448, 14, 14], "float32"], ["TENSOR", [128, 448, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 2, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 64 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax2)
                                v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 32, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 28, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #69: "fused_multiply_add_nn_relu_9"
[14:44:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 416, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 416, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 416, 1, 1), "float32"], T_relu: T.Buffer[(1, 416, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 416, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 416, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 416, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 416, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 416, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 416, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 416, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 416, 1, 1), "float32"], T_relu: T.Buffer[(1, 416, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 416, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:44:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #70: "fused_nn_conv2d_add_nn_relu_33"
[14:44:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 416, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 416, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 416, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 416, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 416, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 416, 14, 14], "float32"], ["TENSOR", [128, 416, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 416, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 416, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 416, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 416, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(5096):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(416, i4_0 * 52 + ax0_ax1_ax2_ax3_fused // 98)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 98 // 7)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(6656):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 52)
                                    v1 = T.axis.spatial(416, i4_0 * 52 + ax0_ax1_ax2_ax3_fused % 52)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 1, 26, 1, 1, 1, 64, 1, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i1_3 * 64 + i1_4)
                                    yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + i3_4)
                                    rc = T.axis.reduce(416, i4_0 * 52 + i4_1 * 26 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 416, 14, 14], "float32"], ["TENSOR", [128, 416, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 128, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 64])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 26])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #71: "fused_multiply_add_multiply_add_nn_relu_25"
[14:44:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 384, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 384, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 384, 14, 14], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 384, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 384, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 384, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 384, 14, 14):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 384, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 384, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 384, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 384, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:44:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #72: "fused_nn_conv2d_add_nn_relu_34"
[14:44:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 384, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 384, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 14, 14], "float32"], ["TENSOR", [128, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 384, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 384, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(28, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1176):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i4_0 * 24 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 7 + ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(768):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + ax0_ax1_ax2_ax3_fused // 24)
                                    v1 = T.axis.spatial(384, i4_0 * 24 + ax0_ax1_ax2_ax3_fused % 24)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4)
                                    yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    rc = T.axis.reduce(384, i4_0 * 24 + i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 14, 14], "float32"], ["TENSOR", [128, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                                v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 4, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 6, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #73: "fused_multiply_add_nn_relu_10"
[14:44:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 352, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 352, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 352, 1, 1), "float32"], T_relu: T.Buffer[(1, 352, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 352, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 352, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 352, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 352, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 352, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 352, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 352, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 352, 1, 1), "float32"], T_relu: T.Buffer[(1, 352, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 352, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:44:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #74: "fused_nn_conv2d_add_nn_relu_35"
[14:44:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 352, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 352, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 352, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 352, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 352, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 352, 14, 14], "float32"], ["TENSOR", [128, 352, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 352, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 352, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 352, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 352, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1078):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(352, i4_0 * 11 + ax0_ax1_ax2_ax3_fused // 98)
                                    v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax0_ax1_ax2_ax3_fused % 98 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(352):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + ax0_ax1_ax2_ax3_fused // 11)
                                    v1 = T.axis.spatial(352, i4_0 * 11 + ax0_ax1_ax2_ax3_fused % 11)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 7, 11, 1, 1, 1, 2, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i2_4)
                                    xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused * 7 + i3_3)
                                    rc = T.axis.reduce(352, i4_0 * 11 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 352, 14, 14], "float32"], ["TENSOR", [128, 352, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 8 + ax1)
                                v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax2)
                                v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 1, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 11])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #75: "fused_multiply_add_nn_relu_11"
[14:44:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 320, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1), "float32"], T_relu: T.Buffer[(1, 320, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 320, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 320, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 320, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 320, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 320, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 320, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1), "float32"], T_relu: T.Buffer[(1, 320, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 320, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:44:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #76: "fused_nn_conv2d_add_nn_relu_36"
[14:44:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 320, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 320, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 320, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 320, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 320, 14, 14], "float32"], ["TENSOR", [128, 320, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:44:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:44:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 320, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 320, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 320, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(64, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(7, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(4480):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(320, i4_0 * 160 + ax0_ax1_ax2_ax3_fused // 28)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 28 // 2)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(20480):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 160)
                                    v1 = T.axis.spatial(320, i4_0 * 160 + ax0_ax1_ax2_ax3_fused % 160)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 2, 40, 1, 1, 1, 4, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i1_4)
                                    yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + i3_3)
                                    rc = T.axis.reduce(320, i4_0 * 160 + i4_1 * 40 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 320, 14, 14], "float32"], ["TENSOR", [128, 320, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 32, 1, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 40])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:44:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #77: "fused_multiply_add_nn_relu_12"
[14:44:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 288, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 288, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 288, 1, 1), "float32"], T_relu: T.Buffer[(1, 288, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 288, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 288, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 288, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 288, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 288, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 288, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 288, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 288, 1, 1), "float32"], T_relu: T.Buffer[(1, 288, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 288, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:45:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #78: "fused_nn_conv2d_add_nn_relu_37"
[14:45:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 288, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 288, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 288, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 288, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 288, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 288, 14, 14], "float32"], ["TENSOR", [128, 288, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 288, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 288, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 288, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 288, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(252):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(288, i4_0 * 18 + ax0_ax1_ax2_ax3_fused // 14)
                                    v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 2 + ax0_ax1_ax2_ax3_fused % 14 // 7)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(576):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + ax0_ax1_ax2_ax3_fused // 18)
                                    v1 = T.axis.spatial(288, i4_0 * 18 + ax0_ax1_ax2_ax3_fused % 18)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(18, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 8, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i1_4)
                                    yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i3_3)
                                    rc = T.axis.reduce(288, i4_0 * 18 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 288, 14, 14], "float32"], ["TENSOR", [128, 288, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + ax1)
                                v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 1, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 18, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #79: "fused_multiply_add_nn_relu_13"
[14:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #80: "fused_nn_conv2d_add_nn_relu_38"
[14:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 14, 14, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [128, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 256, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(256, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(98):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused // 7)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(16):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + ax0_ax1_ax2_ax3_fused)
                                    v1 = T.axis.spatial(256, i4_0)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 14, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3)
                                    yy = T.axis.spatial(14, i2_4)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    rc = T.axis.reduce(256, i4_0)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 14, 14], "float32"], ["TENSOR", [128, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 14, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                                v2 = T.axis.spatial(14, ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #81: "fused_nn_conv2d_1"
[14:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14), "float32"], placeholder_1: T.Buffer[(32, 128, 3, 3), "float32"], conv2d_nchw: T.Buffer[(1, 32, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 16, 16], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 16, 16):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 14, 14, 128, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 14, 14], "float32"], ["TENSOR", [32, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
    

[14:45:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 14, 14), "float32"], placeholder_1: T.Buffer[(32, 128, 3, 3), "float32"], conv2d_nchw: T.Buffer[(1, 32, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 16, 16], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 128, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(14336):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 64 + ax0_ax1_ax2_ax3_fused % 14336 // 224)
                                    v2 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused % 224 // 14)
                                    v3 = T.axis.spatial(16, i6_0 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1536):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 8 + ax0_ax1_ax2_ax3_fused // 192)
                                    v1 = T.axis.spatial(128, i4_0 * 64 + ax0_ax1_ax2_ax3_fused % 192 // 3)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 3, 1, 1, 8, 1, 1, 2, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 8 + i1_3)
                                    yy, xx = T.axis.remap("SS", [i0_1_i1_1_i2_1_i3_1_fused, i0_2_i1_2_i2_2_i3_2_fused])
                                    rc = T.axis.reduce(128, i4_0 * 64 + i4_1 * 2 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 14, 14], "float32"], ["TENSOR", [32, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 8 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused + ax2)
                                v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                                T.writes(conv2d_nchw[v0, v1, v2, v3])
                                conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[4, 1, 1, 8, 1])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[2, 32, 2])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58])
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
sch.compute_inline(block=b0)
v98 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v98)
[14:45:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #82: "fused_multiply_add_multiply_add_nn_relu_26"
[14:45:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 480, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 480, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 480, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 480, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 480, 1, 1), "float32"], T_relu: T.Buffer[(1, 480, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 480, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 480, 28, 28], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 480, 28, 28], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 480, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 480, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 480, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 480, 28, 28):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 480, 28, 28):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 480, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 480, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 480, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 480, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 480, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 480, 1, 1), "float32"], T_relu: T.Buffer[(1, 480, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 480, 28, 28):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:45:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #83: "fused_nn_conv2d_add_nn_relu_39"
[14:45:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 480, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 480, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 480, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 480, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 480, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 480, 28, 28], "float32"], ["TENSOR", [128, 480, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 480, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 480, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 480, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 480, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(80, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(4704):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(480, i4_0 * 6 + ax0_ax1_ax2_ax3_fused // 784)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 784 // 28)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(768):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 6)
                                    v1 = T.axis.spatial(480, i4_0 * 6 + ax0_ax1_ax2_ax3_fused % 6)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 4, 14, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 14 + i2_4)
                                    xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    rc = T.axis.reduce(480, i4_0 * 6 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 480, 28, 28], "float32"], ["TENSOR", [128, 480, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 14, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + ax1)
                                v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 14 + ax2)
                                v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 4, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[80, 6, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #84: "fused_multiply_add_multiply_add_nn_relu_27"
[14:45:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 448, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 448, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 448, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 448, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 448, 1, 1), "float32"], T_relu: T.Buffer[(1, 448, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 448, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 448, 28, 28], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 448, 28, 28], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 448, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 448, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 448, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 448, 28, 28):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 448, 28, 28):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 448, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 448, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 448, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 448, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 448, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 448, 1, 1), "float32"], T_relu: T.Buffer[(1, 448, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 448, 28, 28):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:45:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #85: "fused_nn_conv2d_add_nn_relu_40"
[14:45:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 448, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 448, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 448, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 448, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 448, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 448, 28, 28], "float32"], ["TENSOR", [128, 448, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 448, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 448, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 448, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 448, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(43904):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(448, i4_0 * 56 + ax0_ax1_ax2_ax3_fused // 784)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 784 // 28)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(112):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax0_ax1_ax2_ax3_fused // 56)
                                    v1 = T.axis.spatial(448, i4_0 * 56 + ax0_ax1_ax2_ax3_fused % 56)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 7, 1, 56, 1, 1, 1, 1, 4, 14):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                                    yy = T.axis.spatial(28, i2_3 * 4 + i2_4)
                                    xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused * 14 + i3_4)
                                    rc = T.axis.reduce(448, i4_0 * 56 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 448, 28, 28], "float32"], ["TENSOR", [128, 448, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 28, 14):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_1_i1_1_i2_1_i3_1_fused + ax1)
                                v2 = T.axis.spatial(28, ax2)
                                v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused * 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[64, 2, 1, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 56])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #86: "fused_multiply_add_nn_relu_14"
[14:45:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 416, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 416, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 416, 1, 1), "float32"], T_relu: T.Buffer[(1, 416, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 416, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 416, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 416, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 416, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 416, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 416, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 416, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 416, 1, 1), "float32"], T_relu: T.Buffer[(1, 416, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 416, 28, 28):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:45:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #87: "fused_nn_conv2d_add_nn_relu_41"
[14:45:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 416, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 416, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 416, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 416, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 416, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 416, 28, 28], "float32"], ["TENSOR", [128, 416, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 416, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 416, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 416, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 416, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(81536):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(416, ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(6656):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + ax0_ax1_ax2_ax3_fused // 416)
                                    v1 = T.axis.spatial(416, ax0_ax1_ax2_ax3_fused % 416)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(208, 1, 1, 1, 1, 2, 7, 2, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i2_3)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3)
                                    rc = T.axis.reduce(416, i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 416, 28, 28], "float32"], ["TENSOR", [128, 416, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 16, 1, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 2, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 208, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #88: "fused_multiply_add_nn_relu_15"
[14:45:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 384, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 384, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 384, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 384, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 384, 28, 28):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:45:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #89: "fused_nn_conv2d_add_nn_relu_42"
[14:45:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 384, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 384, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [128, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 384, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(256, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(75264):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i4_0 * 96 + ax0_ax1_ax2_ax3_fused // 784)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 784 // 28)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(12288):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 96)
                                    v1 = T.axis.spatial(384, i4_0 * 96 + ax0_ax1_ax2_ax3_fused % 96)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 2, 2, 12, 1, 1, 1, 1, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                                    yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 14 + i2_3 * 7 + i2_4)
                                    xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                    rc = T.axis.reduce(384, i4_0 * 96 + i4_1 * 12 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [128, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 4 + ax1)
                                v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 14 + ax2)
                                v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 64, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 2, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 8, 12])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #90: "fused_multiply_add_nn_relu_16"
[14:45:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 352, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 352, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 352, 1, 1), "float32"], T_relu: T.Buffer[(1, 352, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 352, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 352, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 352, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 352, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 352, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 352, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 352, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 352, 1, 1), "float32"], T_relu: T.Buffer[(1, 352, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 352, 28, 28):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:45:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #91: "fused_nn_conv2d_add_nn_relu_43"
[14:45:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 352, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 352, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 352, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 352, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 352, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 352, 28, 28], "float32"], ["TENSOR", [128, 352, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 352, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 352, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 352, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 352, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(88, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1568):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(352, i4_0 * 4 + ax0_ax1_ax2_ax3_fused // 392)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 392 // 14)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(256):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + ax0_ax1_ax2_ax3_fused // 4)
                                    v1 = T.axis.spatial(352, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 14, 1, 4, 1, 1, 1, 4, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 32 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(28, i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused)
                                    rc = T.axis.reduce(352, i4_0 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 352, 28, 28], "float32"], ["TENSOR", [128, 352, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 28, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 32 + ax1)
                                v2 = T.axis.spatial(28, ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 2, 8, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 14, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 14, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[88, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #92: "fused_multiply_add_nn_relu_17"
[14:45:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 320, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1), "float32"], T_relu: T.Buffer[(1, 320, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 320, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 320, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 320, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 320, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 320, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 320, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1), "float32"], T_relu: T.Buffer[(1, 320, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 320, 28, 28):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:45:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #93: "fused_nn_conv2d_add_nn_relu_44"
[14:45:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 320, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 320, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 320, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 320, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 320, 28, 28], "float32"], ["TENSOR", [128, 320, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 320, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 320, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 320, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(784, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1280):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(320, ax0_ax1_ax2_ax3_fused // 4)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 196 // 28 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(10240):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 196 * 32 + ax0_ax1_ax2_ax3_fused // 320)
                                    v1 = T.axis.spatial(320, ax0_ax1_ax2_ax3_fused % 320)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(320, 1, 1, 1, 32, 2, 1, 1, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 196 * 32 + i1_3)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 196 // 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i2_3)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28)
                                    rc = T.axis.reduce(320, i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 320, 28, 28], "float32"], ["TENSOR", [128, 320, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 2, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 196 * 32 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 196 // 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 1, 32, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 320, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #94: "fused_multiply_add_multiply_add_nn_relu_28"
[14:45:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 288, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 288, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 288, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 288, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 288, 1, 1), "float32"], T_relu: T.Buffer[(1, 288, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 288, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 288, 28, 28], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 288, 28, 28], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 288, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 288, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 288, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 288, 28, 28):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 288, 28, 28):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 288, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 288, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 288, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 288, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 288, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 288, 1, 1), "float32"], T_relu: T.Buffer[(1, 288, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 288, 28, 28):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:45:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #95: "fused_nn_conv2d_add_nn_relu_45"
[14:45:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 288, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 288, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 288, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 288, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 288, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 288, 28, 28], "float32"], ["TENSOR", [128, 288, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 288, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 288, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 288, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 288, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(18, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(896):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(288, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 56)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + ax0_ax1_ax2_ax3_fused % 56 // 28)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1024):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(288, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 8, 1, 2, 1, 1, 1, 1, 4, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 32 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i2_4)
                                    xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                    rc = T.axis.reduce(288, i4_0 * 16 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 288, 28, 28], "float32"], ["TENSOR", [128, 288, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 2, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 32 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + ax2)
                                v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 2, 8, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[18, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #96: "fused_multiply_add_multiply_add_nn_relu_29"
[14:45:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:45:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #97: "fused_nn_conv2d_add_nn_relu_46"
[14:45:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [128, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 256, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3136):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 4 + ax0_ax1_ax2_ax3_fused // 784)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 784 // 28)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(512):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 4)
                                    v1 = T.axis.spatial(256, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 2, 14, 2, 1, 1, 1, 64, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i1_3 * 64 + i1_4)
                                    yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused * 2 + i2_3)
                                    xx = T.axis.spatial(28, i3_3 * 2 + i3_4)
                                    rc = T.axis.reduce(256, i4_0 * 4 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 28, 28], "float32"], ["TENSOR", [128, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 128, 2, 28):
                            with T.block("conv2d_nchw_local"):
                                v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused * 2 + ax2)
                                v3 = T.axis.spatial(28, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 64])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 14, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #98: "fused_multiply_add_nn_relu_18"
[14:45:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 224, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 224, 1, 1), "float32"], T_relu: T.Buffer[(1, 224, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 224, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 224, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 224, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 224, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 224, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 224, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 224, 1, 1), "float32"], T_relu: T.Buffer[(1, 224, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 224, 28, 28):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:45:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #99: "fused_nn_conv2d_add_nn_relu_47"
[14:45:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 224, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 224, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 224, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 224, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 224, 28, 28], "float32"], ["TENSOR", [128, 224, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 224, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 224, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 224, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(7, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(12544):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(224, i4_0 * 32 + ax0_ax1_ax2_ax3_fused // 392)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 392 // 14)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(2048):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + ax0_ax1_ax2_ax3_fused // 32)
                                    v1 = T.axis.spatial(224, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 8, 1, 2, 8, 1, 1, 1, 2, 4, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i2_4)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_3 * 7 + i3_4)
                                    rc = T.axis.reduce(224, i4_0 * 32 + i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 224, 28, 28], "float32"], ["TENSOR", [128, 224, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 4, 14):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + ax1)
                                v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 1, 8, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[7, 4, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #100: "fused_multiply_add_multiply_add_nn_relu_30"
[14:45:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 192, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 192, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 192, 28, 28], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 192, 28, 28], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 192, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 192, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:45:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #101: "fused_nn_conv2d_add_nn_relu_48"
[14:45:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 192, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 192, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [128, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 192, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(28, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3136):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1024):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(192, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 8, 1, 7, 1, 1, 1, 1, 1, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_3)
                                    rc = T.axis.reduce(192, i4_0 * 16 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [128, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 4, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[12, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #102: "fused_multiply_add_nn_relu_19"
[14:45:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1), "float32"], T_relu: T.Buffer[(1, 160, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 160, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 160, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 160, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 160, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 160, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1), "float32"], T_relu: T.Buffer[(1, 160, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 160, 28, 28):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:45:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #103: "fused_nn_conv2d_add_nn_relu_49"
[14:45:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 160, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 160, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 160, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 28, 28], "float32"], ["TENSOR", [128, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 160, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 160, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(31360):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(160, i4_0 * 80 + ax0_ax1_ax2_ax3_fused // 392)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax0_ax1_ax2_ax3_fused % 392 // 28)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1280):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + ax0_ax1_ax2_ax3_fused // 80)
                                    v1 = T.axis.spatial(160, i4_0 * 80 + ax0_ax1_ax2_ax3_fused % 80)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(10, 1, 1, 1, 2, 1, 1, 8, 1, 1, 1, 1, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_3)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 * 7 + i2_4)
                                    xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                    rc = T.axis.reduce(160, i4_0 * 80 + i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 28, 28], "float32"], ["TENSOR", [128, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 * 7 + ax2)
                                v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 10, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #104: "fused_multiply_add_nn_relu_20"
[14:45:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:45:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #105: "fused_nn_conv2d_add_nn_relu_50"
[14:45:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 28, 28, 128, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1024):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 8)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + ax0_ax1_ax2_ax3_fused % 8 // 4)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(16384):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 128)
                                    v1 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused % 128)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(64, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 16, 2, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 16 + i1_4)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i2_4)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_4)
                                    rc = T.axis.reduce(128, i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 16 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 4, 1, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 64, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #106: "fused_nn_conv2d_2"
[14:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 128, 3, 3), "float32"], conv2d_nchw: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 30, 30], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 30, 30):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 28, 28, 128, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [32, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
    

[14:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 128, 3, 3), "float32"], conv2d_nchw: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 30, 30], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 128, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(8640):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 8640 // 270)
                                    v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused * 7 + ax0_ax1_ax2_ax3_fused % 270 // 30)
                                    v3 = T.axis.spatial(30, ax0_ax1_ax2_ax3_fused % 30)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(9216):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused // 288)
                                    v1 = T.axis.spatial(128, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 288 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 7, 7, 32, 1, 3, 1, 4, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 7 + i2_3)
                                    xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_3 * 2 + i3_4)
                                    rc = T.axis.reduce(128, i4_0 * 32 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [32, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 14):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 7 + ax2)
                                v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                                T.writes(conv2d_nchw[v0, v1, v2, v3])
                                conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 2, 4, 1, 4])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 1, 1, 7, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 2])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[4, 1, 32])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58])
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
sch.compute_inline(block=b0)
v98 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v98)
[14:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #107: "fused_multiply_add_multiply_add_nn_relu_31"
[14:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 224, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 224, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 224, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 224, 1, 1), "float32"], T_relu: T.Buffer[(1, 224, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 224, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 224, 56, 56], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 224, 56, 56], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 224, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 224, 56, 56):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 224, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 224, 56, 56):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 224, 56, 56):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 224, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 224, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 224, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 224, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 224, 1, 1), "float32"], T_relu: T.Buffer[(1, 224, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 224, 56, 56):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #108: "fused_nn_conv2d_add_nn_relu_51"
[14:45:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 224, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 224, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 224, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 56, 56, 224, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 224, 56, 56], "float32"], ["TENSOR", [128, 224, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 224, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 224, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 224, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(64, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(28, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(87808):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(224, i4_0 * 112 + ax0_ax1_ax2_ax3_fused // 784)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + ax0_ax1_ax2_ax3_fused % 784 // 28)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(3584):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + ax0_ax1_ax2_ax3_fused // 112)
                                    v1 = T.axis.spatial(224, i4_0 * 112 + ax0_ax1_ax2_ax3_fused % 112)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 2, 1, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 16 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4)
                                    yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i3_4)
                                    rc = T.axis.reduce(224, i4_0 * 112 + i4_1 * 7 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 224, 56, 56], "float32"], ["TENSOR", [128, 224, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 16 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                                v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 4, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 4, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 4, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 16, 7])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #109: "fused_multiply_add_multiply_add_nn_relu_32"
[14:45:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 192, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 192, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 192, 56, 56], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 192, 56, 56], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 192, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 192, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:45:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #110: "fused_nn_conv2d_add_nn_relu_52"
[14:45:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 192, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 56, 56, 192, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [128, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 192, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 192, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(896, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(10752):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i4_0 * 48 + ax0_ax1_ax2_ax3_fused // 224)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + ax0_ax1_ax2_ax3_fused % 224 // 28)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(768):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + ax0_ax1_ax2_ax3_fused // 48)
                                    v1 = T.axis.spatial(192, i4_0 * 48 + ax0_ax1_ax2_ax3_fused % 48)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 2, 16, 1, 1, 1, 1, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 56)
                                    yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 56 // 7)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i3_3 * 2 + i3_4)
                                    rc = T.axis.reduce(192, i4_0 * 48 + i4_1 * 16 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [128, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 4):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 56 + ax1)
                                v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 56 // 7 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 16, 1, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 8, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 3, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #111: "fused_multiply_add_multiply_add_nn_relu_33"
[14:45:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 160, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 160, 1, 1), "float32"], T_relu: T.Buffer[(1, 160, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 160, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 160, 56, 56], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 160, 56, 56], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 160, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 160, 56, 56):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 160, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 160, 56, 56):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 160, 56, 56):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 160, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 160, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 160, 1, 1), "float32"], T_relu: T.Buffer[(1, 160, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 160, 56, 56):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #112: "fused_nn_conv2d_add_nn_relu_53"
[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 160, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 160, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 56, 56, 160, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 56, 56], "float32"], ["TENSOR", [128, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 160, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 160, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(448, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(5, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(25088):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(160, i4_0 * 32 + ax0_ax1_ax2_ax3_fused // 784)
                                    v2 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused % 784 // 14)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(2048):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + ax0_ax1_ax2_ax3_fused // 32)
                                    v1 = T.axis.spatial(160, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 1, 1, 8, 1, 1, 1, 1, 14, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                    yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 14 + i2_4)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                    rc = T.axis.reduce(160, i4_0 * 32 + i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 56, 56], "float32"], ["TENSOR", [128, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 14, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                                v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 14 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 32, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 1, 1, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[5, 4, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #113: "fused_multiply_add_nn_relu_21"
[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #114: "fused_nn_conv2d_add_nn_relu_54"
[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 56, 56, 128, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 128, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(14336):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 112)
                                    v2 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused % 112 // 2)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(4096):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + ax0_ax1_ax2_ax3_fused // 128)
                                    v1 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused % 128)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 4, 2, 64, 1, 1, 1, 4, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4)
                                    yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i2_3 * 7 + i2_4)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + i3_3)
                                    rc = T.axis.reduce(128, i4_1 * 64 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [128, 128, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 28, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                                v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 4, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 2, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #115: "fused_multiply_add_multiply_add_nn_relu_34"
[14:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 96, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 96, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 96, 56, 56], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 96, 56, 56], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 96, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 96, 56, 56):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 96, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 96, 56, 56):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 96, 56, 56):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 96, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 96, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 96, 56, 56):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_multiply_1", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[14:45:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #116: "fused_nn_conv2d_add_nn_relu_55"
[14:45:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 96, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 96, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 56, 56, 96, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [128, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 96, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(392, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(24, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(64):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(96, i4_0 * 4 + ax0_ax1_ax2_ax3_fused // 16)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 196 // 7 * 2 + ax0_ax1_ax2_ax3_fused % 16 // 8)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(256):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 196 * 64 + ax0_ax1_ax2_ax3_fused // 4)
                                    v1 = T.axis.spatial(96, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 8):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 196 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                    yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 196 // 7 * 2 + i2_3)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i3_4)
                                    rc = T.axis.reduce(96, i4_0 * 4 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [128, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 8):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 196 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                                v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 196 // 7 * 2 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 16, 2, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[28, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 8])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[24, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #117: "fused_multiply_add_nn_relu_22"
[14:45:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_relu", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[14:45:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #118: "fused_nn_conv2d_add_nn_relu_56"
[14:45:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 56, 56, 64, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 64, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(12544):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + ax0_ax1_ax2_ax3_fused // 1568)
                                    v2 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused % 1568 // 28)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1024):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 8)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 7, 2, 8, 1, 1, 1, 4, 2, 14):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i3_3 * 14 + i3_4)
                                    rc = T.axis.reduce(64, i4_0 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 56, 56], "float32"], ["TENSOR", [128, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 14, 28):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                                v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 2, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 1, 7, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:45:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #119: "fused_nn_conv2d_3"
[14:45:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(32, 128, 3, 3), "float32"], conv2d_nchw: T.Buffer[(1, 32, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 58, 58], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 58, 58):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 56, 56, 128, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [32, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
    

[14:45:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(32, 128, 3, 3), "float32"], conv2d_nchw: T.Buffer[(1, 32, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 58, 58], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 128, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(28, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(430592):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 3364)
                                    v2 = T.axis.spatial(58, ax0_ax1_ax2_ax3_fused % 3364 // 58)
                                    v3 = T.axis.spatial(58, ax0_ax1_ax2_ax3_fused % 58)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(18432):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + ax0_ax1_ax2_ax3_fused // 1152)
                                    v1 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused % 1152 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 3, 3, 1, 4, 2, 4, 4, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 8 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i2_3)
                                    xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3)
                                    rc = T.axis.reduce(128, i4_1 * 4 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 56, 56], "float32"], ["TENSOR", [32, 128, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 4):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 8 + ax1)
                                v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax2)
                                v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                                T.writes(conv2d_nchw[v0, v1, v2, v3])
                                conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 2, 1, 4, 2])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 14, 2, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 7, 2, 4, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[1, 32, 4])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58])
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
sch.compute_inline(block=b0)
v98 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v98)
[14:45:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #120: "fused_nn_conv2d_add_multiply_add_nn_relu_1"
[14:45:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 3, 230, 230], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
        T_multiply = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 64, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 230, 230):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(3 <= i2_1 and i2_1 < 227 and 3 <= i3_1 and i3_1 < 227, placeholder[i0_1, i1_1, i2_1 - 3, i3_1 - 3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 112, 112, 3, 7, 7):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 112, 112):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(64, 3, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 64, 1, 1), "float32"], T_relu: T.Buffer[(1, 64, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 112, 112], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 3, 230, 230], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 3, 7, 7], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(392, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(2331):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 2331 // 777)
                                    v2 = T.axis.spatial(230, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 16 + ax0_ax1_ax2_ax3_fused % 777 // 37)
                                    v3 = T.axis.spatial(230, i0_0_i1_0_i2_0_i3_0_fused % 7 * 32 + ax0_ax1_ax2_ax3_fused % 37)
                                    T.reads(placeholder[v0, v1, v2 - 3, v3 - 3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(3 <= v2 and v2 < 227 and 3 <= v3 and v3 < 227, placeholder[v0, v1, v2 - 3, v3 - 3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(2352):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 98 * 16 + ax0_ax1_ax2_ax3_fused // 147)
                                    v1 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 147 // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 7, 1, 1, 1, 2, 3, 7, 1, 1, 1, 4, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 98 * 16 + i0_1_i1_1_i2_1_i3_1_fused)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i2_4)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_3 * 2 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_2, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [64, 3, 7, 7], "float32"], [2, 2], [3, 3, 3, 3], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 4):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 98 * 16 + i0_1_i1_1_i2_1_i3_1_fused + ax1)
                                v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 98 // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax2)
                                v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0], placeholder_4[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max((conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * placeholder_3[v0, v1, 0, 0] + placeholder_4[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_multiply", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l19, l20, l21, l22, l23 = sch.split(loop=l7, factors=[v14, v15, v16, v17, v18])
v24, v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 16, 1, 1, 1])
l29, l30, l31, l32, l33 = sch.split(loop=l8, factors=[v24, v25, v26, v27, v28])
v34, v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 4])
l39, l40, l41, l42, l43 = sch.split(loop=l9, factors=[v34, v35, v36, v37, v38])
v44, v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l10, n=5, max_innermost_factor=64, decision=[7, 1, 4, 2, 2])
l49, l50, l51, l52, l53 = sch.split(loop=l10, factors=[v44, v45, v46, v47, v48])
v54, v55, v56 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l57, l58, l59 = sch.split(loop=l11, factors=[v54, v55, v56])
v60, v61, v62 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 7])
l63, l64, l65 = sch.split(loop=l12, factors=[v60, v61, v62])
v66, v67, v68 = sch.sample_perfect_tile(loop=l13, n=3, max_innermost_factor=64, decision=[1, 7, 1])
l69, l70, l71 = sch.split(loop=l13, factors=[v66, v67, v68])
sch.reorder(l19, l29, l39, l49, l20, l30, l40, l50, l21, l31, l41, l51, l57, l63, l69, l58, l64, l70, l22, l32, l42, l52, l59, l65, l71, l23, l33, l43, l53)
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="vthread.x")
l74 = sch.fuse(l21, l31, l41, l51)
sch.bind(loop=l74, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b75 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b75, loop=l74, preserve_unit_loops=True)
b76 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b76, loop=l69, preserve_unit_loops=True)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b76)
l87 = sch.fuse(l83, l84, l85, l86)
v88 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch", ann_val=v88)
b89 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b89, loop=l69, preserve_unit_loops=True)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b89)
l100 = sch.fuse(l96, l97, l98, l99)
v101 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
[14:45:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #121: "fused_nn_max_pool2d"
[14:45:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], tensor: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 114, 114], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 114, 114):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                T.writes(pad_temp[ax0, ax1, ax2, ax3])
                pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 113 and 1 <= ax3 and ax3 < 113, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 64, 56, 56, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[14:45:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 112, 112), "float32"], tensor: T.Buffer[(1, 64, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 64, 56, 56, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], T.if_then_else(1 <= ax2 * 2 + rv0 and ax2 * 2 + rv0 < 113 and 1 <= ax3 * 2 + rv1 and ax3 * 2 + rv1 < 113, placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1], T.float32(-3.4028234663852886e+38), dtype="float32"))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[14:45:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #122: "fused_concatenate"
[14:45:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 32, 56, 56), "float32"], T_concat: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 96, 56, 56):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 64, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(64 <= ax1, placeholder_1[ax0, ax1 - 64, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 32, 56, 56), "float32"], T_concat: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 96, 56, 56):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 64, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(64 <= ax1, placeholder_1[ax0, ax1 - 64, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #123: "fused_concatenate_1"
[14:45:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 32, 56, 56), "float32"], T_concat: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 96, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(96 <= ax1, placeholder_1[ax0, ax1 - 96, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 32, 56, 56), "float32"], T_concat: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 128, 56, 56):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 96, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(96 <= ax1, placeholder_1[ax0, ax1 - 96, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #124: "fused_concatenate_2"
[14:45:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 32, 56, 56), "float32"], T_concat: T.Buffer[(1, 160, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 160, 56, 56):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 128, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(128 <= ax1, placeholder_1[ax0, ax1 - 128, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 32, 56, 56), "float32"], T_concat: T.Buffer[(1, 160, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 160, 56, 56):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 128, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(128 <= ax1, placeholder_1[ax0, ax1 - 128, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #125: "fused_concatenate_3"
[14:45:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 32, 56, 56), "float32"], T_concat: T.Buffer[(1, 192, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 160, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(160 <= ax1, placeholder_1[ax0, ax1 - 160, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 32, 56, 56), "float32"], T_concat: T.Buffer[(1, 192, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 160, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(160 <= ax1, placeholder_1[ax0, ax1 - 160, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #126: "fused_concatenate_4"
[14:45:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 32, 56, 56), "float32"], T_concat: T.Buffer[(1, 224, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 224, 56, 56):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 192, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(192 <= ax1, placeholder_1[ax0, ax1 - 192, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 32, 56, 56), "float32"], T_concat: T.Buffer[(1, 224, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 224, 56, 56):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 192, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(192 <= ax1, placeholder_1[ax0, ax1 - 192, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #127: "fused_concatenate_multiply_add_multiply_add_nn_relu"
[14:45:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 32, 56, 56), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_5: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_concat = T.alloc_buffer([1, 256, 56, 56], dtype="float32")
        T_multiply = T.alloc_buffer([1, 256, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 56, 56], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 256, 56, 56], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 256, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 56, 56):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 224, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(224 <= ax1, placeholder_1[ax0, ax1 - 224, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 56, 56):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_concat[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = T_concat[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 56, 56):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 56, 56):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_5[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_5[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 56, 56), "float32"], placeholder_1: T.Buffer[(1, 32, 56, 56), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 256, 1, 1), "float32"], placeholder_5: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 256, 56, 56):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 224, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0], placeholder_5[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((T.if_then_else(224 <= ax1, placeholder_1[ax0, ax1 - 224, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32") * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0]) * placeholder_4[ax0, ax1, 0, 0] + placeholder_5[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v6 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v6)
[14:45:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #128: "fused_nn_conv2d_4"
[14:45:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1), "float32"], conv2d_nchw: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 56, 56, 256, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 56, 56], "float32"], ["TENSOR", [128, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
    

[14:45:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 56, 56), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1), "float32"], conv2d_nchw: T.Buffer[(1, 128, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 256, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(392, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(6272):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 4 + ax0_ax1_ax2_ax3_fused // 1568)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + ax0_ax1_ax2_ax3_fused % 1568 // 56)
                                    v3 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused % 56)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(512):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused // 4)
                                    v1 = T.axis.spatial(256, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 1, 2, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 98 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3)
                                    yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused % 98 // 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_4)
                                    xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4)
                                    rc = T.axis.reduce(256, i4_0 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 56, 56], "float32"], ["TENSOR", [128, 256, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 98 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax1)
                                v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused % 98 // 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + ax2)
                                v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                                T.writes(conv2d_nchw[v0, v1, v2, v3])
                                conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 4, 8, 4, 1])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 7, 2, 1, 2])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 14, 2, 1, 2])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[64, 1, 4])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58])
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
sch.compute_inline(block=b0)
v98 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v98)
[14:45:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #129: "fused_nn_avg_pool2d"
[14:45:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], tensor: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 128, 28, 28, 2, 2):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 1, 55) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 1, 55) + 1 - ax3 * 2), 1), "float32")
    

[14:45:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 56, 56), "float32"], tensor: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            tensor_1 = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 128, 28, 28, 2, 2):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1]
            for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 1, 55) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 1, 55) + 1 - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #130: "fused_concatenate_5"
[14:45:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 160, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 160, 28, 28):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 128, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(128 <= ax1, placeholder_1[ax0, ax1 - 128, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 160, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 160, 28, 28):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 128, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(128 <= ax1, placeholder_1[ax0, ax1 - 128, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #131: "fused_concatenate_6"
[14:45:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 160, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(160 <= ax1, placeholder_1[ax0, ax1 - 160, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 160, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(160 <= ax1, placeholder_1[ax0, ax1 - 160, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #132: "fused_concatenate_7"
[14:45:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 224, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 224, 28, 28):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 192, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(192 <= ax1, placeholder_1[ax0, ax1 - 192, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 224, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 224, 28, 28):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 192, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(192 <= ax1, placeholder_1[ax0, ax1 - 192, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #133: "fused_concatenate_8"
[14:45:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 224, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(224 <= ax1, placeholder_1[ax0, ax1 - 224, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 224, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(224 <= ax1, placeholder_1[ax0, ax1 - 224, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #134: "fused_concatenate_9"
[14:45:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 288, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 288, 28, 28):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 256, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(256 <= ax1, placeholder_1[ax0, ax1 - 256, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 288, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 288, 28, 28):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 256, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(256 <= ax1, placeholder_1[ax0, ax1 - 256, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #135: "fused_concatenate_10"
[14:45:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 288, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 320, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 320, 28, 28):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 288, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(288 <= ax1, placeholder_1[ax0, ax1 - 288, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 288, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 320, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 320, 28, 28):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 288, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(288 <= ax1, placeholder_1[ax0, ax1 - 288, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #136: "fused_concatenate_11"
[14:45:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 352, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 352, 28, 28):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 320, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(320 <= ax1, placeholder_1[ax0, ax1 - 320, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 352, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 352, 28, 28):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 320, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(320 <= ax1, placeholder_1[ax0, ax1 - 320, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #137: "fused_concatenate_12"
[14:45:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 352, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 384, 28, 28):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 352, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(352 <= ax1, placeholder_1[ax0, ax1 - 352, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 352, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 384, 28, 28):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 352, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(352 <= ax1, placeholder_1[ax0, ax1 - 352, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #138: "fused_concatenate_13"
[14:45:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 416, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 416, 28, 28):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 384, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(384 <= ax1, placeholder_1[ax0, ax1 - 384, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 416, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 416, 28, 28):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 384, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(384 <= ax1, placeholder_1[ax0, ax1 - 384, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #139: "fused_concatenate_14"
[14:45:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 416, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 448, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 448, 28, 28):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 416, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(416 <= ax1, placeholder_1[ax0, ax1 - 416, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 416, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 448, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 448, 28, 28):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 416, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(416 <= ax1, placeholder_1[ax0, ax1 - 416, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #140: "fused_concatenate_15"
[14:45:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 448, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 480, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 480, 28, 28):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 448, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(448 <= ax1, placeholder_1[ax0, ax1 - 448, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 448, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], T_concat: T.Buffer[(1, 480, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 480, 28, 28):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 448, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(448 <= ax1, placeholder_1[ax0, ax1 - 448, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #141: "fused_concatenate_multiply_add_multiply_add_nn_relu_1"
[14:45:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 480, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_5: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_concat = T.alloc_buffer([1, 512, 28, 28], dtype="float32")
        T_multiply = T.alloc_buffer([1, 512, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 28, 28], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 512, 28, 28], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 512, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 28, 28):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 480, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(480 <= ax1, placeholder_1[ax0, ax1 - 480, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_concat[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = T_concat[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 28, 28):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 28, 28):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_5[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_5[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:45:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 480, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 32, 28, 28), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 512, 1, 1), "float32"], placeholder_5: T.Buffer[(1, 512, 1, 1), "float32"], T_relu: T.Buffer[(1, 512, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 512, 28, 28):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 480, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0], placeholder_5[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((T.if_then_else(480 <= ax1, placeholder_1[ax0, ax1 - 480, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32") * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0]) * placeholder_4[ax0, ax1, 0, 0] + placeholder_5[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v6 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v6)
[14:45:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #142: "fused_nn_conv2d_5"
[14:45:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 512, 1, 1), "float32"], conv2d_nchw: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 28, 28, 512, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 28, 28], "float32"], ["TENSOR", [256, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
    

[14:45:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 512, 1, 1), "float32"], conv2d_nchw: T.Buffer[(1, 256, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 512, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(14336):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 128 + ax0_ax1_ax2_ax3_fused // 112)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 112 // 4)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(16384):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + ax0_ax1_ax2_ax3_fused // 128)
                                    v1 = T.axis.spatial(512, i4_0 * 128 + ax0_ax1_ax2_ax3_fused % 128)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 2, 7, 4, 16, 1, 1, 1, 16, 4, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 32 + i1_3 * 16 + i1_4)
                                    yy = T.axis.spatial(28, i2_3 * 4 + i2_4)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_3)
                                    rc = T.axis.reduce(512, i4_0 * 128 + i4_1 * 16 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 28, 28], "float32"], ["TENSOR", [256, 512, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 28, 4):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 7 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 32 + ax1)
                                v2 = T.axis.spatial(28, ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                                T.writes(conv2d_nchw[v0, v1, v2, v3])
                                conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[2, 1, 4, 2, 16])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 4])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 1, 1, 4, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[4, 8, 16])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58])
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
sch.compute_inline(block=b0)
v98 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v98)
[14:45:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #143: "fused_nn_avg_pool2d_1"
[14:45:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], tensor: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 14, 14, 2, 2):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 1, 27) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 1, 27) + 1 - ax3 * 2), 1), "float32")
    

[14:45:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 28, 28), "float32"], tensor: T.Buffer[(1, 256, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            tensor_1 = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 14, 14, 2, 2):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1]
            for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 1, 27) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 1, 27) + 1 - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #144: "fused_concatenate_16"
[14:45:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 288, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 288, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 256, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(256 <= ax1, placeholder_1[ax0, ax1 - 256, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 288, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 288, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 256, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(256 <= ax1, placeholder_1[ax0, ax1 - 256, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #145: "fused_concatenate_17"
[14:45:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 288, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 320, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 320, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 288, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(288 <= ax1, placeholder_1[ax0, ax1 - 288, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 288, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 320, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 320, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 288, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(288 <= ax1, placeholder_1[ax0, ax1 - 288, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #146: "fused_concatenate_18"
[14:45:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 352, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 352, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 320, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(320 <= ax1, placeholder_1[ax0, ax1 - 320, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 352, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 352, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 320, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(320 <= ax1, placeholder_1[ax0, ax1 - 320, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #147: "fused_concatenate_19"
[14:45:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 352, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 384, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 384, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 352, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(352 <= ax1, placeholder_1[ax0, ax1 - 352, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 352, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 384, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 384, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 352, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(352 <= ax1, placeholder_1[ax0, ax1 - 352, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #148: "fused_concatenate_20"
[14:45:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 416, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 416, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 384, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(384 <= ax1, placeholder_1[ax0, ax1 - 384, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 416, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 416, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 384, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(384 <= ax1, placeholder_1[ax0, ax1 - 384, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #149: "fused_concatenate_21"
[14:45:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 416, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 448, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 448, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 416, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(416 <= ax1, placeholder_1[ax0, ax1 - 416, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 416, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 448, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 448, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 416, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(416 <= ax1, placeholder_1[ax0, ax1 - 416, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #150: "fused_concatenate_22"
[14:45:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 448, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 480, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 480, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 448, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(448 <= ax1, placeholder_1[ax0, ax1 - 448, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 448, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 480, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 480, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 448, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(448 <= ax1, placeholder_1[ax0, ax1 - 448, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #151: "fused_concatenate_23"
[14:45:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 480, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 480, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(480 <= ax1, placeholder_1[ax0, ax1 - 480, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 480, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 480, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(480 <= ax1, placeholder_1[ax0, ax1 - 480, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #152: "fused_concatenate_24"
[14:45:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 544, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 544, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 512, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(512 <= ax1, placeholder_1[ax0, ax1 - 512, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 544, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 544, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 512, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(512 <= ax1, placeholder_1[ax0, ax1 - 512, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #153: "fused_concatenate_25"
[14:45:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 576, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 544, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(544 <= ax1, placeholder_1[ax0, ax1 - 544, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 576, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 544, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(544 <= ax1, placeholder_1[ax0, ax1 - 544, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #154: "fused_concatenate_26"
[14:45:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 608, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 608, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 576, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(576 <= ax1, placeholder_1[ax0, ax1 - 576, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 608, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 608, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 576, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(576 <= ax1, placeholder_1[ax0, ax1 - 576, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #155: "fused_concatenate_27"
[14:45:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 608, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 640, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 640, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 608, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(608 <= ax1, placeholder_1[ax0, ax1 - 608, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 608, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 640, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 640, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 608, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(608 <= ax1, placeholder_1[ax0, ax1 - 608, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #156: "fused_concatenate_28"
[14:45:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 640, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 672, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 640, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(640 <= ax1, placeholder_1[ax0, ax1 - 640, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 640, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 672, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 640, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(640 <= ax1, placeholder_1[ax0, ax1 - 640, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #157: "fused_concatenate_29"
[14:45:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 704, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 704, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 672, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(672 <= ax1, placeholder_1[ax0, ax1 - 672, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 704, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 704, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 672, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(672 <= ax1, placeholder_1[ax0, ax1 - 672, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #158: "fused_concatenate_30"
[14:45:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 704, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 736, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 736, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 704, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(704 <= ax1, placeholder_1[ax0, ax1 - 704, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 704, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 736, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 736, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 704, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(704 <= ax1, placeholder_1[ax0, ax1 - 704, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #159: "fused_concatenate_31"
[14:45:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 736, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 768, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 768, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 736, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(736 <= ax1, placeholder_1[ax0, ax1 - 736, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 736, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 768, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 768, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 736, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(736 <= ax1, placeholder_1[ax0, ax1 - 736, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #160: "fused_concatenate_32"
[14:45:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 768, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 800, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 800, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 768, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(768 <= ax1, placeholder_1[ax0, ax1 - 768, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 768, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 800, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 800, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 768, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(768 <= ax1, placeholder_1[ax0, ax1 - 768, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #161: "fused_concatenate_33"
[14:45:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 832, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 832, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 800, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(800 <= ax1, placeholder_1[ax0, ax1 - 800, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 832, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 832, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 800, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(800 <= ax1, placeholder_1[ax0, ax1 - 800, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #162: "fused_concatenate_34"
[14:45:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 864, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 864, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 832, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(832 <= ax1, placeholder_1[ax0, ax1 - 832, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:45:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:45:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 864, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 864, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 832, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(832 <= ax1, placeholder_1[ax0, ax1 - 832, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:45:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #163: "fused_concatenate_35"
[14:45:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 864, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 896, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 896, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 864, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(864 <= ax1, placeholder_1[ax0, ax1 - 864, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 864, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 896, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 896, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 864, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(864 <= ax1, placeholder_1[ax0, ax1 - 864, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #164: "fused_concatenate_36"
[14:46:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 928, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 928, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 896, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(896 <= ax1, placeholder_1[ax0, ax1 - 896, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 928, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 928, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 896, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(896 <= ax1, placeholder_1[ax0, ax1 - 896, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #165: "fused_concatenate_37"
[14:46:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 928, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 960, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 960, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 928, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(928 <= ax1, placeholder_1[ax0, ax1 - 928, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 928, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 960, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 960, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 928, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(928 <= ax1, placeholder_1[ax0, ax1 - 928, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #166: "fused_concatenate_38"
[14:46:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 992, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 992, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 960, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(960 <= ax1, placeholder_1[ax0, ax1 - 960, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], T_concat: T.Buffer[(1, 992, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 992, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 960, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(960 <= ax1, placeholder_1[ax0, ax1 - 960, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #167: "fused_concatenate_multiply_add_nn_relu"
[14:46:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 1024, 1, 1), "float32"], T_relu: T.Buffer[(1, 1024, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_concat = T.alloc_buffer([1, 1024, 14, 14], dtype="float32")
        T_multiply = T.alloc_buffer([1, 1024, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 1024, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 992, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(992 <= ax1, placeholder_1[ax0, ax1 - 992, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_concat[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = T_concat[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1024, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1024, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:46:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 32, 14, 14), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 1024, 1, 1), "float32"], T_relu: T.Buffer[(1, 1024, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 1024, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 992, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(T.if_then_else(992 <= ax1, placeholder_1[ax0, ax1 - 992, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32") * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
[14:46:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #168: "fused_nn_conv2d_6"
[14:46:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1024, 1, 1), "float32"], conv2d_nchw: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1024, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 14, 14, 1024, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 14, 14], "float32"], ["TENSOR", [512, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
    

[14:46:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 14, 14), "float32"], placeholder_1: T.Buffer[(512, 1024, 1, 1), "float32"], conv2d_nchw: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 1024, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 1024, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(32):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 2)
                                    v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(8192):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(1024, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                    rc = T.axis.reduce(1024, i4_0 * 16 + i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 14, 14], "float32"], ["TENSOR", [512, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                                v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                                T.writes(conv2d_nchw[v0, v1, v2, v3])
                                conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 32, 16, 1, 1])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[64, 2, 8])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58])
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
sch.compute_inline(block=b0)
v98 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v98)
[14:46:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #169: "fused_nn_avg_pool2d_2"
[14:46:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], tensor: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 512, 7, 7, 2, 2):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 1, 13) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 1, 13) + 1 - ax3 * 2), 1), "float32")
    

[14:46:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], tensor: T.Buffer[(1, 512, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            tensor_1 = T.alloc_buffer([1, 512, 7, 7], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 512, 7, 7, 2, 2):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1]
            for i0, i1, i2, i3 in T.grid(1, 512, 7, 7):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2, 0) * 2 + T.min(ax2 * 2 + 1, 13) + 1 - ax2 * 2) * (T.min(ax3, 0) * 2 + T.min(ax3 * 2 + 1, 13) + 1 - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #170: "fused_concatenate_39"
[14:46:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 512, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(512 <= ax1, placeholder_1[ax0, ax1 - 512, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 512, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(512 <= ax1, placeholder_1[ax0, ax1 - 512, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #171: "fused_concatenate_40"
[14:46:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 576, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 576, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 544, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(544 <= ax1, placeholder_1[ax0, ax1 - 544, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 576, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 576, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 544, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(544 <= ax1, placeholder_1[ax0, ax1 - 544, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #172: "fused_concatenate_41"
[14:46:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 608, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 608, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 576, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(576 <= ax1, placeholder_1[ax0, ax1 - 576, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 608, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 608, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 576, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(576 <= ax1, placeholder_1[ax0, ax1 - 576, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #173: "fused_concatenate_42"
[14:46:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 608, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 640, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 640, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 608, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(608 <= ax1, placeholder_1[ax0, ax1 - 608, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 608, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 640, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 640, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 608, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(608 <= ax1, placeholder_1[ax0, ax1 - 608, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #174: "fused_concatenate_43"
[14:46:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 640, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 672, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 672, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 640, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(640 <= ax1, placeholder_1[ax0, ax1 - 640, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 640, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 672, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 672, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 640, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(640 <= ax1, placeholder_1[ax0, ax1 - 640, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #175: "fused_concatenate_44"
[14:46:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 704, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 704, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 672, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(672 <= ax1, placeholder_1[ax0, ax1 - 672, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 704, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 704, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 672, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(672 <= ax1, placeholder_1[ax0, ax1 - 672, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #176: "fused_concatenate_45"
[14:46:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 704, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 736, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 736, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 704, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(704 <= ax1, placeholder_1[ax0, ax1 - 704, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 704, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 736, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 736, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 704, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(704 <= ax1, placeholder_1[ax0, ax1 - 704, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #177: "fused_concatenate_46"
[14:46:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 736, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 768, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 768, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 736, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(736 <= ax1, placeholder_1[ax0, ax1 - 736, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 736, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 768, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 768, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 736, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(736 <= ax1, placeholder_1[ax0, ax1 - 736, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #178: "fused_concatenate_47"
[14:46:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 768, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 800, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 768, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(768 <= ax1, placeholder_1[ax0, ax1 - 768, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 768, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 800, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 768, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(768 <= ax1, placeholder_1[ax0, ax1 - 768, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #179: "fused_concatenate_48"
[14:46:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 832, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 800, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(800 <= ax1, placeholder_1[ax0, ax1 - 800, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 832, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 800, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(800 <= ax1, placeholder_1[ax0, ax1 - 800, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #180: "fused_concatenate_49"
[14:46:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 864, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 832, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(832 <= ax1, placeholder_1[ax0, ax1 - 832, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 864, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 832, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(832 <= ax1, placeholder_1[ax0, ax1 - 832, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #181: "fused_concatenate_50"
[14:46:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 864, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 896, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 864, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(864 <= ax1, placeholder_1[ax0, ax1 - 864, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 864, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 896, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 864, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(864 <= ax1, placeholder_1[ax0, ax1 - 864, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #182: "fused_concatenate_51"
[14:46:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 928, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 896, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(896 <= ax1, placeholder_1[ax0, ax1 - 896, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 928, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 896, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(896 <= ax1, placeholder_1[ax0, ax1 - 896, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #183: "fused_concatenate_52"
[14:46:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 928, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 928, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(928 <= ax1, placeholder_1[ax0, ax1 - 928, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 928, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 928, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(928 <= ax1, placeholder_1[ax0, ax1 - 928, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #184: "fused_concatenate_53"
[14:46:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 992, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 960, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(960 <= ax1, placeholder_1[ax0, ax1 - 960, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

[14:46:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], T_concat: T.Buffer[(1, 992, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 960, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_concat[ax0, ax1, ax2, ax3])
                    T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(960 <= ax1, placeholder_1[ax0, ax1 - 960, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #185: "fused_concatenate_multiply_add_multiply_add_nn_relu_2"
[14:46:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 1024, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 1024, 1, 1), "float32"], placeholder_5: T.Buffer[(1, 1024, 1, 1), "float32"], T_relu: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_concat = T.alloc_buffer([1, 1024, 7, 7], dtype="float32")
        T_multiply = T.alloc_buffer([1, 1024, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 1024, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 1024, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 1024, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 992, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(992 <= ax1, placeholder_1[ax0, ax1 - 992, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_concat[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = T_concat[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1024, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1024, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1024, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_5[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_5[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1024, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[14:46:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 32, 7, 7), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 1024, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 1024, 1, 1), "float32"], placeholder_5: T.Buffer[(1, 1024, 1, 1), "float32"], T_relu: T.Buffer[(1, 1024, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 1024, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 992, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0], placeholder_5[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((T.if_then_else(992 <= ax1, placeholder_1[ax0, ax1 - 992, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32") * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0]) * placeholder_4[ax0, ax1, 0, 0] + placeholder_5[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_multiply_1", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v6 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v6)
[14:46:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #186: "fused_nn_global_avg_pool2d"
[14:46:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], tensor: T.Buffer[(1, 1024, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 1024, 1, 1], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 1024, 1, 1, 7, 7):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 7 + rv0, ax3 * 7 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 * 7 + rv0, ax3 * 7 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 1024, 1, 1):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] * T.float32(0.020408163265306121)
    

[14:46:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 2 design space(s) generated
[14:46:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], tensor: T.Buffer[(1, 1024, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            tensor_shared = T.alloc_buffer([1, 1024, 1, 1], dtype="float32", scope="shared")
            for i0, i1, i2, i3_0 in T.grid(1, 1024, 1, 1):
                for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(1, 1, 1, 1, 4):
                    for ax4_ax5_fused_1 in T.thread_binding(16, thread="threadIdx.x"):
                        with T.block("tensor"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(1024, i1)
                            ax2_1 = T.axis.spatial(1, 0)
                            ax3_1 = T.axis.spatial(1, 0)
                            rv0 = T.axis.reduce(7, (ax4_ax5_fused_0 * 16 + ax4_ax5_fused_1) // 7)
                            rv1 = T.axis.reduce(7, (ax4_ax5_fused_0 * 16 + ax4_ax5_fused_1) % 7)
                            T.where(ax4_ax5_fused_0 * 16 + ax4_ax5_fused_1 < 49)
                            T.reads(placeholder[ax0_1, ax1_1, ax2_1 * 7 + rv0, ax3_1 * 7 + rv1])
                            T.writes(tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1])
                            with T.init():
                                tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] = T.float32(0)
                            tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] = tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] + placeholder[ax0_1, ax1_1, ax2_1 * 7 + rv0, ax3_1 * 7 + rv1]
                for i3_1 in T.thread_binding(16, thread="threadIdx.x"):
                    with T.block("tensor_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(1024, i1)
                        ax2 = T.axis.spatial(1, 0)
                        ax3 = T.axis.spatial(1, 0)
                        T.where(i3_1 < 1)
                        T.reads(tensor_shared[ax0, ax1, ax2, ax3])
                        T.writes(tensor[ax0, ax1, ax2, ax3])
                        tensor[ax0, ax1, ax2, ax3] = tensor_shared[ax0, ax1, ax2, ax3] * T.float32(0.020408163265306121)
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
b2, = sch.get_consumers(block=b0)
l3, l4, l5, l6 = sch.get_loops(block=b2)
v7 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=2)
l8, l9 = sch.split(loop=l6, factors=[None, v7])
sch.bind(loop=l9, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l8, preserve_unit_loops=True)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l10, l11, l12, l13, l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b0)
l20 = sch.fuse(l18, l19)
l21, l22 = sch.split(loop=l20, factors=[None, v7])
sch.bind(loop=l22, thread_axis="threadIdx.x")
v23 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v23)
[14:46:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 7, 7), "float32"], tensor: T.Buffer[(1, 1024, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            tensor_1 = T.alloc_buffer([1, 1024, 1, 1], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 1024, 1, 1, 7, 7):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 7 + rv0, ax3 * 7 + rv1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 * 7 + rv0, ax3 * 7 + rv1]
            for i0, i1, i2, i3 in T.grid(1, 1024, 1, 1):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] * T.float32(0.020408163265306121)
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:46:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #187: "fused_nn_conv2d_add"
[14:46:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 1, 1), "float32"], placeholder_1: T.Buffer[(1000, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1000, 1, 1), "float32"], T_add: T.Buffer[(1, 1000, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1024, 1, 1], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 1000, 1, 1], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 1, 1):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 1000, 1, 1, 1024, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 1, 1], "float32"], ["TENSOR", [1000, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 1000, 1, 1):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, ax2, ax3]
    

[14:46:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:46:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 1, 1), "float32"], placeholder_1: T.Buffer[(1000, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1000, 1, 1), "float32"], T_add: T.Buffer[(1, 1000, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 1000, 1, 1], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 1024, 1, 1], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1000, 1024, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(50, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(20, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(128):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 128 + ax0_ax1_ax2_ax3_fused)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(2560):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1000, i0_0_i1_0_i2_0_i3_0_fused * 20 + ax0_ax1_ax2_ax3_fused // 128)
                                    v1 = T.axis.spatial(1024, i4_0 * 128 + ax0_ax1_ax2_ax3_fused % 128)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 64, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(1000, i0_0_i1_0_i2_0_i3_0_fused * 20 + i0_1_i1_1_i2_1_i3_1_fused)
                                    yy = T.axis.spatial(1, 0)
                                    xx = T.axis.spatial(1, 0)
                                    rc = T.axis.reduce(1024, i4_0 * 128 + i4_1 * 64 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 1, 1], "float32"], ["TENSOR", [1000, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1000, i0_0_i1_0_i2_0_i3_0_fused * 20 + i0_1_i1_1_i2_1_i3_1_fused + ax1)
                                v2, v3 = T.axis.remap("SS", [ax2, ax3])
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[50, 20, 1, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[8, 2, 64])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:46:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |            N/A |          N/A |                   N/A |      0 |            
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |            N/A |          N/A |                   N/A |      0 |            
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |            N/A |          N/A |                   N/A |      0 |            
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |            N/A |          N/A |                   N/A |      0 |            
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |            N/A |          N/A |                   N/A |      0 |            
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |            N/A |          N/A |                   N/A |      0 |            
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |            N/A |          N/A |                   N/A |      0 |            
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[14:46:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_multiply_add_multiply_add_nn_relu"
[14:46:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:46:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:46:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a303748)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a85d698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e19ff04b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d6fa518)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a85d618)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1b42e118)]: 0 failure(s)
[14:46:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[14:46:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a303748)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a85d698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e19ff04b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d6fa518)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a85d618)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1b42e118)]: 0 failure(s)
[14:46:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a303748)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a85d698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e19ff04b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d6fa518)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a85d618)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1b42e118)]: 0 failure(s)
[14:46:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a303748)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a85d698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e19ff04b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d6fa518)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a85d618)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1b42e118)]: 0 failure(s)
[14:47:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a303748)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a85d698)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e19ff04b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d6fa518)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a85d618)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1b42e118)]: 0 failure(s)
[14:47:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.8849  0.2436  0.2410  0.1841  0.0648
[14:47:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[14:47:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[14:47:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[14:47:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[14:47:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_conv2d_add_nn_relu"
[14:47:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:47:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:47:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a2e2c88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a3f0d18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d7c12c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a2e9858)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a1b8d28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d864838)]: 1895 failure(s)
[14:47:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 153 candidate(s)
[14:48:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a2e2c88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a3f0d18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d7c12c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a2e9858)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a1b8d28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d864838)]: 491 failure(s)
[14:48:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a2e2c88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a3f0d18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d7c12c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a2e9858)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a1b8d28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d864838)]: 448 failure(s)
[14:49:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a2e2c88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a3f0d18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d7c12c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a2e9858)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a1b8d28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d864838)]: 396 failure(s)
[14:49:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a2e2c88)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a3f0d18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d7c12c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a2e9858)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a1b8d28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d864838)]: 403 failure(s)
[14:49:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9996  0.9996  0.9996  0.9994  0.9993  0.9992  0.9991  0.9989  0.9988  0.9987  0.9982  0.9981  0.9980  0.9980
[17 : 32]:	0.9979  0.9973  0.9972  0.9970  0.9969  0.9968  0.9964  0.9963  0.9963  0.9962  0.9962  0.9961  0.9960  0.9954  0.9953  0.9953
[14:49:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[14:49:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[14:49:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:49:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [14:49:55] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:921
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:1264
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:159
  25: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:125
  26: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  27: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3469
  28: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  29: function_code_fastcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:284
  30: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:411
  31: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  32: _PyObject_FastCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:147
  33: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1465
  34: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:6838
  35: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:310
  36: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:328
  37: subtype_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1221
  38: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  39: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  40: frame_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/frameobject.c:430
  41: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  42: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  43: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  44: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:168
  45: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  46: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  47: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  48: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:167
  49: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  50: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  51: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:2119
  52: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  53: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  54: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  55: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  56: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  57: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  58: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  59: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  60: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4327
  61: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:718
  62: builtin_exec_impl.isra.15
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/bltinmodule.c:1025
  63: builtin_exec
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/clinic/bltinmodule.c.h:396
  64: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/methodobject.c:426
  65: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  66: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  67: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  68: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  69: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  70: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  71: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  72: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  73: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  74: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  75: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  76: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  77: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:200
  78: PyObject_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:228
  79: pymain_run_module
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:309
  80: pymain_run_python
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:610
  81: Py_RunMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:695
  82: Py_BytesMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:1127
  83: __libc_start_main
  84: 0x0000558ebbd01426
  85: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [14:49:58] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:921
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:1264
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:159
  25: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:125
  26: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  27: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3469
  28: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  29: function_code_fastcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:284
  30: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:411
  31: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  32: _PyObject_FastCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:147
  33: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1465
  34: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:6838
  35: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:310
  36: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:328
  37: subtype_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1221
  38: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  39: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  40: frame_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/frameobject.c:430
  41: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  42: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  43: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  44: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:168
  45: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  46: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  47: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  48: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:167
  49: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  50: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  51: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:2119
  52: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  53: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  54: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  55: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  56: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  57: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  58: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  59: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  60: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4327
  61: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:718
  62: builtin_exec_impl.isra.15
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/bltinmodule.c:1025
  63: builtin_exec
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/clinic/bltinmodule.c.h:396
  64: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/methodobject.c:426
  65: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  66: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  67: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  68: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  69: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  70: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  71: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  72: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  73: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  74: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  75: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  76: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  77: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:200
  78: PyObject_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:228
  79: pymain_run_module
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:309
  80: pymain_run_python
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:610
  81: Py_RunMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:695
  82: Py_BytesMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:1127
  83: __libc_start_main
  84: 0x00005555e2534426
  85: 0xffffffffffffffff


[14:50:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_multiply_add_multiply_add_nn_relu_1"
[14:50:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:50:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:50:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a1b7fb8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1acbd1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d6e5f98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a1a4538)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a1ba128)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d6f1288)]: 0 failure(s)
[14:50:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[14:50:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a1b7fb8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1acbd1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d6e5f98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a1a4538)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a1ba128)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d6f1288)]: 0 failure(s)
[14:50:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a1b7fb8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1acbd1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d6e5f98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a1a4538)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a1ba128)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d6f1288)]: 0 failure(s)
[14:50:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a1b7fb8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1acbd1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d6e5f98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a1a4538)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a1ba128)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d6f1288)]: 0 failure(s)
[14:51:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a1b7fb8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1acbd1b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d6e5f98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a1a4538)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a1ba128)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d6f1288)]: 0 failure(s)
[14:51:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9703  0.8937  0.8873  0.6412  0.4920
[14:51:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[14:51:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[14:51:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[14:51:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[14:51:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_conv2d_add_nn_relu_1"
[14:51:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:51:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:51:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a1966e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a00a038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1acdc278)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a1b9978)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1ab18ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1ace7528)]: 1899 failure(s)
[14:51:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 149 candidate(s)
[14:52:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a1966e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a00a038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1acdc278)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a1b9978)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1ab18ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1ace7528)]: 530 failure(s)
[14:52:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a1966e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a00a038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1acdc278)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a1b9978)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1ab18ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1ace7528)]: 417 failure(s)
[14:53:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a1966e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a00a038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1acdc278)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a1b9978)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1ab18ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1ace7528)]: 412 failure(s)
[14:54:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a1966e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a00a038)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1acdc278)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1a1b9978)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1ab18ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1ace7528)]: 433 failure(s)
[14:54:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9996  0.9992  0.9990  0.9990  0.9989  0.9988  0.9986  0.9985  0.9981  0.9980  0.9980  0.9974  0.9974  0.9971  0.9970  0.9968
[17 : 32]:	0.9964  0.9963  0.9963  0.9961  0.9960  0.9955  0.9951  0.9949  0.9947  0.9946  0.9943  0.9943  0.9942  0.9940  0.9940  0.9939
[14:54:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[14:54:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[14:54:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:54:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[14:54:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_multiply_add_multiply_add_nn_relu_2"
[14:54:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:54:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:54:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a7b1378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d88a108)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1ce4a248)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d889928)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1aad83f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1abc6968)]: 0 failure(s)
[14:54:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[14:54:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a7b1378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d88a108)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1ce4a248)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d889928)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1aad83f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1abc6968)]: 0 failure(s)
[14:54:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a7b1378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d88a108)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1ce4a248)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d889928)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1aad83f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1abc6968)]: 0 failure(s)
[14:54:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a7b1378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d88a108)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1ce4a248)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d889928)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1aad83f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1abc6968)]: 0 failure(s)
[14:54:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a7b1378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d88a108)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1ce4a248)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d889928)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1aad83f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1abc6968)]: 0 failure(s)
[14:54:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9699  0.9092  0.7782  0.5994  0.2065
[14:54:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[14:54:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[14:54:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[14:54:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[14:54:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_conv2d_add_nn_relu_2"
[14:54:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:54:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:55:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d506688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d21ebd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d6fe9d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d86bcb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1cfdd4e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d275d38)]: 1891 failure(s)
[14:55:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 157 candidate(s)
[14:55:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d506688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d21ebd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d6fe9d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d86bcb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1cfdd4e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d275d38)]: 473 failure(s)
[14:56:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d506688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d21ebd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d6fe9d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d86bcb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1cfdd4e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d275d38)]: 394 failure(s)
[14:57:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d506688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d21ebd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d6fe9d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d86bcb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1cfdd4e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d275d38)]: 413 failure(s)
[14:57:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d506688)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d21ebd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d6fe9d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d86bcb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1cfdd4e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d275d38)]: 384 failure(s)
[14:57:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9987  0.9986  0.9985  0.9984  0.9982  0.9979  0.9977  0.9977  0.9977  0.9975  0.9974  0.9970  0.9968  0.9966  0.9964
[17 : 32]:	0.9961  0.9957  0.9952  0.9949  0.9949  0.9947  0.9946  0.9944  0.9943  0.9943  0.9939  0.9938  0.9937  0.9932  0.9932  0.9932
[14:58:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[14:58:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[14:58:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:58:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[14:58:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_multiply_add_multiply_add_nn_relu_3"
[14:58:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:58:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:58:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d876d58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d8720c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a3ddd38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1ab113d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d86c568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d68d848)]: 0 failure(s)
[14:58:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[14:58:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d876d58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d8720c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a3ddd38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1ab113d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d86c568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d68d848)]: 0 failure(s)
[14:59:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d876d58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d8720c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a3ddd38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1ab113d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d86c568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d68d848)]: 0 failure(s)
[14:59:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d876d58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d8720c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a3ddd38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1ab113d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d86c568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d68d848)]: 0 failure(s)
[14:59:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d876d58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d8720c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a3ddd38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1ab113d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d86c568)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d68d848)]: 0 failure(s)
[14:59:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.7990  0.6066  0.4986  0.4677  0.1263
[14:59:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[14:59:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[14:59:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[14:59:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[14:59:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_conv2d_add_nn_relu_3"
[14:59:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:59:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:59:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d5079e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a0a8248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a3f30b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d7b1998)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a3e45e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a400e18)]: 1882 failure(s)
[14:59:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 166 candidate(s)
[15:00:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d5079e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a0a8248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a3f30b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d7b1998)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a3e45e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a400e18)]: 517 failure(s)
[15:01:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d5079e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a0a8248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a3f30b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d7b1998)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a3e45e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a400e18)]: 420 failure(s)
[15:01:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d5079e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a0a8248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a3f30b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d7b1998)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a3e45e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a400e18)]: 412 failure(s)
[15:02:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d5079e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1a0a8248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a3f30b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d7b1998)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a3e45e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a400e18)]: 413 failure(s)
[15:02:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9996  0.9994  0.9993  0.9990  0.9989  0.9989  0.9988  0.9988  0.9984  0.9984  0.9980  0.9973  0.9971  0.9970
[17 : 32]:	0.9969  0.9968  0.9967  0.9962  0.9962  0.9961  0.9958  0.9957  0.9955  0.9955  0.9950  0.9945  0.9942  0.9940  0.9938  0.9937
[15:02:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[15:02:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[15:02:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:02:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:03:00] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:921
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:1264
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:159
  25: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:125
  26: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  27: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3469
  28: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  29: function_code_fastcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:284
  30: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:411
  31: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  32: _PyObject_FastCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:147
  33: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1465
  34: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:6838
  35: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:310
  36: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:328
  37: subtype_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1221
  38: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  39: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  40: frame_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/frameobject.c:430
  41: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  42: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  43: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  44: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:168
  45: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  46: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  47: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  48: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:167
  49: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  50: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  51: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:2119
  52: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  53: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  54: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  55: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  56: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  57: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  58: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  59: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  60: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4327
  61: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:718
  62: builtin_exec_impl.isra.15
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/bltinmodule.c:1025
  63: builtin_exec
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/clinic/bltinmodule.c.h:396
  64: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/methodobject.c:426
  65: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  66: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  67: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  68: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  69: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  70: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  71: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  72: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  73: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  74: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  75: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  76: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  77: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:200
  78: PyObject_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:228
  79: pymain_run_module
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:309
  80: pymain_run_python
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:610
  81: Py_RunMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:695
  82: Py_BytesMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:1127
  83: __libc_start_main
  84: 0x000055c54b935426
  85: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:03:05] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:921
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:1264
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:159
  25: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:125
  26: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  27: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3469
  28: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  29: function_code_fastcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:284
  30: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:411
  31: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  32: _PyObject_FastCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:147
  33: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1465
  34: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:6838
  35: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:310
  36: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:328
  37: subtype_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1221
  38: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  39: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  40: frame_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/frameobject.c:430
  41: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  42: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  43: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  44: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:168
  45: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  46: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  47: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  48: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:167
  49: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  50: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  51: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:2119
  52: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  53: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  54: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  55: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  56: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  57: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  58: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  59: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  60: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4327
  61: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:718
  62: builtin_exec_impl.isra.15
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/bltinmodule.c:1025
  63: builtin_exec
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/clinic/bltinmodule.c.h:396
  64: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/methodobject.c:426
  65: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  66: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  67: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  68: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  69: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  70: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  71: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  72: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  73: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  74: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  75: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  76: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  77: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:200
  78: PyObject_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:228
  79: pymain_run_module
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:309
  80: pymain_run_python
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:610
  81: Py_RunMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:695
  82: Py_BytesMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:1127
  83: __libc_start_main
  84: 0x000055aa2ad68426
  85: 0xffffffffffffffff


[15:03:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_multiply_add_multiply_add_nn_relu_4"
[15:03:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:03:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:03:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d7b8e68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d7bfff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d0320e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d7be8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a3e51f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1aa7b288)]: 0 failure(s)
[15:03:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[15:03:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d7b8e68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d7bfff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d0320e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d7be8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a3e51f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1aa7b288)]: 0 failure(s)
[15:03:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d7b8e68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d7bfff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d0320e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d7be8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a3e51f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1aa7b288)]: 0 failure(s)
[15:03:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d7b8e68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d7bfff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d0320e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d7be8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a3e51f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1aa7b288)]: 0 failure(s)
[15:03:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d7b8e68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d7bfff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d0320e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d7be8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1a3e51f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1aa7b288)]: 0 failure(s)
[15:03:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9399  0.8540  0.7034  0.5421  0.5218
[15:03:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[15:03:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[15:03:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[15:03:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[15:03:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_conv2d_add_nn_relu_4"
[15:03:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:03:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:03:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a123b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d7af978)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1aa9d968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1aa87dd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d460d38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a005308)]: 1883 failure(s)
[15:03:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 165 candidate(s)
[15:04:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a123b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d7af978)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1aa9d968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1aa87dd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d460d38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a005308)]: 551 failure(s)
[15:04:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a123b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d7af978)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1aa9d968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1aa87dd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d460d38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a005308)]: 498 failure(s)
[15:05:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a123b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d7af978)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1aa9d968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1aa87dd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d460d38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a005308)]: 461 failure(s)
[15:05:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a123b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d7af978)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1aa9d968)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1aa87dd8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d460d38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a005308)]: 428 failure(s)
[15:05:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9996  0.9991  0.9991  0.9990  0.9989  0.9988  0.9986  0.9985  0.9982  0.9979  0.9976  0.9972  0.9967  0.9964  0.9961
[17 : 32]:	0.9960  0.9959  0.9958  0.9957  0.9957  0.9955  0.9946  0.9944  0.9944  0.9943  0.9941  0.9938  0.9937  0.9936  0.9934  0.9934
[15:05:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[15:05:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[15:05:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:06:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[15:06:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_multiply_add_multiply_add_nn_relu_5"
[15:06:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:06:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:06:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d460658)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d2f8fe8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d44d078)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d46a658)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d556748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d45db08)]: 0 failure(s)
[15:06:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[15:06:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d460658)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d2f8fe8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d44d078)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d46a658)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d556748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d45db08)]: 0 failure(s)
[15:06:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d460658)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d2f8fe8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d44d078)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d46a658)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d556748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d45db08)]: 0 failure(s)
[15:06:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d460658)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d2f8fe8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d44d078)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d46a658)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d556748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d45db08)]: 0 failure(s)
[15:06:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1d460658)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d2f8fe8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d44d078)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d46a658)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d556748)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d45db08)]: 0 failure(s)
[15:06:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.8650  0.7945  0.6554  0.5162  0.3863
[15:06:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[15:06:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[15:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[15:06:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[15:06:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_conv2d_add_nn_relu_5"
[15:06:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:06:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:07:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1ca9d378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d4520d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d87ce68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d448458)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1b2f0538)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d79c308)]: 1884 failure(s)
[15:07:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 164 candidate(s)
[15:07:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1ca9d378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d4520d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d87ce68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d448458)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1b2f0538)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d79c308)]: 425 failure(s)
[15:07:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1ca9d378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d4520d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d87ce68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d448458)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1b2f0538)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d79c308)]: 423 failure(s)
[15:08:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1ca9d378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d4520d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d87ce68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d448458)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1b2f0538)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d79c308)]: 432 failure(s)
[15:09:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1ca9d378)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d4520d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1d87ce68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d448458)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1b2f0538)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1d79c308)]: 348 failure(s)
[15:09:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9997  0.9997  0.9994  0.9992  0.9989  0.9986  0.9985  0.9984  0.9983  0.9981  0.9980  0.9980  0.9978  0.9977  0.9976
[17 : 32]:	0.9967  0.9966  0.9960  0.9959  0.9957  0.9954  0.9953  0.9952  0.9949  0.9949  0.9949  0.9945  0.9939  0.9937  0.9929  0.9928
[15:09:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[15:09:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[15:09:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:09:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:09:55] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:921
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:1264
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:159
  25: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:125
  26: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  27: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3469
  28: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  29: function_code_fastcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:284
  30: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:411
  31: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  32: _PyObject_FastCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:147
  33: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1465
  34: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:6838
  35: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:310
  36: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:328
  37: subtype_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1221
  38: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  39: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  40: frame_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/frameobject.c:430
  41: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  42: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  43: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  44: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:168
  45: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  46: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  47: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  48: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:167
  49: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  50: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  51: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:2119
  52: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  53: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  54: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  55: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  56: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  57: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  58: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  59: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  60: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4327
  61: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:718
  62: builtin_exec_impl.isra.15
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/bltinmodule.c:1025
  63: builtin_exec
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/clinic/bltinmodule.c.h:396
  64: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/methodobject.c:426
  65: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  66: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  67: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  68: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  69: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  70: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  71: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  72: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  73: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  74: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  75: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  76: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  77: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:200
  78: PyObject_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:228
  79: pymain_run_module
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:309
  80: pymain_run_python
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:610
  81: Py_RunMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:695
  82: Py_BytesMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:1127
  83: __libc_start_main
  84: 0x0000563c2e76d426
  85: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:10:02] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:921
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:1264
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:159
  25: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:125
  26: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  27: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3469
  28: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  29: function_code_fastcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:284
  30: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:411
  31: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  32: _PyObject_FastCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:147
  33: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1465
  34: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:6838
  35: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:310
  36: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:328
  37: subtype_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1221
  38: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  39: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  40: frame_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/frameobject.c:430
  41: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  42: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  43: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  44: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:168
  45: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  46: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  47: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  48: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:167
  49: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  50: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  51: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:2119
  52: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  53: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  54: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  55: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  56: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  57: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  58: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  59: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  60: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4327
  61: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:718
  62: builtin_exec_impl.isra.15
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/bltinmodule.c:1025
  63: builtin_exec
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/clinic/bltinmodule.c.h:396
  64: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/methodobject.c:426
  65: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  66: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  67: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  68: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  69: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  70: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  71: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  72: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  73: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  74: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  75: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  76: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  77: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:200
  78: PyObject_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:228
  79: pymain_run_module
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:309
  80: pymain_run_python
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:610
  81: Py_RunMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:695
  82: Py_BytesMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:1127
  83: __libc_start_main
  84: 0x000055c5bb720426
  85: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [15:10:14] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:921
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/callproc.c:1264
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.8.12/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:159
  25: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:125
  26: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  27: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3469
  28: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  29: function_code_fastcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:284
  30: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:411
  31: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  32: _PyObject_FastCall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:147
  33: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1465
  34: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:6838
  35: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:310
  36: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:328
  37: subtype_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/typeobject.c:1221
  38: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  39: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  40: frame_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/frameobject.c:430
  41: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  42: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  43: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  44: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:168
  45: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/object.c:2215
  46: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  47: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  48: tb_dealloc
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/traceback.c:167
  49: _Py_DECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:478
  50: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/object.h:541
  51: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:2119
  52: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  53: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  54: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  55: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  56: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  57: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  58: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  59: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  60: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4327
  61: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:718
  62: builtin_exec_impl.isra.15
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/bltinmodule.c:1025
  63: builtin_exec
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/clinic/bltinmodule.c.h:396
  64: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/methodobject.c:426
  65: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  66: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  67: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  68: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  69: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  70: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  71: _PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Include/cpython/abstract.h:127
  72: call_function
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4963
  73: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:3500
  74: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:741
  75: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1634043551344/work/Python/ceval.c:4298
  76: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:436
  77: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:200
  78: PyObject_Call
        at /tmp/build/80754af9/python-split_1634043551344/work/Objects/call.c:228
  79: pymain_run_module
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:309
  80: pymain_run_python
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:610
  81: Py_RunMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:695
  82: Py_BytesMain
        at /tmp/build/80754af9/python-split_1634043551344/work/Modules/main.c:1127
  83: __libc_start_main
  84: 0x000055ff2cb1b426
  85: 0xffffffffffffffff


[15:10:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_multiply_add_multiply_add_nn_relu_6"
[15:10:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:10:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:10:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a09f9c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e19ff9178)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a2f4888)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d671af8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1aa71578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a307dd8)]: 0 failure(s)
[15:10:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[15:10:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a09f9c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e19ff9178)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a2f4888)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d671af8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1aa71578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a307dd8)]: 0 failure(s)
[15:10:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a09f9c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e19ff9178)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a2f4888)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d671af8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1aa71578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a307dd8)]: 0 failure(s)
[15:10:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a09f9c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e19ff9178)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a2f4888)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d671af8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1aa71578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a307dd8)]: 0 failure(s)
[15:11:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1a09f9c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e19ff9178)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1a2f4888)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1d671af8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1aa71578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a307dd8)]: 0 failure(s)
[15:11:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9023  0.8909  0.8464  0.1575  0.0860
[15:11:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[15:11:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[15:11:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[15:11:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[15:11:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_conv2d_add_nn_relu_6"
[15:11:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:11:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:11:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1aa988f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d54d988)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1abdd2c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1ab207f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d5d3d68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a7df158)]: 1890 failure(s)
[15:11:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 158 candidate(s)
[15:11:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1aa988f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d54d988)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1abdd2c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1ab207f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d5d3d68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a7df158)]: 484 failure(s)
[15:12:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1aa988f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d54d988)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1abdd2c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1ab207f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d5d3d68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a7df158)]: 423 failure(s)
[15:13:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1aa988f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d54d988)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1abdd2c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1ab207f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d5d3d68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a7df158)]: 393 failure(s)
[15:13:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x560e1aa988f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x560e1d54d988)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x560e1abdd2c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x560e1ab207f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x560e1d5d3d68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x560e1a7df158)]: 401 failure(s)
[15:13:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9998  0.9990  0.9990  0.9989  0.9989  0.9988  0.9985  0.9984  0.9980  0.9980  0.9978  0.9975  0.9973  0.9971  0.9969
[17 : 32]:	0.9967  0.9964  0.9962  0.9959  0.9957  0.9949  0.9948  0.9944  0.9943  0.9941  0.9941  0.9940  0.9936  0.9935  0.9935  0.9931
[15:13:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[15:13:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[15:13:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:14:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[15:14:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_multiply_add_multiply_add_nn_relu"] Trial #0: GFLOPs: 18.7404. Time: 0.0130 ms. Best GFLOPs: 18.7404
[15:14:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_multiply_add_multiply_add_nn_relu"] Trial #1: GFLOPs: 19.8608. Time: 0.0122 ms. Best GFLOPs: 19.8608
[15:14:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_multiply_add_multiply_add_nn_relu"] Trial #2: GFLOPs: 31.9426. Time: 0.0076 ms. Best GFLOPs: 31.9426
[15:14:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_multiply_add_multiply_add_nn_relu"] Trial #3: GFLOPs: 70.3358. Time: 0.0035 ms. Best GFLOPs: 70.3358
[15:14:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_multiply_add_multiply_add_nn_relu"] Trial #4: GFLOPs: 16.4195. Time: 0.0148 ms. Best GFLOPs: 70.3358
[15:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_multiply_add_multiply_add_nn_relu"
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |        70.3358 |       3.4554 |                3.4554 |      5 |            
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |            N/A |          N/A |                   N/A |      0 |            
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |            N/A |          N/A |                   N/A |      0 |            
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |            N/A |          N/A |                   N/A |      0 |            
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |            N/A |          N/A |                   N/A |      0 |            
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |            N/A |          N/A |                   N/A |      0 |            
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |            N/A |          N/A |                   N/A |      0 |            
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 5
Total latency (us): 3.45543

[15:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #0 has finished. Remaining task(s): 187
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #0: GFLOPs: 9.8489. Time: 1.2647 ms. Best GFLOPs: 9.8489
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #1: GFLOPs: 22.4513. Time: 0.5548 ms. Best GFLOPs: 22.4513
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #2: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 992, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 992, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 992, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_4_init in T.grid(2, 7, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy, xx = T.axis.remap("SS", [i2_3_init, i3_4_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 992, 7, 7], "float32"], ["TENSOR", [128, 992, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(992, i4_0 * 62 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 49)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3038)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(31):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 62)
                                        v1 = T.axis.spatial(992, i4_0 * 62 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 62)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(62, 1, 1, 1, 2, 7, 1, 1, 1, 1, 1, 1, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy, xx = T.axis.remap("SS", [i2_3, i3_4])
                                rc = T.axis.reduce(992, i4_0 * 62 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 992, 7, 7], "float32"], ["TENSOR", [128, 992, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 64, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 62, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #3: GFLOPs: 49.5128. Time: 0.2516 ms. Best GFLOPs: 49.5128
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #4: GFLOPs: 60.6741. Time: 0.2053 ms. Best GFLOPs: 60.6741
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #5: GFLOPs: 26.0637. Time: 0.4779 ms. Best GFLOPs: 60.6741
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #6: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 992, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 992, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 992, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused)
                            xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 992, 7, 7], "float32"], ["TENSOR", [128, 992, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(992, i4_0 * 62 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 7)
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 434)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(62):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 62)
                                        v1 = T.axis.spatial(992, i4_0 * 62 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 62)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(62, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused)
                                xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                rc = T.axis.reduce(992, i4_0 * 62 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 992, 7, 7], "float32"], ["TENSOR", [128, 992, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused + ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 32, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 62, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #7: GFLOPs: 23.5116. Time: 0.5298 ms. Best GFLOPs: 60.6741
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #8: GFLOPs: 130.2331. Time: 0.0956 ms. Best GFLOPs: 130.2331
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #9: GFLOPs: 14.8878. Time: 0.8367 ms. Best GFLOPs: 130.2331
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #10: GFLOPs: 7.8255. Time: 1.5917 ms. Best GFLOPs: 130.2331
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #11: GFLOPs: 156.9139. Time: 0.0794 ms. Best GFLOPs: 156.9139
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #12: GFLOPs: 63.1764. Time: 0.1972 ms. Best GFLOPs: 156.9139
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #13: GFLOPs: 20.6562. Time: 0.6030 ms. Best GFLOPs: 156.9139
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #14: GFLOPs: 77.1104. Time: 0.1615 ms. Best GFLOPs: 156.9139
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #15: GFLOPs: 95.8579. Time: 0.1299 ms. Best GFLOPs: 156.9139
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #16: GFLOPs: 98.6071. Time: 0.1263 ms. Best GFLOPs: 156.9139
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #17: GFLOPs: 69.1889. Time: 0.1800 ms. Best GFLOPs: 156.9139
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #18: GFLOPs: 102.6413. Time: 0.1214 ms. Best GFLOPs: 156.9139
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #19: GFLOPs: 71.1877. Time: 0.1750 ms. Best GFLOPs: 156.9139
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #20: GFLOPs: 85.5127. Time: 0.1457 ms. Best GFLOPs: 156.9139
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #21: GFLOPs: 106.0462. Time: 0.1175 ms. Best GFLOPs: 156.9139
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #22: GFLOPs: 101.3360. Time: 0.1229 ms. Best GFLOPs: 156.9139
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #23: GFLOPs: 375.9272. Time: 0.0331 ms. Best GFLOPs: 375.9272
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #24: GFLOPs: 47.7573. Time: 0.2608 ms. Best GFLOPs: 375.9272
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #25: GFLOPs: 87.0308. Time: 0.1431 ms. Best GFLOPs: 375.9272
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #26: GFLOPs: 112.1865. Time: 0.1110 ms. Best GFLOPs: 375.9272
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #27: GFLOPs: 206.4979. Time: 0.0603 ms. Best GFLOPs: 375.9272
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #28: GFLOPs: 91.3772. Time: 0.1363 ms. Best GFLOPs: 375.9272
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #29: GFLOPs: 138.4887. Time: 0.0899 ms. Best GFLOPs: 375.9272
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #30: GFLOPs: 232.6797. Time: 0.0535 ms. Best GFLOPs: 375.9272
[15:14:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu"] Trial #31: GFLOPs: 28.2805. Time: 0.4405 ms. Best GFLOPs: 375.9272
[15:14:34] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_conv2d_add_nn_relu"
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |        70.3358 |       3.4554 |                3.4554 |      5 |          Y 
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |       375.9272 |      33.1346 |               33.1346 |     32 |            
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |            N/A |          N/A |                   N/A |      0 |            
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |            N/A |          N/A |                   N/A |      0 |            
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |            N/A |          N/A |                   N/A |      0 |            
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |            N/A |          N/A |                   N/A |      0 |            
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |            N/A |          N/A |                   N/A |      0 |            
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 37
Total latency (us): 36.59

[15:14:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #1 has finished. Remaining task(s): 186
[15:14:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_multiply_add_multiply_add_nn_relu_1"] Trial #0: GFLOPs: 13.5462. Time: 0.0174 ms. Best GFLOPs: 13.5462
[15:14:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_multiply_add_multiply_add_nn_relu_1"] Trial #1: GFLOPs: 22.1248. Time: 0.0106 ms. Best GFLOPs: 22.1248
[15:14:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_multiply_add_multiply_add_nn_relu_1"] Trial #2: GFLOPs: 21.6514. Time: 0.0109 ms. Best GFLOPs: 22.1248
[15:14:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_multiply_add_multiply_add_nn_relu_1"] Trial #3: GFLOPs: 19.3680. Time: 0.0121 ms. Best GFLOPs: 22.1248
[15:14:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_multiply_add_multiply_add_nn_relu_1"] Trial #4: GFLOPs: 24.8110. Time: 0.0095 ms. Best GFLOPs: 24.8110
[15:14:43] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_multiply_add_multiply_add_nn_relu_1"
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |        70.3358 |       3.4554 |                3.4554 |      5 |          Y 
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |       375.9272 |      33.1346 |               33.1346 |     32 |          Y 
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |        24.8110 |       9.4797 |                9.4797 |      5 |            
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |            N/A |          N/A |                   N/A |      0 |            
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |            N/A |          N/A |                   N/A |      0 |            
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |            N/A |          N/A |                   N/A |      0 |            
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |            N/A |          N/A |                   N/A |      0 |            
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 42
Total latency (us): 46.0697

[15:14:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #2 has finished. Remaining task(s): 185
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #0: GFLOPs: 144.3304. Time: 0.0835 ms. Best GFLOPs: 144.3304
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #1: GFLOPs: 38.8011. Time: 0.3107 ms. Best GFLOPs: 144.3304
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #2: GFLOPs: 52.6934. Time: 0.2288 ms. Best GFLOPs: 144.3304
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #3: GFLOPs: 29.4243. Time: 0.4097 ms. Best GFLOPs: 144.3304
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #4: GFLOPs: 175.3764. Time: 0.0687 ms. Best GFLOPs: 175.3764
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #5: GFLOPs: 43.0349. Time: 0.2801 ms. Best GFLOPs: 175.3764
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #6: GFLOPs: 248.6605. Time: 0.0485 ms. Best GFLOPs: 248.6605
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #7: GFLOPs: 16.8536. Time: 0.7153 ms. Best GFLOPs: 248.6605
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #8: GFLOPs: 71.6111. Time: 0.1683 ms. Best GFLOPs: 248.6605
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #9: GFLOPs: 271.2805. Time: 0.0444 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #10: GFLOPs: 88.6540. Time: 0.1360 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #11: GFLOPs: 56.2439. Time: 0.2143 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #12: GFLOPs: 50.7004. Time: 0.2378 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #13: GFLOPs: 30.0729. Time: 0.4009 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #14: GFLOPs: 110.2528. Time: 0.1093 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #15: GFLOPs: 229.3580. Time: 0.0526 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #16: GFLOPs: 59.6130. Time: 0.2022 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #17: GFLOPs: 44.6538. Time: 0.2700 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #18: GFLOPs: 91.3162. Time: 0.1320 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #19: GFLOPs: 34.0346. Time: 0.3542 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #20: GFLOPs: 56.8492. Time: 0.2120 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #21: GFLOPs: 193.6983. Time: 0.0622 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #22: GFLOPs: 29.8935. Time: 0.4033 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #23: GFLOPs: 269.9412. Time: 0.0447 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #24: GFLOPs: 77.5425. Time: 0.1555 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #25: GFLOPs: 33.3382. Time: 0.3616 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #26: GFLOPs: 69.0750. Time: 0.1745 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #27: GFLOPs: 26.4505. Time: 0.4557 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #28: GFLOPs: 11.7815. Time: 1.0232 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #29: GFLOPs: 14.8613. Time: 0.8112 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #30: GFLOPs: 35.2609. Time: 0.3419 ms. Best GFLOPs: 271.2805
[15:14:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #31: GFLOPs: 94.2162. Time: 0.1279 ms. Best GFLOPs: 271.2805
[15:14:50] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_conv2d_add_nn_relu_1"
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |        70.3358 |       3.4554 |                3.4554 |      5 |          Y 
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |       375.9272 |      33.1346 |               33.1346 |     32 |          Y 
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |        24.8110 |       9.4797 |                9.4797 |      5 |          Y 
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |       271.2805 |      44.4366 |               44.4366 |     32 |            
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |            N/A |          N/A |                   N/A |      0 |            
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |            N/A |          N/A |                   N/A |      0 |            
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |            N/A |          N/A |                   N/A |      0 |            
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 74
Total latency (us): 90.5063

[15:14:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #3 has finished. Remaining task(s): 184
[15:14:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_multiply_add_multiply_add_nn_relu_2"] Trial #0: GFLOPs: 99.3216. Time: 0.0023 ms. Best GFLOPs: 99.3216
[15:14:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_multiply_add_multiply_add_nn_relu_2"] Trial #1: GFLOPs: 92.9998. Time: 0.0024 ms. Best GFLOPs: 99.3216
[15:14:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_multiply_add_multiply_add_nn_relu_2"] Trial #2: GFLOPs: 42.4383. Time: 0.0054 ms. Best GFLOPs: 99.3216
[15:14:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_multiply_add_multiply_add_nn_relu_2"] Trial #3: GFLOPs: 99.8071. Time: 0.0023 ms. Best GFLOPs: 99.8071
[15:14:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_multiply_add_multiply_add_nn_relu_2"] Trial #4: GFLOPs: 70.3908. Time: 0.0032 ms. Best GFLOPs: 99.8071
[15:14:53] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_multiply_add_multiply_add_nn_relu_2"
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |        70.3358 |       3.4554 |                3.4554 |      5 |          Y 
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |       375.9272 |      33.1346 |               33.1346 |     32 |          Y 
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |        24.8110 |       9.4797 |                9.4797 |      5 |          Y 
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |       271.2805 |      44.4366 |               44.4366 |     32 |          Y 
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |        99.8071 |       2.2780 |                2.2780 |      5 |            
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |            N/A |          N/A |                   N/A |      0 |            
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |            N/A |          N/A |                   N/A |      0 |            
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 79
Total latency (us): 92.7843

[15:14:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #4 has finished. Remaining task(s): 183
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #0: GFLOPs: 143.1154. Time: 0.0814 ms. Best GFLOPs: 143.1154
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #1: GFLOPs: 87.5773. Time: 0.1331 ms. Best GFLOPs: 143.1154
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #2: GFLOPs: 66.2622. Time: 0.1759 ms. Best GFLOPs: 143.1154
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #3: GFLOPs: 87.0736. Time: 0.1338 ms. Best GFLOPs: 143.1154
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #4: GFLOPs: 55.2013. Time: 0.2111 ms. Best GFLOPs: 143.1154
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #5: GFLOPs: 117.2699. Time: 0.0994 ms. Best GFLOPs: 143.1154
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #6: GFLOPs: 220.3395. Time: 0.0529 ms. Best GFLOPs: 220.3395
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #7: GFLOPs: 152.0354. Time: 0.0766 ms. Best GFLOPs: 220.3395
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #8: GFLOPs: 157.5800. Time: 0.0740 ms. Best GFLOPs: 220.3395
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #9: GFLOPs: 51.9457. Time: 0.2243 ms. Best GFLOPs: 220.3395
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #10: GFLOPs: 393.7335. Time: 0.0296 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #11: GFLOPs: 49.8562. Time: 0.2337 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #12: GFLOPs: 95.0810. Time: 0.1226 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #13: GFLOPs: 58.2572. Time: 0.2000 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #14: GFLOPs: 64.5580. Time: 0.1805 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #15: GFLOPs: 34.5695. Time: 0.3371 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #16: GFLOPs: 50.2186. Time: 0.2321 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #17: GFLOPs: 270.6251. Time: 0.0431 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #18: GFLOPs: 78.4491. Time: 0.1485 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #19: GFLOPs: 71.8884. Time: 0.1621 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #20: GFLOPs: 73.6537. Time: 0.1582 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #21: GFLOPs: 72.6657. Time: 0.1604 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #22: GFLOPs: 163.3020. Time: 0.0714 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #23: GFLOPs: 383.6808. Time: 0.0304 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #24: GFLOPs: 83.8903. Time: 0.1389 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #25: GFLOPs: 57.9073. Time: 0.2012 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #26: GFLOPs: 250.9042. Time: 0.0464 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #27: GFLOPs: 50.6570. Time: 0.2300 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #28: GFLOPs: 137.7080. Time: 0.0846 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #29: GFLOPs: 65.0195. Time: 0.1792 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #30: GFLOPs: 88.4519. Time: 0.1317 ms. Best GFLOPs: 393.7335
[15:14:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_2"] Trial #31: GFLOPs: 88.7057. Time: 0.1314 ms. Best GFLOPs: 393.7335
[15:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_conv2d_add_nn_relu_2"
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |        70.3358 |       3.4554 |                3.4554 |      5 |          Y 
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |       375.9272 |      33.1346 |               33.1346 |     32 |          Y 
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |        24.8110 |       9.4797 |                9.4797 |      5 |          Y 
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |       271.2805 |      44.4366 |               44.4366 |     32 |          Y 
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |        99.8071 |       2.2780 |                2.2780 |      5 |          Y 
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |       393.7335 |      29.5971 |               29.5971 |     32 |            
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |            N/A |          N/A |                   N/A |      0 |            
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 111
Total latency (us): 122.381

[15:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #5 has finished. Remaining task(s): 182
[15:14:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_multiply_add_multiply_add_nn_relu_3"] Trial #0: GFLOPs: 25.4562. Time: 0.0086 ms. Best GFLOPs: 25.4562
[15:14:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_multiply_add_multiply_add_nn_relu_3"] Trial #1: GFLOPs: 12.8933. Time: 0.0170 ms. Best GFLOPs: 25.4562
[15:14:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_multiply_add_multiply_add_nn_relu_3"] Trial #2: GFLOPs: 15.0686. Time: 0.0146 ms. Best GFLOPs: 25.4562
[15:14:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_multiply_add_multiply_add_nn_relu_3"] Trial #3: GFLOPs: 17.9241. Time: 0.0122 ms. Best GFLOPs: 25.4562
[15:14:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_multiply_add_multiply_add_nn_relu_3"] Trial #4: GFLOPs: 17.5685. Time: 0.0125 ms. Best GFLOPs: 25.4562
[15:15:09] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_multiply_add_multiply_add_nn_relu_3"
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |        70.3358 |       3.4554 |                3.4554 |      5 |          Y 
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |       375.9272 |      33.1346 |               33.1346 |     32 |          Y 
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |        24.8110 |       9.4797 |                9.4797 |      5 |          Y 
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |       271.2805 |      44.4366 |               44.4366 |     32 |          Y 
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |        99.8071 |       2.2780 |                2.2780 |      5 |          Y 
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |       393.7335 |      29.5971 |               29.5971 |     32 |          Y 
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        25.4562 |       8.6234 |                8.6234 |      5 |            
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |            N/A |          N/A |                   N/A |      0 |            
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 116
Total latency (us): 131.005

[15:15:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #6 has finished. Remaining task(s): 181
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #0: GFLOPs: 143.5987. Time: 0.0784 ms. Best GFLOPs: 143.5987
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #1: GFLOPs: 24.4580. Time: 0.4601 ms. Best GFLOPs: 143.5987
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #2: GFLOPs: 74.2279. Time: 0.1516 ms. Best GFLOPs: 143.5987
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #3: GFLOPs: 243.7439. Time: 0.0462 ms. Best GFLOPs: 243.7439
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #4: GFLOPs: 188.2444. Time: 0.0598 ms. Best GFLOPs: 243.7439
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #5: GFLOPs: 35.7645. Time: 0.3146 ms. Best GFLOPs: 243.7439
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #6: GFLOPs: 17.8840. Time: 0.6292 ms. Best GFLOPs: 243.7439
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #7: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 896, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 896, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 896, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_4_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4_init)
                            yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 896, 7, 7], "float32"], ["TENSOR", [128, 896, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(896, i4_0 * 14 + ax0_ax1_ax2_ax3_fused_1 // 7)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused_1 % 7)
                                    v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 98)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 14)
                                        v1 = T.axis.spatial(896, i4_0 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(7, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4)
                                yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused)
                                rc = T.axis.reduce(896, i4_0 * 14 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 896, 7, 7], "float32"], ["TENSOR", [128, 896, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 32, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 7, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 224])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 224, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #8: GFLOPs: 632.7557. Time: 0.0178 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #9: GFLOPs: 197.7686. Time: 0.0569 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #10: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 896, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 896, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 896, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                            xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 896, 7, 7], "float32"], ["TENSOR", [128, 896, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(896, i4_0 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 686)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 14)
                                        v1 = T.axis.spatial(896, i4_0 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(14, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(896, i4_0 * 14 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 896, 7, 7], "float32"], ["TENSOR", [128, 896, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 14, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #11: GFLOPs: 137.5813. Time: 0.0818 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #12: GFLOPs: 37.7857. Time: 0.2978 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #13: GFLOPs: 100.8150. Time: 0.1116 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #14: GFLOPs: 119.8397. Time: 0.0939 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #15: GFLOPs: 53.7058. Time: 0.2095 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #16: GFLOPs: 188.6465. Time: 0.0596 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #17: GFLOPs: 71.2646. Time: 0.1579 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #18: GFLOPs: 53.6616. Time: 0.2097 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #19: GFLOPs: 110.0812. Time: 0.1022 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #20: GFLOPs: 352.0677. Time: 0.0320 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #21: GFLOPs: 58.7246. Time: 0.1916 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #22: GFLOPs: 141.0779. Time: 0.0798 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #23: GFLOPs: 38.0134. Time: 0.2960 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #24: GFLOPs: 148.0797. Time: 0.0760 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #25: GFLOPs: 125.5534. Time: 0.0896 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #26: GFLOPs: 66.6453. Time: 0.1688 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #27: GFLOPs: 113.7265. Time: 0.0989 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #28: GFLOPs: 439.1908. Time: 0.0256 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #29: GFLOPs: 162.8637. Time: 0.0691 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #30: GFLOPs: 213.3243. Time: 0.0527 ms. Best GFLOPs: 632.7557
[15:15:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #31: GFLOPs: 148.4118. Time: 0.0758 ms. Best GFLOPs: 632.7557
[15:15:30] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_conv2d_add_nn_relu_3"
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |        70.3358 |       3.4554 |                3.4554 |      5 |          Y 
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |       375.9272 |      33.1346 |               33.1346 |     32 |          Y 
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |        24.8110 |       9.4797 |                9.4797 |      5 |          Y 
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |       271.2805 |      44.4366 |               44.4366 |     32 |          Y 
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |        99.8071 |       2.2780 |                2.2780 |      5 |          Y 
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |       393.7335 |      29.5971 |               29.5971 |     32 |          Y 
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        25.4562 |       8.6234 |                8.6234 |      5 |          Y 
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |       632.7557 |      17.7825 |               17.7825 |     32 |            
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |            N/A |          N/A |                   N/A |      0 |            
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 148
Total latency (us): 148.787

[15:15:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #7 has finished. Remaining task(s): 180
[15:15:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_multiply_add_multiply_add_nn_relu_4"] Trial #0: GFLOPs: 37.3863. Time: 0.0057 ms. Best GFLOPs: 37.3863
[15:15:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_multiply_add_multiply_add_nn_relu_4"] Trial #1: GFLOPs: 35.8921. Time: 0.0059 ms. Best GFLOPs: 37.3863
[15:15:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_multiply_add_multiply_add_nn_relu_4"] Trial #2: GFLOPs: 34.6249. Time: 0.0061 ms. Best GFLOPs: 37.3863
[15:15:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_multiply_add_multiply_add_nn_relu_4"] Trial #3: GFLOPs: 34.6659. Time: 0.0061 ms. Best GFLOPs: 37.3863
[15:15:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_multiply_add_multiply_add_nn_relu_4"] Trial #4: GFLOPs: 36.4665. Time: 0.0058 ms. Best GFLOPs: 37.3863
[15:15:35] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_multiply_add_multiply_add_nn_relu_4"
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |        70.3358 |       3.4554 |                3.4554 |      5 |          Y 
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |       375.9272 |      33.1346 |               33.1346 |     32 |          Y 
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |        24.8110 |       9.4797 |                9.4797 |      5 |          Y 
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |       271.2805 |      44.4366 |               44.4366 |     32 |          Y 
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |        99.8071 |       2.2780 |                2.2780 |      5 |          Y 
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |       393.7335 |      29.5971 |               29.5971 |     32 |          Y 
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        25.4562 |       8.6234 |                8.6234 |      5 |          Y 
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |       632.7557 |      17.7825 |               17.7825 |     32 |          Y 
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |        37.3863 |       5.6620 |                5.6620 |      5 |            
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 153
Total latency (us): 154.449

[15:15:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #8 has finished. Remaining task(s): 179
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #0: GFLOPs: 70.7232. Time: 0.1534 ms. Best GFLOPs: 70.7232
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #1: GFLOPs: 71.0942. Time: 0.1526 ms. Best GFLOPs: 71.0942
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #2: GFLOPs: 107.4037. Time: 0.1010 ms. Best GFLOPs: 107.4037
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #3: GFLOPs: 158.6269. Time: 0.0684 ms. Best GFLOPs: 158.6269
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #4: GFLOPs: 171.2916. Time: 0.0633 ms. Best GFLOPs: 171.2916
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #5: GFLOPs: 386.0800. Time: 0.0281 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #6: GFLOPs: 335.3356. Time: 0.0324 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #7: GFLOPs: 215.7322. Time: 0.0503 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #8: GFLOPs: 106.3701. Time: 0.1020 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #9: GFLOPs: 64.8740. Time: 0.1673 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #10: GFLOPs: 180.4866. Time: 0.0601 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #11: GFLOPs: 159.4932. Time: 0.0680 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #12: GFLOPs: 113.6910. Time: 0.0954 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #13: GFLOPs: 98.1215. Time: 0.1106 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #14: GFLOPs: 103.4230. Time: 0.1049 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #15: GFLOPs: 21.9880. Time: 0.4935 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #16: GFLOPs: 95.0502. Time: 0.1142 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #17: GFLOPs: 81.6763. Time: 0.1328 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #18: GFLOPs: 226.7575. Time: 0.0479 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #19: GFLOPs: 285.5912. Time: 0.0380 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #20: GFLOPs: 205.1743. Time: 0.0529 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #21: GFLOPs: 116.2917. Time: 0.0933 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #22: GFLOPs: 91.6604. Time: 0.1184 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #23: GFLOPs: 159.9850. Time: 0.0678 ms. Best GFLOPs: 386.0800
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #24: GFLOPs: 419.2557. Time: 0.0259 ms. Best GFLOPs: 419.2557
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #25: GFLOPs: 33.0563. Time: 0.3282 ms. Best GFLOPs: 419.2557
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #26: GFLOPs: 129.0597. Time: 0.0841 ms. Best GFLOPs: 419.2557
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #27: GFLOPs: 156.5739. Time: 0.0693 ms. Best GFLOPs: 419.2557
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #28: GFLOPs: 75.4439. Time: 0.1438 ms. Best GFLOPs: 419.2557
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #29: GFLOPs: 340.0470. Time: 0.0319 ms. Best GFLOPs: 419.2557
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #30: GFLOPs: 117.4170. Time: 0.0924 ms. Best GFLOPs: 419.2557
[15:15:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_4"] Trial #31: GFLOPs: 53.5470. Time: 0.2026 ms. Best GFLOPs: 419.2557
[15:15:51] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_conv2d_add_nn_relu_4"
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |        70.3358 |       3.4554 |                3.4554 |      5 |          Y 
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |       375.9272 |      33.1346 |               33.1346 |     32 |          Y 
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |        24.8110 |       9.4797 |                9.4797 |      5 |          Y 
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |       271.2805 |      44.4366 |               44.4366 |     32 |          Y 
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |        99.8071 |       2.2780 |                2.2780 |      5 |          Y 
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |       393.7335 |      29.5971 |               29.5971 |     32 |          Y 
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        25.4562 |       8.6234 |                8.6234 |      5 |          Y 
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |       632.7557 |      17.7825 |               17.7825 |     32 |          Y 
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |        37.3863 |       5.6620 |                5.6620 |      5 |          Y 
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |       419.2557 |      25.8805 |               25.8805 |     32 |            
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 185
Total latency (us): 180.33

[15:15:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #9 has finished. Remaining task(s): 178
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_multiply_add_multiply_add_nn_relu_5"] Trial #0: GFLOPs: 67.7406. Time: 0.0030 ms. Best GFLOPs: 67.7406
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_multiply_add_multiply_add_nn_relu_5"] Trial #1: GFLOPs: 81.6253. Time: 0.0025 ms. Best GFLOPs: 81.6253
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_multiply_add_multiply_add_nn_relu_5"] Trial #2: GFLOPs: 71.6268. Time: 0.0028 ms. Best GFLOPs: 81.6253
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_multiply_add_multiply_add_nn_relu_5"] Trial #3: GFLOPs: 71.5184. Time: 0.0029 ms. Best GFLOPs: 81.6253
[15:15:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_multiply_add_multiply_add_nn_relu_5"] Trial #4: GFLOPs: 77.8302. Time: 0.0026 ms. Best GFLOPs: 81.6253
[15:16:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_multiply_add_multiply_add_nn_relu_5"
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |        70.3358 |       3.4554 |                3.4554 |      5 |          Y 
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |       375.9272 |      33.1346 |               33.1346 |     32 |          Y 
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |        24.8110 |       9.4797 |                9.4797 |      5 |          Y 
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |       271.2805 |      44.4366 |               44.4366 |     32 |          Y 
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |        99.8071 |       2.2780 |                2.2780 |      5 |          Y 
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |       393.7335 |      29.5971 |               29.5971 |     32 |          Y 
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        25.4562 |       8.6234 |                8.6234 |      5 |          Y 
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |       632.7557 |      17.7825 |               17.7825 |     32 |          Y 
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |        37.3863 |       5.6620 |                5.6620 |      5 |          Y 
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |       419.2557 |      25.8805 |               25.8805 |     32 |          Y 
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |        81.6253 |       2.4973 |                2.4973 |      5 |            
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |            N/A |          N/A |                   N/A |      0 |            
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 190
Total latency (us): 182.827

[15:16:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #10 has finished. Remaining task(s): 177
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #0: GFLOPs: 132.9227. Time: 0.0786 ms. Best GFLOPs: 132.9227
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #1: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 832, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 832, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_4_init in T.serial(7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                            xx = T.axis.spatial(7, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 7, 7], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(40):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(832, i4_0 * 26 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 1274)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 26)
                                        v1 = T.axis.spatial(832, i4_0 * 26 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 26)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                xx = T.axis.spatial(7, i3_4)
                                rc = T.axis.reduce(832, i4_0 * 26 + i4_1 * 13 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 7, 7], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 13])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #2: GFLOPs: 148.3551. Time: 0.0704 ms. Best GFLOPs: 148.3551
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #3: Error in building: LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 153, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 379, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/yj/tvm/python/tvm/runtime/module.py", line 297, in evaluator
    blob = feval(*args)
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  7: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  5: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  4: operator()
        at /home/yj/tvm/src/runtime/rpc/rpc_module.cc:375
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1221
...
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/library_module.cc:80
  4: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  3: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  1: tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/src/runtime/cuda/../pack_args.h:183
  0: tvm::runtime::CUDAWrappedFunc::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*, void**) const
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:190
  File "/home/yj/tvm/src/runtime/cuda/cuda_module.cc", line 190
  File "/home/yj/tvm/src/runtime/library_module.cc", line 80
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: ret == 0 (-1 vs. 0) : TVMError: CUDALaunch Error: CUDA_ERROR_OUT_OF_MEMORY
 grid=(2,1,1),  block=(32,1,1)

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 832, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 832, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init, i3_4_init in T.grid(2, 7, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy, xx = T.axis.remap("SS", [i2_4_init, i3_4_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 7, 7], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(832, i4_0 * 52 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 49)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 2548)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(52):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 52)
                                        v1 = T.axis.spatial(832, i4_0 * 52 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 52)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 52, 1, 1, 1, 1, 7, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy, xx = T.axis.remap("SS", [i2_4, i3_4])
                                rc = T.axis.reduce(832, i4_0 * 52 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 7, 7], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 32, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 52])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #4: GFLOPs: 55.3705. Time: 0.1887 ms. Best GFLOPs: 148.3551
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #5: GFLOPs: 136.8776. Time: 0.0763 ms. Best GFLOPs: 148.3551
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #6: GFLOPs: 55.6436. Time: 0.1878 ms. Best GFLOPs: 148.3551
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #7: GFLOPs: 130.5426. Time: 0.0800 ms. Best GFLOPs: 148.3551
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #8: GFLOPs: 111.2086. Time: 0.0940 ms. Best GFLOPs: 148.3551
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #9: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 832, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 832, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init in T.grid(7, 4, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i2_4_init, i3_3_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 7, 7], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(832, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(832, i4_0)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 49)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(832, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 4, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4)
                                yy, xx, rc = T.axis.remap("SSR", [i2_4, i3_3, i4_0])
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 7, 7], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[832, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #10: GFLOPs: 444.7690. Time: 0.0235 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #11: GFLOPs: 78.1792. Time: 0.1337 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #12: GFLOPs: 121.5385. Time: 0.0860 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #13: GFLOPs: 55.0306. Time: 0.1899 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #14: GFLOPs: 56.4260. Time: 0.1852 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #15: GFLOPs: 83.5516. Time: 0.1251 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #16: GFLOPs: 76.8060. Time: 0.1360 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #17: GFLOPs: 116.0373. Time: 0.0900 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #18: GFLOPs: 194.1649. Time: 0.0538 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #19: GFLOPs: 66.6512. Time: 0.1568 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #20: GFLOPs: 29.6197. Time: 0.3528 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #21: GFLOPs: 99.6859. Time: 0.1048 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #22: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 832, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 832, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(7, 7, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i2_3_init, i3_3_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 7, 7], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(832, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(832, i4_0)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 49)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(832, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 7, 7, 1, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4)
                                yy, xx, rc = T.axis.remap("SSR", [i2_3, i3_3, i4_0])
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 7, 7], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[832, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #23: GFLOPs: 219.3988. Time: 0.0476 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #24: GFLOPs: 36.8391. Time: 0.2836 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #25: GFLOPs: 111.8874. Time: 0.0934 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #26: GFLOPs: 166.4851. Time: 0.0628 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #27: GFLOPs: 69.8614. Time: 0.1496 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #28: GFLOPs: 130.5892. Time: 0.0800 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #29: GFLOPs: 23.2594. Time: 0.4492 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #30: GFLOPs: 86.2677. Time: 0.1211 ms. Best GFLOPs: 444.7690
[15:16:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_nn_relu_5"] Trial #31: GFLOPs: 95.9897. Time: 0.1089 ms. Best GFLOPs: 444.7690
[15:16:19] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_conv2d_add_nn_relu_5"
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |        70.3358 |       3.4554 |                3.4554 |      5 |          Y 
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |       375.9272 |      33.1346 |               33.1346 |     32 |          Y 
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |        24.8110 |       9.4797 |                9.4797 |      5 |          Y 
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |       271.2805 |      44.4366 |               44.4366 |     32 |          Y 
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |        99.8071 |       2.2780 |                2.2780 |      5 |          Y 
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |       393.7335 |      29.5971 |               29.5971 |     32 |          Y 
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        25.4562 |       8.6234 |                8.6234 |      5 |          Y 
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |       632.7557 |      17.7825 |               17.7825 |     32 |          Y 
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |        37.3863 |       5.6620 |                5.6620 |      5 |          Y 
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |       419.2557 |      25.8805 |               25.8805 |     32 |          Y 
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |        81.6253 |       2.4973 |                2.4973 |      5 |          Y 
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |       444.7690 |      23.4934 |               23.4934 |     32 |            
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 222
Total latency (us): 206.321

[15:16:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #11 has finished. Remaining task(s): 176
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_multiply_add_multiply_add_nn_relu_6"] Trial #0: GFLOPs: 13.9779. Time: 0.0140 ms. Best GFLOPs: 13.9779
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_multiply_add_multiply_add_nn_relu_6"] Trial #1: GFLOPs: 16.0090. Time: 0.0122 ms. Best GFLOPs: 16.0090
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_multiply_add_multiply_add_nn_relu_6"] Trial #2: GFLOPs: 16.9200. Time: 0.0116 ms. Best GFLOPs: 16.9200
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_multiply_add_multiply_add_nn_relu_6"] Trial #3: GFLOPs: 20.7200. Time: 0.0095 ms. Best GFLOPs: 20.7200
[15:16:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_multiply_add_multiply_add_nn_relu_6"] Trial #4: GFLOPs: 22.5855. Time: 0.0087 ms. Best GFLOPs: 22.5855
[15:16:39] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_multiply_add_multiply_add_nn_relu_6"
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |        70.3358 |       3.4554 |                3.4554 |      5 |          Y 
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |       375.9272 |      33.1346 |               33.1346 |     32 |          Y 
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |        24.8110 |       9.4797 |                9.4797 |      5 |          Y 
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |       271.2805 |      44.4366 |               44.4366 |     32 |          Y 
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |        99.8071 |       2.2780 |                2.2780 |      5 |          Y 
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |       393.7335 |      29.5971 |               29.5971 |     32 |          Y 
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        25.4562 |       8.6234 |                8.6234 |      5 |          Y 
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |       632.7557 |      17.7825 |               17.7825 |     32 |          Y 
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |        37.3863 |       5.6620 |                5.6620 |      5 |          Y 
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |       419.2557 |      25.8805 |               25.8805 |     32 |          Y 
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |        81.6253 |       2.4973 |                2.4973 |      5 |          Y 
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |       444.7690 |      23.4934 |               23.4934 |     32 |          Y 
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |        22.5855 |       8.6782 |                8.6782 |      5 |            
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |            N/A |          N/A |                   N/A |      0 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 227
Total latency (us): 214.999

[15:16:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #12 has finished. Remaining task(s): 175
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #0: GFLOPs: 380.1419. Time: 0.0264 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #1: GFLOPs: 73.7770. Time: 0.1362 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #2: GFLOPs: 60.4454. Time: 0.1662 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #3: GFLOPs: 291.2668. Time: 0.0345 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #4: GFLOPs: 44.8480. Time: 0.2240 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #5: GFLOPs: 128.8973. Time: 0.0780 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #6: GFLOPs: 63.0682. Time: 0.1593 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #7: GFLOPs: 169.7502. Time: 0.0592 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #8: GFLOPs: 37.7704. Time: 0.2660 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #9: GFLOPs: 261.3879. Time: 0.0384 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #10: GFLOPs: 46.6633. Time: 0.2153 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 800, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 800, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 800, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                            xx = T.axis.spatial(7, i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 800, 7, 7], "float32"], ["TENSOR", [128, 800, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(20, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(62):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(800, i4_0 * 40 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 1960)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(40):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 40)
                                        v1 = T.axis.spatial(800, i4_0 * 40 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 40)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 2, 1, 7, 5, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                xx = T.axis.spatial(7, i3_3)
                                rc = T.axis.reduce(800, i4_0 * 40 + i4_1 * 5 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 800, 7, 7], "float32"], ["TENSOR", [128, 800, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 32, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[20, 8, 5])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #12: GFLOPs: 282.8813. Time: 0.0355 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #13: GFLOPs: 303.8626. Time: 0.0331 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #14: GFLOPs: 79.8385. Time: 0.1259 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #15: GFLOPs: 366.7346. Time: 0.0274 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #16: GFLOPs: 37.9166. Time: 0.2650 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #17: GFLOPs: 35.2865. Time: 0.2847 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #18: GFLOPs: 139.5420. Time: 0.0720 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #19: GFLOPs: 32.3129. Time: 0.3110 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #20: GFLOPs: 120.9648. Time: 0.0831 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #21: GFLOPs: 169.5056. Time: 0.0593 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #22: GFLOPs: 11.4522. Time: 0.8774 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #23: GFLOPs: 87.0913. Time: 0.1154 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #24: GFLOPs: 71.2417. Time: 0.1410 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #25: GFLOPs: 134.9100. Time: 0.0745 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #26: GFLOPs: 281.7855. Time: 0.0357 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #27: GFLOPs: 266.1284. Time: 0.0378 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #28: GFLOPs: 11.1656. Time: 0.8999 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #29: GFLOPs: 28.4880. Time: 0.3527 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #30: GFLOPs: 188.5220. Time: 0.0533 ms. Best GFLOPs: 380.1419
[15:16:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_6"] Trial #31: GFLOPs: 40.0955. Time: 0.2506 ms. Best GFLOPs: 380.1419
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_nn_conv2d_add_nn_relu_6"
  ID |                                                  Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------------------------
   0 |               fused_multiply_add_multiply_add_nn_relu |    243040 |      1 |        70.3358 |       3.4554 |                3.4554 |      5 |          Y 
   1 |                           fused_nn_conv2d_add_nn_relu |  12456192 |      1 |       375.9272 |      33.1346 |               33.1346 |     32 |          Y 
   2 |             fused_multiply_add_multiply_add_nn_relu_1 |    235200 |      1 |        24.8110 |       9.4797 |                9.4797 |      5 |          Y 
   3 |                         fused_nn_conv2d_add_nn_relu_1 |  12054784 |      1 |       271.2805 |      44.4366 |               44.4366 |     32 |          Y 
   4 |             fused_multiply_add_multiply_add_nn_relu_2 |    227360 |      1 |        99.8071 |       2.2780 |                2.2780 |      5 |          Y 
   5 |                         fused_nn_conv2d_add_nn_relu_2 |  11653376 |      1 |       393.7335 |      29.5971 |               29.5971 |     32 |          Y 
   6 |             fused_multiply_add_multiply_add_nn_relu_3 |    219520 |      1 |        25.4562 |       8.6234 |                8.6234 |      5 |          Y 
   7 |                         fused_nn_conv2d_add_nn_relu_3 |  11251968 |      1 |       632.7557 |      17.7825 |               17.7825 |     32 |          Y 
   8 |             fused_multiply_add_multiply_add_nn_relu_4 |    211680 |      1 |        37.3863 |       5.6620 |                5.6620 |      5 |          Y 
   9 |                         fused_nn_conv2d_add_nn_relu_4 |  10850560 |      1 |       419.2557 |      25.8805 |               25.8805 |     32 |          Y 
  10 |             fused_multiply_add_multiply_add_nn_relu_5 |    203840 |      1 |        81.6253 |       2.4973 |                2.4973 |      5 |          Y 
  11 |                         fused_nn_conv2d_add_nn_relu_5 |  10449152 |      1 |       444.7690 |      23.4934 |               23.4934 |     32 |          Y 
  12 |             fused_multiply_add_multiply_add_nn_relu_6 |    196000 |      1 |        22.5855 |       8.6782 |                8.6782 |      5 |          Y 
  13 |                         fused_nn_conv2d_add_nn_relu_6 |  10047744 |      1 |       380.1419 |      26.4316 |               26.4316 |     32 |            
  14 |             fused_multiply_add_multiply_add_nn_relu_7 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  15 |                         fused_nn_conv2d_add_nn_relu_7 |   9646336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  16 |             fused_multiply_add_multiply_add_nn_relu_8 |    180320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  17 |                         fused_nn_conv2d_add_nn_relu_8 |   9244928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  18 |             fused_multiply_add_multiply_add_nn_relu_9 |    172480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  19 |              fused_nn_conv2d_add_multiply_add_nn_relu |   8856064 |      1 |            N/A |          N/A |                   N/A |      0 |            
  20 |            fused_multiply_add_multiply_add_nn_relu_10 |    164640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  21 |                         fused_nn_conv2d_add_nn_relu_9 |   8442112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  22 |            fused_multiply_add_multiply_add_nn_relu_11 |    156800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  23 |                        fused_nn_conv2d_add_nn_relu_10 |   8040704 |      1 |            N/A |          N/A |                   N/A |      0 |            
  24 |            fused_multiply_add_multiply_add_nn_relu_12 |    148960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  25 |                        fused_nn_conv2d_add_nn_relu_11 |   7639296 |      1 |            N/A |          N/A |                   N/A |      0 |            
  26 |            fused_multiply_add_multiply_add_nn_relu_13 |    141120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  27 |                        fused_nn_conv2d_add_nn_relu_12 |   7237888 |      1 |            N/A |          N/A |                   N/A |      0 |            
  28 |                            fused_multiply_add_nn_relu |     79968 |      1 |            N/A |          N/A |                   N/A |      0 |            
  29 |                        fused_nn_conv2d_add_nn_relu_13 |   6836480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  30 |                          fused_multiply_add_nn_relu_1 |     75264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  31 |                        fused_nn_conv2d_add_nn_relu_14 |   6435072 |      1 |            N/A |          N/A |                   N/A |      0 |            
  32 |                                       fused_nn_conv2d |   3612672 |     16 |            N/A |          N/A |                   N/A |      0 |            
  33 |            fused_multiply_add_multiply_add_nn_relu_14 |    972160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  34 |                        fused_nn_conv2d_add_nn_relu_15 |  49824768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  35 |            fused_multiply_add_multiply_add_nn_relu_15 |    940800 |      1 |            N/A |          N/A |                   N/A |      0 |            
  36 |                        fused_nn_conv2d_add_nn_relu_16 |  48219136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  37 |            fused_multiply_add_multiply_add_nn_relu_16 |    909440 |      1 |            N/A |          N/A |                   N/A |      0 |            
  38 |                        fused_nn_conv2d_add_nn_relu_17 |  46613504 |      1 |            N/A |          N/A |                   N/A |      0 |            
  39 |                          fused_multiply_add_nn_relu_2 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  40 |                        fused_nn_conv2d_add_nn_relu_18 |  45007872 |      1 |            N/A |          N/A |                   N/A |      0 |            
  41 |            fused_multiply_add_multiply_add_nn_relu_17 |    846720 |      1 |            N/A |          N/A |                   N/A |      0 |            
  42 |                        fused_nn_conv2d_add_nn_relu_19 |  43402240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  43 |                          fused_multiply_add_nn_relu_3 |    489216 |      1 |            N/A |          N/A |                   N/A |      0 |            
  44 |                        fused_nn_conv2d_add_nn_relu_20 |  41796608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  45 |            fused_multiply_add_multiply_add_nn_relu_18 |    784000 |      1 |            N/A |          N/A |                   N/A |      0 |            
  46 |                        fused_nn_conv2d_add_nn_relu_21 |  40190976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  47 |                          fused_multiply_add_nn_relu_4 |    451584 |      1 |            N/A |          N/A |                   N/A |      0 |            
  48 |                        fused_nn_conv2d_add_nn_relu_22 |  38585344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  49 |            fused_multiply_add_multiply_add_nn_relu_19 |    721280 |      1 |            N/A |          N/A |                   N/A |      0 |            
  50 |                        fused_nn_conv2d_add_nn_relu_23 |  36979712 |      1 |            N/A |          N/A |                   N/A |      0 |            
  51 |            fused_multiply_add_multiply_add_nn_relu_20 |    689920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  52 |                        fused_nn_conv2d_add_nn_relu_24 |  35374080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  53 |                          fused_multiply_add_nn_relu_5 |    395136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  54 |                        fused_nn_conv2d_add_nn_relu_25 |  33768448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  55 |            fused_multiply_add_multiply_add_nn_relu_21 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  56 |                        fused_nn_conv2d_add_nn_relu_26 |  32162816 |      1 |            N/A |          N/A |                   N/A |      0 |            
  57 |            fused_multiply_add_multiply_add_nn_relu_22 |    595840 |      1 |            N/A |          N/A |                   N/A |      0 |            
  58 |                        fused_nn_conv2d_add_nn_relu_27 |  30557184 |      1 |            N/A |          N/A |                   N/A |      0 |            
  59 |                          fused_multiply_add_nn_relu_6 |    338688 |      1 |            N/A |          N/A |                   N/A |      0 |            
  60 |                        fused_nn_conv2d_add_nn_relu_28 |  28951552 |      1 |            N/A |          N/A |                   N/A |      0 |            
  61 |            fused_multiply_add_multiply_add_nn_relu_23 |    533120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  62 |                        fused_nn_conv2d_add_nn_relu_29 |  27345920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  63 |                          fused_multiply_add_nn_relu_7 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
  64 |                        fused_nn_conv2d_add_nn_relu_30 |  25740288 |      1 |            N/A |          N/A |                   N/A |      0 |            
  65 |                          fused_multiply_add_nn_relu_8 |    282240 |      1 |            N/A |          N/A |                   N/A |      0 |            
  66 |                        fused_nn_conv2d_add_nn_relu_31 |  24134656 |      1 |            N/A |          N/A |                   N/A |      0 |            
  67 |            fused_multiply_add_multiply_add_nn_relu_24 |    439040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  68 |                        fused_nn_conv2d_add_nn_relu_32 |  22529024 |      1 |            N/A |          N/A |                   N/A |      0 |            
  69 |                          fused_multiply_add_nn_relu_9 |    244608 |      1 |            N/A |          N/A |                   N/A |      0 |            
  70 |                        fused_nn_conv2d_add_nn_relu_33 |  20923392 |      1 |            N/A |          N/A |                   N/A |      0 |            
  71 |            fused_multiply_add_multiply_add_nn_relu_25 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
  72 |                        fused_nn_conv2d_add_nn_relu_34 |  19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  73 |                         fused_multiply_add_nn_relu_10 |    206976 |      1 |            N/A |          N/A |                   N/A |      0 |            
  74 |                        fused_nn_conv2d_add_nn_relu_35 |  17712128 |      1 |            N/A |          N/A |                   N/A |      0 |            
  75 |                         fused_multiply_add_nn_relu_11 |    188160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  76 |                        fused_nn_conv2d_add_nn_relu_36 |  16106496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  77 |                         fused_multiply_add_nn_relu_12 |    169344 |      1 |            N/A |          N/A |                   N/A |      0 |            
  78 |                        fused_nn_conv2d_add_nn_relu_37 |  14500864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  79 |                         fused_multiply_add_nn_relu_13 |    150528 |      1 |            N/A |          N/A |                   N/A |      0 |            
  80 |                        fused_nn_conv2d_add_nn_relu_38 |  12895232 |      1 |            N/A |          N/A |                   N/A |      0 |            
  81 |                                     fused_nn_conv2d_1 |  14450688 |     24 |            N/A |          N/A |                   N/A |      0 |            
  82 |            fused_multiply_add_multiply_add_nn_relu_26 |   1881600 |      1 |            N/A |          N/A |                   N/A |      0 |            
  83 |                        fused_nn_conv2d_add_nn_relu_39 |  96538624 |      1 |            N/A |          N/A |                   N/A |      0 |            
  84 |            fused_multiply_add_multiply_add_nn_relu_27 |   1756160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  85 |                        fused_nn_conv2d_add_nn_relu_40 |  90116096 |      1 |            N/A |          N/A |                   N/A |      0 |            
  86 |                         fused_multiply_add_nn_relu_14 |    978432 |      1 |            N/A |          N/A |                   N/A |      0 |            
  87 |                        fused_nn_conv2d_add_nn_relu_41 |  83693568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  88 |                         fused_multiply_add_nn_relu_15 |    903168 |      1 |            N/A |          N/A |                   N/A |      0 |            
  89 |                        fused_nn_conv2d_add_nn_relu_42 |  77271040 |      1 |            N/A |          N/A |                   N/A |      0 |            
  90 |                         fused_multiply_add_nn_relu_16 |    827904 |      1 |            N/A |          N/A |                   N/A |      0 |            
  91 |                        fused_nn_conv2d_add_nn_relu_43 |  70848512 |      1 |            N/A |          N/A |                   N/A |      0 |            
  92 |                         fused_multiply_add_nn_relu_17 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  93 |                        fused_nn_conv2d_add_nn_relu_44 |  64425984 |      1 |            N/A |          N/A |                   N/A |      0 |            
  94 |            fused_multiply_add_multiply_add_nn_relu_28 |   1128960 |      1 |            N/A |          N/A |                   N/A |      0 |            
  95 |                        fused_nn_conv2d_add_nn_relu_45 |  58003456 |      1 |            N/A |          N/A |                   N/A |      0 |            
  96 |            fused_multiply_add_multiply_add_nn_relu_29 |   1003520 |      1 |            N/A |          N/A |                   N/A |      0 |            
  97 |                        fused_nn_conv2d_add_nn_relu_46 |  51580928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  98 |                         fused_multiply_add_nn_relu_18 |    526848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  99 |                        fused_nn_conv2d_add_nn_relu_47 |  45158400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 100 |            fused_multiply_add_multiply_add_nn_relu_30 |    752640 |      1 |            N/A |          N/A |                   N/A |      0 |            
 101 |                        fused_nn_conv2d_add_nn_relu_48 |  38735872 |      1 |            N/A |          N/A |                   N/A |      0 |            
 102 |                         fused_multiply_add_nn_relu_19 |    376320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 103 |                        fused_nn_conv2d_add_nn_relu_49 |  32313344 |      1 |            N/A |          N/A |                   N/A |      0 |            
 104 |                         fused_multiply_add_nn_relu_20 |    301056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 105 |                        fused_nn_conv2d_add_nn_relu_50 |  25890816 |      1 |            N/A |          N/A |                   N/A |      0 |            
 106 |                                     fused_nn_conv2d_2 |  57802752 |     12 |            N/A |          N/A |                   N/A |      0 |            
 107 |            fused_multiply_add_multiply_add_nn_relu_31 |   3512320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 108 |                        fused_nn_conv2d_add_nn_relu_51 | 180633600 |      1 |            N/A |          N/A |                   N/A |      0 |            
 109 |            fused_multiply_add_multiply_add_nn_relu_32 |   3010560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 110 |                        fused_nn_conv2d_add_nn_relu_52 | 154943488 |      1 |            N/A |          N/A |                   N/A |      0 |            
 111 |            fused_multiply_add_multiply_add_nn_relu_33 |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 112 |                        fused_nn_conv2d_add_nn_relu_53 | 129253376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 113 |                         fused_multiply_add_nn_relu_21 |   1204224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 114 |                        fused_nn_conv2d_add_nn_relu_54 | 103563264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 115 |            fused_multiply_add_multiply_add_nn_relu_34 |   1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 116 |                        fused_nn_conv2d_add_nn_relu_55 |  77873152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 117 |                         fused_multiply_add_nn_relu_22 |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 118 |                        fused_nn_conv2d_add_nn_relu_56 |  52183040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 119 |                                     fused_nn_conv2d_3 | 231211008 |      6 |            N/A |          N/A |                   N/A |      0 |            
 120 |            fused_nn_conv2d_add_multiply_add_nn_relu_1 | 239239168 |      1 |            N/A |          N/A |                   N/A |      0 |            
 121 |                                   fused_nn_max_pool2d |   1806336 |      1 |            N/A |          N/A |                   N/A |      0 |            
 122 |                                     fused_concatenate |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 123 |                                   fused_concatenate_1 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 124 |                                   fused_concatenate_2 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 125 |                                   fused_concatenate_3 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 126 |                                   fused_concatenate_4 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 127 |   fused_concatenate_multiply_add_multiply_add_nn_relu |   4014080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 128 |                                     fused_nn_conv2d_4 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 129 |                                   fused_nn_avg_pool2d |   2508800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 130 |                                   fused_concatenate_5 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 131 |                                   fused_concatenate_6 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 132 |                                   fused_concatenate_7 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 133 |                                   fused_concatenate_8 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 134 |                                   fused_concatenate_9 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 135 |                                  fused_concatenate_10 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 136 |                                  fused_concatenate_11 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 137 |                                  fused_concatenate_12 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 138 |                                  fused_concatenate_13 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 139 |                                  fused_concatenate_14 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 140 |                                  fused_concatenate_15 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 141 | fused_concatenate_multiply_add_multiply_add_nn_relu_1 |   2007040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 142 |                                     fused_nn_conv2d_5 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 143 |                                 fused_nn_avg_pool2d_1 |   1254400 |      1 |            N/A |          N/A |                   N/A |      0 |            
 144 |                                  fused_concatenate_16 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 145 |                                  fused_concatenate_17 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 146 |                                  fused_concatenate_18 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 147 |                                  fused_concatenate_19 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 148 |                                  fused_concatenate_20 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 149 |                                  fused_concatenate_21 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 150 |                                  fused_concatenate_22 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 151 |                                  fused_concatenate_23 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 152 |                                  fused_concatenate_24 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 153 |                                  fused_concatenate_25 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 154 |                                  fused_concatenate_26 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 155 |                                  fused_concatenate_27 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 156 |                                  fused_concatenate_28 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 157 |                                  fused_concatenate_29 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 158 |                                  fused_concatenate_30 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 159 |                                  fused_concatenate_31 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 160 |                                  fused_concatenate_32 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 161 |                                  fused_concatenate_33 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 162 |                                  fused_concatenate_34 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 163 |                                  fused_concatenate_35 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 164 |                                  fused_concatenate_36 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 165 |                                  fused_concatenate_37 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 166 |                                  fused_concatenate_38 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 167 |                fused_concatenate_multiply_add_nn_relu |    602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 168 |                                     fused_nn_conv2d_6 | 205520896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 169 |                                 fused_nn_avg_pool2d_2 |    627200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 170 |                                  fused_concatenate_39 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 171 |                                  fused_concatenate_40 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 172 |                                  fused_concatenate_41 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 173 |                                  fused_concatenate_42 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 174 |                                  fused_concatenate_43 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 175 |                                  fused_concatenate_44 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 176 |                                  fused_concatenate_45 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 177 |                                  fused_concatenate_46 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 178 |                                  fused_concatenate_47 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 179 |                                  fused_concatenate_48 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 180 |                                  fused_concatenate_49 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 181 |                                  fused_concatenate_50 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 182 |                                  fused_concatenate_51 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 183 |                                  fused_concatenate_52 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 184 |                                  fused_concatenate_53 |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 185 | fused_concatenate_multiply_add_multiply_add_nn_relu_2 |    250880 |      1 |            N/A |          N/A |                   N/A |      0 |            
 186 |                            fused_nn_global_avg_pool2d |     51200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 187 |                                   fused_nn_conv2d_add |   2049000 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 259
Total latency (us): 241.43

[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #13 has finished. Remaining task(s): 174
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #14 has finished. Remaining task(s): 173
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #15 has finished. Remaining task(s): 172
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #16 has finished. Remaining task(s): 171
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #17 has finished. Remaining task(s): 170
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #18 has finished. Remaining task(s): 169
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #19 has finished. Remaining task(s): 168
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #20 has finished. Remaining task(s): 167
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #21 has finished. Remaining task(s): 166
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #22 has finished. Remaining task(s): 165
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #23 has finished. Remaining task(s): 164
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #24 has finished. Remaining task(s): 163
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #25 has finished. Remaining task(s): 162
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #26 has finished. Remaining task(s): 161
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #27 has finished. Remaining task(s): 160
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #28 has finished. Remaining task(s): 159
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #29 has finished. Remaining task(s): 158
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #30 has finished. Remaining task(s): 157
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #31 has finished. Remaining task(s): 156
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #32 has finished. Remaining task(s): 155
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #33 has finished. Remaining task(s): 154
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #34 has finished. Remaining task(s): 153
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #35 has finished. Remaining task(s): 152
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #36 has finished. Remaining task(s): 151
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #37 has finished. Remaining task(s): 150
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #38 has finished. Remaining task(s): 149
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #39 has finished. Remaining task(s): 148
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #40 has finished. Remaining task(s): 147
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #41 has finished. Remaining task(s): 146
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #42 has finished. Remaining task(s): 145
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #43 has finished. Remaining task(s): 144
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #44 has finished. Remaining task(s): 143
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #45 has finished. Remaining task(s): 142
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #46 has finished. Remaining task(s): 141
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #47 has finished. Remaining task(s): 140
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #48 has finished. Remaining task(s): 139
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #49 has finished. Remaining task(s): 138
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #50 has finished. Remaining task(s): 137
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #51 has finished. Remaining task(s): 136
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #52 has finished. Remaining task(s): 135
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #53 has finished. Remaining task(s): 134
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #54 has finished. Remaining task(s): 133
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #55 has finished. Remaining task(s): 132
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #56 has finished. Remaining task(s): 131
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #57 has finished. Remaining task(s): 130
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #58 has finished. Remaining task(s): 129
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #59 has finished. Remaining task(s): 128
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #60 has finished. Remaining task(s): 127
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #61 has finished. Remaining task(s): 126
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #62 has finished. Remaining task(s): 125
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #63 has finished. Remaining task(s): 124
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #64 has finished. Remaining task(s): 123
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #65 has finished. Remaining task(s): 122
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #66 has finished. Remaining task(s): 121
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #67 has finished. Remaining task(s): 120
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #68 has finished. Remaining task(s): 119
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #69 has finished. Remaining task(s): 118
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #70 has finished. Remaining task(s): 117
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #71 has finished. Remaining task(s): 116
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #72 has finished. Remaining task(s): 115
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #73 has finished. Remaining task(s): 114
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #74 has finished. Remaining task(s): 113
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #75 has finished. Remaining task(s): 112
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #76 has finished. Remaining task(s): 111
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #77 has finished. Remaining task(s): 110
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #78 has finished. Remaining task(s): 109
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #79 has finished. Remaining task(s): 108
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #80 has finished. Remaining task(s): 107
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #81 has finished. Remaining task(s): 106
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #82 has finished. Remaining task(s): 105
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #83 has finished. Remaining task(s): 104
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #84 has finished. Remaining task(s): 103
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #85 has finished. Remaining task(s): 102
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #86 has finished. Remaining task(s): 101
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #87 has finished. Remaining task(s): 100
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #88 has finished. Remaining task(s): 99
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #89 has finished. Remaining task(s): 98
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #90 has finished. Remaining task(s): 97
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #91 has finished. Remaining task(s): 96
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #92 has finished. Remaining task(s): 95
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #93 has finished. Remaining task(s): 94
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #94 has finished. Remaining task(s): 93
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #95 has finished. Remaining task(s): 92
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #96 has finished. Remaining task(s): 91
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #97 has finished. Remaining task(s): 90
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #98 has finished. Remaining task(s): 89
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #99 has finished. Remaining task(s): 88
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #100 has finished. Remaining task(s): 87
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #101 has finished. Remaining task(s): 86
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #102 has finished. Remaining task(s): 85
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #103 has finished. Remaining task(s): 84
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #104 has finished. Remaining task(s): 83
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #105 has finished. Remaining task(s): 82
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #106 has finished. Remaining task(s): 81
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #107 has finished. Remaining task(s): 80
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #108 has finished. Remaining task(s): 79
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #109 has finished. Remaining task(s): 78
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #110 has finished. Remaining task(s): 77
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #111 has finished. Remaining task(s): 76
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #112 has finished. Remaining task(s): 75
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #113 has finished. Remaining task(s): 74
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #114 has finished. Remaining task(s): 73
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #115 has finished. Remaining task(s): 72
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #116 has finished. Remaining task(s): 71
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #117 has finished. Remaining task(s): 70
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #118 has finished. Remaining task(s): 69
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #119 has finished. Remaining task(s): 68
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #120 has finished. Remaining task(s): 67
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #121 has finished. Remaining task(s): 66
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #122 has finished. Remaining task(s): 65
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #123 has finished. Remaining task(s): 64
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #124 has finished. Remaining task(s): 63
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #125 has finished. Remaining task(s): 62
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #126 has finished. Remaining task(s): 61
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #127 has finished. Remaining task(s): 60
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #128 has finished. Remaining task(s): 59
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #129 has finished. Remaining task(s): 58
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #130 has finished. Remaining task(s): 57
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #131 has finished. Remaining task(s): 56
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #132 has finished. Remaining task(s): 55
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #133 has finished. Remaining task(s): 54
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #134 has finished. Remaining task(s): 53
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #135 has finished. Remaining task(s): 52
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #136 has finished. Remaining task(s): 51
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #137 has finished. Remaining task(s): 50
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #138 has finished. Remaining task(s): 49
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #139 has finished. Remaining task(s): 48
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #140 has finished. Remaining task(s): 47
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #141 has finished. Remaining task(s): 46
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #142 has finished. Remaining task(s): 45
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #143 has finished. Remaining task(s): 44
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #144 has finished. Remaining task(s): 43
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #145 has finished. Remaining task(s): 42
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #146 has finished. Remaining task(s): 41
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #147 has finished. Remaining task(s): 40
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #148 has finished. Remaining task(s): 39
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #149 has finished. Remaining task(s): 38
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #150 has finished. Remaining task(s): 37
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #151 has finished. Remaining task(s): 36
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #152 has finished. Remaining task(s): 35
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #153 has finished. Remaining task(s): 34
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #154 has finished. Remaining task(s): 33
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #155 has finished. Remaining task(s): 32
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #156 has finished. Remaining task(s): 31
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #157 has finished. Remaining task(s): 30
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #158 has finished. Remaining task(s): 29
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #159 has finished. Remaining task(s): 28
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #160 has finished. Remaining task(s): 27
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #161 has finished. Remaining task(s): 26
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #162 has finished. Remaining task(s): 25
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #163 has finished. Remaining task(s): 24
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #164 has finished. Remaining task(s): 23
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #165 has finished. Remaining task(s): 22
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #166 has finished. Remaining task(s): 21
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #167 has finished. Remaining task(s): 20
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #168 has finished. Remaining task(s): 19
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #169 has finished. Remaining task(s): 18
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #170 has finished. Remaining task(s): 17
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #171 has finished. Remaining task(s): 16
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #172 has finished. Remaining task(s): 15
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #173 has finished. Remaining task(s): 14
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #174 has finished. Remaining task(s): 13
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #175 has finished. Remaining task(s): 12
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #176 has finished. Remaining task(s): 11
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #177 has finished. Remaining task(s): 10
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #178 has finished. Remaining task(s): 9
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #179 has finished. Remaining task(s): 8
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #180 has finished. Remaining task(s): 7
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #181 has finished. Remaining task(s): 6
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #182 has finished. Remaining task(s): 5
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #183 has finished. Remaining task(s): 4
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #184 has finished. Remaining task(s): 3
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #185 has finished. Remaining task(s): 2
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #186 has finished. Remaining task(s): 1
[15:17:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #187 has finished. Remaining task(s): 0
[15:21:04] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_multiply_add_nn_relu
[15:21:04] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_max_pool2d
[15:21:04] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu
[15:21:04] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu
[15:21:05] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform
[15:21:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate
[15:21:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu
[15:21:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_1
[15:21:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_1
[15:21:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_1
[15:21:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_2
[15:21:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_2
[15:21:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_1
[15:21:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_3
[15:21:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_3
[15:21:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_2
[15:21:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_4
[15:21:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_4
[15:21:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_3
[15:21:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_5
[15:21:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_multiply_add_multiply_add_nn_relu
[15:21:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d
[15:21:10] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_avg_pool2d
[15:21:10] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_2
[15:21:10] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_6
[15:21:10] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform_1
[15:21:10] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_5
[15:21:10] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_3
[15:21:10] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_7
[15:21:10] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_6
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_4
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_8
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_7
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_4
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_9
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_8
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_5
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_10
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_9
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_6
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_11
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_10
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_5
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_12
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_11
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_6
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_13
[15:21:11] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_12
[15:21:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_7
[15:21:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_14
[15:21:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_13
[15:21:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_8
[15:21:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_15
[15:21:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_14
[15:21:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_7
[15:21:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_16
[15:21:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_15
[15:21:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_8
[15:21:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_17
[15:21:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_multiply_add_multiply_add_nn_relu_1
[15:21:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_1
[15:21:13] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_avg_pool2d_1
[15:21:13] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_9
[15:21:13] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_18
[15:21:13] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform_2
[15:21:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_16
[15:21:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_10
[15:21:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_19
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_17
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_11
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_20
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_18
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_12
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_21
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_19
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_9
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_22
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_20
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_13
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_23
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_21
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_10
[15:21:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_24
[15:21:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_22
[15:21:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_14
[15:21:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_25
[15:21:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_23
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_15
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_26
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_24
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_11
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_27
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_25
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_16
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_28
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_26
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_12
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_29
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_27
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_13
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_30
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_28
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_17
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_31
[15:21:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_29
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_14
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_32
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_30
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_15
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_33
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_31
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_18
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_34
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_32
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_16
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_35
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_33
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_19
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_36
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_34
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_17
[15:21:20] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_37
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_35
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_20
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_38
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_36
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_18
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_39
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_37
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_19
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_40
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_38
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_20
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_41
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_multiply_add_nn_relu
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_2
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_avg_pool2d_2
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_21
[15:21:21] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_42
[15:21:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_contrib_conv2d_winograd_without_weight_transform_3
[15:21:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_39
[15:21:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_nn_relu_22
[15:21:22] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_43
[15:21:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_40
[15:21:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_21
[15:21:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_44
[15:21:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_41
[15:21:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_22
[15:21:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_45
[15:21:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_42
[15:21:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_23
[15:21:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_46
[15:21:23] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_43
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_24
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_47
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_44
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_25
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_multiply_add_nn_relu_1
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_45
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_26
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_48
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_46
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_multiply_add_multiply_add_nn_relu_27
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_nn_relu_49
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_47
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 800, 1, 1), "float32"], T_relu: T.Buffer[(1, 800, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 800, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 800, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 800, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 800, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_28(placeholder: T.Buffer[(1, 800, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 800, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 800, 1, 1), "float32"], T_relu: T.Buffer[(1, 800, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_i2_i3_fused_0 in T.thread_binding(39, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_i1_i2_i3_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("T_multiply"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(800, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) // 49)
                    ax2 = T.axis.spatial(7, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 49 // 7)
                    ax3 = T.axis.spatial(7, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 7)
                    T.where(i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1 < 39200)
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.reverse_compute_inline(block=b1)", "sch.reverse_compute_inline(block=b0)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "l7, l8, l9, l10 = sch.get_loops(block=b6)", "l11 = sch.fuse(l7, l8, l9, l10)", "l12, l13 = sch.split(loop=l11, factors=[None, 1024])", "sch.bind(loop=l12, thread_axis=\"blockIdx.x\")", "sch.bind(loop=l13, thread_axis=\"threadIdx.x\")", "b14 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b14, ann_key=\"meta_schedule.unroll_explicit\")", "b15, = sch.get_child_blocks(b14)", "l16, l17 = sch.get_loops(block=b15)", "sch.annotate(block_or_loop=l16, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l16, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 800, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 800, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 800, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 800, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 800, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 800, 7, 7], "float32"], ["TENSOR", [128, 800, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_nn_relu_50(placeholder: T.Buffer[(1, 800, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 800, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 800, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 800, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(196, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    with T.block("conv2d_nchw_init"):
                        nn = T.axis.spatial(1, 0)
                        ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                        yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7)
                        xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                        T.reads()
                        T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 800, 7, 7], "float32"], ["TENSOR", [128, 800, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(800, i4_0 * 50 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1))
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 50)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(50):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 50)
                                    v1 = T.axis.spatial(800, i4_0 * 50 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 50)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(25, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7)
                                xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                rc = T.axis.reduce(800, i4_0 * 50 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 800, 7, 7], "float32"], ["TENSOR", [128, 800, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 49 * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 + ax2)
                            v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:21:24] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 32, 1, 1])", "l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])", "l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 25, 2])", "l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])", "v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])", "v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])", "sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"blockIdx.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"vthread.x\")", "l72 = sch.fuse(l19, l29, l39, l49)", "sch.bind(loop=l72, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)", "b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)", "l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)", "l85 = sch.fuse(l81, l82, l83, l84)", "v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v86)", "b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)", "l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)", "l98 = sch.fuse(l94, l95, l96, l97)", "v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v99)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v100)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)", "l108, l109 = sch.split(loop=l107, factors=[None, 32])", "sch.bind(loop=l109, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\")", "l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)", "l117, l118 = sch.split(loop=l116, factors=[None, 32])", "sch.bind(loop=l118, thread_axis=\"threadIdx.x\")", "b119 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b119, ann_key=\"meta_schedule.unroll_explicit\")", "b120, b121, b122, b123 = sch.get_child_blocks(b119)", "l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)", "sch.annotate(block_or_loop=l124, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l124, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l132, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l132, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l140, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l140, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l160, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l160, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b167 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)", "b188 = sch.decompose_reduction(block=b167, loop=l171)"]
[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_48
[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 832, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 832, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 832, 1, 1), "float32"], T_relu: T.Buffer[(1, 832, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 832, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 832, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 832, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 832, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_29(placeholder: T.Buffer[(1, 832, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 832, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 832, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 832, 1, 1), "float32"], T_relu: T.Buffer[(1, 832, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_i2_i3_fused_0 in T.thread_binding(40, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_i1_i2_i3_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("T_multiply"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(832, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) // 49)
                    ax2 = T.axis.spatial(7, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 49 // 7)
                    ax3 = T.axis.spatial(7, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 7)
                    T.where(i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1 < 40768)
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.reverse_compute_inline(block=b1)", "sch.reverse_compute_inline(block=b0)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "l7, l8, l9, l10 = sch.get_loops(block=b6)", "l11 = sch.fuse(l7, l8, l9, l10)", "l12, l13 = sch.split(loop=l11, factors=[None, 1024])", "sch.bind(loop=l12, thread_axis=\"blockIdx.x\")", "sch.bind(loop=l13, thread_axis=\"threadIdx.x\")", "b14 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b14, ann_key=\"meta_schedule.unroll_explicit\")", "b15, = sch.get_child_blocks(b14)", "l16, l17 = sch.get_loops(block=b15)", "sch.annotate(block_or_loop=l16, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l16, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 832, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 832, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 832, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 832, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 7, 7], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_nn_relu_51(placeholder: T.Buffer[(1, 832, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 832, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 832, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 832, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3_init)
                            yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 7, 7], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(13, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(832, i4_0 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 7)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 64)
                                    v1 = T.axis.spatial(832, i4_0 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 64)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 2048)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(64, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3)
                                yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                rc = T.axis.reduce(832, i4_0 * 64 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 832, 7, 7], "float32"], ["TENSOR", [128, 832, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 16, 2, 1])", "l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])", "l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[13, 64, 1])", "l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])", "v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])", "v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])", "sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"blockIdx.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"vthread.x\")", "l72 = sch.fuse(l19, l29, l39, l49)", "sch.bind(loop=l72, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)", "b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)", "l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)", "l85 = sch.fuse(l81, l82, l83, l84)", "v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v86)", "b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)", "l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)", "l98 = sch.fuse(l94, l95, l96, l97)", "v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v99)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v100)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)", "l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2])", "sch.vectorize(loop=l110)", "sch.bind(loop=l109, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\")", "l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)", "l118, l119 = sch.split(loop=l117, factors=[None, 112])", "sch.bind(loop=l119, thread_axis=\"threadIdx.x\")", "b120 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b120, ann_key=\"meta_schedule.unroll_explicit\")", "b121, b122, b123, b124 = sch.get_child_blocks(b120)", "l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l125, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l125, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l134, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l134, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l142, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l142, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)", "sch.annotate(block_or_loop=l162, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l162, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b169 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)", "b190 = sch.decompose_reduction(block=b169, loop=l173)"]
[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_49
[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 864, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 864, 1, 1), "float32"], T_relu: T.Buffer[(1, 864, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 864, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 864, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 864, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 864, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_30(placeholder: T.Buffer[(1, 864, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 864, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 864, 1, 1), "float32"], T_relu: T.Buffer[(1, 864, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_i2_i3_fused_0 in T.thread_binding(42, thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("T_multiply"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(864, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) // 49)
                    ax2 = T.axis.spatial(7, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 49 // 7)
                    ax3 = T.axis.spatial(7, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 7)
                    T.where(i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1 < 42336)
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.reverse_compute_inline(block=b1)", "sch.reverse_compute_inline(block=b0)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "l7, l8, l9, l10 = sch.get_loops(block=b6)", "l11 = sch.fuse(l7, l8, l9, l10)", "l12, l13 = sch.split(loop=l11, factors=[None, 1024])", "sch.bind(loop=l12, thread_axis=\"blockIdx.x\")", "sch.bind(loop=l13, thread_axis=\"threadIdx.x\")", "b14 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b14, ann_key=\"meta_schedule.unroll_explicit\")", "b15, = sch.get_child_blocks(b14)", "l16, l17 = sch.get_loops(block=b15)"]
[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 864, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 864, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 864, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 864, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 864, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 864, 7, 7], "float32"], ["TENSOR", [128, 864, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_nn_relu_52(placeholder: T.Buffer[(1, 864, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 864, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 864, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 864, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_3_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3_init)
                            yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 864, 7, 7], "float32"], ["TENSOR", [128, 864, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(9, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(864, i4_0 * 96 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 7)
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 96)
                                        v1 = T.axis.spatial(864, i4_0 * 96 + (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 96)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 6144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 2, 1, 1, 6, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3)
                                yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(864, i4_0 * 96 + i4_1 * 6 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 864, 7, 7], "float32"], ["TENSOR", [128, 864, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:21:25] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 32, 2, 1])", "l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])", "l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[9, 16, 6])", "l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])", "v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])", "v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])", "sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"blockIdx.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"vthread.x\")", "l72 = sch.fuse(l19, l29, l39, l49)", "sch.bind(loop=l72, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)", "b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)", "l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)", "l85 = sch.fuse(l81, l82, l83, l84)", "v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v86)", "b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)", "l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)", "l98 = sch.fuse(l94, l95, l96, l97)", "v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v99)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v100)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)", "l108, l109 = sch.split(loop=l107, factors=[None, 224])", "sch.bind(loop=l109, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\")", "l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)", "l117, l118, l119 = sch.split(loop=l116, factors=[None, 224, 4])", "sch.vectorize(loop=l119)", "sch.bind(loop=l118, thread_axis=\"threadIdx.x\")", "b120 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b120, ann_key=\"meta_schedule.unroll_explicit\")", "b121, b122, b123, b124 = sch.get_child_blocks(b120)", "l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l125, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l125, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l133, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l133, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l142, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l142, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)", "sch.annotate(block_or_loop=l162, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l162, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b169 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)", "b190 = sch.decompose_reduction(block=b169, loop=l173)"]
[15:21:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_50
[15:21:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 896, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 896, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 896, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 896, 1, 1), "float32"], T_relu: T.Buffer[(1, 896, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 896, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 896, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 896, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 896, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:21:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:21:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_31(placeholder: T.Buffer[(1, 896, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 896, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 896, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 896, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 896, 1, 1), "float32"], T_relu: T.Buffer[(1, 896, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_i2_i3_fused_0 in T.thread_binding(43, thread="blockIdx.x"):
            for i0_i1_i2_i3_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("T_multiply"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(896, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) // 49)
                    ax2 = T.axis.spatial(7, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 49 // 7)
                    ax3 = T.axis.spatial(7, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 7)
                    T.where(i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1 < 43904)
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

[15:21:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:21:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.reverse_compute_inline(block=b1)", "sch.reverse_compute_inline(block=b0)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "l7, l8, l9, l10 = sch.get_loops(block=b6)", "l11 = sch.fuse(l7, l8, l9, l10)", "l12, l13 = sch.split(loop=l11, factors=[None, 1024])", "sch.bind(loop=l12, thread_axis=\"blockIdx.x\")", "sch.bind(loop=l13, thread_axis=\"threadIdx.x\")", "b14 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b14, ann_key=\"meta_schedule.unroll_explicit\")", "b15, = sch.get_child_blocks(b14)", "l16, l17 = sch.get_loops(block=b15)"]
[15:21:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 896, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 896, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 896, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 896, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 896, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 896, 7, 7], "float32"], ["TENSOR", [128, 896, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:21:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:21:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_nn_relu_53(placeholder: T.Buffer[(1, 896, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 896, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 896, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 896, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_4_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4_init)
                            yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 896, 7, 7], "float32"], ["TENSOR", [128, 896, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(7, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(896, i4_0 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 7)
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(37):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 128)
                                    v1 = T.axis.spatial(896, i4_0 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 128)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 4096)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(128, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4)
                                yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(896, i4_0 * 128 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 896, 7, 7], "float32"], ["TENSOR", [128, 896, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

[15:21:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:21:26] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 16, 1, 2])", "l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])", "l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[7, 128, 1])", "l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])", "v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])", "v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])", "sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"blockIdx.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"vthread.x\")", "l72 = sch.fuse(l19, l29, l39, l49)", "sch.bind(loop=l72, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)", "b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)", "l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)", "l85 = sch.fuse(l81, l82, l83, l84)", "v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v86)", "b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)", "l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)", "l98 = sch.fuse(l94, l95, l96, l97)", "v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v99)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v100)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)", "l108, l109 = sch.split(loop=l107, factors=[None, 112])", "sch.bind(loop=l109, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\")", "l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)", "l117, l118 = sch.split(loop=l116, factors=[None, 112])", "sch.bind(loop=l118, thread_axis=\"threadIdx.x\")", "b119 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b119, ann_key=\"meta_schedule.unroll_explicit\")", "b120, b121, b122, b123 = sch.get_child_blocks(b119)", "l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)", "sch.annotate(block_or_loop=l124, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l124, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l132, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l132, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l140, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l140, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l160, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l160, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b167 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)", "b188 = sch.decompose_reduction(block=b167, loop=l171)"]
[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_51
[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 928, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 928, 1, 1), "float32"], T_relu: T.Buffer[(1, 928, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 928, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 928, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 928, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 928, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_32(placeholder: T.Buffer[(1, 928, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 928, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 928, 1, 1), "float32"], T_relu: T.Buffer[(1, 928, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_i2_i3_fused_0 in T.thread_binding(45, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_i1_i2_i3_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("T_multiply"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(928, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) // 49)
                    ax2 = T.axis.spatial(7, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 49 // 7)
                    ax3 = T.axis.spatial(7, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 7)
                    T.where(i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1 < 45472)
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.reverse_compute_inline(block=b1)", "sch.reverse_compute_inline(block=b0)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "l7, l8, l9, l10 = sch.get_loops(block=b6)", "l11 = sch.fuse(l7, l8, l9, l10)", "l12, l13 = sch.split(loop=l11, factors=[None, 1024])", "sch.bind(loop=l12, thread_axis=\"blockIdx.x\")", "sch.bind(loop=l13, thread_axis=\"threadIdx.x\")", "b14 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b14, ann_key=\"meta_schedule.unroll_explicit\")", "b15, = sch.get_child_blocks(b14)", "l16, l17 = sch.get_loops(block=b15)", "sch.annotate(block_or_loop=l16, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l16, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 928, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 928, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 928, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 928, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 928, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 928, 7, 7], "float32"], ["TENSOR", [128, 928, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_nn_relu_54(placeholder: T.Buffer[(1, 928, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 928, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 928, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 928, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused // 7)
                            xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 928, 7, 7], "float32"], ["TENSOR", [128, 928, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(116):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(928, i4_0 * 116 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 116)
                                        v1 = T.axis.spatial(928, i4_0 * 116 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 116)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 464)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(116, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused // 7)
                                xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(928, i4_0 * 116 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 928, 7, 7], "float32"], ["TENSOR", [128, 928, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused // 7 + ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 1, 2, 2])", "l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])", "l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 116, 1])", "l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])", "v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])", "v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])", "sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"blockIdx.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"vthread.x\")", "l72 = sch.fuse(l19, l29, l39, l49)", "sch.bind(loop=l72, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)", "b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)", "l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)", "l85 = sch.fuse(l81, l82, l83, l84)", "v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v86)", "b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)", "l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)", "l98 = sch.fuse(l94, l95, l96, l97)", "v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v99)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v100)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)", "l108, l109 = sch.split(loop=l107, factors=[None, 49])", "sch.bind(loop=l109, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\")", "l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)", "l117, l118, l119 = sch.split(loop=l116, factors=[None, 49, 2])", "sch.vectorize(loop=l119)", "sch.bind(loop=l118, thread_axis=\"threadIdx.x\")", "b120 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b120, ann_key=\"meta_schedule.unroll_explicit\")", "b121, b122, b123, b124 = sch.get_child_blocks(b120)", "l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)", "l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)", "l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)", "l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)", "b169 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)", "b190 = sch.decompose_reduction(block=b169, loop=l173)"]
[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_52
[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_33(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 960, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_i2_i3_fused_0 in T.thread_binding(46, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_i1_i2_i3_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("T_multiply"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(960, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) // 49)
                    ax2 = T.axis.spatial(7, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 49 // 7)
                    ax3 = T.axis.spatial(7, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 7)
                    T.where(i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1 < 47040)
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.reverse_compute_inline(block=b1)", "sch.reverse_compute_inline(block=b0)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "l7, l8, l9, l10 = sch.get_loops(block=b6)", "l11 = sch.fuse(l7, l8, l9, l10)", "l12, l13 = sch.split(loop=l11, factors=[None, 1024])", "sch.bind(loop=l12, thread_axis=\"blockIdx.x\")", "sch.bind(loop=l13, thread_axis=\"threadIdx.x\")", "b14 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b14, ann_key=\"meta_schedule.unroll_explicit\")", "b15, = sch.get_child_blocks(b14)", "l16, l17 = sch.get_loops(block=b15)", "sch.annotate(block_or_loop=l16, ann_key=\"pragma_auto_unroll_max_step\", ann_val=64)", "sch.annotate(block_or_loop=l16, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 960, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [128, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_nn_relu_55(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 960, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(49, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused // 7)
                            xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [128, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(960, i4_0 * 30 + ax0_ax1_ax2_ax3_fused_1)
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused // 7)
                                    v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 30)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(120):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 30)
                                    v1 = T.axis.spatial(960, i4_0 * 30 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 30)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(15, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused // 7)
                                xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                rc = T.axis.reduce(960, i4_0 * 30 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [128, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused // 7 + ax2)
                            v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:21:27] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 32, 2, 1])", "l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])", "l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 15, 2])", "l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])", "v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])", "v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])", "sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"blockIdx.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"vthread.x\")", "l72 = sch.fuse(l19, l29, l39, l49)", "sch.bind(loop=l72, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)", "b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)", "l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)", "l85 = sch.fuse(l81, l82, l83, l84)", "v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v86)", "b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)", "l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)", "l98 = sch.fuse(l94, l95, l96, l97)", "v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v99)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v100)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)", "l108, l109 = sch.split(loop=l107, factors=[None, 32])", "sch.bind(loop=l109, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\")", "l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)", "l117, l118 = sch.split(loop=l116, factors=[None, 32])", "sch.bind(loop=l118, thread_axis=\"threadIdx.x\")", "b119 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b119, ann_key=\"meta_schedule.unroll_explicit\")", "b120, b121, b122, b123 = sch.get_child_blocks(b119)", "l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)", "sch.annotate(block_or_loop=l124, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l124, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l132, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l132, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l140, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l140, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l160, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l160, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b167 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)", "b188 = sch.decompose_reduction(block=b167, loop=l171)"]
[15:21:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_53
[15:21:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 992, 1, 1), "float32"], T_relu: T.Buffer[(1, 992, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 992, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 992, 7, 7], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 992, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 992, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3])
                T_multiply_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] * placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply_1[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_multiply_1[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:21:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:21:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_multiply_add_multiply_add_nn_relu_34(placeholder: T.Buffer[(1, 992, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 992, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 992, 1, 1), "float32"], T_relu: T.Buffer[(1, 992, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_i2_i3_fused_0 in T.thread_binding(48, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_i1_i2_i3_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("T_multiply"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(992, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) // 49)
                    ax2 = T.axis.spatial(7, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 49 // 7)
                    ax3 = T.axis.spatial(7, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 7)
                    T.where(i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1 < 48608)
                    T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[ax0, ax1, 0, 0], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max((placeholder[ax0, ax1, ax2, ax3] * placeholder_1[ax0, ax1, 0, 0] + placeholder_2[ax0, ax1, 0, 0]) * placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, 0, 0], T.float32(0))
    

[15:21:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:21:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b1 = sch.get_block(name=\"T_multiply_1\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.reverse_compute_inline(block=b1)", "sch.reverse_compute_inline(block=b0)", "v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v5)", "sch.enter_postproc()", "b6 = sch.get_block(name=\"T_multiply\", func_name=\"main\")", "l7, l8, l9, l10 = sch.get_loops(block=b6)", "l11 = sch.fuse(l7, l8, l9, l10)", "l12, l13 = sch.split(loop=l11, factors=[None, 1024])", "sch.bind(loop=l12, thread_axis=\"blockIdx.x\")", "sch.bind(loop=l13, thread_axis=\"threadIdx.x\")", "b14 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b14, ann_key=\"meta_schedule.unroll_explicit\")", "b15, = sch.get_child_blocks(b14)", "l16, l17 = sch.get_loops(block=b15)", "sch.annotate(block_or_loop=l16, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l16, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[15:21:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 992, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 992, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 992, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 992, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 7, 7, 992, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 992, 7, 7], "float32"], ["TENSOR", [128, 992, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[15:21:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:21:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_nn_relu_56(placeholder: T.Buffer[(1, 992, 7, 7), "float32"], placeholder_1: T.Buffer[(128, 992, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_relu: T.Buffer[(1, 128, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 992, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 992, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_4_init in T.serial(7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            xx = T.axis.spatial(7, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 992, 7, 7], "float32"], ["TENSOR", [128, 992, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(992, i4_0 * 248 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 7)
                                        v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1736)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(62):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 248)
                                        v1 = T.axis.spatial(992, i4_0 * 248 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 248)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 1, 1, 31, 1, 1, 1, 1, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                xx = T.axis.spatial(7, i3_4)
                                rc = T.axis.reduce(992, i4_0 * 248 + i4_1 * 31 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 992, 7, 7], "float32"], ["TENSOR", [128, 992, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

[15:21:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:21:28] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_relu\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 32, 1, 1])", "l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])", "l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])", "l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 8, 31])", "l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])", "v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])", "v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])", "sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"blockIdx.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"vthread.x\")", "l72 = sch.fuse(l19, l29, l39, l49)", "sch.bind(loop=l72, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)", "b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)", "l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)", "l85 = sch.fuse(l81, l82, l83, l84)", "v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v86)", "b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)", "l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)", "l98 = sch.fuse(l94, l95, l96, l97)", "v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v99)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v100)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)", "l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])", "sch.vectorize(loop=l110)", "sch.bind(loop=l109, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\")", "l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)", "l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4])", "sch.vectorize(loop=l120)", "sch.bind(loop=l119, thread_axis=\"threadIdx.x\")", "b121 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b121, ann_key=\"meta_schedule.unroll_explicit\")", "b122, b123, b124, b125 = sch.get_child_blocks(b121)", "l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l126, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l126, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l135, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l135, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)", "sch.annotate(block_or_loop=l144, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l144, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)", "sch.annotate(block_or_loop=l164, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l164, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b171 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)", "b192 = sch.decompose_reduction(block=b171, loop=l175)"]
[15:21:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_concatenate_multiply_add_multiply_add_nn_relu_2
[15:21:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_global_avg_pool2d
[15:21:29] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add
https://storage.cloud.google.com/octoml-aquarium-models/onnx_model_zoo/vision_classification_densenet.onnx
file existed. Skipping downloading.
/home/yj/models/densenet.onnx
Starting to build with relay.
/home/yj/anaconda3/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
The result is correct!
