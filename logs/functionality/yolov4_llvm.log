nohup: ignoring input
Starting to build with relay.
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
[19:06:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"
[19:06:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 27, 27, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 27, 27, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 27 and 1 <= i3_1 and i3_1 < 27, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 13, 13, 4, 256, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4] * T.float32(0.10000000149011612))
    

[19:06:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 27, 27, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 27, 27, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 27 and 1 <= i3_1 and i3_1 < 27, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 13, 1, 1, 1, 32, 1, 13, 1, 256, 3, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_1_1 * 4 + i1_2 * 2 + i1_3)
                    oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSRRR", [i2_0, i3_1_1, i4_3, i5_0, i6_0, i7_0])
                    T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
                with T.block("T_leaky_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                    T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 32, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[256, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:06:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 27, 27, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1 in T.grid(1, 1, 13, 1, 1, 1, 32, 1, 13):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 3, 3, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(27, i2_0 * 2 + ax2)
                        i3 = T.axis.spatial(27, i3_1 * 2 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 27 and 1 <= i3 and i3 < 27, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1 in T.serial(1):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 3, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 4):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSRRR", [i2_0, i3_1, i4_3, i5_0, i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 1, 4):
                        with T.block("T_leaky_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(128, i1_1 * 4 + ax1)
                            ax2_1 = T.axis.spatial(13, i2_0 + ax2)
                            ax3_1 = T.axis.spatial(13, i3_1 + ax3)
                            ax4_1 = T.axis.spatial(4, ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 32, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[256, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[19:06:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 27, 27, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 13, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0 in T.grid(1, 32, 1, 13, 1, 256, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 3, 1):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(64, i5_0 // 4 + ax1)
                            i2 = T.axis.spatial(27, i2_0 * 2 + i6_0 + ax2)
                            i3 = T.axis.spatial(27, i3_1 * 2 + ax3)
                            i4 = T.axis.spatial(4, i5_0 % 4 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 27 and 1 <= i3 and i3 < 27, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 4):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSRRR", [i2_0, i3_1, i4_3, i5_0, i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 1, 13, 4):
                    with T.block("T_leaky_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(13, i2_0 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 32, 2, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[256, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[19:06:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #1: "fused_concatenate"
[19:06:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 128, 13, 13, 4), "float32"], T_concat: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(128 <= ax1, placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[19:06:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 128, 13, 13, 4), "float32"], T_concat: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(128 <= ax1, placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[19:06:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #2: "fused_layout_transform"
[19:06:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], T_layout_trans: T.Buffer[(1, 512, 13, 13, 2), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 512, 13, 13, 2):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 2 + ax4 < 1024 and ax2 < 13 and ax3 < 13, placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4], T.float32(0), dtype="float32")
    

[19:06:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], T_layout_trans: T.Buffer[(1, 512, 13, 13, 2), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 512, 13, 13, 2):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 2 + ax4 < 1024 and ax2 < 13 and ax3 < 13, placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[19:06:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #3: "fused_nn_contrib_conv2d_NCHWc_add"
[19:06:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13, 2), "float32"], placeholder_1: T.Buffer[(85, 512, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 85, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 85, 13, 13, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 85, 13, 13, 3], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 85, 13, 13, 3, 1024, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 13, 13, 2], "float32"], ["TENSOR", [85, 512, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 85, 13, 13, 3):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:06:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13, 2), "float32"], placeholder_1: T.Buffer[(85, 512, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 85, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 85, 13, 13, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 85, 13, 13, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 85, 1, 1, 3, 1, 1, 1, 13, 1, 256, 1, 1, 1, 1, 13, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk, oh, ow, oc_block = T.axis.remap("SSSS", [i1_0, i2_2, i3_1, i4_0])
                    ic = T.axis.reduce(1024, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 13, 13, 2], "float32"], ["TENSOR", [85, 512, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 85, 13, 13, 3):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[85, 1, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[256, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:06:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13, 2), "float32"], placeholder_1: T.Buffer[(85, 512, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 85, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 85, 13, 13, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 85, 13, 13, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 85, 1, 1, 3, 1, 1, 1, 13, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 1, 13, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk, oh, ow, oc_block = T.axis.remap("SSSS", [i1_0, i2_2, i3_1, i4_0])
                        ic = T.axis.reduce(1024, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 13, 13, 2], "float32"], ["TENSOR", [85, 512, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 13, 1, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(85, i1_0 + ax1)
                        ax2_1 = T.axis.spatial(13, ax2)
                        ax3_1 = T.axis.spatial(13, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(3, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[85, 1, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[256, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13, 2), "float32"], placeholder_1: T.Buffer[(85, 512, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 85, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 85, 13, 13, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 85, 13, 13, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 85, 1, 1, 3):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 13, 1, 256, 1, 1, 1, 1, 13, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk, oh, ow, oc_block = T.axis.remap("SSSS", [i1_0, i2_2, i3_1, i4_0])
                        ic = T.axis.reduce(1024, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 13, 13, 2], "float32"], ["TENSOR", [85, 512, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 13, 13, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(85, i1_0 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(3, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[85, 1, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[256, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"
[19:06:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 85, 13, 13, 3), "float32"], T_concat: T.Buffer[(1, 13, 13, 3, 85), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 255, 13, 13], dtype="float32")
        T_transpose = T.alloc_buffer([1, 13, 13, 255], dtype="float32")
        T_reshape = T.alloc_buffer([1, 13, 13, 3, 85], dtype="float32")
        T_split = T.alloc_buffer([1, 13, 13, 3, 80], dtype="float32")
        T_sigmoid = T.alloc_buffer([1, 13, 13, 3, 80], dtype="float32")
        T_split_1 = T.alloc_buffer([1, 13, 13, 3, 1], dtype="float32")
        T_sigmoid_1 = T.alloc_buffer([1, 13, 13, 3, 1], dtype="float32")
        T_split_2 = T.alloc_buffer([1, 13, 13, 3, 4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 255, 13, 13):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 3, ax2, ax3, ax1 % 3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 255 and ax2 < 13 and ax3 < 13, placeholder[ax0, ax1 // 3, ax2, ax3, ax1 % 3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 13, 13, 255):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_layout_trans[ax0, ax3, ax1, ax2])
                T.writes(T_transpose[ax0, ax1, ax2, ax3])
                T_transpose[ax0, ax1, ax2, ax3] = T_layout_trans[ax0, ax3, ax1, ax2]
        for i0, i1, i2, i3, i4 in T.grid(1, 13, 13, 3, 85):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_transpose[0, ((ax2 * 255 + ax3 * 85 + ax4) // 3315 + ax1) % 13, ((ax3 * 85 + ax4) // 255 + ax2) % 13, (ax3 * 85 + ax4) % 255])
                T.writes(T_reshape[ax0, ax1, ax2, ax3, ax4])
                T_reshape[ax0, ax1, ax2, ax3, ax4] = T_transpose[0, ((ax2 * 255 + ax3 * 85 + ax4) // 3315 + ax1) % 13, ((ax3 * 85 + ax4) // 255 + ax2) % 13, (ax3 * 85 + ax4) % 255]
        for i0, i1, i2, i3, i4 in T.grid(1, 13, 13, 3, 80):
            with T.block("T_split"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape[ax0, ax1, ax2, ax3, ax4 + 5])
                T.writes(T_split[ax0, ax1, ax2, ax3, ax4])
                T_split[ax0, ax1, ax2, ax3, ax4] = T_reshape[ax0, ax1, ax2, ax3, ax4 + 5]
        for i0, i1, i2, i3, i4 in T.grid(1, 13, 13, 3, 80):
            with T.block("T_sigmoid"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_split[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_sigmoid[ax0, ax1, ax2, ax3, ax4])
                T_sigmoid[ax0, ax1, ax2, ax3, ax4] = T.sigmoid(T_split[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 13, 13, 3, 1):
            with T.block("T_split_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape[ax0, ax1, ax2, ax3, ax4 + 4])
                T.writes(T_split_1[ax0, ax1, ax2, ax3, ax4])
                T_split_1[ax0, ax1, ax2, ax3, ax4] = T_reshape[ax0, ax1, ax2, ax3, ax4 + 4]
        for i0, i1, i2, i3, i4 in T.grid(1, 13, 13, 3, 1):
            with T.block("T_sigmoid_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_split_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_sigmoid_1[ax0, ax1, ax2, ax3, ax4])
                T_sigmoid_1[ax0, ax1, ax2, ax3, ax4] = T.sigmoid(T_split_1[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 13, 13, 3, 4):
            with T.block("T_split_2"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_split_2[ax0, ax1, ax2, ax3, ax4])
                T_split_2[ax0, ax1, ax2, ax3, ax4] = T_reshape[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 13, 13, 3, 85):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_sigmoid[ax0, ax1, ax2, ax3, ax4 - 5], T_sigmoid_1[ax0, ax1, ax2, ax3, ax4 - 4], T_split_2[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(5 <= ax4, T_sigmoid[ax0, ax1, ax2, ax3, ax4 - 5], T.if_then_else(4 <= ax4, T_sigmoid_1[ax0, ax1, ax2, ax3, ax4 - 4], T_split_2[ax0, ax1, ax2, ax3, ax4], dtype="float32"), dtype="float32")
    

[19:06:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 85, 13, 13, 3), "float32"], T_concat: T.Buffer[(1, 13, 13, 3, 85), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 255, 13, 13], dtype="float32")
            T_transpose = T.alloc_buffer([1, 13, 13, 255], dtype="float32")
            T_reshape = T.alloc_buffer([1, 13, 13, 3, 85], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 13, 13, 255):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("T_layout_trans"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(255, i3 + ax1)
                        ax2_1 = T.axis.spatial(13, i1 + ax2)
                        ax3_1 = T.axis.spatial(13, i2 + ax3)
                        T.reads(placeholder[ax0_1, ax1_1 // 3, ax2_1, ax3_1, ax1_1 % 3])
                        T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 255 and ax2_1 < 13 and ax3_1 < 13, placeholder[ax0_1, ax1_1 // 3, ax2_1, ax3_1, ax1_1 % 3], T.float32(0), dtype="float32")
                with T.block("T_transpose"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(T_layout_trans[ax0, ax3, ax1, ax2])
                    T.writes(T_transpose[ax0, ax1, ax2, ax3])
                    T_transpose[ax0, ax1, ax2, ax3] = T_layout_trans[ax0, ax3, ax1, ax2]
            for i0, i1, i2, i3, i4 in T.grid(1, 13, 13, 3, 85):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_reshape"):
                        ax0_2 = T.axis.spatial(1, ax0)
                        ax1_2 = T.axis.spatial(13, i1 + ax1)
                        ax2_2 = T.axis.spatial(13, i2 + ax2)
                        ax3_2 = T.axis.spatial(3, i3 + ax3)
                        ax4_1 = T.axis.spatial(85, i4 + ax4)
                        T.reads(T_transpose[0, ((ax2_2 * 255 + ax3_2 * 85 + ax4_1) // 3315 + ax1_2) % 13, ((ax3_2 * 85 + ax4_1) // 255 + ax2_2) % 13, (ax3_2 * 85 + ax4_1) % 255])
                        T.writes(T_reshape[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1])
                        T_reshape[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1] = T_transpose[0, ((ax2_2 * 255 + ax3_2 * 85 + ax4_1) // 3315 + ax1_2) % 13, ((ax3_2 * 85 + ax4_1) // 255 + ax2_2) % 13, (ax3_2 * 85 + ax4_1) % 255]
                with T.block("T_concat"):
                    ax0_3, ax1_3, ax2_3, ax3_3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(T_reshape[ax0_3, ax1_3, ax2_3, ax3_3, ax4])
                    T.writes(T_concat[ax0_3, ax1_3, ax2_3, ax3_3, ax4])
                    T_concat[ax0_3, ax1_3, ax2_3, ax3_3, ax4] = T.if_then_else(5 <= ax4, T.sigmoid(T_reshape[ax0_3, ax1_3, ax2_3, ax3_3, ax4 - 5 + 5], dtype="float32"), T.if_then_else(4 <= ax4, T.sigmoid(T_reshape[ax0_3, ax1_3, ax2_3, ax3_3, ax4 - 4 + 4], dtype="float32"), T_reshape[ax0_3, ax1_3, ax2_3, ax3_3, ax4], dtype="float32"), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape", func_name="main")
b3 = sch.get_block(name="T_split", func_name="main")
b4 = sch.get_block(name="T_sigmoid", func_name="main")
b5 = sch.get_block(name="T_split_1", func_name="main")
b6 = sch.get_block(name="T_sigmoid_1", func_name="main")
b7 = sch.get_block(name="T_split_2", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b7)
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.vectorize", ann_val=64)
v9 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v9)
l10 = sch.sample_compute_location(block=b2, decision=4)
sch.compute_at(block=b2, loop=l10, preserve_unit_loops=True)
l11 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True)
l12 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l12, preserve_unit_loops=True)
[19:06:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"
[19:06:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 53, 53, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 53, 53, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 26, 26, 4, 128, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4] * T.float32(0.10000000149011612))
    

[19:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 53, 53, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 2, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 53, 27, 4):
                    with T.block("data_pad"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(53, i3_0 * 26 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 13, 2, 32, 3, 1, 1, 4, 1, 1, 1, 4, 1, 3, 1, 8, 26, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_2 * 8 + i1_3)
                        oh = T.axis.spatial(26, i2_3)
                        ow = T.axis.spatial(26, i3_0 * 13 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
                with T.block("T_leaky_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                    T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 26])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 13, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 53, 53, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 2, 2, 1, 1, 1, 13, 2):
                for i5_0, i6_0 in T.grid(32, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 51, 3, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(32, i5_0 + ax1)
                            i2 = T.axis.spatial(53, i6_0 + ax2)
                            i3 = T.axis.spatial(53, i3_0 * 26 + i3_1 * 2 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 1, 1, 1, 4, 1, 3, 1, 8, 26, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_2 * 8 + i1_3)
                            oh = T.axis.spatial(26, i2_3)
                            ow = T.axis.spatial(26, i3_0 * 13 + i3_1)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 26, 1, 1):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + ax1)
                        ax2_1 = T.axis.spatial(26, ax2)
                        ax3_1 = T.axis.spatial(26, i3_0 * 13 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 26])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 13, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[19:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 53, 53, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 2, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 13, 2):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 53, 3, 4):
                        with T.block("data_pad"):
                            i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                            i3 = T.axis.spatial(53, i3_0 * 26 + i3_1 * 2 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 3, 1, 1, 4, 1, 1, 1, 4, 1, 3, 1, 8, 26, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_2 * 8 + i1_3)
                            oh = T.axis.spatial(26, i2_3)
                            ow = T.axis.spatial(26, i3_0 * 13 + i3_1)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 26, 13, 2):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + ax1)
                        ax2_1 = T.axis.spatial(26, ax2)
                        ax3_1 = T.axis.spatial(26, i3_0 * 13 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 26])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 13, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[19:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #6: "fused_concatenate_1"
[19:06:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(1, 64, 26, 26, 4), "float32"], T_concat: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(64 <= ax1, placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[19:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(1, 64, 26, 26, 4), "float32"], T_concat: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(64 <= ax1, placeholder_1[ax0, ax1 - 64, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[19:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #7: "fused_layout_transform_1"
[19:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], T_layout_trans: T.Buffer[(1, 256, 26, 26, 2), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 26, 26, 2):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 2 + ax4 < 512 and ax2 < 26 and ax3 < 26, placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4], T.float32(0), dtype="float32")
    

[19:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], T_layout_trans: T.Buffer[(1, 256, 26, 26, 2), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 26, 26, 2):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 2 + ax4 < 512 and ax2 < 26 and ax3 < 26, placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[19:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"
[19:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 26, 26, 2), "float32"], placeholder_1: T.Buffer[(85, 256, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 85, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 85, 26, 26, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 85, 26, 26, 3], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 85, 26, 26, 3, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 26, 26, 2], "float32"], ["TENSOR", [85, 256, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 85, 26, 26, 3):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 26, 26, 2), "float32"], placeholder_1: T.Buffer[(85, 256, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 85, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 85, 26, 26, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 85, 26, 26, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 85, 13, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 1, 1, 13, 1, 32, 1, 1, 1, 1, 2, 2, 3):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(85, i1_0)
                    oh = T.axis.spatial(26, i2_0 * 2 + i2_3)
                    ow = T.axis.spatial(26, i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(3, i4_3)
                    ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 26, 26, 2], "float32"], ["TENSOR", [85, 256, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 85, 26, 26, 3):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[85, 1, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[13, 1, 1, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 3])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[16, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 26, 26, 2), "float32"], placeholder_1: T.Buffer[(85, 256, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 85, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 85, 26, 26, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 85, 26, 26, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 85, 13, 1, 1, 1, 1, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 1, 1, 13, 1, 32, 1, 1, 1, 1, 2, 2, 3):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(85, i1_0)
                        oh = T.axis.spatial(26, i2_0 * 2 + i2_3)
                        ow = T.axis.spatial(26, i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(3, i4_3)
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 26, 26, 2], "float32"], ["TENSOR", [85, 256, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 2, 26, 3):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(85, i1_0 + ax1)
                        ax2_1 = T.axis.spatial(26, i2_0 * 2 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[85, 1, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[13, 1, 1, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 3])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[16, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 26, 26, 2), "float32"], placeholder_1: T.Buffer[(85, 256, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 85, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 85, 26, 26, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 85, 26, 26, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 85, 13, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 16, 1, 1, 1, 1, 1, 13, 1, 32, 1, 1, 1, 1, 2, 2, 3):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(85, i1_0)
                        oh = T.axis.spatial(26, i2_0 * 2 + i2_3)
                        ow = T.axis.spatial(26, i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(3, i4_3)
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 26, 26, 2], "float32"], ["TENSOR", [85, 256, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 2, 26, 3):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(85, i1_0 + ax1)
                        ax2_1 = T.axis.spatial(26, i2_0 * 2 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[85, 1, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[13, 1, 1, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 3])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[16, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"
[19:06:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 85, 26, 26, 3), "float32"], T_concat: T.Buffer[(1, 26, 26, 3, 85), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 255, 26, 26], dtype="float32")
        T_transpose = T.alloc_buffer([1, 26, 26, 255], dtype="float32")
        T_reshape = T.alloc_buffer([1, 26, 26, 3, 85], dtype="float32")
        T_split = T.alloc_buffer([1, 26, 26, 3, 80], dtype="float32")
        T_sigmoid = T.alloc_buffer([1, 26, 26, 3, 80], dtype="float32")
        T_split_1 = T.alloc_buffer([1, 26, 26, 3, 1], dtype="float32")
        T_sigmoid_1 = T.alloc_buffer([1, 26, 26, 3, 1], dtype="float32")
        T_split_2 = T.alloc_buffer([1, 26, 26, 3, 4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 255, 26, 26):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 3, ax2, ax3, ax1 % 3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 255 and ax2 < 26 and ax3 < 26, placeholder[ax0, ax1 // 3, ax2, ax3, ax1 % 3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 26, 26, 255):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_layout_trans[ax0, ax3, ax1, ax2])
                T.writes(T_transpose[ax0, ax1, ax2, ax3])
                T_transpose[ax0, ax1, ax2, ax3] = T_layout_trans[ax0, ax3, ax1, ax2]
        for i0, i1, i2, i3, i4 in T.grid(1, 26, 26, 3, 85):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_transpose[0, ((ax2 * 255 + ax3 * 85 + ax4) // 6630 + ax1) % 26, ((ax3 * 85 + ax4) // 255 + ax2) % 26, (ax3 * 85 + ax4) % 255])
                T.writes(T_reshape[ax0, ax1, ax2, ax3, ax4])
                T_reshape[ax0, ax1, ax2, ax3, ax4] = T_transpose[0, ((ax2 * 255 + ax3 * 85 + ax4) // 6630 + ax1) % 26, ((ax3 * 85 + ax4) // 255 + ax2) % 26, (ax3 * 85 + ax4) % 255]
        for i0, i1, i2, i3, i4 in T.grid(1, 26, 26, 3, 80):
            with T.block("T_split"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape[ax0, ax1, ax2, ax3, ax4 + 5])
                T.writes(T_split[ax0, ax1, ax2, ax3, ax4])
                T_split[ax0, ax1, ax2, ax3, ax4] = T_reshape[ax0, ax1, ax2, ax3, ax4 + 5]
        for i0, i1, i2, i3, i4 in T.grid(1, 26, 26, 3, 80):
            with T.block("T_sigmoid"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_split[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_sigmoid[ax0, ax1, ax2, ax3, ax4])
                T_sigmoid[ax0, ax1, ax2, ax3, ax4] = T.sigmoid(T_split[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 26, 26, 3, 1):
            with T.block("T_split_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape[ax0, ax1, ax2, ax3, ax4 + 4])
                T.writes(T_split_1[ax0, ax1, ax2, ax3, ax4])
                T_split_1[ax0, ax1, ax2, ax3, ax4] = T_reshape[ax0, ax1, ax2, ax3, ax4 + 4]
        for i0, i1, i2, i3, i4 in T.grid(1, 26, 26, 3, 1):
            with T.block("T_sigmoid_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_split_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_sigmoid_1[ax0, ax1, ax2, ax3, ax4])
                T_sigmoid_1[ax0, ax1, ax2, ax3, ax4] = T.sigmoid(T_split_1[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 26, 26, 3, 4):
            with T.block("T_split_2"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_split_2[ax0, ax1, ax2, ax3, ax4])
                T_split_2[ax0, ax1, ax2, ax3, ax4] = T_reshape[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 26, 26, 3, 85):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_sigmoid[ax0, ax1, ax2, ax3, ax4 - 5], T_sigmoid_1[ax0, ax1, ax2, ax3, ax4 - 4], T_split_2[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(5 <= ax4, T_sigmoid[ax0, ax1, ax2, ax3, ax4 - 5], T.if_then_else(4 <= ax4, T_sigmoid_1[ax0, ax1, ax2, ax3, ax4 - 4], T_split_2[ax0, ax1, ax2, ax3, ax4], dtype="float32"), dtype="float32")
    

[19:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 85, 26, 26, 3), "float32"], T_concat: T.Buffer[(1, 26, 26, 3, 85), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 255, 26, 26], dtype="float32")
            T_transpose = T.alloc_buffer([1, 26, 26, 255], dtype="float32")
            T_reshape = T.alloc_buffer([1, 26, 26, 3, 85], dtype="float32")
            for i0, i1, i2 in T.grid(1, 26, 26):
                for ax0, ax1, ax2, ax3 in T.grid(1, 255, 1, 1):
                    with T.block("T_layout_trans"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(26, i1 + ax2)
                        ax3_1 = T.axis.spatial(26, i2 + ax3)
                        T.reads(placeholder[ax0_1, ax1_1 // 3, ax2_1, ax3_1, ax1_1 % 3])
                        T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 255 and ax2_1 < 26 and ax3_1 < 26, placeholder[ax0_1, ax1_1 // 3, ax2_1, ax3_1, ax1_1 % 3], T.float32(0), dtype="float32")
                for i3 in T.serial(255):
                    with T.block("T_transpose"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(T_layout_trans[ax0, ax3, ax1, ax2])
                        T.writes(T_transpose[ax0, ax1, ax2, ax3])
                        T_transpose[ax0, ax1, ax2, ax3] = T_layout_trans[ax0, ax3, ax1, ax2]
            for i0, i1, i2, i3 in T.grid(1, 26, 26, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 85):
                    with T.block("T_reshape"):
                        ax0_2 = T.axis.spatial(1, ax0)
                        ax1_2 = T.axis.spatial(26, i1 + ax1)
                        ax2_2 = T.axis.spatial(26, i2 + ax2)
                        ax3_2 = T.axis.spatial(3, i3 + ax3)
                        ax4_1 = T.axis.spatial(85, ax4)
                        T.reads(T_transpose[0, ((ax2_2 * 255 + ax3_2 * 85 + ax4_1) // 6630 + ax1_2) % 26, ((ax3_2 * 85 + ax4_1) // 255 + ax2_2) % 26, (ax3_2 * 85 + ax4_1) % 255])
                        T.writes(T_reshape[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1])
                        T_reshape[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1] = T_transpose[0, ((ax2_2 * 255 + ax3_2 * 85 + ax4_1) // 6630 + ax1_2) % 26, ((ax3_2 * 85 + ax4_1) // 255 + ax2_2) % 26, (ax3_2 * 85 + ax4_1) % 255]
                for i4 in T.serial(85):
                    with T.block("T_concat"):
                        ax0_3, ax1_3, ax2_3, ax3_3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(T_reshape[ax0_3, ax1_3, ax2_3, ax3_3, ax4])
                        T.writes(T_concat[ax0_3, ax1_3, ax2_3, ax3_3, ax4])
                        T_concat[ax0_3, ax1_3, ax2_3, ax3_3, ax4] = T.if_then_else(5 <= ax4, T.sigmoid(T_reshape[ax0_3, ax1_3, ax2_3, ax3_3, ax4 - 5 + 5], dtype="float32"), T.if_then_else(4 <= ax4, T.sigmoid(T_reshape[ax0_3, ax1_3, ax2_3, ax3_3, ax4 - 4 + 4], dtype="float32"), T_reshape[ax0_3, ax1_3, ax2_3, ax3_3, ax4], dtype="float32"), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape", func_name="main")
b3 = sch.get_block(name="T_split", func_name="main")
b4 = sch.get_block(name="T_sigmoid", func_name="main")
b5 = sch.get_block(name="T_split_1", func_name="main")
b6 = sch.get_block(name="T_sigmoid_1", func_name="main")
b7 = sch.get_block(name="T_split_2", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b7)
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.vectorize", ann_val=64)
v9 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v9)
l10 = sch.sample_compute_location(block=b2, decision=3)
sch.compute_at(block=b2, loop=l10, preserve_unit_loops=True)
l11 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True)
l12 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l12, preserve_unit_loops=True)
[19:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #10: "fused_nn_max_pool2d"
[19:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 17, 17, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 17, 17, 4):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2 - 2, ax3 - 2, ax4])
                T.writes(pad_temp[ax0, ax1, ax2, ax3, ax4])
                pad_temp[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(2 <= ax2 and ax2 < 15 and 2 <= ax3 and ax3 < 15, placeholder[ax0, ax1, ax2 - 2, ax3 - 2, ax4], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 13, 13, 4, 5, 5):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], pad_temp[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
    

[19:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 128, 17, 17, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 128, 13, 13, 4, 1], dtype="float32")
            for i0, i1 in T.grid(1, 128):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 17, 17, 4):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 2, ax3_1 - 2, ax4_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(2 <= ax2_1 and ax2_1 < 15 and 2 <= ax3_1 and ax3_1 < 15, placeholder[ax0_1, ax1_1, ax2_1 - 2, ax3_1 - 2, ax4_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(13, 13, 4, 1, 25):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_0 = T.axis.spatial(1, i5_i6_fused_0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                        rv0 = T.axis.reduce(5, i5_i6_fused_1 // 5)
                        rv1 = T.axis.reduce(5, i5_i6_fused_1 % 5)
                        T.reads(pad_temp[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                        with T.init():
                            tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.float32(-3.4028234663852886e+38)
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], pad_temp[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
            for i0, i1, i2, i3, i4, i5_i6_fused_0 in T.grid(1, 128, 13, 13, 4, 1):
                with T.block("tensor"):
                    vi5_i6_fused_0 = T.axis.reduce(1, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 25])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
l17 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
[19:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 128, 17, 17, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 128, 13, 13, 4, 25], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 128, 13, 13):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 5, 5, 4):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1 + ax1)
                        ax2_1 = T.axis.spatial(17, i2 + ax2)
                        ax3_1 = T.axis.spatial(17, i3 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 2, ax3_1 - 2, ax4_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(2 <= ax2_1 and ax2_1 < 15 and 2 <= ax3_1 and ax3_1 < 15, placeholder[ax0_1, ax1_1, ax2_1 - 2, ax3_1 - 2, ax4_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(4, 1, 25):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_1 = T.axis.spatial(25, i5_i6_fused_1)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                        T.reads(pad_temp[ax0, ax1, ax2 + vi5_i6_fused_1 // 5, ax3 + vi5_i6_fused_1 % 5, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        with T.init():
                            tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(-3.4028234663852886e+38)
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], pad_temp[ax0, ax1, ax2 + vi5_i6_fused_1 // 5, ax3 + vi5_i6_fused_1 % 5, ax4])
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 128, 13, 13, 4, 1, 25):
                with T.block("tensor"):
                    vi5_i6_fused_1 = T.axis.reduce(25, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 25])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l14, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
l17 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
[19:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 13, 13, 4, 5, 5):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder[ax0, ax1, ax2 + rv0 - 2, ax3 + rv1 - 2, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], T.if_then_else(2 <= ax2 + rv0 and ax2 + rv0 < 15 and 2 <= ax3 + rv1 and ax3 + rv1 < 15, placeholder[ax0, ax1, ax2 + rv0 - 2, ax3 + rv1 - 2, ax4], T.float32(-3.4028234663852886e+38), dtype="float32"))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[19:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #11: "fused_nn_max_pool2d_1"
[19:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 21, 21, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 21, 21, 4):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2 - 4, ax3 - 4, ax4])
                T.writes(pad_temp[ax0, ax1, ax2, ax3, ax4])
                pad_temp[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(4 <= ax2 and ax2 < 17 and 4 <= ax3 and ax3 < 17, placeholder[ax0, ax1, ax2 - 4, ax3 - 4, ax4], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 13, 13, 4, 9, 9):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], pad_temp[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
    

[19:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 128, 21, 21, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 128, 13, 13, 4, 3], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0 in T.grid(1, 128, 13, 13, 4, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 3, 9, 1):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1 + ax1)
                        ax2_1 = T.axis.spatial(21, i5_i6_fused_0 * 3 + i2 + ax2)
                        ax3_1 = T.axis.spatial(21, i3 + ax3)
                        ax4_1 = T.axis.spatial(4, i4 + ax4)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 4, ax3_1 - 4, ax4_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(4 <= ax2_1 and ax2_1 < 17 and 4 <= ax3_1 and ax3_1 < 17, placeholder[ax0_1, ax1_1, ax2_1 - 4, ax3_1 - 4, ax4_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i5_i6_fused_1 in T.serial(27):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_0 = T.axis.spatial(3, i5_i6_fused_0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4, vi5_i6_fused_1 = T.axis.remap("SSSSR", [i1, i2, i3, i4, i5_i6_fused_1])
                        T.reads(pad_temp[ax0, ax1, ax2 + (vi5_i6_fused_0 * 27 + vi5_i6_fused_1) // 9, ax3 + (vi5_i6_fused_0 * 27 + vi5_i6_fused_1) % 9, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                        with T.init():
                            tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.float32(-3.4028234663852886e+38)
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], pad_temp[ax0, ax1, ax2 + (vi5_i6_fused_0 * 27 + vi5_i6_fused_1) // 9, ax3 + (vi5_i6_fused_0 * 27 + vi5_i6_fused_1) % 9, ax4])
            for i0, i1, i2, i3, i4, i5_i6_fused_0 in T.grid(1, 128, 13, 13, 4, 3):
                with T.block("tensor"):
                    vi5_i6_fused_0 = T.axis.reduce(3, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 27])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
l17 = sch.sample_compute_location(block=b0, decision=5)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
[19:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 128, 21, 21, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 128, 13, 13, 4, 27], dtype="float32")
            for i0, i1 in T.grid(1, 128):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 21, 21, 4):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 4, ax3_1 - 4, ax4_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(4 <= ax2_1 and ax2_1 < 17 and 4 <= ax3_1 and ax3_1 < 17, placeholder[ax0_1, ax1_1, ax2_1 - 4, ax3_1 - 4, ax4_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(13, 13, 4, 3, 27):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_1 = T.axis.spatial(27, i5_i6_fused_1)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4, vi5_i6_fused_0 = T.axis.remap("SSSSR", [i1, i2, i3, i4, i5_i6_fused_0])
                        T.reads(pad_temp[ax0, ax1, ax2 + (vi5_i6_fused_0 * 27 + vi5_i6_fused_1) // 9, ax3 + (vi5_i6_fused_0 * 27 + vi5_i6_fused_1) % 9, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        with T.init():
                            tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(-3.4028234663852886e+38)
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], pad_temp[ax0, ax1, ax2 + (vi5_i6_fused_0 * 27 + vi5_i6_fused_1) // 9, ax3 + (vi5_i6_fused_0 * 27 + vi5_i6_fused_1) % 9, ax4])
            for i0, i1, i2, i3, i4, i5_i6_fused_1 in T.grid(1, 128, 13, 13, 4, 27):
                with T.block("tensor"):
                    vi5_i6_fused_1 = T.axis.reduce(27, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 27])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l14, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
l17 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
[19:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 128, 21, 21, 4], dtype="float32")
            for i0, i1, i2 in T.grid(1, 128, 13):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 9, 21, 4):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1 + ax1)
                        ax2_1 = T.axis.spatial(21, i2 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 4, ax3_1 - 4, ax4_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(4 <= ax2_1 and ax2_1 < 17 and 4 <= ax3_1 and ax3_1 < 17, placeholder[ax0_1, ax1_1, ax2_1 - 4, ax3_1 - 4, ax4_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i3, i4, i5, i6 in T.grid(13, 4, 9, 9):
                    with T.block("tensor"):
                        ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                        T.reads(pad_temp[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
                        T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                        with T.init():
                            tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], pad_temp[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[19:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"
[19:06:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 27, 27, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 27, 27, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 27 and 1 <= i3_1 and i3_1 < 27, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 256, 13, 13, 4, 512, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [256, 128, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 4, 1, 1, 1, 1, 1, 16, 3, 1, 1, 4, 1, 13, 1, 32, 1, 3, 1, 32, 13, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i1_0 * 128 + i1_2 * 32 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_2, i4_0])
                    ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                    T.reads(placeholder[n, ic // 4, oh * 2 + kh - 1, ow * 2 + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [256, 128, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh * 2 + kh and oh * 2 + kh < 27 and 1 <= ow * 2 + kw and ow * 2 + kw < 27, placeholder[n, ic // 4, oh * 2 + kh - 1, ow * 2 + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 4, 32])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[19:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 27, 27, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 27, 27, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 27 and 1 <= i3_1 and i3_1 < 27, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 2, 1, 1, 4, 1, 1, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 3, 1, 1, 4, 1, 13, 1, 32, 1, 3, 1, 32, 13, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_0 * 128 + i1_2 * 32 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_2, i4_0])
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [256, 128, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 13, 13, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1_0 * 128 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 4, 32])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 27, 27, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 27, 27, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 27 and 1 <= i3 and i3 < 27, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 16, 3, 1, 1, 4, 1, 13, 1, 32, 1, 3, 1, 32, 13, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_0 * 128 + i1_2 * 32 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_2, i4_0])
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [256, 128, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 13, 13, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1_0 * 128 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 4, 32])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"
[19:06:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 13, 13, 4, 1024, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 32, 1, 1, 2, 64, 1, 1, 1, 1, 13, 13, 1, 16, 1, 1, 1, 4, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_1 * 4 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_2, i3_2])
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(1024, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 32, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 32, 1, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 1, 13, 13, 1, 16, 1, 1, 1, 4, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_1 * 4 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_2])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(1024, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 13, 13, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_1 * 4 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 32, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 32, 1, 1, 2, 64, 1, 1, 1, 1, 13, 13, 1, 16, 1, 1, 1, 4, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_1 * 4 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_2])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(1024, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 13, 13, 4):
                    with T.block("T_add"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 32, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"
[19:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_log = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_tanh = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 512 and ax2 < 13 and ax3 < 13, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 512 and ax2 < 13 and ax3 < 13, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
    

[19:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0, i1, i2 in T.grid(1, 128, 13):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 13, 4):
                    for ax0_1, ax1_1, ax2_1, ax3_1 in T.grid(1, 1, 1, 1):
                        with T.block("T_layout_trans"):
                            ax0_2 = T.axis.spatial(1, ax0_1)
                            ax1_2 = T.axis.spatial(512, i1 * 4 + ax4 + ax1_1)
                            ax2_2 = T.axis.spatial(13, i2 + ax2_1)
                            ax3_2 = T.axis.spatial(13, ax3 + ax3_1)
                            T.reads(placeholder[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4])
                            T.writes(T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_2])
                            T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_2] = T.if_then_else(ax0_2 < 1 and ax1_2 < 512 and ax2_2 < 13 and ax3_2 < 13, T.exp(placeholder[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4], dtype="float32"), T.float32(0), dtype="float32")
                    with T.block("T_layout_trans_1"):
                        ax0_3 = T.axis.spatial(1, ax0)
                        ax1_3 = T.axis.spatial(128, i1 + ax1)
                        ax2_3 = T.axis.spatial(13, i2 + ax2)
                        ax3_3, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder_1[ax0_3, 0, 0, 0], T_layout_trans[ax0_3, ax1_3 * 4 + ax4_1, ax2_3, ax3_3])
                        T.writes(T_layout_trans_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4_1])
                        T_layout_trans_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4_1] = T.if_then_else(ax0_3 < 1 and ax1_3 * 4 + ax4_1 < 512 and ax2_3 < 13 and ax3_3 < 13, T.tanh(T.log(placeholder_1[ax0_3, 0, 0, 0] + T_layout_trans[ax0_3, ax1_3 * 4 + ax4_1, ax2_3, ax3_3], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                for i3, i4 in T.grid(13, 4):
                    with T.block("T_multiply"):
                        ax0_4, ax1_4, ax2_4, ax3_4, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder[ax0_4, ax1_4, ax2_4, ax3_4, ax4], T_layout_trans_1[ax0_4, ax1_4, ax2_4, ax3_4, ax4])
                        T.writes(T_multiply[ax0_4, ax1_4, ax2_4, ax3_4, ax4])
                        T_multiply[ax0_4, ax1_4, ax2_4, ax3_4, ax4] = placeholder[ax0_4, ax1_4, ax2_4, ax3_4, ax4] * T_layout_trans_1[ax0_4, ax1_4, ax2_4, ax3_4, ax4]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=2)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=7)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
[19:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"
[19:06:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 15, 15, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 13, 13, 4, 512, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:06:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 15, 15, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 1, 4, 1, 4, 1, 1, 1, 256, 3, 1, 1, 4, 1, 13, 1, 2, 1, 3, 1, 1, 13, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 16 + i1_1_1 * 4 + i1_2)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_2, i4_0])
                    ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                    T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 4, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[19:06:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 15, 15, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 8, 1, 1, 4, 1, 4, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 3, 1, 1, 4, 1, 13, 1, 2, 1, 3, 1, 1, 13, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 16 + i1_1_1 * 4 + i1_2)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_2, i4_0])
                        ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 13, 13, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 16 + i1_1_1 * 4 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 4, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:06:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 1, 1, 4):
                for i0_1, i1_1 in T.grid(1, 4):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 15, 15, 4):
                        with T.block("data_pad"):
                            i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 256, 3, 1, 1, 4, 1, 13, 1, 2, 1, 3, 1, 1, 13, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 16 + i1_1 * 4 + i1_2)
                            oh, ow, oc_block = T.axis.remap("SSS", [i2_3, i3_2, i4_0])
                            ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [128, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 13, 13, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 16 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 4, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:06:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"
[19:06:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 13, 13, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_log = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_tanh = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 512 and ax2 < 13 and ax3 < 13, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 512 and ax2 < 13 and ax3 < 13, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4], T_multiply[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = placeholder_2[ax0, ax1, ax2, ax3, ax4] + T_multiply[ax0, ax1, ax2, ax3, ax4]
    

[19:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 13, 13, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
                with T.block("T_exp"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                    T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 512 and ax2 < 13 and ax3 < 13, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 128, 13, 13):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 4):
                    with T.block("T_layout_trans_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1 + ax1)
                        ax2_1 = T.axis.spatial(13, i2 + ax2)
                        ax3_1 = T.axis.spatial(13, i3 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(placeholder_1[ax0_1, 0, 0, 0], T_layout_trans[ax0_1, ax1_1 * 4 + ax4_1, ax2_1, ax3_1])
                        T.writes(T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(ax0_1 < 1 and ax1_1 * 4 + ax4_1 < 512 and ax2_1 < 13 and ax3_1 < 13, T.tanh(T.log(placeholder_1[ax0_1, 0, 0, 0] + T_layout_trans[ax0_1, ax1_1 * 4 + ax4_1, ax2_1, ax3_1], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                for i4 in T.serial(4):
                    with T.block("T_add_1"):
                        ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = placeholder_2[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v8 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v8)
l9 = sch.sample_compute_location(block=b5, decision=3)
sch.compute_at(block=b5, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l10, preserve_unit_loops=True)
l11 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l11, preserve_unit_loops=True)
[19:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"
[19:06:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 13, 13, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 1, 1, 2, 1, 1, 13, 1, 2, 64, 1, 1, 1, 2, 1, 1, 1, 8, 1, 1, 1, 4, 1, 13, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_2 * 4 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_1, i3_3])
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    ic = T.axis.reduce(512, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[16, 1, 2, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 1, 1, 2, 1, 1, 13, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 2, 1, 1, 1, 8, 1, 1, 1, 4, 1, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_2 * 4 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_3])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(512, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 13, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(13, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(13, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[16, 1, 2, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 16, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 13, 1, 2, 64, 1, 1, 1, 2, 1, 1, 1, 8, 1, 1, 1, 4, 1, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 8 + i1_2 * 4 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_3])
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(512, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 13, 13, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 8 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[16, 1, 2, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"
[19:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 13, 13, 4), "float32"], T_concat: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_log = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_tanh = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        T_exp_1 = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        T_layout_trans_2 = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_log_1 = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_tanh_1 = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_layout_trans_3 = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder_2[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 512 and ax2 < 13 and ax3 < 13, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 512 and ax2 < 13 and ax3 < 13, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder_2[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_exp_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp_1[ax0, ax1, ax2, ax3, ax4])
                T_exp_1[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_layout_trans_2"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_2[ax0, ax1, ax2, ax3])
                T_layout_trans_2[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 512 and ax2 < 13 and ax3 < 13, T_exp_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans_2[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans_2[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_log_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_log_1[ax0, ax1, ax2, ax3])
                T_log_1[ax0, ax1, ax2, ax3] = T.log(T_add_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_tanh_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log_1[ax0, ax1, ax2, ax3])
                T.writes(T_tanh_1[ax0, ax1, ax2, ax3])
                T_tanh_1[ax0, ax1, ax2, ax3] = T.tanh(T_log_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_layout_trans_3"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_3[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_3[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 512 and ax2 < 13 and ax3 < 13, T_tanh_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_3[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1 - 128, ax2, ax3, ax4], T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(128 <= ax1, T_multiply[ax0, ax1 - 128, ax2, ax3, ax4], T_multiply_1[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[19:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 128, 13, 13, 4), "float32"], T_concat: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            T_layout_trans_2 = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
            T_layout_trans_3 = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_exp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1 // 4 + ax1)
                        ax2_1 = T.axis.spatial(13, i2 + ax2)
                        ax3_1 = T.axis.spatial(13, i3 + ax3)
                        ax4_1 = T.axis.spatial(4, i1 % 4 + ax4)
                        T.reads(placeholder_2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.exp(placeholder_2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], dtype="float32")
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 512 and ax2 < 13 and ax3 < 13, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 128, 13, 13):
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                    with T.block("T_layout_trans_2"):
                        ax0_2 = T.axis.spatial(1, ax0)
                        ax1_2 = T.axis.spatial(512, i1 * 4 + ax1)
                        ax2_2 = T.axis.spatial(13, i2 + ax2)
                        ax3_2 = T.axis.spatial(13, i3 + ax3)
                        T.reads(placeholder[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4])
                        T.writes(T_layout_trans_2[ax0_2, ax1_2, ax2_2, ax3_2])
                        T_layout_trans_2[ax0_2, ax1_2, ax2_2, ax3_2] = T.if_then_else(ax0_2 < 1 and ax1_2 < 512 and ax2_2 < 13 and ax3_2 < 13, T.exp(placeholder[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4], dtype="float32"), T.float32(0), dtype="float32")
                for i4 in T.serial(4):
                    with T.block("T_layout_trans_3"):
                        ax0_3, ax1_3, ax2_3, ax3_3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder_1[ax0_3, 0, 0, 0], T_layout_trans_2[ax0_3, ax1_3 * 4 + ax4, ax2_3, ax3_3])
                        T.writes(T_layout_trans_3[ax0_3, ax1_3, ax2_3, ax3_3, ax4])
                        T_layout_trans_3[ax0_3, ax1_3, ax2_3, ax3_3, ax4] = T.if_then_else(ax0_3 < 1 and ax1_3 * 4 + ax4 < 512 and ax2_3 < 13 and ax3_3 < 13, T.tanh(T.log(placeholder_1[ax0_3, 0, 0, 0] + T_layout_trans_2[ax0_3, ax1_3 * 4 + ax4, ax2_3, ax3_3], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
                for ax0_4, ax1_4, ax2_4, ax3_4, ax4 in T.grid(1, 1, 1, 1, 4):
                    with T.block("T_layout_trans_1"):
                        ax0_5 = T.axis.spatial(1, ax0_4)
                        ax1_5 = T.axis.spatial(128, i1 - 128 + ax1_4)
                        ax2_5 = T.axis.spatial(13, i2 + ax2_4)
                        ax3_5 = T.axis.spatial(13, i3 + ax3_4)
                        ax4_2 = T.axis.spatial(4, ax4)
                        T.where(128 <= i1)
                        T.reads(placeholder_1[ax0_5, 0, 0, 0], T_layout_trans[ax0_5, ax1_5 * 4 + ax4_2, ax2_5, ax3_5])
                        T.writes(T_layout_trans_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_2])
                        T_layout_trans_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_2] = T.if_then_else(ax0_5 < 1 and ax1_5 * 4 + ax4_2 < 512 and ax2_5 < 13 and ax3_5 < 13, T.tanh(T.log(placeholder_1[ax0_5, 0, 0, 0] + T_layout_trans[ax0_5, ax1_5 * 4 + ax4_2, ax2_5, ax3_5], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                for i4 in T.serial(4):
                    with T.block("T_concat"):
                        ax0_6, ax1_6, ax2_6, ax3_6, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder_2[ax0_6, ax1_6 - 128, ax2_6, ax3_6, ax4_3], T_layout_trans_1[ax0_6, ax1_6 - 128, ax2_6, ax3_6, ax4_3], placeholder[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3], T_layout_trans_3[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3])
                        T.writes(T_concat[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3])
                        T_concat[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3] = T.if_then_else(128 <= ax1_6, placeholder_2[ax0_6, ax1_6 - 128, ax2_6, ax3_6, ax4_3] * T_layout_trans_1[ax0_6, ax1_6 - 128, ax2_6, ax3_6, ax4_3], placeholder[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3] * T_layout_trans_3[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3], dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="T_exp_1", func_name="main")
b8 = sch.get_block(name="T_layout_trans_2", func_name="main")
b9 = sch.get_block(name="T_add_1", func_name="main")
b10 = sch.get_block(name="T_log_1", func_name="main")
b11 = sch.get_block(name="T_tanh_1", func_name="main")
b12 = sch.get_block(name="T_layout_trans_3", func_name="main")
b13 = sch.get_block(name="T_multiply_1", func_name="main")
b14 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b13)
sch.compute_inline(block=b11)
sch.compute_inline(block=b10)
sch.compute_inline(block=b9)
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
l16 = sch.sample_compute_location(block=b12, decision=-1)
sch.compute_at(block=b12, loop=l16, preserve_unit_loops=True)
l17 = sch.sample_compute_location(block=b8, decision=3)
sch.compute_at(block=b8, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b7, decision=-2)
sch.compute_at(block=b7, loop=l18, preserve_unit_loops=True)
l19 = sch.sample_compute_location(block=b5, decision=3)
sch.compute_at(block=b5, loop=l19, preserve_unit_loops=True)
l20 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l21, preserve_unit_loops=True)
[19:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"
[19:06:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 256, 13, 13, 4, 1024, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 1, 4, 13, 13, 2, 32, 1, 1, 1, 4, 1, 1, 2, 32, 1, 1, 1, 8, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i1_0 * 128 + i1_1 * 32 + i1_2 * 8 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_1, i3_1])
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                    ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 4, 4, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1, 1, 4, 13, 13, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 4, 1, 1, 2, 32, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_0 * 128 + i1_1 * 32 + i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_1])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 1, 1, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1_0 * 128 + i1_1 * 32 + ax1)
                        ax2_1 = T.axis.spatial(13, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(13, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 4, 4, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(256, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 13, 13, 2, 32, 1, 1, 1, 4, 1, 1, 2, 32, 1, 1, 1, 8, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_0 * 128 + i1_1 * 32 + i1_2 * 8 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_1, i3_1])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [256, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 13, 13, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1_0 * 128 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 4, 4, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"
[19:06:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 1024, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 1024, 13, 13], dtype="float32")
        T_log = T.alloc_buffer([1, 1024, 13, 13], dtype="float32")
        T_tanh = T.alloc_buffer([1, 1024, 13, 13], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 13, 13):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 1024 and ax2 < 13 and ax3 < 13, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 1024, 13, 13):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 13, 13):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 1024 and ax2 < 13 and ax3 < 13, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
    

[19:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 1024, 13, 13], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
            for i0, i1 in T.grid(1, 256):
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 13, 13):
                    with T.block("T_layout_trans"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(1024, i1 * 4 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                        T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 1024 and ax2_1 < 13 and ax3_1 < 13, T.exp(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], dtype="float32"), T.float32(0), dtype="float32")
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 13, 13, 4):
                    with T.block("T_layout_trans_1"):
                        ax0_2 = T.axis.spatial(1, ax0)
                        ax1_2 = T.axis.spatial(256, i1 + ax1)
                        ax2_2, ax3_2, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder_1[ax0_2, 0, 0, 0], T_layout_trans[ax0_2, ax1_2 * 4 + ax4_1, ax2_2, ax3_2])
                        T.writes(T_layout_trans_1[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1])
                        T_layout_trans_1[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1] = T.if_then_else(ax0_2 < 1 and ax1_2 * 4 + ax4_1 < 1024 and ax2_2 < 13 and ax3_2 < 13, T.tanh(T.log(placeholder_1[ax0_2, 0, 0, 0] + T_layout_trans[ax0_2, ax1_2 * 4 + ax4_1, ax2_2, ax3_2], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                for i2, i3, i4 in T.grid(13, 13, 4):
                    with T.block("T_multiply"):
                        ax0_3, ax1_3, ax2_3, ax3_3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder[ax0_3, ax1_3, ax2_3, ax3_3, ax4], T_layout_trans_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4])
                        T.writes(T_multiply[ax0_3, ax1_3, ax2_3, ax3_3, ax4])
                        T_multiply[ax0_3, ax1_3, ax2_3, ax3_3, ax4] = placeholder[ax0_3, ax1_3, ax2_3, ax3_3, ax4] * T_layout_trans_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=1)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
[19:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #21: "fused_nn_max_pool2d_2"
[19:06:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 25, 25, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 25, 25, 4):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2 - 6, ax3 - 6, ax4])
                T.writes(pad_temp[ax0, ax1, ax2, ax3, ax4])
                pad_temp[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(6 <= ax2 and ax2 < 19 and 6 <= ax3 and ax3 < 19, placeholder[ax0, ax1, ax2 - 6, ax3 - 6, ax4], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 13, 13, 4, 13, 13):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], pad_temp[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
    

[19:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 128, 25, 25, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 128, 13, 13, 4, 169], dtype="float32")
            for i0, i1 in T.grid(1, 128):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 25, 25, 4):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 6, ax3_1 - 6, ax4_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(6 <= ax2_1 and ax2_1 < 19 and 6 <= ax3_1 and ax3_1 < 19, placeholder[ax0_1, ax1_1, ax2_1 - 6, ax3_1 - 6, ax4_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(13, 13, 4, 169, 1):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_0 = T.axis.spatial(169, i5_i6_fused_0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                        T.reads(pad_temp[ax0, ax1, ax2 + vi5_i6_fused_0 // 13, ax3 + vi5_i6_fused_0 % 13, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                        with T.init():
                            tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.float32(-3.4028234663852886e+38)
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], pad_temp[ax0, ax1, ax2 + vi5_i6_fused_0 // 13, ax3 + vi5_i6_fused_0 % 13, ax4])
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 128, 13, 13, 4, 169, 1):
                with T.block("tensor"):
                    vi5_i6_fused_0 = T.axis.reduce(169, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[169, 1])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
l17 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
[19:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 128, 25, 25, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 128, 13, 13, 4, 1], dtype="float32")
            for i0, i1, i2 in T.grid(1, 128, 13):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 13, 25, 4):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1 + ax1)
                        ax2_1 = T.axis.spatial(25, i2 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 6, ax3_1 - 6, ax4_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(6 <= ax2_1 and ax2_1 < 19 and 6 <= ax3_1 and ax3_1 < 19, placeholder[ax0_1, ax1_1, ax2_1 - 6, ax3_1 - 6, ax4_1], T.float32(-3.4028234663852886e+38), dtype="float32")
                for i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(13, 4, 169, 1):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_1 = T.axis.spatial(1, i5_i6_fused_1)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                        rv0 = T.axis.reduce(13, i5_i6_fused_0 // 13)
                        rv1 = T.axis.reduce(13, i5_i6_fused_0 % 13)
                        T.reads(pad_temp[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        with T.init():
                            tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(-3.4028234663852886e+38)
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], pad_temp[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
            for i0, i1, i2, i3, i4, i5_i6_fused_1 in T.grid(1, 128, 13, 13, 4, 1):
                with T.block("tensor"):
                    vi5_i6_fused_1 = T.axis.reduce(1, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[169, 1])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l14, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
l17 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
[19:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], tensor: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 128, 25, 25, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 25, 25, 4):
                with T.block("pad_temp"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2 - 6, ax3 - 6, ax4])
                    T.writes(pad_temp[ax0, ax1, ax2, ax3, ax4])
                    pad_temp[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(6 <= ax2 and ax2 < 19 and 6 <= ax3 and ax3 < 19, placeholder[ax0, ax1, ax2 - 6, ax3 - 6, ax4], T.float32(-3.4028234663852886e+38), dtype="float32")
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 13, 13, 4, 13, 13):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(pad_temp[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], pad_temp[ax0, ax1, ax2 + rv0, ax3 + rv1, ax4])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[19:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #22: "fused_concatenate_2"
[19:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 13, 13, 4), "float32"], T_concat: T.Buffer[(1, 512, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 512, 13, 13, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_3[ax0, ax1 - 384, ax2, ax3, ax4], placeholder_2[ax0, ax1 - 256, ax2, ax3, ax4], placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(384 <= ax1, placeholder_3[ax0, ax1 - 384, ax2, ax3, ax4], T.if_then_else(256 <= ax1, placeholder_2[ax0, ax1 - 256, ax2, ax3, ax4], T.if_then_else(128 <= ax1, placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32"), dtype="float32"), dtype="float32")
    

[19:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_3: T.Buffer[(1, 128, 13, 13, 4), "float32"], T_concat: T.Buffer[(1, 512, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 512, 13, 13, 4):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_3[ax0, ax1 - 384, ax2, ax3, ax4], placeholder_2[ax0, ax1 - 256, ax2, ax3, ax4], placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(384 <= ax1, placeholder_3[ax0, ax1 - 384, ax2, ax3, ax4], T.if_then_else(256 <= ax1, placeholder_2[ax0, ax1 - 256, ax2, ax3, ax4], T.if_then_else(128 <= ax1, placeholder_1[ax0, ax1 - 128, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32"), dtype="float32"), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[19:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"
[19:06:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 512, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 13, 13, 4, 2048, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 13, 13, 4], "float32"], ["TENSOR", [128, 512, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4] * T.float32(0.10000000149011612))
    

[19:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 512, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 13, 1, 1, 1, 32, 1, 13, 2, 128, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 2, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 2 + i1_3)
                    oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(2048, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 13, 13, 4], "float32"], ["TENSOR", [128, 512, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
                with T.block("T_leaky_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                    T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 32, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 512, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 13, 1, 1, 1, 32, 1, 13, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(2048, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 13, 13, 4], "float32"], ["TENSOR", [128, 512, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 1, 1, 2):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(13, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(13, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 32, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[19:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 512, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 13, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 32, 1, 13, 2, 128, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(2048, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 512, 13, 13, 4], "float32"], ["TENSOR", [128, 512, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 1, 13, 4):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + ax1)
                        ax2_1 = T.axis.spatial(13, i2_0 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 32, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[19:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"
[19:06:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 15, 15, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 256, 13, 13, 4, 512, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [256, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 4):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4] * T.float32(0.10000000149011612))
    

[19:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 128, 15, 15, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(13, 1, 1, 1, 32, 1, 1, 4, 64, 1, 3, 1, 1, 1, 13, 1, 8, 3, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_0 * 64 + i1_1 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_0, i3_2, i4_1])
                        ic = T.axis.reduce(512, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [256, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 4):
                with T.block("T_leaky_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                    T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 32, 1, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 15, 15, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 4, 13, 1, 1, 1, 32, 1, 1, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 3, 1, 1, 1, 13, 1, 8, 3, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(256, i1_0 * 64 + i1_1_1 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_0, i3_2, i4_1_1])
                        ic = T.axis.reduce(512, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [256, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 1, 13, 1):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1_0 * 64 + i1_1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(13, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(13, ax3)
                        ax4_1 = T.axis.spatial(4, i4_1_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 32, 1, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[19:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 13, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 32, 1, 1, 4, 64):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 3, 15, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(128, i5_0 * 2 + ax1)
                            i2 = T.axis.spatial(15, i2_0 + ax2)
                            i3, i4 = T.axis.remap("SS", [ax3, ax4])
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 1, 1, 13, 1, 8, 3, 1, 1, 2, 1, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(256, i1_0 * 64 + i1_1 * 2 + i1_3)
                            oh, ow, oc_block = T.axis.remap("SSS", [i2_0, i3_2, i4_1])
                            ic = T.axis.reduce(512, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [256, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 1, 13, 4):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1_0 * 64 + ax1)
                        ax2_1 = T.axis.spatial(13, i2_0 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 32, 1, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[19:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"
[19:06:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 13, 13, 4, 1024, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4] * T.float32(0.10000000149011612))
    

[19:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 1, 1, 13, 13, 1, 32, 1, 1, 1, 2, 1, 1, 1, 32, 1, 1, 1, 32, 1, 1, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_2 * 32 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_1, i4_3])
                    ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 13, 13, 4):
                with T.block("T_leaky_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                    T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 2, 32])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1, 1, 1, 13, 13, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 2, 1, 1, 1, 32, 1, 1, 1, 32, 1, 1, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_2 * 32 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_1, i4_3])
                        ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 1, 1, 4):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + ax1)
                        ax2_1 = T.axis.spatial(13, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(13, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 2, 32])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[19:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 13, 13, 1, 32, 1, 1, 1, 2, 1, 1, 1, 32, 1, 1, 1, 32, 1, 1, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_2 * 32 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_1, i4_3])
                        ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 13, 13, 4):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 2, 32])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[19:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"
[19:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 13, 13, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 13, 13, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 13, 13, 4):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4] * T.float32(0.10000000149011612))
    

[19:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 1, 4, 1, 4, 13, 1, 1, 16, 1, 1, 1, 2, 1, 13, 1, 32, 1, 1, 1, 1, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_0 * 8 + i1_1 * 2 + i1_2)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_2, i4_0])
                    ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 13, 13, 4):
                with T.block("T_leaky_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                    T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 1, 4, 1, 4, 13, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 2, 1, 13, 1, 32, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 8 + i1_1 * 2 + i1_2)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_2, i4_0])
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 1, 13, 1):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 8 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(13, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(13, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[19:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 13, 13, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 1, 1, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 13, 1, 1, 16, 1, 1, 1, 2, 1, 13, 1, 32, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 8 + i1_1 * 2 + i1_2)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_2, i4_0])
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 13, 13, 1):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 8 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[19:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"
[19:06:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 53, 53, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 53, 53, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 26, 26, 4, 256, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 53, 53, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1 in T.grid(1, 2, 2, 1, 2, 1, 16, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 27, 27, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(53, i2_0 * 26 + ax2)
                        i3 = T.axis.spatial(53, i3_1 * 26 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 256, 3, 1, 1, 2, 1, 13, 1, 1, 1, 3, 1, 2, 13, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(26, i2_0 * 13 + i2_3)
                        ow = T.axis.spatial(26, i3_1 * 13 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_1])
                        T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 16, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[19:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 53, 53, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 2, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 27, 53, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(53, i2_0 * 26 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 1, 2, 2):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 3, 1, 1, 2, 1, 13, 1, 1, 1, 3, 1, 2, 13, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(26, i2_0 * 13 + i2_3)
                            ow = T.axis.spatial(26, i3_1 * 13 + i3_2)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 13, 13, 1):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(128, i1_0 * 64 + i1_1 * 4 + ax1)
                            ax2_1 = T.axis.spatial(26, i2_0 * 13 + ax2)
                            ax3_1 = T.axis.spatial(26, i3_1 * 13 + ax3)
                            ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 16, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 53, 53, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 2, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 16, 1, 2, 2, 256):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 27, 27, 1):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(64, i5_0 // 4 + ax1)
                            i2 = T.axis.spatial(53, i2_0 * 26 + ax2)
                            i3 = T.axis.spatial(53, i3_1 * 26 + ax3)
                            i4 = T.axis.spatial(4, i5_0 % 4 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 2, 1, 13, 1, 1, 1, 3, 1, 2, 13, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 64 + i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(26, i2_0 * 13 + i2_3)
                            ow = T.axis.spatial(26, i3_1 * 13 + i3_2)
                            oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 13, 26, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 64 + ax1)
                        ax2_1 = T.axis.spatial(26, i2_0 * 13 + ax2)
                        ax3_1 = T.axis.spatial(26, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 16, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"
[19:06:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 26, 26, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 1, 2, 1, 2, 1, 13, 1, 16, 1, 1, 1, 1, 26, 2, 2, 32, 1, 1, 1, 4, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_0 * 8 + i1_1 * 4 + i1_3)
                    oh = T.axis.spatial(26, i2_2)
                    ow = T.axis.spatial(26, i3_1 * 2 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 2, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 26, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[16, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 1, 2, 1, 2, 1, 13, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 1, 26, 2, 2, 32, 1, 1, 1, 4, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 8 + i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(26, i2_2)
                        ow = T.axis.spatial(26, i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 26, 2, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 8 + i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(26, ax2)
                        ax3_1 = T.axis.spatial(26, i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 2, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 26, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[16, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 1, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 13, 1, 16, 1, 1, 1, 1, 26, 2, 2, 32, 1, 1, 1, 4, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 8 + i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(26, i2_2)
                        ow = T.axis.spatial(26, i3_1 * 2 + i3_2)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 26, 26, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 8 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 2, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 26, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 2, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[16, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"
[19:06:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_log = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_tanh = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 256 and ax2 < 26 and ax3 < 26, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 26 and ax3 < 26, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
    

[19:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0, i1 in T.grid(1, 64):
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 26, 26):
                    for ax0_1, ax1_1, ax2_1, ax3_1, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_exp"):
                            ax0_2 = T.axis.spatial(1, ax0_1)
                            ax1_2 = T.axis.spatial(64, i1 + ax1_1)
                            ax2_2 = T.axis.spatial(26, ax2 + ax2_1)
                            ax3_2 = T.axis.spatial(26, ax3 + ax3_1)
                            ax4_1 = T.axis.spatial(4, ax1 + ax4)
                            T.reads(placeholder[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1])
                            T.writes(T_exp[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1])
                            T_exp[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1] = T.exp(placeholder[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1], dtype="float32")
                    with T.block("T_layout_trans"):
                        ax0_3 = T.axis.spatial(1, ax0)
                        ax1_3 = T.axis.spatial(256, i1 * 4 + ax1)
                        ax2_3, ax3_3 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(T_exp[ax0_3, ax1_3 // 4, ax2_3, ax3_3, ax1_3 % 4])
                        T.writes(T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3])
                        T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3] = T.if_then_else(ax0_3 < 1 and ax1_3 < 256 and ax2_3 < 26 and ax3_3 < 26, T_exp[ax0_3, ax1_3 // 4, ax2_3, ax3_3, ax1_3 % 4], T.float32(0), dtype="float32")
                for ax0_4, ax1_4, ax2_4, ax3_4, ax4 in T.grid(1, 1, 26, 26, 4):
                    with T.block("T_layout_trans_1"):
                        ax0_5 = T.axis.spatial(1, ax0_4)
                        ax1_5 = T.axis.spatial(64, i1 + ax1_4)
                        ax2_5, ax3_5, ax4_2 = T.axis.remap("SSS", [ax2_4, ax3_4, ax4])
                        T.reads(placeholder_1[ax0_5, 0, 0, 0], T_layout_trans[ax0_5, ax1_5 * 4 + ax4_2, ax2_5, ax3_5])
                        T.writes(T_layout_trans_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_2])
                        T_layout_trans_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_2] = T.if_then_else(ax0_5 < 1 and ax1_5 * 4 + ax4_2 < 256 and ax2_5 < 26 and ax3_5 < 26, T.tanh(T.log(placeholder_1[ax0_5, 0, 0, 0] + T_layout_trans[ax0_5, ax1_5 * 4 + ax4_2, ax2_5, ax3_5], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                for i2, i3, i4 in T.grid(26, 26, 4):
                    with T.block("T_multiply"):
                        ax0_6, ax1_6, ax2_6, ax3_6, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3], T_layout_trans_1[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3])
                        T.writes(T_multiply[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3])
                        T_multiply[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3] = placeholder[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3] * T_layout_trans_1[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=1)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=5)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
[19:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"
[19:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 27 and 1 <= i3_1 and i3_1 < 27, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 26, 26, 4, 256, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:06:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 27 and 1 <= i3_1 and i3_1 < 27, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 13, 4, 1, 16, 1, 1, 1, 16, 3, 3, 1, 1, 13, 2, 1, 16, 1, 1, 1, 1, 2, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1_1)
                    oh = T.axis.spatial(26, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(26, i3_0 * 2 + i3_2)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(256, i5_0 * 16 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                    T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 16, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[19:06:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 13, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 28, 4, 4):
                    with T.block("data_pad"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(28, i3_0 * 2 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 27 and 1 <= i3 and i3 < 27, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 16, 1, 1, 1):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 3, 3, 1, 1, 13, 2, 1, 16, 1, 1, 1, 1, 2, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1)
                            oh = T.axis.spatial(26, i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(26, i3_0 * 2 + i3_2)
                            oc_block = T.axis.spatial(4, i4_0)
                            ic = T.axis.reduce(256, i5_0 * 16 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 26, 2, 1):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(64, i1_0 * 16 + i1_1 + ax1)
                            ax2_1 = T.axis.spatial(26, ax2)
                            ax3_1 = T.axis.spatial(26, i3_0 * 2 + ax3)
                            ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 16, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:06:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 13, 4):
                for i0_1, i1_1 in T.grid(1, 16):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 28, 4, 4):
                        with T.block("data_pad"):
                            i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                            i3 = T.axis.spatial(28, i3_0 * 2 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 27 and 1 <= i3 and i3 < 27, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 16, 3, 3, 1, 1, 13, 2, 1, 16, 1, 1, 1, 1, 2, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1)
                            oh = T.axis.spatial(26, i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(26, i3_0 * 2 + i3_2)
                            oc_block = T.axis.spatial(4, i4_0)
                            ic = T.axis.reduce(256, i5_0 * 16 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 26, 2, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 16 + ax1)
                        ax2_1 = T.axis.spatial(26, ax2)
                        ax3_1 = T.axis.spatial(26, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 16, 1, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 2, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:06:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"
[19:06:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 26, 26, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_log = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_tanh = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 256 and ax2 < 26 and ax3 < 26, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 26 and ax3 < 26, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4], T_multiply[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = placeholder_2[ax0, ax1, ax2, ax3, ax4] + T_multiply[ax0, ax1, ax2, ax3, ax4]
    

[19:06:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 26, 26, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_exp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1 + ax1)
                        ax2_1 = T.axis.spatial(26, i2 + ax2)
                        ax3_1 = T.axis.spatial(26, i3 + ax3)
                        ax4_1 = T.axis.spatial(4, i4 + ax4)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.exp(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], dtype="float32")
                with T.block("T_add_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, 0, 0, 0], T_exp[ax0, ax4 // 4 + ax1, ax2, ax3, ax4 % 4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = placeholder_2[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2, ax3, ax4] * T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 26 and ax3 < 26, T.tanh(T.log(placeholder_1[ax0, 0, 0, 0] + T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 26 and ax3 < 26, T_exp[ax0, (ax1 * 4 + ax4) // 4, ax2, ax3, (ax1 * 4 + ax4) % 4], T.float32(0), dtype="float32"), dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v8 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v8)
l9 = sch.sample_compute_location(block=b5, decision=-2)
sch.compute_at(block=b5, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l10, preserve_unit_loops=True)
l11 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l11, preserve_unit_loops=True)
[19:06:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"
[19:06:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 26, 26, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:06:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 128, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 16, 13, 13, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1 * 16 + i1_3)
                    oh = T.axis.spatial(26, i2_2 * 13 + i2_3)
                    ow = T.axis.spatial(26, i3_1 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i4_3)
                    ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 2, 1, 16])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:06:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1, 1, 2, 1, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 16, 13, 13, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1 * 16 + i1_3)
                        oh = T.axis.spatial(26, i2_2 * 13 + i2_3)
                        ow = T.axis.spatial(26, i3_1 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 26, 13, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + i1_1 * 16 + ax1)
                        ax2_1 = T.axis.spatial(26, ax2)
                        ax3_1 = T.axis.spatial(26, i3_1 * 13 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 2, 1, 16])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 2, 1, 128, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 16, 13, 13, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1 * 16 + i1_3)
                        oh = T.axis.spatial(26, i2_2 * 13 + i2_3)
                        ow = T.axis.spatial(26, i3_1 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 26, 26, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 2, 1, 16])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 2, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"
[19:06:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 26, 26, 4), "float32"], T_concat: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_log = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_tanh = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        T_exp_1 = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        T_layout_trans_2 = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_log_1 = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_tanh_1 = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_layout_trans_3 = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder_2[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 256 and ax2 < 26 and ax3 < 26, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 26 and ax3 < 26, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder_2[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_exp_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp_1[ax0, ax1, ax2, ax3, ax4])
                T_exp_1[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_layout_trans_2"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_2[ax0, ax1, ax2, ax3])
                T_layout_trans_2[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 256 and ax2 < 26 and ax3 < 26, T_exp_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans_2[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans_2[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_log_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_log_1[ax0, ax1, ax2, ax3])
                T_log_1[ax0, ax1, ax2, ax3] = T.log(T_add_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_tanh_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log_1[ax0, ax1, ax2, ax3])
                T.writes(T_tanh_1[ax0, ax1, ax2, ax3])
                T_tanh_1[ax0, ax1, ax2, ax3] = T.tanh(T_log_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_layout_trans_3"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_3[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_3[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 26 and ax3 < 26, T_tanh_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_3[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1 - 64, ax2, ax3, ax4], T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(64 <= ax1, T_multiply[ax0, ax1 - 64, ax2, ax3, ax4], T_multiply_1[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[19:06:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 26, 26, 4), "float32"], T_concat: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            T_exp_1 = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            T_layout_trans_2 = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
            T_layout_trans_3 = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
                with T.block("T_exp"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                    T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder_2[ax0, ax1, ax2, ax3, ax4], dtype="float32")
            for i0, i1, i2 in T.grid(1, 64, 26):
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 26):
                    with T.block("T_layout_trans"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(26, i2 + ax2)
                        ax3_1 = T.axis.spatial(26, ax3)
                        T.reads(T_exp[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                        T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 256 and ax2_1 < 26 and ax3_1 < 26, T_exp[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], T.float32(0), dtype="float32")
                for i3, i4 in T.grid(26, 4):
                    with T.block("T_layout_trans_1"):
                        ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1 * 4 + ax4, ax2, ax3])
                        T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                        T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 26 and ax3 < 26, T.tanh(T.log(placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1 * 4 + ax4, ax2, ax3], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 64, 26, 26):
                for ax0, ax1 in T.grid(1, 4):
                    for ax0_2, ax1_2, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_exp_1"):
                            ax0_3 = T.axis.spatial(1, ax0_2)
                            ax1_3 = T.axis.spatial(64, i1 + ax1_2)
                            ax2_2 = T.axis.spatial(26, i2 + ax2)
                            ax3_2 = T.axis.spatial(26, i3 + ax3)
                            ax4_1 = T.axis.spatial(4, ax1 + ax4)
                            T.reads(placeholder[ax0_3, ax1_3, ax2_2, ax3_2, ax4_1])
                            T.writes(T_exp_1[ax0_3, ax1_3, ax2_2, ax3_2, ax4_1])
                            T_exp_1[ax0_3, ax1_3, ax2_2, ax3_2, ax4_1] = T.exp(placeholder[ax0_3, ax1_3, ax2_2, ax3_2, ax4_1], dtype="float32")
                    for ax2_3, ax3_3 in T.grid(1, 1):
                        with T.block("T_layout_trans_2"):
                            ax0_4 = T.axis.spatial(1, ax0)
                            ax1_4 = T.axis.spatial(256, i1 * 4 + ax1)
                            ax2_4 = T.axis.spatial(26, i2 + ax2_3)
                            ax3_4 = T.axis.spatial(26, i3 + ax3_3)
                            T.reads(T_exp_1[ax0_4, ax1_4 // 4, ax2_4, ax3_4, ax1_4 % 4])
                            T.writes(T_layout_trans_2[ax0_4, ax1_4, ax2_4, ax3_4])
                            T_layout_trans_2[ax0_4, ax1_4, ax2_4, ax3_4] = T.if_then_else(ax0_4 < 1 and ax1_4 < 256 and ax2_4 < 26 and ax3_4 < 26, T_exp_1[ax0_4, ax1_4 // 4, ax2_4, ax3_4, ax1_4 % 4], T.float32(0), dtype="float32")
                for i4 in T.serial(4):
                    with T.block("T_layout_trans_3"):
                        ax0_5, ax1_5, ax2_5, ax3_5, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder_1[ax0_5, 0, 0, 0], T_layout_trans_2[ax0_5, ax1_5 * 4 + ax4, ax2_5, ax3_5])
                        T.writes(T_layout_trans_3[ax0_5, ax1_5, ax2_5, ax3_5, ax4])
                        T_layout_trans_3[ax0_5, ax1_5, ax2_5, ax3_5, ax4] = T.if_then_else(ax0_5 < 1 and ax1_5 * 4 + ax4 < 256 and ax2_5 < 26 and ax3_5 < 26, T.tanh(T.log(placeholder_1[ax0_5, 0, 0, 0] + T_layout_trans_2[ax0_5, ax1_5 * 4 + ax4, ax2_5, ax3_5], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
                with T.block("T_concat"):
                    ax0_6, ax1_6, ax2_6, ax3_6, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_2[ax0_6, ax1_6 - 64, ax2_6, ax3_6, ax4], T_layout_trans_1[ax0_6, ax1_6 - 64, ax2_6, ax3_6, ax4], placeholder[ax0_6, ax1_6, ax2_6, ax3_6, ax4], T_layout_trans_3[ax0_6, ax1_6, ax2_6, ax3_6, ax4])
                    T.writes(T_concat[ax0_6, ax1_6, ax2_6, ax3_6, ax4])
                    T_concat[ax0_6, ax1_6, ax2_6, ax3_6, ax4] = T.if_then_else(64 <= ax1_6, placeholder_2[ax0_6, ax1_6 - 64, ax2_6, ax3_6, ax4] * T_layout_trans_1[ax0_6, ax1_6 - 64, ax2_6, ax3_6, ax4], placeholder[ax0_6, ax1_6, ax2_6, ax3_6, ax4] * T_layout_trans_3[ax0_6, ax1_6, ax2_6, ax3_6, ax4], dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="T_exp_1", func_name="main")
b8 = sch.get_block(name="T_layout_trans_2", func_name="main")
b9 = sch.get_block(name="T_add_1", func_name="main")
b10 = sch.get_block(name="T_log_1", func_name="main")
b11 = sch.get_block(name="T_tanh_1", func_name="main")
b12 = sch.get_block(name="T_layout_trans_3", func_name="main")
b13 = sch.get_block(name="T_multiply_1", func_name="main")
b14 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b13)
sch.compute_inline(block=b11)
sch.compute_inline(block=b10)
sch.compute_inline(block=b9)
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
l16 = sch.sample_compute_location(block=b12, decision=-1)
sch.compute_at(block=b12, loop=l16, preserve_unit_loops=True)
l17 = sch.sample_compute_location(block=b8, decision=3)
sch.compute_at(block=b8, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b7, decision=5)
sch.compute_at(block=b7, loop=l18, preserve_unit_loops=True)
l19 = sch.sample_compute_location(block=b5, decision=-1)
sch.compute_at(block=b5, loop=l19, preserve_unit_loops=True)
l20 = sch.sample_compute_location(block=b1, decision=2)
sch.compute_at(block=b1, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l21, preserve_unit_loops=True)
[19:06:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"
[19:06:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 26, 26, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:06:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 1, 1, 16, 13, 1, 1, 8, 1, 1, 1, 1, 1, 2, 4, 64, 1, 1, 1, 2, 2, 13, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 2 + i1_3)
                    oh = T.axis.spatial(26, i2_1 * 2 + i2_3)
                    ow = T.axis.spatial(26, i3_2 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(512, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 16, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 13, 1, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 64])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:06:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 1, 1, 16, 13, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 1, 2, 4, 64, 1, 1, 1, 2, 2, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(26, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(26, i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(512, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 26, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 32 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(26, i2_1 * 2 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 16, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 13, 1, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 64])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 13, 1, 1, 8, 1, 1, 1, 1, 1, 2, 4, 64, 1, 1, 1, 2, 2, 13, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(26, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(26, i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(512, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [128, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 26, 26, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1_0 * 32 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 16, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 13, 1, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 64])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"
[19:06:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 512, 26, 26], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 26, 26], dtype="float32")
        T_log = T.alloc_buffer([1, 512, 26, 26], dtype="float32")
        T_tanh = T.alloc_buffer([1, 512, 26, 26], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 26, 26):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 512 and ax2 < 26 and ax3 < 26, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 26, 26):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 512, 26, 26):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 26, 26):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 512 and ax2 < 26 and ax3 < 26, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
    

[19:06:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
            for i0, i1 in T.grid(1, 128):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 26, 26, 4):
                    with T.block("T_exp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(128, i1 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.exp(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], dtype="float32")
                for i2, i3 in T.grid(26, 26):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 4):
                        with T.block("T_layout_trans_1"):
                            ax0_2 = T.axis.spatial(1, ax0)
                            ax1_2 = T.axis.spatial(128, i1 + ax1)
                            ax2_2 = T.axis.spatial(26, i2 + ax2)
                            ax3_2 = T.axis.spatial(26, i3 + ax3)
                            ax4_2 = T.axis.spatial(4, ax4)
                            T.reads(placeholder_1[ax0_2, 0, 0, 0], T_exp[ax0_2, ax4_2 // 4 + ax1_2, ax2_2, ax3_2, ax4_2 % 4])
                            T.writes(T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2])
                            T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] = T.if_then_else(ax0_2 < 1 and ax1_2 * 4 + ax4_2 < 512 and ax2_2 < 26 and ax3_2 < 26, T.tanh(T.log(placeholder_1[ax0_2, 0, 0, 0] + T.if_then_else(ax0_2 < 1 and ax1_2 * 4 + ax4_2 < 512 and ax2_2 < 26 and ax3_2 < 26, T_exp[ax0_2, (ax1_2 * 4 + ax4_2) // 4, ax2_2, ax3_2, (ax1_2 * 4 + ax4_2) % 4], T.float32(0), dtype="float32"), dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                    for i4 in T.serial(4):
                        with T.block("T_multiply"):
                            ax0_3, ax1_3, ax2_3, ax3_3, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                            T.reads(placeholder[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3], T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                            T.writes(T_multiply[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                            T_multiply[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] = placeholder[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] * T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=3)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
[19:06:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"
[19:06:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(1, 64, 13, 13, 4), "float32"], T_layout_trans: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans_1 = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
        resize = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_layout_trans_2 = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_concat = T.alloc_buffer([1, 512, 26, 26], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3])
                T_layout_trans_1[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 256 and ax2 < 13 and ax3 < 13, placeholder_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("resize"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_layout_trans_1[i0_1, i1_1, 0 : 13, 0 : 13])
                T.writes(resize[i0_1, i1_1, i2_1, i3_1])
                resize[i0_1, i1_1, i2_1, i3_1] = T_layout_trans_1[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.5) - T.float32(0.5) + T.float32(9.9999997473787516e-06), dtype="float32"), "int32"), 12), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.5) - T.float32(0.5) + T.float32(9.9999997473787516e-06), dtype="float32"), "int32"), 12), 0)]
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_2[ax0, ax1, ax2, ax3])
                T_layout_trans_2[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 256 and ax2 < 26 and ax3 < 26, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 26, 26):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(resize[ax0, ax1 - 256, ax2, ax3], T_layout_trans_2[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(256 <= ax1, resize[ax0, ax1 - 256, ax2, ax3], T_layout_trans_2[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
            with T.block("T_layout_trans_2"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_concat[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 512 and ax2 < 26 and ax3 < 26, T_concat[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[19:06:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(1, 64, 13, 13, 4), "float32"], T_layout_trans: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_layout_trans_1 = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
            T_concat = T.alloc_buffer([1, 512, 26, 26], dtype="float32")
            for i0, i1 in T.grid(1, 128):
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 13, 13):
                    with T.block("T_layout_trans"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1 * 4 - 256 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        T.where(256 <= i1 * 4 + ax1)
                        T.reads(placeholder_1[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                        T.writes(T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 256 and ax2_1 < 13 and ax3_1 < 13, placeholder_1[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], T.float32(0), dtype="float32")
                for i2, i3 in T.grid(26, 26):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("T_concat"):
                            ax0_2 = T.axis.spatial(1, ax0)
                            ax1_2 = T.axis.spatial(512, i1 * 4 + ax1)
                            ax2_2 = T.axis.spatial(26, i2 + ax2)
                            ax3_2 = T.axis.spatial(26, i3 + ax3)
                            T.reads(T_layout_trans_1[ax0_2, ax1_2 - 256, 0 : 13, 0 : 13], placeholder[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4])
                            T.writes(T_concat[ax0_2, ax1_2, ax2_2, ax3_2])
                            T_concat[ax0_2, ax1_2, ax2_2, ax3_2] = T.if_then_else(256 <= ax1_2, T_layout_trans_1[ax0_2, ax1_2 - 256, T.max(T.min(T.cast(T.floor((T.cast(ax2_2, "float32") + T.float32(0.5)) * T.float32(0.5) - T.float32(0.5) + T.float32(9.9999997473787516e-06), dtype="float32"), "int32"), 12), 0), T.max(T.min(T.cast(T.floor((T.cast(ax3_2, "float32") + T.float32(0.5)) * T.float32(0.5) - T.float32(0.5) + T.float32(9.9999997473787516e-06), dtype="float32"), "int32"), 12), 0)], T.if_then_else(ax0_2 < 1 and ax1_2 < 256 and ax2_2 < 26 and ax3_2 < 26, placeholder[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4], T.float32(0), dtype="float32"), dtype="float32")
                    for i4 in T.serial(4):
                        with T.block("T_layout_trans_2"):
                            ax0_3, ax1_3, ax2_3, ax3_3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                            T.reads(T_concat[ax0_3, ax1_3 * 4 + ax4, ax2_3, ax3_3])
                            T.writes(T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4])
                            T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4] = T.if_then_else(ax0_3 < 1 and ax1_3 * 4 + ax4 < 512 and ax2_3 < 26 and ax3_3 < 26, T_concat[ax0_3, ax1_3 * 4 + ax4, ax2_3, ax3_3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="resize", func_name="main")
b2 = sch.get_block(name="T_layout_trans_1", func_name="main")
b3 = sch.get_block(name="T_concat", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=-2)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l8, preserve_unit_loops=True)
[19:06:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"
[19:06:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 28, 28, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 27 and 1 <= i3_1 and i3_1 < 27, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 128, 26, 26, 4, 256, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4] * T.float32(0.10000000149011612))
    

[19:06:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 28, 28, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 27 and 1 <= i3 and i3 < 27, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(13, 13, 1, 1, 16, 2, 2, 2, 32, 3, 3, 1, 1, 1, 1, 1, 8, 1, 1, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(26, i2_0 * 2 + i2_1)
                        ow = T.axis.spatial(26, i3_0 * 2 + i3_1)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 26, 26, 4):
                with T.block("T_leaky_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                    T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 16, 1, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 2, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:06:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1 in T.grid(1, 4, 13, 13, 1, 1, 16, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 3, 4, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(28, i2_0 * 2 + i2_1 + ax2)
                        i3 = T.axis.spatial(28, i3_0 * 2 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 27 and 1 <= i3 and i3 < 27, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_1, i4_1 in T.grid(2, 2):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 3, 3, 1, 1, 1, 1, 1, 8, 1, 1, 1, 2, 1, 1, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(26, i2_0 * 2 + i2_1)
                            ow = T.axis.spatial(26, i3_0 * 2 + i3_1)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                            ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 1, 1, 2):
                        with T.block("T_leaky_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(128, i1_0 * 32 + i1_1 * 2 + ax1)
                            ax2_1 = T.axis.spatial(26, i2_0 * 2 + i2_1 + ax2)
                            ax3_1 = T.axis.spatial(26, i3_0 * 2 + i3_1 + ax3)
                            ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 16, 1, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 2, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[19:06:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 4, 13):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 4, 28, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(28, i2_0 * 2 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 27 and 1 <= i3 and i3 < 27, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0 in T.grid(13, 1):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 2, 2, 2, 32, 3, 3, 1, 1, 1, 1, 1, 8, 1, 1, 1, 2, 1, 1, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_0 * 32 + i1_1 * 2 + i1_3)
                            oh = T.axis.spatial(26, i2_0 * 2 + i2_1)
                            ow = T.axis.spatial(26, i3_0 * 2 + i3_1)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                            ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 2, 2, 4):
                        with T.block("T_leaky_relu"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(128, i1_0 * 32 + ax1)
                            ax2_1 = T.axis.spatial(26, i2_0 * 2 + ax2)
                            ax3_1 = T.axis.spatial(26, i3_0 * 2 + ax3)
                            ax4_1 = T.axis.spatial(4, ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 16, 1, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 2, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[19:06:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"
[19:06:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 26, 26, 4, 512, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4] * T.float32(0.10000000149011612))
    

[19:06:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 1, 1, 2, 26, 1, 4, 256, 1, 1, 1, 4, 1, 26, 1, 2, 1, 1, 1, 2, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_2, i4_1])
                    ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 26, 26, 4):
                with T.block("T_leaky_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                    T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 26, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 26, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 1, 1, 2, 26, 1, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 4, 1, 26, 1, 2, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_2, i4_1])
                        ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 26, 1):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 16 + i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(26, i2_1 + ax2)
                        ax3_1 = T.axis.spatial(26, ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 26, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 26, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[19:06:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 26, 1, 4, 256, 1, 1, 1, 4, 1, 26, 1, 2, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 16 + i1_1 * 8 + i1_2 * 2 + i1_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_1, i3_2, i4_1])
                        ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 26, 26, 4):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 16 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 2, 4, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 26, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 26, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[19:06:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"
[19:06:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 32, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 26, 26, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 26, 26, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 26, 26, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 26, 26, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 26, 26, 4):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4] * T.float32(0.10000000149011612))
    

[19:06:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:06:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 32, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 13, 1, 1, 1, 2, 1, 1, 2, 128, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 13, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + i1_2)
                    oh = T.axis.spatial(26, i2_0 * 2 + i2_3)
                    ow = T.axis.spatial(26, i3_2 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 26, 26, 4):
                with T.block("T_leaky_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                    T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:06:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 32, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 13, 1, 1, 1, 2, 1, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 13, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(26, i2_0 * 2 + i2_3)
                        ow = T.axis.spatial(26, i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 26, 2):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(26, i2_0 * 2 + ax2)
                        ax3_1 = T.axis.spatial(26, ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[19:06:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 32, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 26, 26, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 13, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 2, 128, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 13, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(26, i2_0 * 2 + i2_3)
                        ow = T.axis.spatial(26, i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 2, 26, 4):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(26, i2_0 * 2 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 2, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 1, 1, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[128, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[19:06:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #40: "fused_transpose_layout_transform"
[19:06:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 416, 416, 3), "float32"], T_layout_trans: T.Buffer[(1, 1, 416, 416, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_transpose = T.alloc_buffer([1, 3, 416, 416], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 416, 416):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax2, ax3, ax1])
                T.writes(T_transpose[ax0, ax1, ax2, ax3])
                T_transpose[ax0, ax1, ax2, ax3] = placeholder[ax0, ax2, ax3, ax1]
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 416, 416, 3):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_transpose[ax0, ax1 * 3 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 416 and ax3 < 416, T_transpose[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[19:06:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:06:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 416, 416, 3), "float32"], T_layout_trans: T.Buffer[(1, 1, 416, 416, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_transpose = T.alloc_buffer([1, 3, 416, 416], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 3, 416, 416):
                with T.block("T_transpose"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax2, ax3, ax1])
                    T.writes(T_transpose[ax0, ax1, ax2, ax3])
                    T_transpose[ax0, ax1, ax2, ax3] = placeholder[ax0, ax2, ax3, ax1]
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 416, 416, 3):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(T_transpose[ax0, ax1 * 3 + ax4, ax2, ax3])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 416 and ax3 < 416, T_transpose[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_transpose", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[19:06:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"
[19:06:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 416, 416, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 416, 416, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 418, 418, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 416, 416, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 418, 418, 3):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 417 and 1 <= i3_1 and i3_1 < 417, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 416, 416, 4, 3, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 416, 416, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 416, 416, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 416, 416, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 418, 418, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 8, 416, 416, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1 in T.grid(1, 4, 1, 8, 1, 1, 1, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 106, 54, 3):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(418, i2_1 * 104 + ax2)
                        i3 = T.axis.spatial(418, i3_0 * 52 + ax3)
                        i4 = T.axis.spatial(3, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 417 and 1 <= i3 and i3 < 417, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(26, 2, 1, 1, 1, 1, 2, 104, 1, 2, 3, 3, 3, 1, 1, 1, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_2)
                        oh = T.axis.spatial(416, i2_1 * 104 + i2_2)
                        ow = T.axis.spatial(416, i3_0 * 52 + i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_1])
                        T.reads(data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 416, 416, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 104, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 26, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[19:07:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 416, 416, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 416, 416, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 418, 418, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 8, 416, 416, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 418, 418, 3):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 417 and 1 <= i3_1 and i3_1 < 417, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 4, 1, 8, 1, 1, 1, 4, 26, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 104, 1, 2, 3, 3, 3, 1, 1, 1, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_2)
                        oh = T.axis.spatial(416, i2_1_1 * 104 + i2_2)
                        ow = T.axis.spatial(416, i3_0 * 52 + i3_1_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1_1 * 2 + i4_2)
                        ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_1])
                        T.reads(data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 104, 2, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(416, i2_1_1 * 104 + ax2)
                        ax3_1 = T.axis.spatial(416, i3_0 * 52 + i3_1_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 104, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 26, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 416, 416, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 416, 416, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 418, 418, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 8, 416, 416, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 8, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 4, 26, 2):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 106, 4, 3):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(418, i2_1 * 104 + ax2)
                            i3 = T.axis.spatial(418, i3_0 * 52 + i3_1 * 2 + ax3)
                            i4 = T.axis.spatial(3, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 417 and 1 <= i3 and i3 < 417, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 104, 1, 2, 3, 3, 3, 1, 1, 1, 2, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_2)
                            oh = T.axis.spatial(416, i2_1 * 104 + i2_2)
                            ow = T.axis.spatial(416, i3_0 * 52 + i3_1 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_1])
                            T.reads(data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 416, 52, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(416, ax2)
                        ax3_1 = T.axis.spatial(416, i3_0 * 52 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 104, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 26, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 3])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"
[19:07:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 416, 416, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 8, 416, 416, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 8, 416, 416, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 32, 416, 416], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 416, 416], dtype="float32")
        T_log = T.alloc_buffer([1, 32, 416, 416], dtype="float32")
        T_tanh = T.alloc_buffer([1, 32, 416, 416], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 8, 416, 416, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 416, 416, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 416, 416):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 32 and ax2 < 416 and ax3 < 416, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 416, 416):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 32, 416, 416):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 416, 416):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 416, 416, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 32 and ax2 < 416 and ax3 < 416, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 416, 416, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
    

[19:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 416, 416, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 8, 416, 416, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 8, 416, 416, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 8, 416, 416, 4], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 8, 416, 416):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 4):
                    with T.block("T_exp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1 + ax1)
                        ax2_1 = T.axis.spatial(416, i2 + ax2)
                        ax3_1 = T.axis.spatial(416, i3 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.exp(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], dtype="float32")
                for i4 in T.serial(4):
                    with T.block("T_layout_trans_1"):
                        ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder_1[ax0, 0, 0, 0], T_exp[ax0, ax4 // 4 + ax1, ax2, ax3, ax4 % 4])
                        T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                        T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 32 and ax2 < 416 and ax3 < 416, T.tanh(T.log(placeholder_1[ax0, 0, 0, 0] + T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 32 and ax2 < 416 and ax3 < 416, T_exp[ax0, (ax1 * 4 + ax4) // 4, ax2, ax3, (ax1 * 4 + ax4) % 4], T.float32(0), dtype="float32"), dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 416, 416, 4):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                    T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=-1)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
[19:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"
[19:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 416, 416, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 8, 417, 417, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 417, 417, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 417 and 1 <= i3_1 and i3_1 < 417, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 208, 208, 4, 32, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 416, 416, 4], "float32"], ["TENSOR", [16, 8, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 416, 416, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 8, 417, 417, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0 in T.grid(1, 2, 104, 13, 1, 1, 1, 2, 1, 4, 4, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 1, 33, 4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(8, i5_0 * 2 + ax1)
                        i2 = T.axis.spatial(417, i2_0 * 4 + i2_1 * 2 + i6_0 + ax2)
                        i3 = T.axis.spatial(417, i3_0 * 32 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 417 and 1 <= i3 and i3 < 417, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 2, 1, 4, 1, 8, 1, 1, 1, 4, 1, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(208, i2_0 * 2 + i2_1)
                        ow = T.axis.spatial(208, i3_0 * 16 + i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(32, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 416, 416, 4], "float32"], ["TENSOR", [16, 8, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[104, 2, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 4, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[19:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 416, 416, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 8, 417, 417, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 104, 13, 1, 1, 1, 2, 1, 4):
                for i5_0, i6_0 in T.grid(4, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 1, 33, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(8, i5_0 * 2 + ax1)
                            i2 = T.axis.spatial(417, i2_0 * 4 + i2_1 * 2 + i6_0 + ax2)
                            i3 = T.axis.spatial(417, i3_0 * 32 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 417 and 1 <= i3 and i3 < 417, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 2, 1, 4, 1, 8, 1, 1, 1, 4, 1, 4, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(208, i2_0 * 2 + i2_1)
                            ow = T.axis.spatial(208, i3_0 * 16 + i3_2 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1)
                            ic = T.axis.reduce(32, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 416, 416, 4], "float32"], ["TENSOR", [16, 8, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 16, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(208, i2_0 * 2 + i2_1 + ax2)
                        ax3_1 = T.axis.spatial(208, i3_0 * 16 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[104, 2, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 4, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 416, 416, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 104, 13, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 4, 4, 3, 3, 1, 2, 1, 4, 1, 8, 1, 1, 1, 4, 1, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(208, i2_0 * 2 + i2_1)
                        ow = T.axis.spatial(208, i3_0 * 16 + i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(32, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(placeholder[n, ic // 4, oh * 2 + kh - 1, ow * 2 + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 416, 416, 4], "float32"], ["TENSOR", [16, 8, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh * 2 + kh and oh * 2 + kh < 417 and 1 <= ow * 2 + kw and ow * 2 + kw < 417, placeholder[n, ic // 4, oh * 2 + kh - 1, ow * 2 + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 2, 16, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(208, i2_0 * 2 + ax2)
                        ax3_1 = T.axis.spatial(208, i3_0 * 16 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[104, 2, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 4, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"
[19:07:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(8, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 208, 208, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 8, 208, 208, 4, 64, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 208, 208, 4], "float32"], ["TENSOR", [8, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 208, 208, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(8, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 2, 16, 1, 1, 1, 2, 1, 1, 64, 1, 1, 1, 1, 52, 13, 4, 1, 1, 1, 1, 2, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_3)
                    oh = T.axis.spatial(208, i2_0 * 104 + i2_1 * 52 + i2_2)
                    ow = T.axis.spatial(208, i3_0 * 13 + i3_2)
                    oc_block, ic = T.axis.remap("SR", [i4_2, i5_0])
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 208, 208, 4], "float32"], ["TENSOR", [8, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 208, 208, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 52, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 1, 13, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:07:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(8, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 2, 16, 1, 1, 1, 2, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 1, 52, 13, 4, 1, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_3)
                        oh = T.axis.spatial(208, i2_0 * 104 + i2_1 * 52 + i2_2)
                        ow = T.axis.spatial(208, i3_0 * 13 + i3_2)
                        oc_block, ic = T.axis.remap("SR", [i4_2, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 208, 208, 4], "float32"], ["TENSOR", [8, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 52, 13, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(208, i2_0 * 104 + i2_1 * 52 + ax2)
                        ax3_1 = T.axis.spatial(208, i3_0 * 13 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 52, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 1, 13, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(8, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 8, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 2, 16, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 1, 64, 1, 1, 1, 1, 52, 13, 4, 1, 1, 1, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(8, i1_0 * 2 + i1_3)
                        oh = T.axis.spatial(208, i2_0 * 104 + i2_1 * 52 + i2_2)
                        ow = T.axis.spatial(208, i3_0 * 13 + i3_2)
                        oc_block, ic = T.axis.remap("SR", [i4_2, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 208, 208, 4], "float32"], ["TENSOR", [8, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 104, 13, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(8, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(208, i2_0 * 104 + ax2)
                        ax3_1 = T.axis.spatial(208, i3_0 * 13 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 52, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 1, 13, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"
[19:07:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 8, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 8, 208, 208, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 32, 208, 208], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 208, 208], dtype="float32")
        T_log = T.alloc_buffer([1, 32, 208, 208], dtype="float32")
        T_tanh = T.alloc_buffer([1, 32, 208, 208], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 8, 208, 208, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 208, 208, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 208, 208):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 32 and ax2 < 208 and ax3 < 208, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 208, 208):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 32, 208, 208):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 208, 208):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 208, 208, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 32 and ax2 < 208 and ax3 < 208, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 208, 208, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
    

[19:07:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 8, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 8, 208, 208, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 32, 208, 208], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 8, 208, 208, 4], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 8, 208, 208):
                for ax0, ax1 in T.grid(1, 4):
                    for ax0_1, ax1_1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_exp"):
                            ax0_2 = T.axis.spatial(1, ax0_1)
                            ax1_2 = T.axis.spatial(8, i1 + ax1_1)
                            ax2_1 = T.axis.spatial(208, i2 + ax2)
                            ax3_1 = T.axis.spatial(208, i3 + ax3)
                            ax4_1 = T.axis.spatial(4, ax1 + ax4)
                            T.reads(placeholder[ax0_2, ax1_2, ax2_1, ax3_1, ax4_1])
                            T.writes(T_exp[ax0_2, ax1_2, ax2_1, ax3_1, ax4_1])
                            T_exp[ax0_2, ax1_2, ax2_1, ax3_1, ax4_1] = T.exp(placeholder[ax0_2, ax1_2, ax2_1, ax3_1, ax4_1], dtype="float32")
                    for ax2, ax3 in T.grid(1, 1):
                        with T.block("T_layout_trans"):
                            ax0_3 = T.axis.spatial(1, ax0)
                            ax1_3 = T.axis.spatial(32, i1 * 4 + ax1)
                            ax2_2 = T.axis.spatial(208, i2 + ax2)
                            ax3_2 = T.axis.spatial(208, i3 + ax3)
                            T.reads(T_exp[ax0_3, ax1_3 // 4, ax2_2, ax3_2, ax1_3 % 4])
                            T.writes(T_layout_trans[ax0_3, ax1_3, ax2_2, ax3_2])
                            T_layout_trans[ax0_3, ax1_3, ax2_2, ax3_2] = T.if_then_else(ax0_3 < 1 and ax1_3 < 32 and ax2_2 < 208 and ax3_2 < 208, T_exp[ax0_3, ax1_3 // 4, ax2_2, ax3_2, ax1_3 % 4], T.float32(0), dtype="float32")
                for ax0_4, ax1_4, ax2_3, ax3_3, ax4 in T.grid(1, 1, 1, 1, 4):
                    with T.block("T_layout_trans_1"):
                        ax0_5 = T.axis.spatial(1, ax0_4)
                        ax1_5 = T.axis.spatial(8, i1 + ax1_4)
                        ax2_4 = T.axis.spatial(208, i2 + ax2_3)
                        ax3_4 = T.axis.spatial(208, i3 + ax3_3)
                        ax4_2 = T.axis.spatial(4, ax4)
                        T.reads(placeholder_1[ax0_5, 0, 0, 0], T_layout_trans[ax0_5, ax1_5 * 4 + ax4_2, ax2_4, ax3_4])
                        T.writes(T_layout_trans_1[ax0_5, ax1_5, ax2_4, ax3_4, ax4_2])
                        T_layout_trans_1[ax0_5, ax1_5, ax2_4, ax3_4, ax4_2] = T.if_then_else(ax0_5 < 1 and ax1_5 * 4 + ax4_2 < 32 and ax2_4 < 208 and ax3_4 < 208, T.tanh(T.log(placeholder_1[ax0_5, 0, 0, 0] + T_layout_trans[ax0_5, ax1_5 * 4 + ax4_2, ax2_4, ax3_4], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                for i4 in T.serial(4):
                    with T.block("T_multiply"):
                        ax0_6, ax1_6, ax2_5, ax3_5, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder[ax0_6, ax1_6, ax2_5, ax3_5, ax4_3], T_layout_trans_1[ax0_6, ax1_6, ax2_5, ax3_5, ax4_3])
                        T.writes(T_multiply[ax0_6, ax1_6, ax2_5, ax3_5, ax4_3])
                        T_multiply[ax0_6, ax1_6, ax2_5, ax3_5, ax4_3] = placeholder[ax0_6, ax1_6, ax2_5, ax3_5, ax4_3] * T_layout_trans_1[ax0_6, ax1_6, ax2_5, ax3_5, ax4_3]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=3)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=3)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=5)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
[19:07:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"
[19:07:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 8, 210, 210, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 8, 210, 210, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 209 and 1 <= i3_1 and i3_1 < 209, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 208, 208, 4, 32, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 208, 208, 4], "float32"], ["TENSOR", [16, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 8, 210, 210, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 2, 26, 52, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 10, 6, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(210, i2_0 * 8 + ax2)
                        i3 = T.axis.spatial(210, i3_0 * 4 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 209 and 1 <= i3 and i3 < 209, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 32, 3, 3, 1, 2, 1, 2, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(208, i2_0 * 8 + i2_1)
                        ow = T.axis.spatial(208, i3_0 * 4 + i3_1 * 2 + i3_3)
                        oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_3, i5_1, i6_1, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 208, 208, 4], "float32"], ["TENSOR", [16, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[26, 8, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[52, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[19:07:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 8, 210, 210, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 8, 210, 210, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 209 and 1 <= i3_1 and i3_1 < 209, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 2, 26, 52, 1, 1, 2, 8, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 32, 3, 3, 1, 2, 1, 2, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(208, i2_0 * 8 + i2_1_1)
                        ow = T.axis.spatial(208, i3_0 * 4 + i3_1_1 * 2 + i3_3)
                        oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_3, i5_1, i6_1, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 208, 208, 4], "float32"], ["TENSOR", [16, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 2, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + i1_1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(208, i2_0 * 8 + i2_1_1 + ax2)
                        ax3_1 = T.axis.spatial(208, i3_0 * 4 + i3_1_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[26, 8, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[52, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 8, 210, 210, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 26, 52, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1 in T.grid(1, 2, 8, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 32, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 4, 1):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(8, i5_1 // 4 + ax1)
                            i2 = T.axis.spatial(210, i2_0 * 8 + i2_1 + i6_1 + ax2)
                            i3 = T.axis.spatial(210, i3_0 * 4 + i3_1 * 2 + ax3)
                            i4 = T.axis.spatial(4, i5_1 % 4 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 209 and 1 <= i3 and i3 < 209, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 2, 1, 2, 4):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(208, i2_0 * 8 + i2_1)
                            ow = T.axis.spatial(208, i3_0 * 4 + i3_1 * 2 + i3_3)
                            oc_block, ic, kh, kw = T.axis.remap("SRRR", [i4_3, i5_1, i6_1, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 208, 208, 4], "float32"], ["TENSOR", [16, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 8, 4, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(208, i2_0 * 8 + ax2)
                        ax3_1 = T.axis.spatial(208, i3_0 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[26, 8, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[52, 2, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=19)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"
[19:07:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 208, 208, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_log = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_tanh = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 208 and ax3 < 208, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 208 and ax3 < 208, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4], T_multiply[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = placeholder_2[ax0, ax1, ax2, ax3, ax4] + T_multiply[ax0, ax1, ax2, ax3, ax4]
    

[19:07:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 208, 208, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0, i1, i2 in T.grid(1, 16, 208):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 208, 4):
                    with T.block("T_layout_trans_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1 + ax1)
                        ax2_1 = T.axis.spatial(208, i2 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder_1[ax0_1, 0, 0, 0], placeholder[ax0_1, ax4_1 // 4 + ax1_1, ax2_1, ax3_1, ax4_1 % 4])
                        T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(ax0_1 < 1 and ax1_1 * 4 + ax4_1 < 64 and ax2_1 < 208 and ax3_1 < 208, T.tanh(T.log(placeholder_1[ax0_1, 0, 0, 0] + T.if_then_else(ax0_1 < 1 and ax1_1 * 4 + ax4_1 < 64 and ax2_1 < 208 and ax3_1 < 208, T.exp(placeholder[ax0_1, (ax1_1 * 4 + ax4_1) // 4, ax2_1, ax3_1, (ax1_1 * 4 + ax4_1) % 4], dtype="float32"), T.float32(0), dtype="float32"), dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                for i3, i4 in T.grid(208, 4):
                    with T.block("T_add_1"):
                        ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = placeholder_2[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans[ax0, ax1, ax2, ax3, ax4]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v8 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v8)
l9 = sch.sample_compute_location(block=b5, decision=2)
sch.compute_at(block=b5, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l10, preserve_unit_loops=True)
l11 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l11, preserve_unit_loops=True)
[19:07:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"
[19:07:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 208, 208, 4, 64, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 208, 208, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 4, 1, 1, 1, 8, 2, 4, 32, 1, 1, 1, 4, 26, 1, 1, 2, 1, 1, 1, 2, 1, 26, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(208, i2_1 * 26 + i2_2)
                    ow = T.axis.spatial(208, i3_0 * 52 + i3_1 * 26 + i3_3)
                    oc_block = T.axis.spatial(4, i4_1)
                    ic = T.axis.reduce(64, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 208, 208, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 26, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 1, 26])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:07:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 4, 1, 1, 1, 8, 2, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 4, 26, 1, 1, 2, 1, 1, 1, 2, 1, 26, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(208, i2_1 * 26 + i2_2)
                        ow = T.axis.spatial(208, i3_0 * 52 + i3_1 * 26 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(64, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 208, 208, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 26, 26, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(208, i2_1 * 26 + ax2)
                        ax3_1 = T.axis.spatial(208, i3_0 * 52 + i3_1 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 26, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 1, 26])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 4, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 8, 2, 4, 32, 1, 1, 1, 4, 26, 1, 1, 2, 1, 1, 1, 2, 1, 26, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(208, i2_1 * 26 + i2_2)
                        ow = T.axis.spatial(208, i3_0 * 52 + i3_1 * 26 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(64, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 208, 208, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 208, 52, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(208, ax2)
                        ax3_1 = T.axis.spatial(208, i3_0 * 52 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 26, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 1, 26])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"
[19:07:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 208, 208, 4), "float32"], T_concat: T.Buffer[(1, 32, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_log = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_tanh = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        T_exp_1 = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        T_layout_trans_2 = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_log_1 = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_tanh_1 = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_layout_trans_3 = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder_2[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 208 and ax3 < 208, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 208 and ax3 < 208, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder_2[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_exp_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp_1[ax0, ax1, ax2, ax3, ax4])
                T_exp_1[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_layout_trans_2"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_2[ax0, ax1, ax2, ax3])
                T_layout_trans_2[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 208 and ax3 < 208, T_exp_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans_2[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans_2[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_log_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_log_1[ax0, ax1, ax2, ax3])
                T_log_1[ax0, ax1, ax2, ax3] = T.log(T_add_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_tanh_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log_1[ax0, ax1, ax2, ax3])
                T.writes(T_tanh_1[ax0, ax1, ax2, ax3])
                T_tanh_1[ax0, ax1, ax2, ax3] = T.tanh(T_log_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_layout_trans_3"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_3[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_3[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 208 and ax3 < 208, T_tanh_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_3[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 208, 208, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1 - 16, ax2, ax3, ax4], T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(16 <= ax1, T_multiply[ax0, ax1 - 16, ax2, ax3, ax4], T_multiply_1[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[19:07:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 208, 208, 4), "float32"], T_concat: T.Buffer[(1, 32, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
            T_layout_trans_2 = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0, i1, i2 in T.grid(1, 32, 208):
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 208):
                    with T.block("T_layout_trans_2"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(208, i2 + ax2)
                        ax3_1 = T.axis.spatial(208, ax3)
                        T.where(i1 * 4 + ax1 < 64)
                        T.reads(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                        T.writes(T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 64 and ax2_1 < 208 and ax3_1 < 208, T.exp(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], dtype="float32"), T.float32(0), dtype="float32")
                for i3 in T.serial(208):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 4):
                        with T.block("T_layout_trans_3"):
                            ax0_2 = T.axis.spatial(1, ax0)
                            ax1_2 = T.axis.spatial(16, i1 + ax1)
                            ax2_2 = T.axis.spatial(208, i2 + ax2)
                            ax3_2 = T.axis.spatial(208, i3 + ax3)
                            ax4_1 = T.axis.spatial(4, ax4)
                            T.where(i1 < 16)
                            T.reads(placeholder_1[ax0_2, 0, 0, 0], T_layout_trans_1[ax0_2, ax1_2 * 4 + ax4_1, ax2_2, ax3_2])
                            T.writes(T_layout_trans_2[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1])
                            T_layout_trans_2[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1] = T.if_then_else(ax0_2 < 1 and ax1_2 * 4 + ax4_1 < 64 and ax2_2 < 208 and ax3_2 < 208, T.tanh(T.log(placeholder_1[ax0_2, 0, 0, 0] + T_layout_trans_1[ax0_2, ax1_2 * 4 + ax4_1, ax2_2, ax3_2], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                    for i4 in T.serial(4):
                        for ax0_3, ax1_3, ax2_3, ax3_3, ax4 in T.grid(1, 1, 1, 1, 1):
                            with T.block("T_layout_trans_1"):
                                ax0_4 = T.axis.spatial(1, ax0_3)
                                ax1_4 = T.axis.spatial(16, i1 - 16 + ax1_3)
                                ax2_4 = T.axis.spatial(208, i2 + ax2_3)
                                ax3_4 = T.axis.spatial(208, i3 + ax3_3)
                                ax4_2 = T.axis.spatial(4, i4 + ax4)
                                T.where(16 <= i1)
                                T.reads(placeholder_1[ax0_4, 0, 0, 0], placeholder_2[ax0_4, ax4_2 // 4 + ax1_4, ax2_4, ax3_4, ax4_2 % 4])
                                T.writes(T_layout_trans[ax0_4, ax1_4, ax2_4, ax3_4, ax4_2])
                                T_layout_trans[ax0_4, ax1_4, ax2_4, ax3_4, ax4_2] = T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4_2 < 64 and ax2_4 < 208 and ax3_4 < 208, T.tanh(T.log(placeholder_1[ax0_4, 0, 0, 0] + T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4_2 < 64 and ax2_4 < 208 and ax3_4 < 208, T.exp(placeholder_2[ax0_4, (ax1_4 * 4 + ax4_2) // 4, ax2_4, ax3_4, (ax1_4 * 4 + ax4_2) % 4], dtype="float32"), T.float32(0), dtype="float32"), dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                        with T.block("T_concat"):
                            ax0_5, ax1_5, ax2_5, ax3_5, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                            T.reads(placeholder_2[ax0_5, ax1_5 - 16, ax2_5, ax3_5, ax4_3], T_layout_trans[ax0_5, ax1_5 - 16, ax2_5, ax3_5, ax4_3], placeholder[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3], T_layout_trans_2[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3])
                            T.writes(T_concat[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3])
                            T_concat[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3] = T.if_then_else(16 <= ax1_5, placeholder_2[ax0_5, ax1_5 - 16, ax2_5, ax3_5, ax4_3] * T_layout_trans[ax0_5, ax1_5 - 16, ax2_5, ax3_5, ax4_3], placeholder[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3] * T_layout_trans_2[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3], dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="T_exp_1", func_name="main")
b8 = sch.get_block(name="T_layout_trans_2", func_name="main")
b9 = sch.get_block(name="T_add_1", func_name="main")
b10 = sch.get_block(name="T_log_1", func_name="main")
b11 = sch.get_block(name="T_tanh_1", func_name="main")
b12 = sch.get_block(name="T_layout_trans_3", func_name="main")
b13 = sch.get_block(name="T_multiply_1", func_name="main")
b14 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b13)
sch.compute_inline(block=b11)
sch.compute_inline(block=b10)
sch.compute_inline(block=b9)
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
l16 = sch.sample_compute_location(block=b12, decision=3)
sch.compute_at(block=b12, loop=l16, preserve_unit_loops=True)
l17 = sch.sample_compute_location(block=b8, decision=2)
sch.compute_at(block=b8, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b7, decision=-2)
sch.compute_at(block=b7, loop=l18, preserve_unit_loops=True)
l19 = sch.sample_compute_location(block=b5, decision=4)
sch.compute_at(block=b5, loop=l19, preserve_unit_loops=True)
l20 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l21, preserve_unit_loops=True)
[19:07:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"
[19:07:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 208, 208, 4, 128, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 208, 208, 4], "float32"], ["TENSOR", [16, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 26, 13, 1, 1, 1, 2, 1, 1, 64, 1, 1, 1, 2, 4, 4, 1, 2, 1, 1, 1, 2, 1, 4, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(208, i2_0 * 8 + i2_1 * 4 + i2_2)
                    ow = T.axis.spatial(208, i3_0 * 16 + i3_2 * 4 + i3_3)
                    oc_block = T.axis.spatial(4, i4_3)
                    ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 208, 208, 4], "float32"], ["TENSOR", [16, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 2, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[26, 2, 4, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 1, 4, 4])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:07:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 26, 13, 1, 1, 1, 2, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 2, 4, 4, 1, 2, 1, 1, 1, 2, 1, 4, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(208, i2_0 * 8 + i2_1 * 4 + i2_2)
                        ow = T.axis.spatial(208, i3_0 * 16 + i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 208, 208, 4], "float32"], ["TENSOR", [16, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 4, 16, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(208, i2_0 * 8 + i2_1 * 4 + ax2)
                        ax3_1 = T.axis.spatial(208, i3_0 * 16 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 2, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[26, 2, 4, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 1, 4, 4])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 26, 13, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 1, 64, 1, 1, 1, 2, 4, 4, 1, 2, 1, 1, 1, 2, 1, 4, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(208, i2_0 * 8 + i2_1 * 4 + i2_2)
                        ow = T.axis.spatial(208, i3_0 * 16 + i3_2 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3)
                        ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 208, 208, 4], "float32"], ["TENSOR", [16, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 8, 16, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(208, i2_0 * 8 + ax2)
                        ax3_1 = T.axis.spatial(208, i3_0 * 16 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 2, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[26, 2, 4, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 1, 4, 4])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[64, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"
[19:07:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_log = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_tanh = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 208 and ax3 < 208, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 208, 208):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 208 and ax3 < 208, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 208, 208, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
    

[19:07:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 64, 208, 208], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
            for i0, i1 in T.grid(1, 16):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 208, 208, 4):
                    with T.block("T_exp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.exp(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], dtype="float32")
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 208, 208):
                    with T.block("T_layout_trans"):
                        ax0_2 = T.axis.spatial(1, ax0)
                        ax1_2 = T.axis.spatial(64, i1 * 4 + ax1)
                        ax2_2, ax3_2 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(T_exp[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4])
                        T.writes(T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_2])
                        T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_2] = T.if_then_else(ax0_2 < 1 and ax1_2 < 64 and ax2_2 < 208 and ax3_2 < 208, T_exp[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4], T.float32(0), dtype="float32")
                for i2, i3, i4 in T.grid(208, 208, 4):
                    for ax0_3, ax1_3, ax2_3, ax3_3, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_layout_trans_1"):
                            ax0_4 = T.axis.spatial(1, ax0_3)
                            ax1_4 = T.axis.spatial(16, i1 + ax1_3)
                            ax2_4 = T.axis.spatial(208, i2 + ax2_3)
                            ax3_4 = T.axis.spatial(208, i3 + ax3_3)
                            ax4_2 = T.axis.spatial(4, i4 + ax4)
                            T.reads(placeholder_1[ax0_4, 0, 0, 0], T_layout_trans[ax0_4, ax1_4 * 4 + ax4_2, ax2_4, ax3_4])
                            T.writes(T_layout_trans_1[ax0_4, ax1_4, ax2_4, ax3_4, ax4_2])
                            T_layout_trans_1[ax0_4, ax1_4, ax2_4, ax3_4, ax4_2] = T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4_2 < 64 and ax2_4 < 208 and ax3_4 < 208, T.tanh(T.log(placeholder_1[ax0_4, 0, 0, 0] + T_layout_trans[ax0_4, ax1_4 * 4 + ax4_2, ax2_4, ax3_4], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                    with T.block("T_multiply"):
                        ax0_5, ax1_5, ax2_5, ax3_5, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3], T_layout_trans_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3])
                        T.writes(T_multiply[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3])
                        T_multiply[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3] = placeholder[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3] * T_layout_trans_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=4)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
[19:07:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"
[19:07:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 209, 209, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 209, 209, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 209 and 1 <= i3_1 and i3_1 < 209, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 104, 104, 4, 64, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 208, 208, 4], "float32"], ["TENSOR", [32, 16, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 104, 104, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 209, 209, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0 in T.grid(1, 1, 1, 1, 1, 1, 8, 4, 2, 1, 8, 3, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 51, 103, 4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(16, i5_0 * 2 + ax1)
                        i2 = T.axis.spatial(209, i2_1 * 52 + i6_0 + ax2)
                        i3 = T.axis.spatial(209, i3_1 * 104 + i7_0 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 209 and 1 <= i3 and i3 < 209, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 4, 2, 8, 1, 1, 1, 4, 13, 13, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_3)
                        oh = T.axis.spatial(104, i2_1 * 26 + i2_2 * 13 + i2_3)
                        ow = T.axis.spatial(104, i3_1 * 52 + i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 208, 208, 4], "float32"], ["TENSOR", [32, 16, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 104, 104, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 2, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 4, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[19:07:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 209, 209, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 1, 1, 8, 4, 2, 1):
                for i5_0, i6_0, i7_0 in T.grid(8, 3, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 51, 103, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(16, i5_0 * 2 + ax1)
                            i2 = T.axis.spatial(209, i2_1 * 52 + i6_0 + ax2)
                            i3 = T.axis.spatial(209, i3_1 * 104 + i7_0 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 209 and 1 <= i3 and i3 < 209, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 4, 2, 8, 1, 1, 1, 4, 13, 13, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_3)
                            oh = T.axis.spatial(104, i2_1 * 26 + i2_2 * 13 + i2_3)
                            ow = T.axis.spatial(104, i3_1 * 52 + i3_2 * 13 + i3_3)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 208, 208, 4], "float32"], ["TENSOR", [32, 16, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 26, 52, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_1 * 26 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_1 * 52 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 2, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 4, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 209, 209, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 8, 4, 2, 1, 8):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 53, 105, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(16, i5_0 * 2 + ax1)
                            i2 = T.axis.spatial(209, i2_1 * 52 + ax2)
                            i3 = T.axis.spatial(209, i3_1 * 104 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 209 and 1 <= i3 and i3 < 209, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 2, 4, 2, 8, 1, 1, 1, 4, 13, 13, 2):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i1_1 * 4 + i1_3)
                            oh = T.axis.spatial(104, i2_1 * 26 + i2_2 * 13 + i2_3)
                            ow = T.axis.spatial(104, i3_1 * 52 + i3_2 * 13 + i3_3)
                            oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            ic = T.axis.reduce(64, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 208, 208, 4], "float32"], ["TENSOR", [32, 16, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 104, 104, 4):
                    with T.block("T_add"):
                        ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 8, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 2, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 4, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"
[19:07:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 104, 104, 4, 128, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [16, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 26, 1, 1, 1, 1, 1, 4, 1, 32, 1, 1, 1, 1, 4, 13, 4, 4, 1, 1, 1, 8, 1, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_3)
                    oh = T.axis.spatial(104, i2_0 * 4 + i2_2)
                    ow = T.axis.spatial(104, i3_1 * 26 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [16, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 1, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[26, 1, 4, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 13, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:07:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 26, 1, 1, 1, 1, 1, 4, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 1, 4, 13, 4, 4, 1, 1, 1, 8, 1, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_3)
                        oh = T.axis.spatial(104, i2_0 * 4 + i2_2)
                        ow = T.axis.spatial(104, i3_1 * 26 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [16, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 4, 26, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_0 * 4 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_1 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 1, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[26, 1, 4, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 13, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 26, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 4, 1, 32, 1, 1, 1, 1, 4, 13, 4, 4, 1, 1, 1, 8, 1, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_3)
                        oh = T.axis.spatial(104, i2_0 * 4 + i2_2)
                        ow = T.axis.spatial(104, i3_1 * 26 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [16, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 4, 104, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_0 * 4 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 1, 1, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[26, 1, 4, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 13, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"
[19:07:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_log = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_tanh = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 104 and ax3 < 104, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 104 and ax3 < 104, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
    

[19:07:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0, i1, i2 in T.grid(1, 16, 104):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 104, 4):
                    for ax0_1, ax1_1, ax2_1, ax3_1, ax4_1 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_exp"):
                            ax0_2 = T.axis.spatial(1, ax0_1)
                            ax1_2 = T.axis.spatial(16, i1 + ax1_1)
                            ax2_2 = T.axis.spatial(104, i2 + ax2_1)
                            ax3_2 = T.axis.spatial(104, ax3 + ax3_1)
                            ax4_2 = T.axis.spatial(4, ax4 + ax4_1)
                            T.reads(placeholder[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2])
                            T.writes(T_exp[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2])
                            T_exp[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] = T.exp(placeholder[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2], dtype="float32")
                    for ax0_3, ax1_3, ax2_3, ax3_3 in T.grid(1, 1, 1, 1):
                        with T.block("T_layout_trans"):
                            ax0_4 = T.axis.spatial(1, ax0_3)
                            ax1_4 = T.axis.spatial(64, i1 * 4 + ax4 + ax1_3)
                            ax2_4 = T.axis.spatial(104, i2 + ax2_3)
                            ax3_4 = T.axis.spatial(104, ax3 + ax3_3)
                            T.reads(T_exp[ax0_4, ax1_4 // 4, ax2_4, ax3_4, ax1_4 % 4])
                            T.writes(T_layout_trans[ax0_4, ax1_4, ax2_4, ax3_4])
                            T_layout_trans[ax0_4, ax1_4, ax2_4, ax3_4] = T.if_then_else(ax0_4 < 1 and ax1_4 < 64 and ax2_4 < 104 and ax3_4 < 104, T_exp[ax0_4, ax1_4 // 4, ax2_4, ax3_4, ax1_4 % 4], T.float32(0), dtype="float32")
                    with T.block("T_layout_trans_1"):
                        ax0_5 = T.axis.spatial(1, ax0)
                        ax1_5 = T.axis.spatial(16, i1 + ax1)
                        ax2_5 = T.axis.spatial(104, i2 + ax2)
                        ax3_5, ax4_3 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder_1[ax0_5, 0, 0, 0], T_layout_trans[ax0_5, ax1_5 * 4 + ax4_3, ax2_5, ax3_5])
                        T.writes(T_layout_trans_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3])
                        T_layout_trans_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3] = T.if_then_else(ax0_5 < 1 and ax1_5 * 4 + ax4_3 < 64 and ax2_5 < 104 and ax3_5 < 104, T.tanh(T.log(placeholder_1[ax0_5, 0, 0, 0] + T_layout_trans[ax0_5, ax1_5 * 4 + ax4_3, ax2_5, ax3_5], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                for i3, i4 in T.grid(104, 4):
                    with T.block("T_multiply"):
                        ax0_6, ax1_6, ax2_6, ax3_6, ax4_4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder[ax0_6, ax1_6, ax2_6, ax3_6, ax4_4], T_layout_trans_1[ax0_6, ax1_6, ax2_6, ax3_6, ax4_4])
                        T.writes(T_multiply[ax0_6, ax1_6, ax2_6, ax3_6, ax4_4])
                        T_multiply[ax0_6, ax1_6, ax2_6, ax3_6, ax4_4] = placeholder[ax0_6, ax1_6, ax2_6, ax3_6, ax4_4] * T_layout_trans_1[ax0_6, ax1_6, ax2_6, ax3_6, ax4_4]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=2)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=7)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=7)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
[19:07:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"
[19:07:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 16, 106, 106, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 106, 106, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 105 and 1 <= i3_1 and i3_1 < 105, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 104, 104, 4, 64, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 106, 106, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 2, 2, 8, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 54, 15, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(106, i2_0 * 52 + ax2)
                        i3 = T.axis.spatial(106, i3_0 * 13 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 105 and 1 <= i3 and i3 < 105, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(26, 1, 1, 2, 3, 1, 1, 2, 2, 13, 4, 32, 1, 3, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(104, i2_0 * 52 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(104, i3_0 * 13 + i3_2)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 26, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[19:07:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 106, 106, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1 in T.grid(1, 2, 2, 8, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 54, 15, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(106, i2_0 * 52 + ax2)
                        i3 = T.axis.spatial(106, i3_0 * 13 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 105 and 1 <= i3 and i3 < 105, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_1, i3_1, i4_1 in T.grid(26, 1, 1):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 3, 1, 1, 2, 2, 13, 4, 32, 1, 3, 1, 2, 1, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(104, i2_0 * 52 + i2_1 * 2 + i2_2)
                            ow = T.axis.spatial(104, i3_0 * 13 + i3_2)
                            oc_block = T.axis.spatial(4, i4_2)
                            ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 2, 13, 4):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + ax1)
                            ax2_1 = T.axis.spatial(104, i2_0 * 52 + i2_1 * 2 + ax2)
                            ax3_1 = T.axis.spatial(104, i3_0 * 13 + ax3)
                            ax4_1 = T.axis.spatial(4, ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 26, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 16, 106, 106, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 2, 8, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 2, 26, 1, 1, 2):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 4, 15, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(16, i5_0 * 8 + ax1)
                            i2 = T.axis.spatial(106, i2_0 * 52 + i2_1 * 2 + ax2)
                            i3 = T.axis.spatial(106, i3_0 * 13 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 105 and 1 <= i3 and i3 < 105, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 2, 2, 13, 4, 32, 1, 3, 1, 2, 1, 1, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(104, i2_0 * 52 + i2_1 * 2 + i2_2)
                            ow = T.axis.spatial(104, i3_0 * 13 + i3_2)
                            oc_block = T.axis.spatial(4, i4_2)
                            ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 52, 13, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_0 * 52 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 13 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 26, 2, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[8, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"
[19:07:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 104, 104, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_log = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_tanh = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 104 and ax3 < 104, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 104 and ax3 < 104, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4], T_multiply[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = placeholder_2[ax0, ax1, ax2, ax3, ax4] + T_multiply[ax0, ax1, ax2, ax3, ax4]
    

[19:07:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 104, 104, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0, i1 in T.grid(1, 16):
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 104, 104):
                    for ax0_1, ax1_1, ax2_1, ax3_1, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_exp"):
                            ax0_2 = T.axis.spatial(1, ax0_1)
                            ax1_2 = T.axis.spatial(16, i1 + ax1_1)
                            ax2_2 = T.axis.spatial(104, ax2 + ax2_1)
                            ax3_2 = T.axis.spatial(104, ax3 + ax3_1)
                            ax4_1 = T.axis.spatial(4, ax1 + ax4)
                            T.reads(placeholder[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1])
                            T.writes(T_exp[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1])
                            T_exp[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1] = T.exp(placeholder[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1], dtype="float32")
                    with T.block("T_layout_trans"):
                        ax0_3 = T.axis.spatial(1, ax0)
                        ax1_3 = T.axis.spatial(64, i1 * 4 + ax1)
                        ax2_3, ax3_3 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(T_exp[ax0_3, ax1_3 // 4, ax2_3, ax3_3, ax1_3 % 4])
                        T.writes(T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3])
                        T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3] = T.if_then_else(ax0_3 < 1 and ax1_3 < 64 and ax2_3 < 104 and ax3_3 < 104, T_exp[ax0_3, ax1_3 // 4, ax2_3, ax3_3, ax1_3 % 4], T.float32(0), dtype="float32")
                for ax0_4, ax1_4, ax2_4, ax3_4, ax4 in T.grid(1, 1, 104, 104, 4):
                    with T.block("T_layout_trans_1"):
                        ax0_5 = T.axis.spatial(1, ax0_4)
                        ax1_5 = T.axis.spatial(16, i1 + ax1_4)
                        ax2_5, ax3_5, ax4_2 = T.axis.remap("SSS", [ax2_4, ax3_4, ax4])
                        T.reads(placeholder_1[ax0_5, 0, 0, 0], T_layout_trans[ax0_5, ax1_5 * 4 + ax4_2, ax2_5, ax3_5])
                        T.writes(T_layout_trans_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_2])
                        T_layout_trans_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_2] = T.if_then_else(ax0_5 < 1 and ax1_5 * 4 + ax4_2 < 64 and ax2_5 < 104 and ax3_5 < 104, T.tanh(T.log(placeholder_1[ax0_5, 0, 0, 0] + T_layout_trans[ax0_5, ax1_5 * 4 + ax4_2, ax2_5, ax3_5], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                for i2, i3, i4 in T.grid(104, 104, 4):
                    with T.block("T_add_1"):
                        ax0_6, ax1_6, ax2_6, ax3_6, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder_2[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3], placeholder[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3], T_layout_trans_1[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3])
                        T.writes(T_add[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3])
                        T_add[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3] = placeholder_2[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3] + placeholder[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3] * T_layout_trans_1[ax0_6, ax1_6, ax2_6, ax3_6, ax4_3]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v8 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v8)
l9 = sch.sample_compute_location(block=b5, decision=1)
sch.compute_at(block=b5, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l10, preserve_unit_loops=True)
l11 = sch.sample_compute_location(block=b0, decision=5)
sch.compute_at(block=b0, loop=l11, preserve_unit_loops=True)
[19:07:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"
[19:07:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 104, 104, 4, 64, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 1, 8, 4, 1, 1, 52, 13, 1, 2, 1, 1, 1, 2, 1, 1, 1, 32, 1, 1, 1, 1, 2, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(16, i1_0 * 2 + i1_2)
                    oh = T.axis.spatial(104, i2_1 * 2 + i2_3)
                    ow = T.axis.spatial(104, i3_0 * 13 + i3_1)
                    oc_block = T.axis.spatial(4, i4_0)
                    ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 1, 2, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 52, 1, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 13, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[2, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:07:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 8, 4, 1, 1, 52, 13, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 2, 1, 1, 1, 32, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 2 + i1_2)
                        oh = T.axis.spatial(104, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(104, i3_0 * 13 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 2, 1, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 13 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 1, 2, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 52, 1, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 13, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[2, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 16, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 1, 8, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 52, 13, 1, 2, 1, 1, 1, 2, 1, 1, 1, 32, 1, 1, 1, 1, 2, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_0 * 2 + i1_2)
                        oh = T.axis.spatial(104, i2_1 * 2 + i2_3)
                        ow = T.axis.spatial(104, i3_0 * 13 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0)
                        ic = T.axis.reduce(64, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 16, 104, 104, 4], "float32"], ["TENSOR", [16, 16, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 104, 13, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(104, ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 13 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 1, 2, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 52, 1, 2])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 13, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[2, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"
[19:07:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 104, 104, 4), "float32"], T_concat: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_log = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_tanh = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_exp_1 = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_layout_trans_2 = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_log_1 = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_tanh_1 = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_layout_trans_3 = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder_2[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 104 and ax3 < 104, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 104 and ax3 < 104, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder_2[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_exp_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp_1[ax0, ax1, ax2, ax3, ax4])
                T_exp_1[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_layout_trans_2"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_2[ax0, ax1, ax2, ax3])
                T_layout_trans_2[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 104 and ax3 < 104, T_exp_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans_2[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans_2[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_log_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_log_1[ax0, ax1, ax2, ax3])
                T_log_1[ax0, ax1, ax2, ax3] = T.log(T_add_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_tanh_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log_1[ax0, ax1, ax2, ax3])
                T.writes(T_tanh_1[ax0, ax1, ax2, ax3])
                T_tanh_1[ax0, ax1, ax2, ax3] = T.tanh(T_log_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_layout_trans_3"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_3[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_3[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 104 and ax3 < 104, T_tanh_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 104, 104, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_3[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 104, 104, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1 - 16, ax2, ax3, ax4], T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(16 <= ax1, T_multiply[ax0, ax1 - 16, ax2, ax3, ax4], T_multiply_1[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[19:07:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 104, 104, 4), "float32"], T_concat: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            T_layout_trans_2 = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
            T_layout_trans_3 = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
            for i0, i1, i2 in T.grid(1, 32, 104):
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 104):
                    with T.block("T_layout_trans_2"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(104, i2 + ax2)
                        ax3_1 = T.axis.spatial(104, ax3)
                        T.where(i1 * 4 + ax1 < 64)
                        T.reads(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                        T.writes(T_layout_trans_2[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_layout_trans_2[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 64 and ax2_1 < 104 and ax3_1 < 104, T.exp(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], dtype="float32"), T.float32(0), dtype="float32")
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 104, 4):
                    with T.block("T_layout_trans_3"):
                        ax0_2 = T.axis.spatial(1, ax0)
                        ax1_2 = T.axis.spatial(16, i1 + ax1)
                        ax2_2 = T.axis.spatial(104, i2 + ax2)
                        ax3_2, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.where(i1 < 16)
                        T.reads(placeholder_1[ax0_2, 0, 0, 0], T_layout_trans_2[ax0_2, ax1_2 * 4 + ax4_1, ax2_2, ax3_2])
                        T.writes(T_layout_trans_3[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1])
                        T_layout_trans_3[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1] = T.if_then_else(ax0_2 < 1 and ax1_2 * 4 + ax4_1 < 64 and ax2_2 < 104 and ax3_2 < 104, T.tanh(T.log(placeholder_1[ax0_2, 0, 0, 0] + T_layout_trans_2[ax0_2, ax1_2 * 4 + ax4_1, ax2_2, ax3_2], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                for i3 in T.serial(104):
                    for ax0_3, ax1_3 in T.grid(1, 4):
                        for ax0_4, ax1_4, ax2_3, ax3_3, ax4 in T.grid(1, 1, 1, 1, 1):
                            with T.block("T_exp"):
                                ax0_5 = T.axis.spatial(1, ax0_4)
                                ax1_5 = T.axis.spatial(16, i1 - 16 + ax1_4)
                                ax2_4 = T.axis.spatial(104, i2 + ax2_3)
                                ax3_4 = T.axis.spatial(104, i3 + ax3_3)
                                ax4_2 = T.axis.spatial(4, ax1_3 + ax4)
                                T.where(16 <= i1)
                                T.reads(placeholder_2[ax0_5, ax1_5, ax2_4, ax3_4, ax4_2])
                                T.writes(T_exp[ax0_5, ax1_5, ax2_4, ax3_4, ax4_2])
                                T_exp[ax0_5, ax1_5, ax2_4, ax3_4, ax4_2] = T.exp(placeholder_2[ax0_5, ax1_5, ax2_4, ax3_4, ax4_2], dtype="float32")
                        for ax2_5, ax3_5 in T.grid(1, 1):
                            with T.block("T_layout_trans"):
                                ax0_6 = T.axis.spatial(1, ax0_3)
                                ax1_6 = T.axis.spatial(64, i1 * 4 - 64 + ax1_3)
                                ax2_6 = T.axis.spatial(104, i2 + ax2_5)
                                ax3_6 = T.axis.spatial(104, i3 + ax3_5)
                                T.where(64 <= i1 * 4 + ax1_3)
                                T.reads(T_exp[ax0_6, ax1_6 // 4, ax2_6, ax3_6, ax1_6 % 4])
                                T.writes(T_layout_trans[ax0_6, ax1_6, ax2_6, ax3_6])
                                T_layout_trans[ax0_6, ax1_6, ax2_6, ax3_6] = T.if_then_else(ax0_6 < 1 and ax1_6 < 64 and ax2_6 < 104 and ax3_6 < 104, T_exp[ax0_6, ax1_6 // 4, ax2_6, ax3_6, ax1_6 % 4], T.float32(0), dtype="float32")
                    for i4 in T.serial(4):
                        for ax0_7, ax1_7, ax2_7, ax3_7, ax4_3 in T.grid(1, 1, 1, 1, 1):
                            with T.block("T_layout_trans_1"):
                                ax0_8 = T.axis.spatial(1, ax0_7)
                                ax1_8 = T.axis.spatial(16, i1 - 16 + ax1_7)
                                ax2_8 = T.axis.spatial(104, i2 + ax2_7)
                                ax3_8 = T.axis.spatial(104, i3 + ax3_7)
                                ax4_4 = T.axis.spatial(4, i4 + ax4_3)
                                T.where(16 <= i1)
                                T.reads(placeholder_1[ax0_8, 0, 0, 0], T_layout_trans[ax0_8, ax1_8 * 4 + ax4_4, ax2_8, ax3_8])
                                T.writes(T_layout_trans_1[ax0_8, ax1_8, ax2_8, ax3_8, ax4_4])
                                T_layout_trans_1[ax0_8, ax1_8, ax2_8, ax3_8, ax4_4] = T.if_then_else(ax0_8 < 1 and ax1_8 * 4 + ax4_4 < 64 and ax2_8 < 104 and ax3_8 < 104, T.tanh(T.log(placeholder_1[ax0_8, 0, 0, 0] + T_layout_trans[ax0_8, ax1_8 * 4 + ax4_4, ax2_8, ax3_8], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                        with T.block("T_concat"):
                            ax0_9, ax1_9, ax2_9, ax3_9, ax4_5 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                            T.reads(placeholder_2[ax0_9, ax1_9 - 16, ax2_9, ax3_9, ax4_5], T_layout_trans_1[ax0_9, ax1_9 - 16, ax2_9, ax3_9, ax4_5], placeholder[ax0_9, ax1_9, ax2_9, ax3_9, ax4_5], T_layout_trans_3[ax0_9, ax1_9, ax2_9, ax3_9, ax4_5])
                            T.writes(T_concat[ax0_9, ax1_9, ax2_9, ax3_9, ax4_5])
                            T_concat[ax0_9, ax1_9, ax2_9, ax3_9, ax4_5] = T.if_then_else(16 <= ax1_9, placeholder_2[ax0_9, ax1_9 - 16, ax2_9, ax3_9, ax4_5] * T_layout_trans_1[ax0_9, ax1_9 - 16, ax2_9, ax3_9, ax4_5], placeholder[ax0_9, ax1_9, ax2_9, ax3_9, ax4_5] * T_layout_trans_3[ax0_9, ax1_9, ax2_9, ax3_9, ax4_5], dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="T_exp_1", func_name="main")
b8 = sch.get_block(name="T_layout_trans_2", func_name="main")
b9 = sch.get_block(name="T_add_1", func_name="main")
b10 = sch.get_block(name="T_log_1", func_name="main")
b11 = sch.get_block(name="T_tanh_1", func_name="main")
b12 = sch.get_block(name="T_layout_trans_3", func_name="main")
b13 = sch.get_block(name="T_multiply_1", func_name="main")
b14 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b13)
sch.compute_inline(block=b11)
sch.compute_inline(block=b10)
sch.compute_inline(block=b9)
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
l16 = sch.sample_compute_location(block=b12, decision=2)
sch.compute_at(block=b12, loop=l16, preserve_unit_loops=True)
l17 = sch.sample_compute_location(block=b8, decision=2)
sch.compute_at(block=b8, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b7, decision=-2)
sch.compute_at(block=b7, loop=l18, preserve_unit_loops=True)
l19 = sch.sample_compute_location(block=b5, decision=4)
sch.compute_at(block=b5, loop=l19, preserve_unit_loops=True)
l20 = sch.sample_compute_location(block=b1, decision=3)
sch.compute_at(block=b1, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b0, decision=5)
sch.compute_at(block=b0, loop=l21, preserve_unit_loops=True)
[19:07:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"
[19:07:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 104, 104, 4, 128, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 104, 104, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 13, 4, 1, 1, 1, 8, 1, 2, 16, 1, 1, 1, 1, 1, 26, 1, 8, 1, 1, 1, 4, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_3)
                    oh = T.axis.spatial(104, i2_0 * 8 + i2_1)
                    ow = T.axis.spatial(104, i3_0 * 26 + i3_2)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                    ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 104, 104, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 1, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[13, 8, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 26, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[16, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:07:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 13, 4, 1, 1, 1, 8, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 1, 1, 26, 1, 8, 1, 1, 1, 4, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_3)
                        oh = T.axis.spatial(104, i2_0 * 8 + i2_1)
                        ow = T.axis.spatial(104, i3_0 * 26 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 26, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_0 * 8 + i2_1 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 1, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[13, 8, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 26, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[16, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 13, 4, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 8, 1, 2, 16, 1, 1, 1, 1, 1, 26, 1, 8, 1, 1, 1, 4, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 4 + i1_3)
                        oh = T.axis.spatial(104, i2_0 * 8 + i2_1)
                        ow = T.axis.spatial(104, i3_0 * 26 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 8, 26, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(104, i2_0 * 8 + ax2)
                        ax3_1 = T.axis.spatial(104, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 1, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[13, 8, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 26, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[16, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"
[19:07:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 128, 104, 104], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 104, 104], dtype="float32")
        T_log = T.alloc_buffer([1, 128, 104, 104], dtype="float32")
        T_tanh = T.alloc_buffer([1, 128, 104, 104], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 104, 104, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 104, 104):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 128 and ax2 < 104 and ax3 < 104, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 104, 104):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 128, 104, 104):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 104, 104):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 104, 104, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 128 and ax2 < 104 and ax3 < 104, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 104, 104, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
    

[19:07:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 128, 104, 104], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
            for i0, i1, i2 in T.grid(1, 32, 104):
                for ax0, ax1 in T.grid(1, 4):
                    for ax0_1, ax1_1, ax2, ax3, ax4 in T.grid(1, 1, 1, 104, 1):
                        with T.block("T_exp"):
                            ax0_2 = T.axis.spatial(1, ax0_1)
                            ax1_2 = T.axis.spatial(32, i1 + ax1_1)
                            ax2_1 = T.axis.spatial(104, i2 + ax2)
                            ax3_1 = T.axis.spatial(104, ax3)
                            ax4_1 = T.axis.spatial(4, ax1 + ax4)
                            T.reads(placeholder[ax0_2, ax1_2, ax2_1, ax3_1, ax4_1])
                            T.writes(T_exp[ax0_2, ax1_2, ax2_1, ax3_1, ax4_1])
                            T_exp[ax0_2, ax1_2, ax2_1, ax3_1, ax4_1] = T.exp(placeholder[ax0_2, ax1_2, ax2_1, ax3_1, ax4_1], dtype="float32")
                    for ax2, ax3 in T.grid(1, 104):
                        with T.block("T_layout_trans"):
                            ax0_3 = T.axis.spatial(1, ax0)
                            ax1_3 = T.axis.spatial(128, i1 * 4 + ax1)
                            ax2_2 = T.axis.spatial(104, i2 + ax2)
                            ax3_2 = T.axis.spatial(104, ax3)
                            T.reads(T_exp[ax0_3, ax1_3 // 4, ax2_2, ax3_2, ax1_3 % 4])
                            T.writes(T_layout_trans[ax0_3, ax1_3, ax2_2, ax3_2])
                            T_layout_trans[ax0_3, ax1_3, ax2_2, ax3_2] = T.if_then_else(ax0_3 < 1 and ax1_3 < 128 and ax2_2 < 104 and ax3_2 < 104, T_exp[ax0_3, ax1_3 // 4, ax2_2, ax3_2, ax1_3 % 4], T.float32(0), dtype="float32")
                for i3 in T.serial(104):
                    for ax0_4, ax1_4, ax2_3, ax3_3, ax4 in T.grid(1, 1, 1, 1, 4):
                        with T.block("T_layout_trans_1"):
                            ax0_5 = T.axis.spatial(1, ax0_4)
                            ax1_5 = T.axis.spatial(32, i1 + ax1_4)
                            ax2_4 = T.axis.spatial(104, i2 + ax2_3)
                            ax3_4 = T.axis.spatial(104, i3 + ax3_3)
                            ax4_2 = T.axis.spatial(4, ax4)
                            T.reads(placeholder_1[ax0_5, 0, 0, 0], T_layout_trans[ax0_5, ax1_5 * 4 + ax4_2, ax2_4, ax3_4])
                            T.writes(T_layout_trans_1[ax0_5, ax1_5, ax2_4, ax3_4, ax4_2])
                            T_layout_trans_1[ax0_5, ax1_5, ax2_4, ax3_4, ax4_2] = T.if_then_else(ax0_5 < 1 and ax1_5 * 4 + ax4_2 < 128 and ax2_4 < 104 and ax3_4 < 104, T.tanh(T.log(placeholder_1[ax0_5, 0, 0, 0] + T_layout_trans[ax0_5, ax1_5 * 4 + ax4_2, ax2_4, ax3_4], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                    for i4 in T.serial(4):
                        with T.block("T_multiply"):
                            ax0_6, ax1_6, ax2_5, ax3_5, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                            T.reads(placeholder[ax0_6, ax1_6, ax2_5, ax3_5, ax4_3], T_layout_trans_1[ax0_6, ax1_6, ax2_5, ax3_5, ax4_3])
                            T.writes(T_multiply[ax0_6, ax1_6, ax2_5, ax3_5, ax4_3])
                            T_multiply[ax0_6, ax1_6, ax2_5, ax3_5, ax4_3] = placeholder[ax0_6, ax1_6, ax2_5, ax3_5, ax4_3] * T_layout_trans_1[ax0_6, ax1_6, ax2_5, ax3_5, ax4_3]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=3)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=2)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
[19:07:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"
[19:07:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 105, 105, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 105, 105, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 105 and 1 <= i3_1 and i3_1 < 105, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 52, 52, 4, 128, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 105, 105, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 8, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 53, 105, 4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(105, i2_0 * 52 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 105 and 1 <= i3 and i3 < 105, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 4, 2, 13, 1, 2, 3, 1, 1, 2, 13, 1, 4, 64, 1, 3, 1, 1, 1, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 8 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(52, i2_0 * 26 + i2_1 * 13 + i2_2)
                        ow = T.axis.spatial(52, i3_1 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2)
                        ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[19:07:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 105, 105, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 2, 1, 1, 1, 4, 2, 13, 1):
                for i5_0, i6_0 in T.grid(2, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 25, 9, 4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(32, i5_0 * 16 + ax1)
                            i2 = T.axis.spatial(105, i2_0 * 52 + i2_1 * 26 + i6_0 + ax2)
                            i3 = T.axis.spatial(105, i3_1 * 8 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 105 and 1 <= i3 and i3 < 105, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 13, 1, 4, 64, 1, 3, 1, 1, 1, 4, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_0 * 8 + i1_1 * 2 + i1_2)
                            oh = T.axis.spatial(52, i2_0 * 26 + i2_1 * 13 + i2_2)
                            ow = T.axis.spatial(52, i3_1 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i4_2)
                            ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 13, 4, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 8 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_0 * 26 + i2_1 * 13 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_1 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 105, 105, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 2, 1, 1):
                for i0_1, i1_1 in T.grid(1, 4):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 53, 105, 4):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(105, i2_0 * 52 + ax2)
                            i3, i4 = T.axis.remap("SS", [ax3, ax4])
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 105 and 1 <= i3 and i3 < 105, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 13, 1, 2, 3, 1, 1, 2, 13, 1, 4, 64, 1, 3, 1, 1, 1, 4, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i1_0 * 8 + i1_1 * 2 + i1_2)
                            oh = T.axis.spatial(52, i2_0 * 26 + i2_1 * 13 + i2_2)
                            ow = T.axis.spatial(52, i3_1 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i4_2)
                            ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 26, 52, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_0 * 26 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"
[19:07:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 52, 52, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 4, 2, 2, 1, 1, 1, 26, 1, 128, 1, 1, 1, 4, 1, 1, 2, 2, 1, 1, 1, 2, 13, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(52, i2_0 * 13 + i2_3)
                    ow = T.axis.spatial(52, i3_0 * 26 + i3_1)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                    ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 1, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 26, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:07:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 4, 2, 2, 1, 1, 1, 26, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 4, 1, 1, 2, 2, 1, 1, 1, 2, 13, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(52, i2_0 * 13 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 13, 1, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_0 * 13 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + i3_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 1, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 26, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 4, 2, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 26, 1, 128, 1, 1, 1, 4, 1, 1, 2, 2, 1, 1, 1, 2, 13, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(52, i2_0 * 13 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 13, 26, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_0 * 13 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 4, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 1, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 26, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"
[19:07:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_log = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_tanh = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 128 and ax2 < 52 and ax3 < 52, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 128 and ax2 < 52 and ax3 < 52, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
    

[19:07:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0, i1 in T.grid(1, 32):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 52, 52, 4):
                    with T.block("T_exp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.exp(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], dtype="float32")
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 52, 52):
                    with T.block("T_layout_trans"):
                        ax0_2 = T.axis.spatial(1, ax0)
                        ax1_2 = T.axis.spatial(128, i1 * 4 + ax1)
                        ax2_2, ax3_2 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(T_exp[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4])
                        T.writes(T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_2])
                        T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_2] = T.if_then_else(ax0_2 < 1 and ax1_2 < 128 and ax2_2 < 52 and ax3_2 < 52, T_exp[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4], T.float32(0), dtype="float32")
                for i2, i3, i4 in T.grid(52, 52, 4):
                    for ax0_3, ax1_3, ax2_3, ax3_3, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_layout_trans_1"):
                            ax0_4 = T.axis.spatial(1, ax0_3)
                            ax1_4 = T.axis.spatial(32, i1 + ax1_3)
                            ax2_4 = T.axis.spatial(52, i2 + ax2_3)
                            ax3_4 = T.axis.spatial(52, i3 + ax3_3)
                            ax4_2 = T.axis.spatial(4, i4 + ax4)
                            T.reads(placeholder_1[ax0_4, 0, 0, 0], T_layout_trans[ax0_4, ax1_4 * 4 + ax4_2, ax2_4, ax3_4])
                            T.writes(T_layout_trans_1[ax0_4, ax1_4, ax2_4, ax3_4, ax4_2])
                            T_layout_trans_1[ax0_4, ax1_4, ax2_4, ax3_4, ax4_2] = T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4_2 < 128 and ax2_4 < 52 and ax3_4 < 52, T.tanh(T.log(placeholder_1[ax0_4, 0, 0, 0] + T_layout_trans[ax0_4, ax1_4 * 4 + ax4_2, ax2_4, ax3_4], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                    with T.block("T_multiply"):
                        ax0_5, ax1_5, ax2_5, ax3_5, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3], T_layout_trans_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3])
                        T.writes(T_multiply[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3])
                        T_multiply[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3] = placeholder[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3] * T_layout_trans_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_3]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=4)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
[19:07:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"
[19:07:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 54, 54, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 54, 54, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 52, 52, 4, 128, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 54, 54, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 54, 54, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 4, 1, 13, 2, 16, 3, 1, 1, 4, 13, 1, 2, 8, 1, 3, 1, 1, 4, 4, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(52, i2_2 * 4 + i2_3)
                        ow = T.axis.spatial(52, i3_1 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
[19:07:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 54, 54, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 54, 54, 4):
                    with T.block("data_pad"):
                        i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 4, 1, 13, 2):
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 3, 1, 1, 4, 13, 1, 2, 8, 1, 3, 1, 1, 4, 4, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + i1_2)
                            oh = T.axis.spatial(52, i2_2 * 4 + i2_3)
                            ow = T.axis.spatial(52, i3_1 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 52, 4, 2):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + ax1)
                            ax2_1 = T.axis.spatial(52, ax2)
                            ax3_1 = T.axis.spatial(52, i3_1 * 4 + ax3)
                            ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 54, 54, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 1):
                for i0_1, i1_1 in T.grid(1, 4):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 54, 54, 4):
                        with T.block("data_pad"):
                            i0, i1, i2, i3, i4 = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 13, 2, 16, 3, 1, 1, 4, 13, 1, 2, 8, 1, 3, 1, 1, 4, 4, 1):
                        with T.block("conv2d_NCHWc"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i1_0 * 16 + i1_1 * 4 + i1_2)
                            oh = T.axis.spatial(52, i2_2 * 4 + i2_3)
                            ow = T.axis.spatial(52, i3_1 * 4 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                            ic = T.axis.reduce(128, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 52, 52, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 16 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 4, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 4])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 4])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"
[19:07:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 52, 52, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_log = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_tanh = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 128 and ax2 < 52 and ax3 < 52, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 128 and ax2 < 52 and ax3 < 52, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4], T_multiply[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = placeholder_2[ax0, ax1, ax2, ax3, ax4] + T_multiply[ax0, ax1, ax2, ax3, ax4]
    

[19:07:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 52, 52, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 32, 52, 52):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 4):
                    with T.block("T_exp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1 + ax1)
                        ax2_1 = T.axis.spatial(52, i2 + ax2)
                        ax3_1 = T.axis.spatial(52, i3 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.exp(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], dtype="float32")
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 4):
                    with T.block("T_layout_trans_1"):
                        ax0_2 = T.axis.spatial(1, ax0)
                        ax1_2 = T.axis.spatial(32, i1 + ax1)
                        ax2_2 = T.axis.spatial(52, i2 + ax2)
                        ax3_2 = T.axis.spatial(52, i3 + ax3)
                        ax4_2 = T.axis.spatial(4, ax4)
                        T.reads(placeholder_1[ax0_2, 0, 0, 0], T_exp[ax0_2, ax4_2 // 4 + ax1_2, ax2_2, ax3_2, ax4_2 % 4])
                        T.writes(T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2])
                        T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] = T.if_then_else(ax0_2 < 1 and ax1_2 * 4 + ax4_2 < 128 and ax2_2 < 52 and ax3_2 < 52, T.tanh(T.log(placeholder_1[ax0_2, 0, 0, 0] + T.if_then_else(ax0_2 < 1 and ax1_2 * 4 + ax4_2 < 128 and ax2_2 < 52 and ax3_2 < 52, T_exp[ax0_2, (ax1_2 * 4 + ax4_2) // 4, ax2_2, ax3_2, (ax1_2 * 4 + ax4_2) % 4], T.float32(0), dtype="float32"), dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                for i4 in T.serial(4):
                    with T.block("T_add_1"):
                        ax0_3, ax1_3, ax2_3, ax3_3, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder_2[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3], placeholder[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3], T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                        T.writes(T_add[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                        T_add[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] = placeholder_2[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] + placeholder[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] * T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v8 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v8)
l9 = sch.sample_compute_location(block=b5, decision=3)
sch.compute_at(block=b5, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l10, preserve_unit_loops=True)
l11 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l11, preserve_unit_loops=True)
[19:07:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"
[19:07:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 52, 52, 4, 128, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 2, 2, 1, 4, 1, 13, 2, 128, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 13, 2, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_3)
                    oh = T.axis.spatial(52, i2_2 * 13 + i2_3)
                    ow = T.axis.spatial(52, i3_0 * 26 + i3_1 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                    ic = T.axis.reduce(128, i5_0)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 4, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 13, 1, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:07:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 2, 2, 1, 4, 1, 13, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 13, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(52, i2_2 * 13 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(128, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 52, 2, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(52, ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + i3_1 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 4, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 13, 1, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 1, 2, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 13, 2, 128, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 13, 2, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_1 * 2 + i1_3)
                        oh = T.axis.spatial(52, i2_2 * 13 + i2_3)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_1 * 2 + i3_3)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(128, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 52, 26, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 4, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 13, 1, 2])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"
[19:07:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 52, 52, 4), "float32"], T_concat: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_log = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_tanh = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_multiply = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_exp_1 = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_layout_trans_2 = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_log_1 = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_tanh_1 = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_layout_trans_3 = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder_2[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 128 and ax2 < 52 and ax3 < 52, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 128 and ax2 < 52 and ax3 < 52, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder_2[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_exp_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp_1[ax0, ax1, ax2, ax3, ax4])
                T_exp_1[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_layout_trans_2"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_2[ax0, ax1, ax2, ax3])
                T_layout_trans_2[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 128 and ax2 < 52 and ax3 < 52, T_exp_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans_2[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans_2[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_log_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_log_1[ax0, ax1, ax2, ax3])
                T_log_1[ax0, ax1, ax2, ax3] = T.log(T_add_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_tanh_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log_1[ax0, ax1, ax2, ax3])
                T.writes(T_tanh_1[ax0, ax1, ax2, ax3])
                T_tanh_1[ax0, ax1, ax2, ax3] = T.tanh(T_log_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_layout_trans_3"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_3[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_3[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 128 and ax2 < 52 and ax3 < 52, T_tanh_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_3[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T_multiply_1[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_3[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_multiply[ax0, ax1 - 32, ax2, ax3, ax4], T_multiply_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(32 <= ax1, T_multiply[ax0, ax1 - 32, ax2, ax3, ax4], T_multiply_1[ax0, ax1, ax2, ax3, ax4], dtype="float32")
    

[19:07:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 52, 52, 4), "float32"], T_concat: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
            T_layout_trans_1 = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            T_exp_1 = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            T_layout_trans_2 = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
            T_layout_trans_3 = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_exp_1"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1 + ax1)
                        ax2_1 = T.axis.spatial(52, i2 + ax2)
                        ax3_1 = T.axis.spatial(52, i3 + ax3)
                        ax4_1 = T.axis.spatial(4, i4 + ax4)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_exp_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_exp_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.exp(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], dtype="float32")
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("T_layout_trans_2"):
                        ax0_2 = T.axis.spatial(1, ax0)
                        ax1_2 = T.axis.spatial(128, i1 * 4 + i4 + ax1)
                        ax2_2 = T.axis.spatial(52, i2 + ax2)
                        ax3_2 = T.axis.spatial(52, i3 + ax3)
                        T.reads(T_exp_1[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4])
                        T.writes(T_layout_trans_2[ax0_2, ax1_2, ax2_2, ax3_2])
                        T_layout_trans_2[ax0_2, ax1_2, ax2_2, ax3_2] = T.if_then_else(ax0_2 < 1 and ax1_2 < 128 and ax2_2 < 52 and ax3_2 < 52, T_exp_1[ax0_2, ax1_2 // 4, ax2_2, ax3_2, ax1_2 % 4], T.float32(0), dtype="float32")
                with T.block("T_layout_trans_3"):
                    ax0_3, ax1_3, ax2_3, ax3_3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder_1[ax0_3, 0, 0, 0], T_layout_trans_2[ax0_3, ax1_3 * 4 + ax4, ax2_3, ax3_3])
                    T.writes(T_layout_trans_3[ax0_3, ax1_3, ax2_3, ax3_3, ax4])
                    T_layout_trans_3[ax0_3, ax1_3, ax2_3, ax3_3, ax4] = T.if_then_else(ax0_3 < 1 and ax1_3 * 4 + ax4 < 128 and ax2_3 < 52 and ax3_3 < 52, T.tanh(T.log(placeholder_1[ax0_3, 0, 0, 0] + T_layout_trans_2[ax0_3, ax1_3 * 4 + ax4, ax2_3, ax3_3], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
            for i0, i1 in T.grid(1, 64):
                for ax0_4, ax1_4, ax2_4, ax3_4, ax4 in T.grid(1, 1, 52, 52, 4):
                    with T.block("T_exp"):
                        ax0_5 = T.axis.spatial(1, ax0_4)
                        ax1_5 = T.axis.spatial(32, i1 - 32 + ax1_4)
                        ax2_5, ax3_5, ax4_2 = T.axis.remap("SSS", [ax2_4, ax3_4, ax4])
                        T.where(32 <= i1)
                        T.reads(placeholder_2[ax0_5, ax1_5, ax2_5, ax3_5, ax4_2])
                        T.writes(T_exp[ax0_5, ax1_5, ax2_5, ax3_5, ax4_2])
                        T_exp[ax0_5, ax1_5, ax2_5, ax3_5, ax4_2] = T.exp(placeholder_2[ax0_5, ax1_5, ax2_5, ax3_5, ax4_2], dtype="float32")
                for ax0_6, ax1_6, ax2_6, ax3_6 in T.grid(1, 4, 52, 52):
                    with T.block("T_layout_trans"):
                        ax0_7 = T.axis.spatial(1, ax0_6)
                        ax1_7 = T.axis.spatial(128, i1 * 4 - 128 + ax1_6)
                        ax2_7, ax3_7 = T.axis.remap("SS", [ax2_6, ax3_6])
                        T.where(128 <= i1 * 4 + ax1_6)
                        T.reads(T_exp[ax0_7, ax1_7 // 4, ax2_7, ax3_7, ax1_7 % 4])
                        T.writes(T_layout_trans[ax0_7, ax1_7, ax2_7, ax3_7])
                        T_layout_trans[ax0_7, ax1_7, ax2_7, ax3_7] = T.if_then_else(ax0_7 < 1 and ax1_7 < 128 and ax2_7 < 52 and ax3_7 < 52, T_exp[ax0_7, ax1_7 // 4, ax2_7, ax3_7, ax1_7 % 4], T.float32(0), dtype="float32")
                for i2, i3, i4 in T.grid(52, 52, 4):
                    for ax0_8, ax1_8, ax2_8, ax3_8, ax4_3 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_layout_trans_1"):
                            ax0_9 = T.axis.spatial(1, ax0_8)
                            ax1_9 = T.axis.spatial(32, i1 - 32 + ax1_8)
                            ax2_9 = T.axis.spatial(52, i2 + ax2_8)
                            ax3_9 = T.axis.spatial(52, i3 + ax3_8)
                            ax4_4 = T.axis.spatial(4, i4 + ax4_3)
                            T.where(32 <= i1)
                            T.reads(placeholder_1[ax0_9, 0, 0, 0], T_layout_trans[ax0_9, ax1_9 * 4 + ax4_4, ax2_9, ax3_9])
                            T.writes(T_layout_trans_1[ax0_9, ax1_9, ax2_9, ax3_9, ax4_4])
                            T_layout_trans_1[ax0_9, ax1_9, ax2_9, ax3_9, ax4_4] = T.if_then_else(ax0_9 < 1 and ax1_9 * 4 + ax4_4 < 128 and ax2_9 < 52 and ax3_9 < 52, T.tanh(T.log(placeholder_1[ax0_9, 0, 0, 0] + T_layout_trans[ax0_9, ax1_9 * 4 + ax4_4, ax2_9, ax3_9], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                    with T.block("T_concat"):
                        ax0_10, ax1_10, ax2_10, ax3_10, ax4_5 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder_2[ax0_10, ax1_10 - 32, ax2_10, ax3_10, ax4_5], T_layout_trans_1[ax0_10, ax1_10 - 32, ax2_10, ax3_10, ax4_5], placeholder[ax0_10, ax1_10, ax2_10, ax3_10, ax4_5], T_layout_trans_3[ax0_10, ax1_10, ax2_10, ax3_10, ax4_5])
                        T.writes(T_concat[ax0_10, ax1_10, ax2_10, ax3_10, ax4_5])
                        T_concat[ax0_10, ax1_10, ax2_10, ax3_10, ax4_5] = T.if_then_else(32 <= ax1_10, placeholder_2[ax0_10, ax1_10 - 32, ax2_10, ax3_10, ax4_5] * T_layout_trans_1[ax0_10, ax1_10 - 32, ax2_10, ax3_10, ax4_5], placeholder[ax0_10, ax1_10, ax2_10, ax3_10, ax4_5] * T_layout_trans_3[ax0_10, ax1_10, ax2_10, ax3_10, ax4_5], dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="T_exp_1", func_name="main")
b8 = sch.get_block(name="T_layout_trans_2", func_name="main")
b9 = sch.get_block(name="T_add_1", func_name="main")
b10 = sch.get_block(name="T_log_1", func_name="main")
b11 = sch.get_block(name="T_tanh_1", func_name="main")
b12 = sch.get_block(name="T_layout_trans_3", func_name="main")
b13 = sch.get_block(name="T_multiply_1", func_name="main")
b14 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b13)
sch.compute_inline(block=b11)
sch.compute_inline(block=b10)
sch.compute_inline(block=b9)
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
l16 = sch.sample_compute_location(block=b12, decision=-1)
sch.compute_at(block=b12, loop=l16, preserve_unit_loops=True)
l17 = sch.sample_compute_location(block=b8, decision=4)
sch.compute_at(block=b8, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b7, decision=4)
sch.compute_at(block=b7, loop=l18, preserve_unit_loops=True)
l19 = sch.sample_compute_location(block=b5, decision=4)
sch.compute_at(block=b5, loop=l19, preserve_unit_loops=True)
l20 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l21, preserve_unit_loops=True)
[19:07:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"
[19:07:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 52, 52, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 2, 1, 1, 16, 13, 2, 1, 256, 1, 1, 1, 4, 2, 1, 2, 1, 1, 1, 1, 1, 1, 13, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_1 * 4 + i1_2)
                    oh = T.axis.spatial(52, i2_0 * 26 + i2_1 * 2 + i2_2)
                    ow = T.axis.spatial(52, i3_0 * 26 + i3_1 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(256, i5_0)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 16, 4, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 13, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[256, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:07:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 2, 1, 1, 16, 13, 2, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 4, 2, 1, 2, 1, 1, 1, 1, 1, 1, 13, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(52, i2_0 * 26 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_1 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(256, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 2, 13, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_0 * 26 + i2_1 * 2 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + i3_1 * 13 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 16, 4, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 13, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[256, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 2, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 16, 13, 2, 1, 256, 1, 1, 1, 4, 2, 1, 2, 1, 1, 1, 1, 1, 1, 13, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_1 * 4 + i1_2)
                        oh = T.axis.spatial(52, i2_0 * 26 + i2_1 * 2 + i2_2)
                        ow = T.axis.spatial(52, i3_0 * 26 + i3_1 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(256, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 26, 26, 4):
                    with T.block("T_add"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(52, i2_0 * 26 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 26 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 16, 4, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 13, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[256, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"
[19:07:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        T_log = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        T_tanh = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_exp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 256 and ax2 < 52 and ax3 < 52, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_log"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_log[ax0, ax1, ax2, ax3])
                T_log[ax0, ax1, ax2, ax3] = T.log(T_add[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_tanh"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_log[ax0, ax1, ax2, ax3])
                T.writes(T_tanh[ax0, ax1, ax2, ax3])
                T_tanh[ax0, ax1, ax2, ax3] = T.tanh(T_log[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans_1[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 52 and ax3 < 52, T_tanh[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], T_layout_trans_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T_layout_trans_1[ax0, ax1, ax2, ax3, ax4]
    

[19:07:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_exp = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            T_layout_trans = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
                with T.block("T_exp"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                    T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
            for i0, i1, i2 in T.grid(1, 64, 52):
                for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 52):
                    with T.block("T_layout_trans"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(256, i1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(52, i2 + ax2)
                        ax3_1 = T.axis.spatial(52, ax3)
                        T.reads(T_exp[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                        T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 256 and ax2_1 < 52 and ax3_1 < 52, T_exp[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], T.float32(0), dtype="float32")
                for i3, i4 in T.grid(52, 4):
                    with T.block("T_multiply"):
                        ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1 * 4 + ax4, ax2, ax3])
                        T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                        T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 52 and ax3 < 52, T.tanh(T.log(placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1 * 4 + ax4, ax2, ax3], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=-2)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=2)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
[19:07:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"
[19:07:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(1, 32, 26, 26, 4), "float32"], T_layout_trans: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans_1 = T.alloc_buffer([1, 128, 26, 26], dtype="float32")
        resize = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_layout_trans_2 = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_concat = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 26, 26):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3])
                T_layout_trans_1[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 128 and ax2 < 26 and ax3 < 26, placeholder_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("resize"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_layout_trans_1[i0_1, i1_1, 0 : 26, 0 : 26])
                T.writes(resize[i0_1, i1_1, i2_1, i3_1])
                resize[i0_1, i1_1, i2_1, i3_1] = T_layout_trans_1[i0_1, i1_1, T.max(T.min(T.cast(T.floor((T.cast(i2_1, "float32") + T.float32(0.5)) * T.float32(0.5) - T.float32(0.5) + T.float32(9.9999997473787516e-06), dtype="float32"), "int32"), 25), 0), T.max(T.min(T.cast(T.floor((T.cast(i3_1, "float32") + T.float32(0.5)) * T.float32(0.5) - T.float32(0.5) + T.float32(9.9999997473787516e-06), dtype="float32"), "int32"), 25), 0)]
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_layout_trans_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans_2[ax0, ax1, ax2, ax3])
                T_layout_trans_2[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 128 and ax2 < 52 and ax3 < 52, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 52, 52):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(resize[ax0, ax1 - 128, ax2, ax3], T_layout_trans_2[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(128 <= ax1, resize[ax0, ax1 - 128, ax2, ax3], T_layout_trans_2[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_layout_trans_2"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_concat[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 256 and ax2 < 52 and ax3 < 52, T_concat[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[19:07:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(1, 32, 26, 26, 4), "float32"], T_layout_trans: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_layout_trans_1 = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
            T_concat = T.alloc_buffer([1, 256, 52, 52], dtype="float32")
            for i0, i1 in T.grid(1, 64):
                for ax0, ax1, ax2 in T.grid(1, 4, 52):
                    for ax0_1, ax1_1, ax2_1, ax3 in T.grid(1, 1, 1, 52):
                        with T.block("T_layout_trans_1"):
                            ax0_2 = T.axis.spatial(1, ax0_1)
                            ax1_2 = T.axis.spatial(128, i1 * 4 + ax1 + ax1_1)
                            ax2_2 = T.axis.spatial(52, ax2 + ax2_1)
                            ax3_1 = T.axis.spatial(52, ax3)
                            T.where(i1 * 4 + ax1 < 128)
                            T.reads(placeholder[ax0_2, ax1_2 // 4, ax2_2, ax3_1, ax1_2 % 4])
                            T.writes(T_layout_trans_1[ax0_2, ax1_2, ax2_2, ax3_1])
                            T_layout_trans_1[ax0_2, ax1_2, ax2_2, ax3_1] = T.if_then_else(ax0_2 < 1 and ax1_2 < 128 and ax2_2 < 52 and ax3_1 < 52, placeholder[ax0_2, ax1_2 // 4, ax2_2, ax3_1, ax1_2 % 4], T.float32(0), dtype="float32")
                    for ax3 in T.serial(52):
                        with T.block("T_concat"):
                            ax0_3 = T.axis.spatial(1, ax0)
                            ax1_3 = T.axis.spatial(256, i1 * 4 + ax1)
                            ax2_3, ax3_2 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(placeholder_1[ax0_3, ax1_3 // 4 - 32, 0 : 26, 0 : 26, ax1_3 % 4], T_layout_trans_1[ax0_3, ax1_3, ax2_3, ax3_2])
                            T.writes(T_concat[ax0_3, ax1_3, ax2_3, ax3_2])
                            T_concat[ax0_3, ax1_3, ax2_3, ax3_2] = T.if_then_else(128 <= ax1_3, T.if_then_else(ax0_3 < 1 and ax1_3 - 128 < 128 and T.max(T.min(T.cast(T.floor((T.cast(ax2_3, "float32") + T.float32(0.5)) * T.float32(0.5) - T.float32(0.5) + T.float32(9.9999997473787516e-06), dtype="float32"), "int32"), 25), 0) < 26 and T.max(T.min(T.cast(T.floor((T.cast(ax3_2, "float32") + T.float32(0.5)) * T.float32(0.5) - T.float32(0.5) + T.float32(9.9999997473787516e-06), dtype="float32"), "int32"), 25), 0) < 26, placeholder_1[ax0_3, (ax1_3 - 128) // 4, T.max(T.min(T.cast(T.floor((T.cast(ax2_3, "float32") + T.float32(0.5)) * T.float32(0.5) - T.float32(0.5) + T.float32(9.9999997473787516e-06), dtype="float32"), "int32"), 25), 0), T.max(T.min(T.cast(T.floor((T.cast(ax3_2, "float32") + T.float32(0.5)) * T.float32(0.5) - T.float32(0.5) + T.float32(9.9999997473787516e-06), dtype="float32"), "int32"), 25), 0), (ax1_3 - 128) % 4], T.float32(0), dtype="float32"), T_layout_trans_1[ax0_3, ax1_3, ax2_3, ax3_2], dtype="float32")
                for i2, i3, i4 in T.grid(52, 52, 4):
                    with T.block("T_layout_trans_2"):
                        ax0_4, ax1_4, ax2_4, ax3_3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(T_concat[ax0_4, ax1_4 * 4 + ax4, ax2_4, ax3_3])
                        T.writes(T_layout_trans[ax0_4, ax1_4, ax2_4, ax3_3, ax4])
                        T_layout_trans[ax0_4, ax1_4, ax2_4, ax3_3, ax4] = T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4 < 256 and ax2_4 < 52 and ax3_3 < 52, T_concat[ax0_4, ax1_4 * 4 + ax4, ax2_4, ax3_3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="resize", func_name="main")
b2 = sch.get_block(name="T_layout_trans_1", func_name="main")
b3 = sch.get_block(name="T_concat", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
l6 = sch.sample_compute_location(block=b3, decision=1)
sch.compute_at(block=b3, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b2, decision=4)
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
l8 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l8, preserve_unit_loops=True)
[19:07:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"
[19:07:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 32, 52, 52, 4, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4] * T.float32(0.10000000149011612))
    

[19:07:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 2, 52, 1, 1, 1, 2, 1, 2, 4, 1, 1, 1, 2, 13, 1, 2, 64, 1, 1, 1, 4, 1, 1, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(52, i2_0 * 26 + i2_1 * 13 + i2_2)
                    ow = T.axis.spatial(52, i3_0)
                    oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                    ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 52, 52, 4):
                with T.block("T_leaky_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                    T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[52, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 2, 52, 1, 1, 1, 2, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 2, 13, 1, 2, 64, 1, 1, 1, 4, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(52, i2_0 * 26 + i2_1 * 13 + i2_2)
                        ow = T.axis.spatial(52, i3_0)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 13, 1, 2):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_0 * 26 + i2_1 * 13 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[52, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[19:07:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 4, 2, 52, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 1, 2, 4, 1, 1, 1, 2, 13, 1, 2, 64, 1, 1, 1, 4, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i1_0 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(52, i2_0 * 26 + i2_1 * 13 + i2_2)
                        ow = T.axis.spatial(52, i3_0)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 26, 1, 4):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(32, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_0 * 26 + ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 2, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 13, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[52, 1, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
[19:07:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"
[19:07:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 54, 54, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 32, 54, 54, 4):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 64, 52, 52, 4, 128, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4], T_add[ax0, ax1, ax2, ax3, ax4] * T.float32(0.10000000149011612))
    

[19:07:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 54, 54, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 2, 1, 26, 2, 1, 4, 1, 2, 2, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 54, 3, 4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(32, i5_0 * 16 + ax1)
                        i2 = T.axis.spatial(54, ax2)
                        i3 = T.axis.spatial(54, i3_0 * 2 + i3_1 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 4, 52, 1, 1, 64, 3, 3, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(52, i2_2)
                        ow = T.axis.spatial(52, i3_0 * 2 + i3_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 64, 52, 52, 4):
                with T.block("T_leaky_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                    T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 4, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 52, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[26, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 64])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
[19:07:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 54, 54, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 54, 54, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 2, 1, 26, 2, 1, 4, 1, 2, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 4, 52, 1, 1, 64, 3, 3, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1_1 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(52, i2_2)
                        ow = T.axis.spatial(52, i3_0 * 2 + i3_1_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1_1)
                        ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 52, 1, 1):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + i1_1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(52, ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 2 + i3_1_1 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1_1 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 4, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 52, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[26, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 64])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[19:07:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 32, 54, 54, 4], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 32, 54, 54, 4):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 26, 2):
                for i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 2, 2, 2, 1, 1, 1, 4, 52, 1, 1, 64, 3, 3, 1, 2, 1, 1, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_0 * 32 + i1_1_1 * 8 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(52, i2_2)
                        ow = T.axis.spatial(52, i3_0 * 2 + i3_1_1)
                        oc_block = T.axis.spatial(4, i4_0 * 2 + i4_1_1)
                        ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 52, 2, 2):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(64, i1_0 * 32 + ax1)
                        ax2_1 = T.axis.spatial(52, ax2)
                        ax3_1 = T.axis.spatial(52, i3_0 * 2 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 4, 4, 2])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 52, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[26, 2, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 64])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[19:07:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #73: "fused_layout_transform_2"
[19:07:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], T_layout_trans: T.Buffer[(1, 128, 52, 52, 2), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 2):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 2 + ax4 < 256 and ax2 < 52 and ax3 < 52, placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4], T.float32(0), dtype="float32")
    

[19:07:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], T_layout_trans: T.Buffer[(1, 128, 52, 52, 2), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 128, 52, 52, 2):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 2 + ax4 < 256 and ax2 < 52 and ax3 < 52, placeholder[ax0, (ax1 * 2 + ax4) // 4, ax2, ax3, (ax1 * 2 + ax4) % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[19:07:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"
[19:07:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 2), "float32"], placeholder_1: T.Buffer[(85, 128, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 85, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 85, 52, 52, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 85, 52, 52, 3], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 85, 52, 52, 3, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 2], "float32"], ["TENSOR", [85, 128, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 85, 52, 52, 3):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[19:07:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[19:07:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 2), "float32"], placeholder_1: T.Buffer[(85, 128, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 85, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 85, 52, 52, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 85, 52, 52, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 17, 1, 1, 3, 1, 1, 4, 1, 1, 8, 1, 1, 1, 1, 13, 1, 1, 32, 1, 1, 1, 5, 1, 52, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(85, i1_0 * 5 + i1_3)
                    oh = T.axis.spatial(52, i2_1 * 13 + i2_2)
                    ow, oc_block = T.axis.remap("SS", [i3_3, i4_0])
                    ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 2], "float32"], ["TENSOR", [85, 128, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 85, 52, 52, 3):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[17, 1, 1, 5])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 13, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 52])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
[19:07:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 2), "float32"], placeholder_1: T.Buffer[(85, 128, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 85, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 85, 52, 52, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 85, 52, 52, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 17, 1, 1, 3, 1, 1, 4, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 13, 1, 1, 32, 1, 1, 1, 5, 1, 52, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(85, i1_0 * 5 + i1_3)
                        oh = T.axis.spatial(52, i2_1 * 13 + i2_2)
                        ow, oc_block = T.axis.remap("SS", [i3_3, i4_0])
                        ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 2], "float32"], ["TENSOR", [85, 128, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 5, 13, 52, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(85, i1_0 * 5 + ax1)
                        ax2_1 = T.axis.spatial(52, i2_1 * 13 + ax2)
                        ax3_1 = T.axis.spatial(52, ax3)
                        ax4_1 = T.axis.spatial(3, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[17, 1, 1, 5])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 13, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 52])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52, 2), "float32"], placeholder_1: T.Buffer[(85, 128, 1, 1, 2, 3), "float32"], placeholder_2: T.Buffer[(1, 85, 1, 1, 3), "float32"], T_add: T.Buffer[(1, 85, 52, 52, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 85, 52, 52, 3], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 17, 1, 1, 3):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 4, 1, 1, 8, 1, 1, 1, 1, 13, 1, 1, 32, 1, 1, 1, 5, 1, 52, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(85, i1_0 * 5 + i1_3)
                        oh = T.axis.spatial(52, i2_1 * 13 + i2_2)
                        ow, oc_block = T.axis.remap("SS", [i3_3, i4_0])
                        ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2], placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 52, 52, 2], "float32"], ["TENSOR", [85, 128, 1, 1, 2, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW2c", "NCHW3c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 2, oh + kh, ow + kw, ic % 2] * placeholder_1[oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 5, 52, 52, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(85, i1_0 * 5 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(3, i4_0 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[17, 1, 1, 5])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 13, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 52])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
[19:07:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"
[19:07:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 85, 52, 52, 3), "float32"], T_concat: T.Buffer[(1, 52, 52, 3, 85), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 255, 52, 52], dtype="float32")
        T_transpose = T.alloc_buffer([1, 52, 52, 255], dtype="float32")
        T_reshape = T.alloc_buffer([1, 52, 52, 3, 85], dtype="float32")
        T_split = T.alloc_buffer([1, 52, 52, 3, 80], dtype="float32")
        T_sigmoid = T.alloc_buffer([1, 52, 52, 3, 80], dtype="float32")
        T_split_1 = T.alloc_buffer([1, 52, 52, 3, 1], dtype="float32")
        T_sigmoid_1 = T.alloc_buffer([1, 52, 52, 3, 1], dtype="float32")
        T_split_2 = T.alloc_buffer([1, 52, 52, 3, 4], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 255, 52, 52):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 3, ax2, ax3, ax1 % 3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 255 and ax2 < 52 and ax3 < 52, placeholder[ax0, ax1 // 3, ax2, ax3, ax1 % 3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 52, 52, 255):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_layout_trans[ax0, ax3, ax1, ax2])
                T.writes(T_transpose[ax0, ax1, ax2, ax3])
                T_transpose[ax0, ax1, ax2, ax3] = T_layout_trans[ax0, ax3, ax1, ax2]
        for i0, i1, i2, i3, i4 in T.grid(1, 52, 52, 3, 85):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_transpose[0, ((ax2 * 255 + ax3 * 85 + ax4) // 13260 + ax1) % 52, ((ax3 * 85 + ax4) // 255 + ax2) % 52, (ax3 * 85 + ax4) % 255])
                T.writes(T_reshape[ax0, ax1, ax2, ax3, ax4])
                T_reshape[ax0, ax1, ax2, ax3, ax4] = T_transpose[0, ((ax2 * 255 + ax3 * 85 + ax4) // 13260 + ax1) % 52, ((ax3 * 85 + ax4) // 255 + ax2) % 52, (ax3 * 85 + ax4) % 255]
        for i0, i1, i2, i3, i4 in T.grid(1, 52, 52, 3, 80):
            with T.block("T_split"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape[ax0, ax1, ax2, ax3, ax4 + 5])
                T.writes(T_split[ax0, ax1, ax2, ax3, ax4])
                T_split[ax0, ax1, ax2, ax3, ax4] = T_reshape[ax0, ax1, ax2, ax3, ax4 + 5]
        for i0, i1, i2, i3, i4 in T.grid(1, 52, 52, 3, 80):
            with T.block("T_sigmoid"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_split[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_sigmoid[ax0, ax1, ax2, ax3, ax4])
                T_sigmoid[ax0, ax1, ax2, ax3, ax4] = T.sigmoid(T_split[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 52, 52, 3, 1):
            with T.block("T_split_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape[ax0, ax1, ax2, ax3, ax4 + 4])
                T.writes(T_split_1[ax0, ax1, ax2, ax3, ax4])
                T_split_1[ax0, ax1, ax2, ax3, ax4] = T_reshape[ax0, ax1, ax2, ax3, ax4 + 4]
        for i0, i1, i2, i3, i4 in T.grid(1, 52, 52, 3, 1):
            with T.block("T_sigmoid_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_split_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_sigmoid_1[ax0, ax1, ax2, ax3, ax4])
                T_sigmoid_1[ax0, ax1, ax2, ax3, ax4] = T.sigmoid(T_split_1[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 52, 52, 3, 4):
            with T.block("T_split_2"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_split_2[ax0, ax1, ax2, ax3, ax4])
                T_split_2[ax0, ax1, ax2, ax3, ax4] = T_reshape[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 52, 52, 3, 85):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_sigmoid[ax0, ax1, ax2, ax3, ax4 - 5], T_sigmoid_1[ax0, ax1, ax2, ax3, ax4 - 4], T_split_2[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(5 <= ax4, T_sigmoid[ax0, ax1, ax2, ax3, ax4 - 5], T.if_then_else(4 <= ax4, T_sigmoid_1[ax0, ax1, ax2, ax3, ax4 - 4], T_split_2[ax0, ax1, ax2, ax3, ax4], dtype="float32"), dtype="float32")
    

[19:07:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[19:07:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 85, 52, 52, 3), "float32"], T_concat: T.Buffer[(1, 52, 52, 3, 85), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 255, 52, 52], dtype="float32")
            T_transpose = T.alloc_buffer([1, 52, 52, 255], dtype="float32")
            T_reshape = T.alloc_buffer([1, 52, 52, 3, 85], dtype="float32")
            for i0, i1 in T.grid(1, 52):
                for ax0, ax1, ax2, ax3 in T.grid(1, 255, 1, 52):
                    with T.block("T_layout_trans"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(52, i1 + ax2)
                        ax3_1 = T.axis.spatial(52, ax3)
                        T.reads(placeholder[ax0_1, ax1_1 // 3, ax2_1, ax3_1, ax1_1 % 3])
                        T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 255 and ax2_1 < 52 and ax3_1 < 52, placeholder[ax0_1, ax1_1 // 3, ax2_1, ax3_1, ax1_1 % 3], T.float32(0), dtype="float32")
                for i2, i3 in T.grid(52, 255):
                    with T.block("T_transpose"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(T_layout_trans[ax0, ax3, ax1, ax2])
                        T.writes(T_transpose[ax0, ax1, ax2, ax3])
                        T_transpose[ax0, ax1, ax2, ax3] = T_layout_trans[ax0, ax3, ax1, ax2]
            for i0, i1, i2, i3, i4 in T.grid(1, 52, 52, 3, 85):
                with T.block("T_reshape"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(T_transpose[0, ((ax2 * 255 + ax3 * 85 + ax4) // 13260 + ax1) % 52, ((ax3 * 85 + ax4) // 255 + ax2) % 52, (ax3 * 85 + ax4) % 255])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3, ax4])
                    T_reshape[ax0, ax1, ax2, ax3, ax4] = T_transpose[0, ((ax2 * 255 + ax3 * 85 + ax4) // 13260 + ax1) % 52, ((ax3 * 85 + ax4) // 255 + ax2) % 52, (ax3 * 85 + ax4) % 255]
            for i0, i1, i2, i3, i4 in T.grid(1, 52, 52, 3, 85):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(T_reshape[ax0, ax1, ax2, ax3, ax4])
                    T.writes(T_concat[ax0, ax1, ax2, ax3, ax4])
                    T_concat[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(5 <= ax4, T.sigmoid(T_reshape[ax0, ax1, ax2, ax3, ax4 - 5 + 5], dtype="float32"), T.if_then_else(4 <= ax4, T.sigmoid(T_reshape[ax0, ax1, ax2, ax3, ax4 - 4 + 4], dtype="float32"), T_reshape[ax0, ax1, ax2, ax3, ax4], dtype="float32"), dtype="float32")
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape", func_name="main")
b3 = sch.get_block(name="T_split", func_name="main")
b4 = sch.get_block(name="T_sigmoid", func_name="main")
b5 = sch.get_block(name="T_split_1", func_name="main")
b6 = sch.get_block(name="T_sigmoid_1", func_name="main")
b7 = sch.get_block(name="T_split_2", func_name="main")
b8 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b7)
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.vectorize", ann_val=64)
v9 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit", ann_val=v9)
l10 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l10, preserve_unit_loops=True)
l11 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l11, preserve_unit_loops=True)
l12 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l12, preserve_unit_loops=True)
[19:07:41] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[19:07:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"
[19:07:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:08:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:08:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_concatenate"
[19:08:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:08:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:08:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_layout_transform"
[19:08:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:08:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:08:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_nn_contrib_conv2d_NCHWc_add"
[19:08:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:09:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:09:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"
[19:09:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:09:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:09:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"
[19:09:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:10:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:10:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_concatenate_1"
[19:10:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:10:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:10:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_layout_transform_1"
[19:10:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:10:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:10:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"
[19:10:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:11:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:11:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"
[19:11:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:11:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:11:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_nn_max_pool2d"
[19:11:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:11:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:11:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_nn_max_pool2d_1"
[19:11:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:12:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:12:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"
[19:12:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:12:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"
[19:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:13:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:14:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"
[19:14:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:14:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:14:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"
[19:14:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:14:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:14:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"
[19:14:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:14:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:15:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"
[19:15:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:15:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:15:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"
[19:15:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:15:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:15:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"
[19:15:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:15:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:16:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"
[19:16:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:16:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:16:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #21: "fused_nn_max_pool2d_2"
[19:16:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:16:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #22: "fused_concatenate_2"
[19:16:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:16:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:16:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"
[19:16:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:17:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:17:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"
[19:17:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:17:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:18:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"
[19:18:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:18:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:18:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"
[19:18:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:18:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:18:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"
[19:18:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:19:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:19:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"
[19:19:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:20:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:20:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"
[19:20:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:20:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:20:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"
[19:20:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:21:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:21:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"
[19:21:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:21:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:21:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"
[19:21:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:21:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:21:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"
[19:21:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:22:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:22:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"
[19:22:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:22:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:22:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"
[19:22:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:22:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:22:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"
[19:22:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:22:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:22:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"
[19:22:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:23:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:23:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"
[19:23:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:24:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:24:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"
[19:24:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:24:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:24:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #40: "fused_transpose_layout_transform"
[19:24:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:24:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:25:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"
[19:25:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:25:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:25:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"
[19:25:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:26:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:26:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"
[19:26:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:27:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:27:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"
[19:27:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:27:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:28:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"
[19:28:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:28:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:28:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"
[19:28:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:28:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:29:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"
[19:29:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:29:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:29:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"
[19:29:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:29:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:29:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"
[19:30:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:30:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:30:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"
[19:30:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:30:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:31:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"
[19:31:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:31:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:31:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"
[19:31:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:32:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:32:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"
[19:32:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:32:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:33:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"
[19:33:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:33:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:33:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"
[19:33:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:33:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:33:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"
[19:33:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:34:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:34:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"
[19:34:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:34:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:34:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"
[19:34:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:35:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
corrupted double-linked list
[19:35:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"
[19:35:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:36:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:36:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"
[19:36:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:36:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:36:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"
[19:36:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:37:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:37:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"
[19:37:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:37:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:37:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"
[19:37:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:38:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:38:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"
[19:38:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:38:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:38:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"
[19:38:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:38:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"
[19:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:39:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:39:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"
[19:39:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:39:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
munmap_chunk(): invalid pointer
[19:39:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"
[19:39:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:40:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:40:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"
[19:40:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:40:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:40:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"
[19:40:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:40:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:41:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"
[19:41:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:41:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:41:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"
[19:41:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:42:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:42:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #73: "fused_layout_transform_2"
[19:42:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:42:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:42:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"
[19:42:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:42:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:42:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"
[19:42:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[19:42:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #0: GFLOPs: 16.7909. Time: 23.7566 ms. Best GFLOPs: 16.7909
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #1: GFLOPs: 9.1989. Time: 43.3632 ms. Best GFLOPs: 16.7909
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #2: GFLOPs: 5.2660. Time: 75.7486 ms. Best GFLOPs: 16.7909
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #3: GFLOPs: 7.9378. Time: 50.2523 ms. Best GFLOPs: 16.7909
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 27, 27, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(13, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 1, 1):
                for i1_2_init, i2_2_init, i1_3_init in T.grid(4, 13, 8):
                    for i2_3_i3_3_i4_3_fused_init in T.vectorized(4):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_1 * 32 + i1_2_init * 8 + i1_3_init)
                            oh, ow, oc_block = T.axis.remap("SSS", [i2_2_init, i0_0_i1_0_i2_0_i3_0_i4_0_fused, i2_3_i3_3_i4_3_fused_init])
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0 in T.grid(256, 3, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 25, 1, 1):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(64, i5_0 // 4 + ax1)
                            i2 = T.axis.spatial(27, i6_0 + ax2)
                            i3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 13 * 2 + i7_0 + ax3)
                            i4 = T.axis.spatial(4, i5_0 % 4 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 27 and 1 <= i3 and i3 < 27, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3 in T.grid(1, 4, 13, 1, 1, 1, 1, 1, 1, 8):
                        for i2_3_i3_3_i4_3_fused in T.vectorized(4):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(128, i1_1 * 32 + i1_2 * 8 + i1_3)
                                oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSRRR", [i2_2, i0_0_i1_0_i2_0_i3_0_i4_0_fused, i2_3_i3_3_i4_3_fused, i5_0, i6_0, i7_0])
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 128, 13):
                for ax3_ax4_fused in T.vectorized(4):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3, ax4 = T.axis.remap("SSSS", [ax1, ax2, i0_0_i1_0_i2_0_i3_0_i4_0_fused, ax3_ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 4, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[256, 1])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b68)
l89 = sch.fuse(l71, l72, l73, l74, l75)
sch.parallel(loop=l89)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b69)
l112 = sch.fuse(l109, l110, l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l90, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l90, ann_key="pragma_unroll_explicit", ann_val=1)
l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b70)
l119 = sch.fuse(l117, l118)
sch.vectorize(loop=l119)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
b120 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b120)
b141 = sch.decompose_reduction(block=b120, loop=l127)
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #5: GFLOPs: 18.2467. Time: 21.8612 ms. Best GFLOPs: 18.2467
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #6: GFLOPs: 24.6074. Time: 16.2103 ms. Best GFLOPs: 24.6074
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #7: GFLOPs: 35.9943. Time: 11.0821 ms. Best GFLOPs: 35.9943
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #8: GFLOPs: 8.5412. Time: 46.7022 ms. Best GFLOPs: 35.9943
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #9: GFLOPs: 23.5704. Time: 16.9235 ms. Best GFLOPs: 35.9943
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #10: GFLOPs: 21.0976. Time: 18.9070 ms. Best GFLOPs: 35.9943
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #11: GFLOPs: 25.8563. Time: 15.4274 ms. Best GFLOPs: 35.9943
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #12: GFLOPs: 69.4381. Time: 5.7446 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #13: GFLOPs: 36.3355. Time: 10.9781 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #14: GFLOPs: 44.4301. Time: 8.9780 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #15: GFLOPs: 4.7379. Time: 84.1918 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #16: GFLOPs: 40.9385. Time: 9.7437 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #17: GFLOPs: 26.3161. Time: 15.1578 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #18: GFLOPs: 29.9598. Time: 13.3143 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #19: GFLOPs: 40.6018. Time: 9.8245 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #20: GFLOPs: 14.8348. Time: 26.8892 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #21: GFLOPs: 29.0268. Time: 13.7423 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #22: GFLOPs: 15.0481. Time: 26.5079 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #23: GFLOPs: 22.7706. Time: 17.5180 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 27, 27, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1728, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(27):
                for i4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_i1_i2_fused // 27)
                        i2 = T.axis.spatial(27, i0_i1_i2_fused % 27)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3_1, i4])
                        data_pad[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 27 and 1 <= i3_1 and i3_1 < 27, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1_1, i4_1 in T.grid(1, 2, 1, 13, 2):
                for i1_2_init, i1_3_init, i2_3_init in T.grid(4, 4, 13):
                    for i3_3_i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 32 + i1_1 * 16 + i1_2_init * 4 + i1_3_init)
                            oh, ow = T.axis.remap("SS", [i2_3_init, i3_1_1])
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i3_3_i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(32, 3, 1, 1, 4, 1, 1, 1, 8, 1, 3, 1, 4, 13):
                    for i3_3_i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 32 + i1_1 * 16 + i1_2 * 4 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_3, i3_1_1])
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i3_3_i4_3_fused)
                            ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 32, 13):
                for ax3_ax4_fused in T.vectorized(52):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 32 + ax1)
                        ax2_1 = T.axis.spatial(13, ax2)
                        ax3 = T.axis.spatial(13, ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 2, 4, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[32, 8])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75 = sch.get_loops(block=b68)
l76 = sch.fuse(l71, l72, l73)
sch.parallel(loop=l76)
l77 = sch.fuse(l75)
sch.vectorize(loop=l77)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b69)
l104 = sch.fuse(l78, l79, l80, l81, l82)
sch.parallel(loop=l104)
l105 = sch.fuse(l102, l103)
sch.vectorize(loop=l105)
sch.annotate(block_or_loop=l104, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l104, ann_key="pragma_unroll_explicit", ann_val=1)
l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b70)
l112 = sch.fuse(l110, l111)
sch.vectorize(loop=l112)
sch.annotate(block_or_loop=l106, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l106, ann_key="pragma_unroll_explicit", ann_val=1)
b113 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b113)
b135 = sch.decompose_reduction(block=b113, loop=l120)
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #25: GFLOPs: 20.6471. Time: 19.3196 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #26: GFLOPs: 23.7796. Time: 16.7746 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #27: GFLOPs: 6.4624. Time: 61.7251 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 27, 27, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 64, 27, 27):
                for ax4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2, i3, i4 = T.axis.remap("SSSS", [ax1, ax2, ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 27 and 1 <= i3 and i3 < 27, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1):
                for i2_2_init, i1_3_init, i3_3_init in T.grid(13, 32, 13):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 64 + i1_1 * 32 + i1_3_init)
                        oh, ow = T.axis.remap("SS", [i2_2_init, i3_3_init])
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 1, 13, 1, 1, 32, 3, 3, 1, 32, 1, 13, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 64 + i1_1 * 32 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_leaky_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(128, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                    T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 32])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[8, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b67)
l80 = sch.fuse(l70, l71, l72, l73, l74)
sch.parallel(loop=l80)
l81 = sch.fuse(l79)
sch.vectorize(loop=l81)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b68)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108 = sch.get_loops(block=b69)
l109 = sch.fuse(l104, l105, l106)
sch.parallel(loop=l109)
l110 = sch.fuse(l107, l108)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b111)
b134 = sch.decompose_reduction(block=b111, loop=l118)
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #29: GFLOPs: 34.7949. Time: 11.4642 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #30: GFLOPs: 57.0848. Time: 6.9877 ms. Best GFLOPs: 69.4381
[19:43:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"] Trial #31: GFLOPs: 9.9624. Time: 40.0399 ms. Best GFLOPs: 69.4381
/home/yj/anaconda3/lib/python3.9/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
[19:43:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 5744.6

[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #0: GFLOPs: 0.0000. Time: 0.0195 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #1: GFLOPs: 0.0000. Time: 0.0196 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #2: GFLOPs: 0.0000. Time: 0.0185 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #3: GFLOPs: 0.0000. Time: 0.0148 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #4: GFLOPs: 0.0000. Time: 0.0185 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #5: GFLOPs: 0.0000. Time: 0.0197 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #6: GFLOPs: 0.0000. Time: 0.0848 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #7: GFLOPs: 0.0000. Time: 0.0183 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #8: GFLOPs: 0.0000. Time: 0.0297 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #9: GFLOPs: 0.0000. Time: 0.0237 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #10: GFLOPs: 0.0000. Time: 0.0231 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #11: GFLOPs: 0.0000. Time: 0.0266 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #12: GFLOPs: 0.0000. Time: 0.0237 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #13: GFLOPs: 0.0000. Time: 0.0282 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #14: GFLOPs: 0.0000. Time: 0.0180 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #15: GFLOPs: 0.0000. Time: 0.0173 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #16: GFLOPs: 0.0000. Time: 0.0194 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #17: GFLOPs: 0.0000. Time: 0.0254 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #18: GFLOPs: 0.0000. Time: 0.0185 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #19: GFLOPs: 0.0000. Time: 0.0221 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #20: GFLOPs: 0.0000. Time: 0.0229 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #21: GFLOPs: 0.0000. Time: 0.0190 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #22: GFLOPs: 0.0000. Time: 0.0167 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #23: GFLOPs: 0.0000. Time: 0.0209 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #24: GFLOPs: 0.0000. Time: 0.0218 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #25: GFLOPs: 0.0000. Time: 0.0228 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #26: GFLOPs: 0.0000. Time: 0.0406 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #27: GFLOPs: 0.0000. Time: 0.0244 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #28: GFLOPs: 0.0000. Time: 0.0446 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #29: GFLOPs: 0.0000. Time: 0.0211 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #30: GFLOPs: 0.0000. Time: 0.0203 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_concatenate"] Trial #31: GFLOPs: 0.0000. Time: 0.0250 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_concatenate"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 64
Total latency (us): 5759.44

[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #0: GFLOPs: 0.0000. Time: 0.0272 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #1: GFLOPs: 0.0000. Time: 0.0239 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #2: GFLOPs: 0.0000. Time: 0.0280 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #3: GFLOPs: 0.0000. Time: 0.0234 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #4: GFLOPs: 0.0000. Time: 0.0604 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #5: GFLOPs: 0.0000. Time: 0.7259 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #6: GFLOPs: 0.0000. Time: 0.1123 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #7: GFLOPs: 0.0000. Time: 0.9368 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #8: GFLOPs: 0.0000. Time: 0.0514 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #9: GFLOPs: 0.0000. Time: 0.0345 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #10: GFLOPs: 0.0000. Time: 0.0370 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #11: GFLOPs: 0.0000. Time: 0.0322 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #12: GFLOPs: 0.0000. Time: 0.0369 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #13: GFLOPs: 0.0000. Time: 0.0265 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #14: GFLOPs: 0.0000. Time: 0.0215 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #15: GFLOPs: 0.0000. Time: 0.0206 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #16: GFLOPs: 0.0000. Time: 0.0213 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #17: GFLOPs: 0.0000. Time: 0.0207 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #18: GFLOPs: 0.0000. Time: 0.0249 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #19: GFLOPs: 0.0000. Time: 0.0257 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #20: GFLOPs: 0.0000. Time: 0.0273 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #21: GFLOPs: 0.0000. Time: 0.0285 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #22: GFLOPs: 0.0000. Time: 0.0270 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #23: GFLOPs: 0.0000. Time: 0.0238 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #24: GFLOPs: 0.0000. Time: 0.0272 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #25: GFLOPs: 0.0000. Time: 0.0273 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #26: GFLOPs: 0.0000. Time: 0.0365 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #27: GFLOPs: 0.0000. Time: 0.0175 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #28: GFLOPs: 0.0000. Time: 0.0278 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #29: GFLOPs: 0.0000. Time: 0.0352 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #30: GFLOPs: 0.0000. Time: 0.0272 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #31: GFLOPs: 0.0000. Time: 0.0229 ms. Best GFLOPs: 0.0000
[19:43:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_layout_transform"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 96
Total latency (us): 5776.94

[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #0: GFLOPs: 13.0696. Time: 6.7563 ms. Best GFLOPs: 13.0696
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #1: GFLOPs: 4.3560. Time: 20.2711 ms. Best GFLOPs: 13.0696
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #2: GFLOPs: 10.7037. Time: 8.2497 ms. Best GFLOPs: 13.0696
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #3: GFLOPs: 17.4617. Time: 5.0569 ms. Best GFLOPs: 17.4617
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #4: GFLOPs: 7.8334. Time: 11.2724 ms. Best GFLOPs: 17.4617
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #5: GFLOPs: 10.4203. Time: 8.4740 ms. Best GFLOPs: 17.4617
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #6: GFLOPs: 30.6047. Time: 2.8852 ms. Best GFLOPs: 30.6047
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #7: GFLOPs: 15.2760. Time: 5.7804 ms. Best GFLOPs: 30.6047
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #8: GFLOPs: 29.1487. Time: 3.0293 ms. Best GFLOPs: 30.6047
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #9: GFLOPs: 23.2949. Time: 3.7906 ms. Best GFLOPs: 30.6047
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #10: GFLOPs: 23.4425. Time: 3.7667 ms. Best GFLOPs: 30.6047
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #11: GFLOPs: 5.2423. Time: 16.8440 ms. Best GFLOPs: 30.6047
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #12: GFLOPs: 28.0496. Time: 3.1480 ms. Best GFLOPs: 30.6047
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #13: GFLOPs: 64.4650. Time: 1.3698 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #14: GFLOPs: 32.3518. Time: 2.7294 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #15: GFLOPs: 53.7356. Time: 1.6433 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #16: GFLOPs: 1.3523. Time: 65.2955 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #17: GFLOPs: 2.5288. Time: 34.9182 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #18: GFLOPs: 12.2857. Time: 7.1874 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #19: GFLOPs: 22.3490. Time: 3.9510 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #20: GFLOPs: 7.4739. Time: 11.8147 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #21: GFLOPs: 13.6006. Time: 6.4925 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #22: GFLOPs: 17.2527. Time: 5.1181 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #23: GFLOPs: 15.5660. Time: 5.6727 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #24: GFLOPs: 15.6973. Time: 5.6253 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #25: GFLOPs: 3.0914. Time: 28.5639 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #26: GFLOPs: 9.0296. Time: 9.7792 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #27: GFLOPs: 47.4627. Time: 1.8604 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #28: GFLOPs: 7.5979. Time: 11.6219 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #29: GFLOPs: 20.5752. Time: 4.2916 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #30: GFLOPs: 10.2832. Time: 8.5870 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_contrib_conv2d_NCHWc_add"] Trial #31: GFLOPs: 15.6633. Time: 5.6375 ms. Best GFLOPs: 64.4650
[19:43:03] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_contrib_conv2d_NCHWc_add"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 128
Total latency (us): 7146.7

[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #0: GFLOPs: 0.0000. Time: 0.0627 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #1: GFLOPs: 0.0000. Time: 0.2441 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #2: GFLOPs: 0.0000. Time: 0.1486 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #3: GFLOPs: 0.0000. Time: 0.0844 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #4: GFLOPs: 0.0000. Time: 0.2431 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #5: GFLOPs: 0.0000. Time: 0.1384 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #6: GFLOPs: 0.0000. Time: 0.1480 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #7: GFLOPs: 0.0000. Time: 0.0773 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #8: GFLOPs: 0.0000. Time: 0.0523 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #9: GFLOPs: 0.0000. Time: 0.1304 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #10: GFLOPs: 0.0000. Time: 0.2196 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #11: GFLOPs: 0.0000. Time: 0.0436 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #12: GFLOPs: 0.0000. Time: 0.0860 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #13: GFLOPs: 0.0000. Time: 0.0901 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #14: GFLOPs: 0.0000. Time: 0.1086 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #15: GFLOPs: 0.0000. Time: 0.1117 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #16: GFLOPs: 0.0000. Time: 0.0984 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #17: GFLOPs: 0.0000. Time: 0.0509 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #18: GFLOPs: 0.0000. Time: 0.0769 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #19: GFLOPs: 0.0000. Time: 0.2303 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #20: GFLOPs: 0.0000. Time: 0.0762 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #21: GFLOPs: 0.0000. Time: 0.1193 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #22: GFLOPs: 0.0000. Time: 0.0793 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #23: GFLOPs: 0.0000. Time: 0.0871 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #24: GFLOPs: 0.0000. Time: 0.0550 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #25: GFLOPs: 0.0000. Time: 0.2910 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #26: GFLOPs: 0.0000. Time: 0.0586 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #27: GFLOPs: 0.0000. Time: 0.1808 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #28: GFLOPs: 0.0000. Time: 1.3234 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #29: GFLOPs: 0.0000. Time: 0.4551 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #30: GFLOPs: 0.0000. Time: 0.1459 ms. Best GFLOPs: 0.0000
[19:43:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"] Trial #31: GFLOPs: 0.0000. Time: 0.1626 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 160
Total latency (us): 7190.34

[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #0: GFLOPs: 19.3460. Time: 20.6279 ms. Best GFLOPs: 19.3460
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #1: GFLOPs: 9.3544. Time: 42.6609 ms. Best GFLOPs: 19.3460
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #2: GFLOPs: 8.5812. Time: 46.5047 ms. Best GFLOPs: 19.3460
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #3: GFLOPs: 29.6605. Time: 13.4545 ms. Best GFLOPs: 29.6605
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #4: GFLOPs: 15.6952. Time: 25.4261 ms. Best GFLOPs: 29.6605
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #5: GFLOPs: 11.3507. Time: 35.1579 ms. Best GFLOPs: 29.6605
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #6: GFLOPs: 17.8525. Time: 22.3536 ms. Best GFLOPs: 29.6605
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #7: GFLOPs: 30.7864. Time: 12.9624 ms. Best GFLOPs: 30.7864
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #8: GFLOPs: 9.1440. Time: 43.6427 ms. Best GFLOPs: 30.7864
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #9: GFLOPs: 37.2423. Time: 10.7154 ms. Best GFLOPs: 37.2423
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #10: GFLOPs: 36.4272. Time: 10.9552 ms. Best GFLOPs: 37.2423
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #11: GFLOPs: 10.6617. Time: 37.4299 ms. Best GFLOPs: 37.2423
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #12: GFLOPs: 6.9844. Time: 57.1373 ms. Best GFLOPs: 37.2423
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #13: GFLOPs: 31.3718. Time: 12.7206 ms. Best GFLOPs: 37.2423
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #14: GFLOPs: 6.0343. Time: 66.1330 ms. Best GFLOPs: 37.2423
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 32, 53, 53, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i2_3_init, i3_3_init in T.grid(2, 13, 26):
                for i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                        oh = T.axis.spatial(26, i2_2_init * 13 + i2_3_init)
                        ow, oc_block = T.axis.remap("SS", [i3_3_init, i4_3_fused_init])
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0 in T.grid(4, 3):
                for ax0, ax1, ax2, ax3 in T.grid(1, 8, 51, 53):
                    for ax4_fused in T.vectorized(4):
                        with T.block("data_pad"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(32, i5_0 * 8 + ax1)
                            i2 = T.axis.spatial(53, i6_0 + ax2)
                            i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3 and i3 < 53, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 2, 1, 1, 32, 1, 3, 1, 1, 13, 26):
                    for i4_3_fused in T.vectorized(4):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                            oh = T.axis.spatial(26, i2_2 * 13 + i2_3)
                            ow, oc_block = T.axis.remap("SS", [i3_3, i4_3_fused])
                            ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [64, 32, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(26):
                for i4_fused in T.vectorized(4):
                    with T.block("T_leaky_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(64, i0_i1_i2_fused // 26)
                        ax2 = T.axis.spatial(26, i0_i1_i2_fused % 26)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                        T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[16, 4, 1, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 13])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 26])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[4, 32])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b67)
l87 = sch.fuse(l70, l71, l72, l73, l74, l75, l76, l77, l78, l79)
sch.parallel(loop=l87)
l88 = sch.fuse(l86)
sch.vectorize(loop=l88)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b68)
l106 = sch.fuse(l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l107, l108, l109, l110, l111 = sch.get_loops(block=b69)
l112 = sch.fuse(l107, l108, l109)
sch.parallel(loop=l112)
l113 = sch.fuse(l111)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l112, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l112, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b114)
b132 = sch.decompose_reduction(block=b114, loop=l116)
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #16: GFLOPs: 2.5785. Time: 154.7681 ms. Best GFLOPs: 37.2423
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #17: GFLOPs: 12.2409. Time: 32.6012 ms. Best GFLOPs: 37.2423
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #18: GFLOPs: 6.3193. Time: 63.1508 ms. Best GFLOPs: 37.2423
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #19: GFLOPs: 92.9105. Time: 4.2952 ms. Best GFLOPs: 92.9105
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #20: GFLOPs: 32.1358. Time: 12.4182 ms. Best GFLOPs: 92.9105
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #21: GFLOPs: 28.9215. Time: 13.7983 ms. Best GFLOPs: 92.9105
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #22: GFLOPs: 23.3890. Time: 17.0622 ms. Best GFLOPs: 92.9105
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #23: GFLOPs: 40.4321. Time: 9.8701 ms. Best GFLOPs: 92.9105
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #24: GFLOPs: 34.1407. Time: 11.6889 ms. Best GFLOPs: 92.9105
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #25: GFLOPs: 27.0102. Time: 14.7747 ms. Best GFLOPs: 92.9105
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #26: GFLOPs: 26.5879. Time: 15.0093 ms. Best GFLOPs: 92.9105
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #27: GFLOPs: 5.2179. Time: 76.4806 ms. Best GFLOPs: 92.9105
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #28: GFLOPs: 31.4369. Time: 12.6942 ms. Best GFLOPs: 92.9105
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #29: GFLOPs: 20.9899. Time: 19.0123 ms. Best GFLOPs: 92.9105
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #30: GFLOPs: 10.5979. Time: 37.6551 ms. Best GFLOPs: 92.9105
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"] Trial #31: GFLOPs: 13.4690. Time: 29.6285 ms. Best GFLOPs: 92.9105
[19:43:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 192
Total latency (us): 11485.5

[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #0: GFLOPs: 0.0000. Time: 0.0345 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #1: GFLOPs: 0.0000. Time: 0.0308 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #2: GFLOPs: 0.0000. Time: 0.0528 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #3: GFLOPs: 0.0000. Time: 0.0377 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #4: GFLOPs: 0.0000. Time: 0.0357 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #5: GFLOPs: 0.0000. Time: 0.0536 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #6: GFLOPs: 0.0000. Time: 0.0507 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #7: GFLOPs: 0.0000. Time: 0.0372 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #8: GFLOPs: 0.0000. Time: 0.5941 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #9: GFLOPs: 0.0000. Time: 0.4343 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #10: GFLOPs: 0.0000. Time: 0.0601 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #11: GFLOPs: 0.0000. Time: 0.0578 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #12: GFLOPs: 0.0000. Time: 0.0358 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #13: GFLOPs: 0.0000. Time: 0.0412 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #14: GFLOPs: 0.0000. Time: 0.0412 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #15: GFLOPs: 0.0000. Time: 0.0561 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #16: GFLOPs: 0.0000. Time: 0.0354 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #17: GFLOPs: 0.0000. Time: 0.0464 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #18: GFLOPs: 0.0000. Time: 0.0509 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #19: GFLOPs: 0.0000. Time: 0.0505 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #20: GFLOPs: 0.0000. Time: 0.0501 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #21: GFLOPs: 0.0000. Time: 0.0437 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #22: GFLOPs: 0.0000. Time: 0.0415 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #23: GFLOPs: 0.0000. Time: 0.0489 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #24: GFLOPs: 0.0000. Time: 0.0426 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #25: GFLOPs: 0.0000. Time: 0.0575 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #26: GFLOPs: 0.0000. Time: 0.0454 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #27: GFLOPs: 0.0000. Time: 0.0271 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #28: GFLOPs: 0.0000. Time: 0.0399 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #29: GFLOPs: 0.0000. Time: 0.0636 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #30: GFLOPs: 0.0000. Time: 0.0316 ms. Best GFLOPs: 0.0000
[19:43:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_concatenate_1"] Trial #31: GFLOPs: 0.0000. Time: 0.0308 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_concatenate_1"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 224
Total latency (us): 11512.6

[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #0: GFLOPs: 0.0000. Time: 0.0634 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #1: GFLOPs: 0.0000. Time: 0.0570 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #2: GFLOPs: 0.0000. Time: 0.0617 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #3: GFLOPs: 0.0000. Time: 0.0778 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #4: GFLOPs: 0.0000. Time: 0.0689 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #5: GFLOPs: 0.0000. Time: 0.0495 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #6: GFLOPs: 0.0000. Time: 0.0626 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #7: GFLOPs: 0.0000. Time: 0.0524 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #8: GFLOPs: 0.0000. Time: 0.0715 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #9: GFLOPs: 0.0000. Time: 0.0580 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #10: GFLOPs: 0.0000. Time: 0.0485 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #11: GFLOPs: 0.0000. Time: 0.0486 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #12: GFLOPs: 0.0000. Time: 0.0576 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #13: GFLOPs: 0.0000. Time: 0.0484 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #14: GFLOPs: 0.0000. Time: 0.0502 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #15: GFLOPs: 0.0000. Time: 0.0609 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #16: GFLOPs: 0.0000. Time: 0.0486 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #17: GFLOPs: 0.0000. Time: 0.1031 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #18: GFLOPs: 0.0000. Time: 0.0463 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #19: GFLOPs: 0.0000. Time: 0.0718 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #20: GFLOPs: 0.0000. Time: 0.0505 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #21: GFLOPs: 0.0000. Time: 0.0444 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #22: GFLOPs: 0.0000. Time: 0.0547 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #23: GFLOPs: 0.0000. Time: 0.0542 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #24: GFLOPs: 0.0000. Time: 0.0440 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #25: GFLOPs: 0.0000. Time: 0.0588 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #26: GFLOPs: 0.0000. Time: 0.0540 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #27: GFLOPs: 0.0000. Time: 0.0517 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #28: GFLOPs: 0.0000. Time: 0.0504 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #29: GFLOPs: 0.0000. Time: 0.0624 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #30: GFLOPs: 0.0000. Time: 0.0552 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_layout_transform_1"] Trial #31: GFLOPs: 0.0000. Time: 0.0497 ms. Best GFLOPs: 0.0000
[19:43:05] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_layout_transform_1"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 256
Total latency (us): 11556.6

[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #0: GFLOPs: 3.9477. Time: 44.7571 ms. Best GFLOPs: 3.9477
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #1: GFLOPs: 7.4224. Time: 23.8050 ms. Best GFLOPs: 7.4224
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #2: GFLOPs: 5.5229. Time: 31.9921 ms. Best GFLOPs: 7.4224
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #3: GFLOPs: 40.5276. Time: 4.3597 ms. Best GFLOPs: 40.5276
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #4: GFLOPs: 21.4854. Time: 8.2237 ms. Best GFLOPs: 40.5276
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #5: GFLOPs: 3.6080. Time: 48.9717 ms. Best GFLOPs: 40.5276
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #6: GFLOPs: 35.8775. Time: 4.9248 ms. Best GFLOPs: 40.5276
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #7: GFLOPs: 33.3378. Time: 5.3000 ms. Best GFLOPs: 40.5276
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #8: GFLOPs: 10.3076. Time: 17.1417 ms. Best GFLOPs: 40.5276
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #9: GFLOPs: 18.4277. Time: 9.5883 ms. Best GFLOPs: 40.5276
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #10: GFLOPs: 34.7112. Time: 5.0903 ms. Best GFLOPs: 40.5276
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #11: GFLOPs: 26.2789. Time: 6.7236 ms. Best GFLOPs: 40.5276
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #12: GFLOPs: 14.7416. Time: 11.9858 ms. Best GFLOPs: 40.5276
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #13: GFLOPs: 42.5448. Time: 4.1530 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #14: GFLOPs: 12.1977. Time: 14.4855 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #15: GFLOPs: 7.9303. Time: 22.2803 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #16: GFLOPs: 11.3020. Time: 15.6335 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #17: GFLOPs: 20.2990. Time: 8.7043 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #18: GFLOPs: 17.6724. Time: 9.9980 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #19: GFLOPs: 13.2542. Time: 13.3308 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #20: GFLOPs: 18.7758. Time: 9.4105 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #21: GFLOPs: 13.6067. Time: 12.9854 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #22: GFLOPs: 16.5181. Time: 10.6967 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #23: GFLOPs: 17.2111. Time: 10.2660 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #24: GFLOPs: 16.7142. Time: 10.5712 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #25: GFLOPs: 24.0541. Time: 7.3455 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #26: GFLOPs: 25.7692. Time: 6.8566 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #27: GFLOPs: 14.0654. Time: 12.5620 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #28: GFLOPs: 4.9819. Time: 35.4665 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #29: GFLOPs: 6.7963. Time: 25.9978 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #30: GFLOPs: 20.8586. Time: 8.4708 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"] Trial #31: GFLOPs: 31.2885. Time: 5.6471 ms. Best GFLOPs: 42.5448
[19:43:06] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 288
Total latency (us): 15709.6

[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #0: GFLOPs: 0.0000. Time: 2.6916 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #1: GFLOPs: 0.0000. Time: 3.5002 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #2: GFLOPs: 0.0000. Time: 3.8205 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #3: GFLOPs: 0.0000. Time: 1.2384 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #4: GFLOPs: 0.0000. Time: 1.1720 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #5: GFLOPs: 0.0000. Time: 1.2590 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #6: GFLOPs: 0.0000. Time: 1.0577 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #7: GFLOPs: 0.0000. Time: 1.0576 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #8: GFLOPs: 0.0000. Time: 1.8273 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #9: GFLOPs: 0.0000. Time: 0.3370 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #10: GFLOPs: 0.0000. Time: 1.8895 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #11: GFLOPs: 0.0000. Time: 0.5289 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #12: GFLOPs: 0.0000. Time: 0.4995 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #13: GFLOPs: 0.0000. Time: 0.3754 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #14: GFLOPs: 0.0000. Time: 0.3743 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #15: GFLOPs: 0.0000. Time: 0.3200 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #16: GFLOPs: 0.0000. Time: 0.5031 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #17: GFLOPs: 0.0000. Time: 0.3835 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #18: GFLOPs: 0.0000. Time: 2.0961 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #19: GFLOPs: 0.0000. Time: 1.5984 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #20: GFLOPs: 0.0000. Time: 0.2521 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #21: GFLOPs: 0.0000. Time: 0.4296 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #22: GFLOPs: 0.0000. Time: 0.4737 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #23: GFLOPs: 0.0000. Time: 0.2786 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #24: GFLOPs: 0.0000. Time: 0.3210 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #25: GFLOPs: 0.0000. Time: 0.4343 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #26: GFLOPs: 0.0000. Time: 2.2217 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #27: GFLOPs: 0.0000. Time: 0.3060 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #28: GFLOPs: 0.0000. Time: 0.1694 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #29: GFLOPs: 0.0000. Time: 1.4736 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #30: GFLOPs: 0.0000. Time: 1.0516 ms. Best GFLOPs: 0.0000
[19:43:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"] Trial #31: GFLOPs: 0.0000. Time: 0.2599 ms. Best GFLOPs: 0.0000
[19:43:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 320
Total latency (us): 15879

[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #0: GFLOPs: 7.5248. Time: 0.2875 ms. Best GFLOPs: 7.5248
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #1: GFLOPs: 2.4296. Time: 0.8904 ms. Best GFLOPs: 7.5248
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #2: GFLOPs: 7.1746. Time: 0.3015 ms. Best GFLOPs: 7.5248
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #3: GFLOPs: 2.1732. Time: 0.9954 ms. Best GFLOPs: 7.5248
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #4: GFLOPs: 10.7884. Time: 0.2005 ms. Best GFLOPs: 10.7884
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #5: GFLOPs: 3.8482. Time: 0.5621 ms. Best GFLOPs: 10.7884
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #6: GFLOPs: 8.3437. Time: 0.2593 ms. Best GFLOPs: 10.7884
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #7: GFLOPs: 4.2338. Time: 0.5109 ms. Best GFLOPs: 10.7884
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #8: GFLOPs: 12.1329. Time: 0.1783 ms. Best GFLOPs: 12.1329
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #9: GFLOPs: 8.6147. Time: 0.2511 ms. Best GFLOPs: 12.1329
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #10: GFLOPs: 6.4700. Time: 0.3343 ms. Best GFLOPs: 12.1329
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #11: GFLOPs: 6.1623. Time: 0.3510 ms. Best GFLOPs: 12.1329
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #12: GFLOPs: 7.5008. Time: 0.2884 ms. Best GFLOPs: 12.1329
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #13: GFLOPs: 12.4914. Time: 0.1732 ms. Best GFLOPs: 12.4914
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #14: GFLOPs: 7.6419. Time: 0.2831 ms. Best GFLOPs: 12.4914
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #15: GFLOPs: 7.8339. Time: 0.2761 ms. Best GFLOPs: 12.4914
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #16: GFLOPs: 7.2695. Time: 0.2976 ms. Best GFLOPs: 12.4914
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #17: GFLOPs: 5.8251. Time: 0.3714 ms. Best GFLOPs: 12.4914
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #18: GFLOPs: 7.9876. Time: 0.2708 ms. Best GFLOPs: 12.4914
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #19: GFLOPs: 8.5388. Time: 0.2533 ms. Best GFLOPs: 12.4914
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #20: GFLOPs: 4.2322. Time: 0.5111 ms. Best GFLOPs: 12.4914
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #21: GFLOPs: 19.1105. Time: 0.1132 ms. Best GFLOPs: 19.1105
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #22: GFLOPs: 8.7991. Time: 0.2458 ms. Best GFLOPs: 19.1105
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #23: GFLOPs: 17.7709. Time: 0.1217 ms. Best GFLOPs: 19.1105
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #24: GFLOPs: 13.2750. Time: 0.1630 ms. Best GFLOPs: 19.1105
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #25: GFLOPs: 3.6333. Time: 0.5954 ms. Best GFLOPs: 19.1105
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #26: GFLOPs: 1.2945. Time: 1.6711 ms. Best GFLOPs: 19.1105
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #27: GFLOPs: 6.1826. Time: 0.3499 ms. Best GFLOPs: 19.1105
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #28: GFLOPs: 8.2306. Time: 0.2628 ms. Best GFLOPs: 19.1105
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #29: GFLOPs: 5.1235. Time: 0.4222 ms. Best GFLOPs: 19.1105
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #30: GFLOPs: 7.6602. Time: 0.2824 ms. Best GFLOPs: 19.1105
[19:43:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #31: GFLOPs: 0.1729. Time: 12.5138 ms. Best GFLOPs: 19.1105
[19:43:08] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_max_pool2d"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 352
Total latency (us): 15992.2

[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #0: GFLOPs: 1.9307. Time: 3.6301 ms. Best GFLOPs: 1.9307
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #1: GFLOPs: 8.0492. Time: 0.8707 ms. Best GFLOPs: 8.0492
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #2: GFLOPs: 5.5070. Time: 1.2727 ms. Best GFLOPs: 8.0492
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #3: GFLOPs: 7.9343. Time: 0.8834 ms. Best GFLOPs: 8.0492
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #4: GFLOPs: 4.9112. Time: 1.4271 ms. Best GFLOPs: 8.0492
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #5: GFLOPs: 2.5559. Time: 2.7422 ms. Best GFLOPs: 8.0492
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #6: GFLOPs: 3.3331. Time: 2.1028 ms. Best GFLOPs: 8.0492
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #7: GFLOPs: 4.1312. Time: 1.6965 ms. Best GFLOPs: 8.0492
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #8: GFLOPs: 15.7969. Time: 0.4437 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #9: GFLOPs: 7.0288. Time: 0.9972 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #10: GFLOPs: 13.2373. Time: 0.5295 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #11: GFLOPs: 4.2289. Time: 1.6573 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #12: GFLOPs: 9.4381. Time: 0.7426 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #13: GFLOPs: 3.7844. Time: 1.8520 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #14: GFLOPs: 5.9171. Time: 1.1845 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #15: GFLOPs: 4.6259. Time: 1.5151 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #16: GFLOPs: 1.5983. Time: 4.3850 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #17: GFLOPs: 4.4546. Time: 1.5734 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #18: GFLOPs: 5.7225. Time: 1.2248 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #19: GFLOPs: 1.8747. Time: 3.7385 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #20: GFLOPs: 6.0572. Time: 1.1571 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #21: GFLOPs: 5.8375. Time: 1.2007 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #22: GFLOPs: 13.5920. Time: 0.5157 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #23: GFLOPs: 6.6140. Time: 1.0597 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #24: GFLOPs: 12.8770. Time: 0.5443 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #25: GFLOPs: 8.8027. Time: 0.7962 ms. Best GFLOPs: 15.7969
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #26: GFLOPs: 17.4000. Time: 0.4028 ms. Best GFLOPs: 17.4000
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #27: GFLOPs: 2.4464. Time: 2.8649 ms. Best GFLOPs: 17.4000
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #28: GFLOPs: 17.9495. Time: 0.3905 ms. Best GFLOPs: 17.9495
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #29: GFLOPs: 4.3685. Time: 1.6044 ms. Best GFLOPs: 17.9495
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #30: GFLOPs: 11.5244. Time: 0.6082 ms. Best GFLOPs: 17.9495
[19:43:08] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_1"] Trial #31: GFLOPs: 16.9693. Time: 0.4130 ms. Best GFLOPs: 17.9495
[19:43:08] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_max_pool2d_1"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 384
Total latency (us): 16382.7

[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #0: GFLOPs: 25.2911. Time: 63.0679 ms. Best GFLOPs: 25.2911
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #1: GFLOPs: 93.6486. Time: 17.0324 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #2: GFLOPs: 41.2617. Time: 38.6571 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 27, 27, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(3456, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(27):
                for i4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(128, i0_i1_i2_fused // 27)
                        i2 = T.axis.spatial(27, i0_i1_i2_fused % 27)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3_1, i4])
                        data_pad[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 27 and 1 <= i3_1 and i3_1 < 27, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1_1, i4_1 in T.grid(1, 2, 13, 1, 1):
                for i1_2_init, i1_3_init, i3_3_init in T.grid(4, 32, 13):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(256, i1_1 * 128 + i1_2_init * 32 + i1_3_init)
                            oh, ow = T.axis.remap("SS", [i2_1, i3_3_init])
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [256, 128, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(32, 3, 3, 1, 4, 1, 1, 1, 16, 1, 1, 1, 32, 1, 13):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(256, i1_1 * 128 + i1_2 * 32 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_1, i3_3])
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i4_3_fused)
                            ic = T.axis.reduce(512, i5_0 * 16 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [256, 128, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 256, 13, 13):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3_1 = T.axis.remap("SSS", [ax1, ax2, ax3])
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 4, 32])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74 = sch.get_loops(block=b67)
l75 = sch.fuse(l70, l71, l72)
sch.parallel(loop=l75)
l76 = sch.fuse(l74)
sch.vectorize(loop=l76)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b68)
l103 = sch.fuse(l77, l78, l79, l80, l81)
sch.parallel(loop=l103)
l104 = sch.fuse(l102)
sch.vectorize(loop=l104)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110 = sch.get_loops(block=b69)
l111 = sch.fuse(l110)
sch.vectorize(loop=l111)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b112 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b112)
b135 = sch.decompose_reduction(block=b112, loop=l119)
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #4: GFLOPs: 15.9640. Time: 99.9160 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #5: GFLOPs: 21.5833. Time: 73.9022 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #6: GFLOPs: 22.2396. Time: 71.7216 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #7: GFLOPs: 59.3222. Time: 26.8880 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #8: GFLOPs: 24.1539. Time: 66.0374 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #9: GFLOPs: 18.6373. Time: 85.5842 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #10: GFLOPs: 17.9107. Time: 89.0559 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #11: GFLOPs: 12.3380. Time: 129.2803 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #12: GFLOPs: 37.2442. Time: 42.8270 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #13: GFLOPs: 2.8457. Time: 560.5224 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #14: GFLOPs: 5.2134. Time: 305.9552 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #15: GFLOPs: 20.6799. Time: 77.1310 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #16: GFLOPs: 71.5410. Time: 22.2957 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #17: GFLOPs: 3.3718. Time: 473.0650 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #18: GFLOPs: 37.2984. Time: 42.7647 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #19: GFLOPs: 26.6713. Time: 59.8042 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #20: GFLOPs: 24.6938. Time: 64.5935 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #21: GFLOPs: 16.2924. Time: 97.9018 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #22: GFLOPs: 39.6340. Time: 40.2446 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #23: GFLOPs: 4.6287. Time: 344.6028 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #24: GFLOPs: 17.5829. Time: 90.7163 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #25: GFLOPs: 7.1950. Time: 221.6908 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #26: GFLOPs: 41.5159. Time: 38.4204 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #27: GFLOPs: 6.5263. Time: 244.4035 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #28: GFLOPs: 20.1157. Time: 79.2940 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #29: GFLOPs: 36.2158. Time: 44.0431 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #30: GFLOPs: 23.0348. Time: 69.2456 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"] Trial #31: GFLOPs: 18.0087. Time: 88.5714 ms. Best GFLOPs: 93.6486
[19:43:09] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |            N/A |          N/A |                   N/A |      0 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 416
Total latency (us): 33415

[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #0: GFLOPs: 26.6517. Time: 6.6523 ms. Best GFLOPs: 26.6517
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 13, 13):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 64 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 * 4 + i1_2_init * 2 + i1_3_init)
                        oh, ow = T.axis.remap("SS", [i2_3_init, i3_3_init])
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(128, 1, 1, 1, 2, 1, 1, 1, 8, 1, 1, 1, 2, 13, 13):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 64 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 * 4 + i1_2 * 2 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_3])
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 2 + i4_3_fused)
                        ic = T.axis.reduce(1024, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 13, 13):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 64 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 * 4 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 16 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 16, 2, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b66)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #2: GFLOPs: 14.9072. Time: 11.8933 ms. Best GFLOPs: 26.6517
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #3: GFLOPs: 146.0186. Time: 1.2142 ms. Best GFLOPs: 146.0186
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #4: GFLOPs: 26.5014. Time: 6.6901 ms. Best GFLOPs: 146.0186
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #5: GFLOPs: 25.7679. Time: 6.8805 ms. Best GFLOPs: 146.0186
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #6: GFLOPs: 110.8857. Time: 1.5989 ms. Best GFLOPs: 146.0186
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_2_init, i1_3_init, i2_3_init in T.grid(13, 2, 13):
                for i3_3_i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 64 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 * 2 + i1_3_init)
                        oh, ow, oc_block = T.axis.remap("SSS", [i2_3_init, i3_2_init, i3_3_i4_3_fused_init])
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(1024, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, 2, 13):
                for i3_3_i4_3_fused in T.vectorized(4):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 64 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 * 2 + i1_3)
                        oh, ow, oc_block, ic = T.axis.remap("SSSR", [i2_3, i3_2, i3_3_i4_3_fused, i5_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_add"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(128, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 32, 1, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[1024, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
l93 = sch.fuse(l90, l91)
sch.vectorize(loop=l93)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b65)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b101)
b118 = sch.decompose_reduction(block=b101, loop=l103)
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #8: GFLOPs: 35.8780. Time: 4.9416 ms. Best GFLOPs: 146.0186
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #9: GFLOPs: 52.1050. Time: 3.4027 ms. Best GFLOPs: 146.0186
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #10: GFLOPs: 46.5386. Time: 3.8097 ms. Best GFLOPs: 146.0186
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #11: GFLOPs: 71.6446. Time: 2.4747 ms. Best GFLOPs: 146.0186
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #12: GFLOPs: 77.0610. Time: 2.3007 ms. Best GFLOPs: 146.0186
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #13: GFLOPs: 24.6645. Time: 7.1883 ms. Best GFLOPs: 146.0186
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(128, 256, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(52, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_3_init, i2_3_init in T.grid(32, 13):
                for i3_3_i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 13 * 32 + i1_3_init)
                        oh = T.axis.spatial(13, i2_3_init)
                        ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 13)
                        oc_block = T.axis.spatial(4, i3_3_i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3 in T.grid(32, 1, 1, 1, 1, 1, 1, 1, 32, 1, 1, 1, 32, 13):
                for i3_3_i4_3_fused in T.vectorized(4):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 13 * 32 + i1_3)
                        oh = T.axis.spatial(13, i2_3)
                        ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 13)
                        oc_block = T.axis.spatial(4, i3_3_i4_3_fused)
                        ic = T.axis.reduce(1024, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 256, 13, 13, 4], "float32"], ["TENSOR", [128, 256, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(52):
                with T.block("T_add"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(128, i0_i1_i2_fused // 13)
                    ax2 = T.axis.spatial(13, i0_i1_i2_fused % 13)
                    ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[4, 1, 1, 32])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
l93 = sch.fuse(l90, l91)
sch.vectorize(loop=l93)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b65)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l97, l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b101)
b118 = sch.decompose_reduction(block=b101, loop=l103)
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #15: GFLOPs: 30.7266. Time: 5.7701 ms. Best GFLOPs: 146.0186
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #16: GFLOPs: 53.3688. Time: 3.3221 ms. Best GFLOPs: 146.0186
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #17: GFLOPs: 31.8437. Time: 5.5677 ms. Best GFLOPs: 146.0186
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #18: GFLOPs: 37.5117. Time: 4.7264 ms. Best GFLOPs: 146.0186
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #19: GFLOPs: 177.3662. Time: 0.9996 ms. Best GFLOPs: 177.3662
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #20: GFLOPs: 50.6012. Time: 3.5038 ms. Best GFLOPs: 177.3662
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #21: GFLOPs: 111.5795. Time: 1.5890 ms. Best GFLOPs: 177.3662
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #22: GFLOPs: 40.7575. Time: 4.3500 ms. Best GFLOPs: 177.3662
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #23: GFLOPs: 60.6864. Time: 2.9215 ms. Best GFLOPs: 177.3662
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #24: GFLOPs: 38.9530. Time: 4.5515 ms. Best GFLOPs: 177.3662
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #25: GFLOPs: 31.4267. Time: 5.6416 ms. Best GFLOPs: 177.3662
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #26: GFLOPs: 62.2043. Time: 2.8502 ms. Best GFLOPs: 177.3662
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #27: GFLOPs: 15.4235. Time: 11.4952 ms. Best GFLOPs: 177.3662
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #28: GFLOPs: 38.3260. Time: 4.6260 ms. Best GFLOPs: 177.3662
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #29: GFLOPs: 61.6023. Time: 2.8781 ms. Best GFLOPs: 177.3662
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #30: GFLOPs: 13.5687. Time: 13.0665 ms. Best GFLOPs: 177.3662
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"] Trial #31: GFLOPs: 17.1412. Time: 10.3433 ms. Best GFLOPs: 177.3662
[19:43:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 448
Total latency (us): 35414.3

[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #0: GFLOPs: 0.9937. Time: 0.1742 ms. Best GFLOPs: 0.9937
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #1: GFLOPs: 1.0112. Time: 0.1711 ms. Best GFLOPs: 1.0112
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #2: GFLOPs: 1.0244. Time: 0.1689 ms. Best GFLOPs: 1.0244
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #3: GFLOPs: 1.1611. Time: 0.1490 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #4: GFLOPs: 0.9905. Time: 0.1747 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #5: GFLOPs: 1.0252. Time: 0.1688 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #6: GFLOPs: 0.9784. Time: 0.1769 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #7: GFLOPs: 1.1581. Time: 0.1494 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #8: GFLOPs: 1.0747. Time: 0.1610 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #9: GFLOPs: 1.0805. Time: 0.1602 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #10: GFLOPs: 1.0255. Time: 0.1688 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #11: GFLOPs: 1.1109. Time: 0.1558 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #12: GFLOPs: 1.0203. Time: 0.1696 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #13: GFLOPs: 1.0247. Time: 0.1689 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #14: GFLOPs: 1.0250. Time: 0.1688 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #15: GFLOPs: 1.0166. Time: 0.1702 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #16: GFLOPs: 1.1465. Time: 0.1509 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #17: GFLOPs: 1.1570. Time: 0.1496 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #18: GFLOPs: 0.9889. Time: 0.1750 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #19: GFLOPs: 0.7881. Time: 0.2196 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #20: GFLOPs: 1.0664. Time: 0.1623 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #21: GFLOPs: 1.0098. Time: 0.1714 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #22: GFLOPs: 1.0040. Time: 0.1724 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #23: GFLOPs: 1.1568. Time: 0.1496 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #24: GFLOPs: 0.9924. Time: 0.1744 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #25: GFLOPs: 1.0936. Time: 0.1582 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #26: GFLOPs: 0.4061. Time: 0.4262 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #27: GFLOPs: 0.4118. Time: 0.4203 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #28: GFLOPs: 0.2956. Time: 0.5854 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #29: GFLOPs: 0.3461. Time: 0.5000 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #30: GFLOPs: 0.5349. Time: 0.3235 ms. Best GFLOPs: 1.1611
[19:43:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"] Trial #31: GFLOPs: 0.2530. Time: 0.6839 ms. Best GFLOPs: 1.1611
[19:43:11] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 480
Total latency (us): 36159.5

[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #0: GFLOPs: 15.0783. Time: 52.8923 ms. Best GFLOPs: 15.0783
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #1: GFLOPs: 22.9943. Time: 34.6838 ms. Best GFLOPs: 22.9943
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #2: GFLOPs: 6.1404. Time: 129.8829 ms. Best GFLOPs: 22.9943
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #3: GFLOPs: 14.3649. Time: 55.5194 ms. Best GFLOPs: 22.9943
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #4: GFLOPs: 15.8169. Time: 50.4226 ms. Best GFLOPs: 22.9943
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #5: GFLOPs: 10.1697. Time: 78.4217 ms. Best GFLOPs: 22.9943
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #6: GFLOPs: 12.5208. Time: 63.6961 ms. Best GFLOPs: 22.9943
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #7: GFLOPs: 91.2437. Time: 8.7406 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #8: GFLOPs: 26.6647. Time: 29.9095 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #9: GFLOPs: 13.0692. Time: 61.0234 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #10: GFLOPs: 11.4613. Time: 69.5842 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #11: GFLOPs: 26.1352. Time: 30.5155 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #12: GFLOPs: 70.7549. Time: 11.2717 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #13: GFLOPs: 19.0700. Time: 41.8210 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #14: GFLOPs: 46.8209. Time: 17.0336 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #15: GFLOPs: 26.1518. Time: 30.4962 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #16: GFLOPs: 18.3226. Time: 43.5270 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #17: GFLOPs: 55.2750. Time: 14.4284 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #18: GFLOPs: 54.0400. Time: 14.7581 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #19: GFLOPs: 49.4043. Time: 16.1429 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #20: GFLOPs: 23.5170. Time: 33.9129 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #21: GFLOPs: 8.6481. Time: 92.2197 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #22: GFLOPs: 12.1816. Time: 65.4702 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #23: GFLOPs: 10.9979. Time: 72.5167 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #24: GFLOPs: 58.7786. Time: 13.5683 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #25: GFLOPs: 58.8365. Time: 13.5550 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #26: GFLOPs: 41.6838. Time: 19.1328 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #27: GFLOPs: 9.5769. Time: 83.2762 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #28: GFLOPs: 38.5933. Time: 20.6649 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #29: GFLOPs: 17.0213. Time: 46.8546 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #30: GFLOPs: 23.3750. Time: 34.1189 ms. Best GFLOPs: 91.2437
[19:43:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"] Trial #31: GFLOPs: 20.6690. Time: 38.5857 ms. Best GFLOPs: 91.2437
[19:43:12] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 512
Total latency (us): 71122

[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #0: GFLOPs: 0.7623. Time: 0.3405 ms. Best GFLOPs: 0.7623
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #1: GFLOPs: 0.7071. Time: 0.3671 ms. Best GFLOPs: 0.7623
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #2: GFLOPs: 0.9163. Time: 0.2833 ms. Best GFLOPs: 0.9163
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #3: GFLOPs: 0.6989. Time: 0.3714 ms. Best GFLOPs: 0.9163
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #4: GFLOPs: 0.7703. Time: 0.3370 ms. Best GFLOPs: 0.9163
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #5: GFLOPs: 0.7277. Time: 0.3567 ms. Best GFLOPs: 0.9163
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #6: GFLOPs: 0.7289. Time: 0.3561 ms. Best GFLOPs: 0.9163
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #7: GFLOPs: 0.7945. Time: 0.3267 ms. Best GFLOPs: 0.9163
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #8: GFLOPs: 0.9210. Time: 0.2819 ms. Best GFLOPs: 0.9210
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #9: GFLOPs: 0.6990. Time: 0.3714 ms. Best GFLOPs: 0.9210
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #10: GFLOPs: 0.7428. Time: 0.3495 ms. Best GFLOPs: 0.9210
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #11: GFLOPs: 0.8247. Time: 0.3148 ms. Best GFLOPs: 0.9210
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #12: GFLOPs: 0.8346. Time: 0.3110 ms. Best GFLOPs: 0.9210
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #13: GFLOPs: 0.8105. Time: 0.3203 ms. Best GFLOPs: 0.9210
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #14: GFLOPs: 0.7903. Time: 0.3284 ms. Best GFLOPs: 0.9210
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #15: GFLOPs: 0.8385. Time: 0.3096 ms. Best GFLOPs: 0.9210
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #16: GFLOPs: 0.7301. Time: 0.3556 ms. Best GFLOPs: 0.9210
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #17: GFLOPs: 0.8884. Time: 0.2922 ms. Best GFLOPs: 0.9210
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #18: GFLOPs: 0.9552. Time: 0.2718 ms. Best GFLOPs: 0.9552
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #19: GFLOPs: 0.7367. Time: 0.3524 ms. Best GFLOPs: 0.9552
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #20: GFLOPs: 0.7615. Time: 0.3409 ms. Best GFLOPs: 0.9552
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #21: GFLOPs: 0.9016. Time: 0.2879 ms. Best GFLOPs: 0.9552
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #22: GFLOPs: 0.8564. Time: 0.3031 ms. Best GFLOPs: 0.9552
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #23: GFLOPs: 0.7996. Time: 0.3247 ms. Best GFLOPs: 0.9552
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #24: GFLOPs: 0.7068. Time: 0.3673 ms. Best GFLOPs: 0.9552
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #25: GFLOPs: 0.7879. Time: 0.3294 ms. Best GFLOPs: 0.9552
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #26: GFLOPs: 0.9303. Time: 0.2790 ms. Best GFLOPs: 0.9552
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #27: GFLOPs: 0.7972. Time: 0.3256 ms. Best GFLOPs: 0.9552
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #28: GFLOPs: 0.7264. Time: 0.3573 ms. Best GFLOPs: 0.9552
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #29: GFLOPs: 0.8229. Time: 0.3155 ms. Best GFLOPs: 0.9552
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #30: GFLOPs: 0.7888. Time: 0.3291 ms. Best GFLOPs: 0.9552
[19:43:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"] Trial #31: GFLOPs: 0.8040. Time: 0.3229 ms. Best GFLOPs: 0.9552
[19:43:13] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 544
Total latency (us): 72209.1

[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #0: GFLOPs: 11.1876. Time: 7.9277 ms. Best GFLOPs: 11.1876
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #1: GFLOPs: 56.2179. Time: 1.5776 ms. Best GFLOPs: 56.2179
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #2: GFLOPs: 54.1975. Time: 1.6364 ms. Best GFLOPs: 56.2179
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #3: GFLOPs: 29.1743. Time: 3.0400 ms. Best GFLOPs: 56.2179
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #4: GFLOPs: 82.2036. Time: 1.0789 ms. Best GFLOPs: 82.2036
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #5: GFLOPs: 53.3879. Time: 1.6613 ms. Best GFLOPs: 82.2036
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #6: GFLOPs: 18.2068. Time: 4.8713 ms. Best GFLOPs: 82.2036
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #7: GFLOPs: 78.1121. Time: 1.1354 ms. Best GFLOPs: 82.2036
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #8: GFLOPs: 74.6282. Time: 1.1884 ms. Best GFLOPs: 82.2036
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #9: GFLOPs: 10.7808. Time: 8.2268 ms. Best GFLOPs: 82.2036
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #10: GFLOPs: 59.1225. Time: 1.5001 ms. Best GFLOPs: 82.2036
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #11: GFLOPs: 12.9702. Time: 6.8381 ms. Best GFLOPs: 82.2036
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #12: GFLOPs: 134.2602. Time: 0.6606 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #13: GFLOPs: 30.6371. Time: 2.8949 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #14: GFLOPs: 26.6904. Time: 3.3230 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #15: GFLOPs: 28.2416. Time: 3.1404 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #16: GFLOPs: 14.5462. Time: 6.0972 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #17: GFLOPs: 22.9494. Time: 3.8646 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #18: GFLOPs: 6.0667. Time: 14.6192 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #19: GFLOPs: 22.6825. Time: 3.9101 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #20: GFLOPs: 19.9089. Time: 4.4549 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #21: GFLOPs: 20.7892. Time: 4.2662 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #22: GFLOPs: 13.0450. Time: 6.7989 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #23: GFLOPs: 11.3754. Time: 7.7968 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #24: GFLOPs: 11.0873. Time: 7.9993 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #25: GFLOPs: 16.9786. Time: 5.2237 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #26: GFLOPs: 20.6482. Time: 4.2954 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #27: GFLOPs: 13.0692. Time: 6.7863 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #28: GFLOPs: 96.5068. Time: 0.9190 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #29: GFLOPs: 17.7231. Time: 5.0043 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #30: GFLOPs: 24.0997. Time: 3.6802 ms. Best GFLOPs: 134.2602
[19:43:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"] Trial #31: GFLOPs: 46.4607. Time: 1.9090 ms. Best GFLOPs: 134.2602
[19:43:14] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 576
Total latency (us): 75512.1

[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #0: GFLOPs: 0.9555. Time: 0.3622 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #1: GFLOPs: 0.0478. Time: 7.2377 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #2: GFLOPs: 0.1149. Time: 3.0127 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #3: GFLOPs: 0.0173. Time: 20.0030 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #4: GFLOPs: 0.1133. Time: 3.0539 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #5: GFLOPs: 0.6619. Time: 0.5229 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #6: GFLOPs: 0.2631. Time: 1.3154 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #7: GFLOPs: 0.2123. Time: 1.6305 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #8: GFLOPs: 0.1075. Time: 3.2211 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #9: GFLOPs: 0.7100. Time: 0.4875 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #10: GFLOPs: 0.0985. Time: 3.5131 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #11: GFLOPs: 0.0206. Time: 16.7757 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #12: GFLOPs: 0.0705. Time: 4.9083 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #13: GFLOPs: 0.3025. Time: 1.1440 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #14: GFLOPs: 0.5397. Time: 0.6413 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #15: GFLOPs: 0.2888. Time: 1.1985 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #16: GFLOPs: 0.8047. Time: 0.4301 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #17: GFLOPs: 0.0119. Time: 28.9990 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #18: GFLOPs: 0.0532. Time: 6.5080 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #19: GFLOPs: 0.7117. Time: 0.4863 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #20: GFLOPs: 0.1427. Time: 2.4253 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #21: GFLOPs: 0.7389. Time: 0.4684 ms. Best GFLOPs: 0.9555
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #22: GFLOPs: 1.0197. Time: 0.3394 ms. Best GFLOPs: 1.0197
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #23: GFLOPs: 0.2707. Time: 1.2785 ms. Best GFLOPs: 1.0197
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #24: GFLOPs: 0.2016. Time: 1.7165 ms. Best GFLOPs: 1.0197
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #25: GFLOPs: 0.6512. Time: 0.5315 ms. Best GFLOPs: 1.0197
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #26: GFLOPs: 0.6741. Time: 0.5135 ms. Best GFLOPs: 1.0197
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #27: GFLOPs: 0.9169. Time: 0.3775 ms. Best GFLOPs: 1.0197
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #28: GFLOPs: 0.3824. Time: 0.9051 ms. Best GFLOPs: 1.0197
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #29: GFLOPs: 0.3825. Time: 0.9049 ms. Best GFLOPs: 1.0197
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #30: GFLOPs: 0.4864. Time: 0.7116 ms. Best GFLOPs: 1.0197
[19:43:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"] Trial #31: GFLOPs: 0.2199. Time: 1.5741 ms. Best GFLOPs: 1.0197
[19:43:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 608
Total latency (us): 75851.5

[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #0: GFLOPs: 8.6417. Time: 41.0324 ms. Best GFLOPs: 8.6417
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #1: GFLOPs: 107.4819. Time: 3.2991 ms. Best GFLOPs: 107.4819
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #2: GFLOPs: 11.5933. Time: 30.5860 ms. Best GFLOPs: 107.4819
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #3: GFLOPs: 67.2725. Time: 5.2710 ms. Best GFLOPs: 107.4819
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #4: GFLOPs: 58.0373. Time: 6.1097 ms. Best GFLOPs: 107.4819
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #5: GFLOPs: 26.3480. Time: 13.4580 ms. Best GFLOPs: 107.4819
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #6: GFLOPs: 19.3698. Time: 18.3064 ms. Best GFLOPs: 107.4819
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #7: GFLOPs: 85.2158. Time: 4.1611 ms. Best GFLOPs: 107.4819
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #8: GFLOPs: 47.7654. Time: 7.4236 ms. Best GFLOPs: 107.4819
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #9: GFLOPs: 17.9594. Time: 19.7440 ms. Best GFLOPs: 107.4819
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #10: GFLOPs: 88.0621. Time: 4.0266 ms. Best GFLOPs: 107.4819
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #11: GFLOPs: 70.2231. Time: 5.0495 ms. Best GFLOPs: 107.4819
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #12: GFLOPs: 10.3150. Time: 34.3763 ms. Best GFLOPs: 107.4819
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #13: GFLOPs: 30.6567. Time: 11.5665 ms. Best GFLOPs: 107.4819
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #14: GFLOPs: 155.3409. Time: 2.2827 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #15: GFLOPs: 7.2051. Time: 49.2137 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #16: GFLOPs: 113.3112. Time: 3.1294 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #17: GFLOPs: 32.1427. Time: 11.0318 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #18: GFLOPs: 36.8609. Time: 9.6197 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #19: GFLOPs: 37.4371. Time: 9.4717 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #20: GFLOPs: 88.5608. Time: 4.0039 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #21: GFLOPs: 35.4399. Time: 10.0054 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #22: GFLOPs: 24.8296. Time: 14.2810 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #23: GFLOPs: 10.4949. Time: 33.7872 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #24: GFLOPs: 37.2955. Time: 9.5076 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #25: GFLOPs: 29.5425. Time: 12.0028 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #26: GFLOPs: 14.3771. Time: 24.6637 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #27: GFLOPs: 30.6594. Time: 11.5655 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #28: GFLOPs: 7.5999. Time: 46.6577 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #29: GFLOPs: 17.7308. Time: 19.9986 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #30: GFLOPs: 26.5269. Time: 13.3672 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"] Trial #31: GFLOPs: 38.7509. Time: 9.1506 ms. Best GFLOPs: 155.3409
[19:43:16] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 640
Total latency (us): 78134.1

[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #0: GFLOPs: 0.3709. Time: 0.9332 ms. Best GFLOPs: 0.3709
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #1: GFLOPs: 0.3128. Time: 1.1065 ms. Best GFLOPs: 0.3709
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #2: GFLOPs: 0.3301. Time: 1.0486 ms. Best GFLOPs: 0.3709
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #3: GFLOPs: 0.2857. Time: 1.2114 ms. Best GFLOPs: 0.3709
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #4: GFLOPs: 0.2915. Time: 1.1875 ms. Best GFLOPs: 0.3709
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #5: GFLOPs: 0.3755. Time: 0.9217 ms. Best GFLOPs: 0.3755
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #6: GFLOPs: 0.3725. Time: 0.9291 ms. Best GFLOPs: 0.3755
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #7: GFLOPs: 0.5652. Time: 0.6124 ms. Best GFLOPs: 0.5652
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #8: GFLOPs: 0.4464. Time: 0.7753 ms. Best GFLOPs: 0.5652
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #9: GFLOPs: 0.4977. Time: 0.6954 ms. Best GFLOPs: 0.5652
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #10: GFLOPs: 0.5204. Time: 0.6651 ms. Best GFLOPs: 0.5652
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #11: GFLOPs: 0.5612. Time: 0.6167 ms. Best GFLOPs: 0.5652
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #12: GFLOPs: 0.5440. Time: 0.6363 ms. Best GFLOPs: 0.5652
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #13: GFLOPs: 0.5437. Time: 0.6365 ms. Best GFLOPs: 0.5652
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #14: GFLOPs: 0.5800. Time: 0.5967 ms. Best GFLOPs: 0.5800
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #15: GFLOPs: 0.5321. Time: 0.6505 ms. Best GFLOPs: 0.5800
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #16: GFLOPs: 0.5482. Time: 0.6314 ms. Best GFLOPs: 0.5800
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #17: GFLOPs: 0.6751. Time: 0.5127 ms. Best GFLOPs: 0.6751
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #18: GFLOPs: 1.0824. Time: 0.3198 ms. Best GFLOPs: 1.0824
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #19: GFLOPs: 1.0737. Time: 0.3224 ms. Best GFLOPs: 1.0824
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #20: GFLOPs: 0.9673. Time: 0.3578 ms. Best GFLOPs: 1.0824
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #21: GFLOPs: 0.9806. Time: 0.3530 ms. Best GFLOPs: 1.0824
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #22: GFLOPs: 1.1523. Time: 0.3004 ms. Best GFLOPs: 1.1523
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #23: GFLOPs: 1.1100. Time: 0.3118 ms. Best GFLOPs: 1.1523
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #24: GFLOPs: 0.9757. Time: 0.3547 ms. Best GFLOPs: 1.1523
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #25: GFLOPs: 0.9731. Time: 0.3557 ms. Best GFLOPs: 1.1523
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #26: GFLOPs: 0.3615. Time: 0.9575 ms. Best GFLOPs: 1.1523
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #27: GFLOPs: 0.3982. Time: 0.8691 ms. Best GFLOPs: 1.1523
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #28: GFLOPs: 0.0675. Time: 5.1248 ms. Best GFLOPs: 1.1523
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #29: GFLOPs: 0.0926. Time: 3.7387 ms. Best GFLOPs: 1.1523
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #30: GFLOPs: 1.0338. Time: 0.3348 ms. Best GFLOPs: 1.1523
[19:43:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"] Trial #31: GFLOPs: 0.3332. Time: 1.0386 ms. Best GFLOPs: 1.1523
[19:43:17] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 672
Total latency (us): 78734.9

[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #0: GFLOPs: 2.5388. Time: 5.7598 ms. Best GFLOPs: 2.5388
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #1: GFLOPs: 3.5880. Time: 4.0756 ms. Best GFLOPs: 3.5880
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #2: GFLOPs: 4.7504. Time: 3.0783 ms. Best GFLOPs: 4.7504
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #3: GFLOPs: 18.1162. Time: 0.8072 ms. Best GFLOPs: 18.1162
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #4: GFLOPs: 19.4338. Time: 0.7525 ms. Best GFLOPs: 19.4338
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #5: GFLOPs: 23.4922. Time: 0.6225 ms. Best GFLOPs: 23.4922
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #6: GFLOPs: 10.0831. Time: 1.4503 ms. Best GFLOPs: 23.4922
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #7: GFLOPs: 4.9513. Time: 2.9534 ms. Best GFLOPs: 23.4922
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #8: GFLOPs: 1.2637. Time: 11.5714 ms. Best GFLOPs: 23.4922
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #9: GFLOPs: 10.0465. Time: 1.4556 ms. Best GFLOPs: 23.4922
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #10: GFLOPs: 11.6106. Time: 1.2595 ms. Best GFLOPs: 23.4922
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #11: GFLOPs: 25.5129. Time: 0.5732 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #12: GFLOPs: 1.6357. Time: 8.9403 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #13: GFLOPs: 5.2686. Time: 2.7756 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #14: GFLOPs: 1.5017. Time: 9.7375 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #15: GFLOPs: 1.6619. Time: 8.7990 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #16: GFLOPs: 5.2779. Time: 2.7707 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #17: GFLOPs: 0.8825. Time: 16.5705 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #18: GFLOPs: 1.0230. Time: 14.2945 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #19: GFLOPs: 2.2498. Time: 6.4998 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #20: GFLOPs: 1.1427. Time: 12.7967 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #21: GFLOPs: 1.1182. Time: 13.0778 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #22: GFLOPs: 1.9185. Time: 7.6223 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #23: GFLOPs: 1.2567. Time: 11.6359 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #24: GFLOPs: 2.7595. Time: 5.2993 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #25: GFLOPs: 9.3694. Time: 1.5607 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #26: GFLOPs: 11.1514. Time: 1.3113 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #27: GFLOPs: 13.0740. Time: 1.1185 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #28: GFLOPs: 3.6618. Time: 3.9935 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #29: GFLOPs: 9.9998. Time: 1.4624 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #30: GFLOPs: 10.9922. Time: 1.3303 ms. Best GFLOPs: 25.5129
[19:43:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_max_pool2d_2"] Trial #31: GFLOPs: 27.8106. Time: 0.5258 ms. Best GFLOPs: 27.8106
[19:43:19] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_nn_max_pool2d_2"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 704
Total latency (us): 79260.7

[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #0: GFLOPs: 0.0000. Time: 0.0106 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #1: GFLOPs: 0.0000. Time: 0.0317 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #2: GFLOPs: 0.0000. Time: 0.0112 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #3: GFLOPs: 0.0000. Time: 0.0103 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #4: GFLOPs: 0.0000. Time: 0.0195 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #5: GFLOPs: 0.0000. Time: 0.0231 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #6: GFLOPs: 0.0000. Time: 0.0240 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #7: GFLOPs: 0.0000. Time: 0.0234 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #8: GFLOPs: 0.0000. Time: 0.0225 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #9: GFLOPs: 0.0000. Time: 0.0232 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #10: GFLOPs: 0.0000. Time: 0.0239 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #11: GFLOPs: 0.0000. Time: 0.0222 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #12: GFLOPs: 0.0000. Time: 0.0236 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #13: GFLOPs: 0.0000. Time: 0.0234 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #14: GFLOPs: 0.0000. Time: 0.0223 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #15: GFLOPs: 0.0000. Time: 0.0238 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #16: GFLOPs: 0.0000. Time: 0.0240 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #17: GFLOPs: 0.0000. Time: 0.0235 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #18: GFLOPs: 0.0000. Time: 0.0250 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #19: GFLOPs: 0.0000. Time: 0.0225 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #20: GFLOPs: 0.0000. Time: 0.0224 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #21: GFLOPs: 0.0000. Time: 0.0227 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #22: GFLOPs: 0.0000. Time: 0.0234 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #23: GFLOPs: 0.0000. Time: 0.0231 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #24: GFLOPs: 0.0000. Time: 0.0297 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #25: GFLOPs: 0.0000. Time: 0.0326 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #26: GFLOPs: 0.0000. Time: 0.0214 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #27: GFLOPs: 0.0000. Time: 0.0236 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #28: GFLOPs: 0.0000. Time: 0.0239 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #29: GFLOPs: 0.0000. Time: 0.0237 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #30: GFLOPs: 0.0000. Time: 0.0226 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_concatenate_2"] Trial #31: GFLOPs: 0.0000. Time: 0.0112 ms. Best GFLOPs: 0.0000
[19:43:19] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_concatenate_2"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 736
Total latency (us): 79271

[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #0: GFLOPs: 43.8252. Time: 8.0911 ms. Best GFLOPs: 43.8252
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #1: GFLOPs: 51.1715. Time: 6.9295 ms. Best GFLOPs: 51.1715
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #2: GFLOPs: 145.3149. Time: 2.4402 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #3: GFLOPs: 31.4399. Time: 11.2784 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #4: GFLOPs: 87.1806. Time: 4.0673 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #5: GFLOPs: 20.4899. Time: 17.3057 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #6: GFLOPs: 43.0739. Time: 8.2322 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #7: GFLOPs: 22.8028. Time: 15.5504 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #8: GFLOPs: 44.9458. Time: 7.8893 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #9: GFLOPs: 30.2172. Time: 11.7348 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #10: GFLOPs: 7.8365. Time: 45.2488 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #11: GFLOPs: 5.2336. Time: 67.7535 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #12: GFLOPs: 11.2534. Time: 31.5098 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #13: GFLOPs: 57.0291. Time: 6.2177 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #14: GFLOPs: 45.9291. Time: 7.7204 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #15: GFLOPs: 10.1323. Time: 34.9963 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #16: GFLOPs: 33.5565. Time: 10.5670 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #17: GFLOPs: 6.5364. Time: 54.2489 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #18: GFLOPs: 10.6064. Time: 33.4318 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #19: GFLOPs: 8.6888. Time: 40.8100 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #20: GFLOPs: 13.0034. Time: 27.2692 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #21: GFLOPs: 52.3997. Time: 6.7671 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #22: GFLOPs: 45.3827. Time: 7.8134 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #23: GFLOPs: 99.3241. Time: 3.5700 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #24: GFLOPs: 65.4628. Time: 5.4167 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #25: GFLOPs: 44.9565. Time: 7.8874 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #26: GFLOPs: 32.5258. Time: 10.9019 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #27: GFLOPs: 14.3556. Time: 24.7007 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #28: GFLOPs: 17.3026. Time: 20.4936 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #29: GFLOPs: 20.1022. Time: 17.6394 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #30: GFLOPs: 17.1394. Time: 20.6886 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"] Trial #31: GFLOPs: 65.3257. Time: 5.4281 ms. Best GFLOPs: 145.3149
[19:43:20] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 768
Total latency (us): 81711.2

[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #0: GFLOPs: 28.2429. Time: 56.4824 ms. Best GFLOPs: 28.2429
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #1: GFLOPs: 39.0598. Time: 40.8408 ms. Best GFLOPs: 39.0598
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #2: GFLOPs: 26.6067. Time: 59.9561 ms. Best GFLOPs: 39.0598
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #3: GFLOPs: 36.8517. Time: 43.2879 ms. Best GFLOPs: 39.0598
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #4: GFLOPs: 9.6141. Time: 165.9267 ms. Best GFLOPs: 39.0598
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #5: GFLOPs: 34.7282. Time: 45.9347 ms. Best GFLOPs: 39.0598
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #6: GFLOPs: 148.2293. Time: 10.7619 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #7: GFLOPs: 32.2159. Time: 49.5169 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(13, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 128, 3):
                for ax3_ax4_fused in T.vectorized(60):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(15, i0_0_i1_0_i2_0_fused % 13 + ax2)
                        i3 = T.axis.spatial(15, ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0 in T.grid(1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 32, 1, 13, 1):
                    for i4_2_init, i1_3_init in T.grid(4, 8):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(256, i1_1 * 8 + i1_3_init)
                            oh, ow, oc_block = T.axis.remap("SSS", [i0_0_i1_0_i2_0_fused, i3_1, i4_2_init])
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [256, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(128, 1, 3, 1, 1, 1, 1, 4, 4, 3, 1, 1, 8, 1, 1, 1):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(256, i1_1 * 8 + i1_3)
                            oh, ow, oc_block = T.axis.remap("SSS", [i0_0_i1_0_i2_0_fused, i3_1, i4_2])
                            ic = T.axis.reduce(512, i5_0 * 4 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_0])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [256, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1 in T.grid(1, 256):
                    for ax2_ax3_ax4_fused in T.vectorized(52):
                        with T.block("T_leaky_relu"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1, ax2 = T.axis.remap("SS", [ax1, i0_0_i1_0_i2_0_fused])
                            ax3 = T.axis.spatial(13, ax2_ax3_ax4_fused // 4)
                            ax4 = T.axis.spatial(4, ax2_ax3_ax4_fused % 4)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_leaky_relu[ax0_1, ax1_1, ax2, ax3, ax4])
                            T_leaky_relu[ax0_1, ax1_1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], (conv2d_NCHWc[ax0_1, ax1_1, ax2, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 32, 1, 8])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[128, 4])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78 = sch.get_loops(block=b68)
l79 = sch.fuse(l71, l72, l73)
sch.parallel(loop=l79)
l80 = sch.fuse(l77, l78)
sch.vectorize(loop=l80)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b70)
l113 = sch.fuse(l110, l111, l112)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l105, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l105, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b114)
b139 = sch.decompose_reduction(block=b114, loop=l123)
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #9: GFLOPs: 23.6710. Time: 67.3916 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #10: GFLOPs: 26.8357. Time: 59.4444 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #11: GFLOPs: 29.7706. Time: 53.5841 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #12: GFLOPs: 16.5375. Time: 96.4611 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #13: GFLOPs: 40.6327. Time: 39.2598 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #14: GFLOPs: 45.7299. Time: 34.8838 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #15: GFLOPs: 23.8551. Time: 66.8716 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #16: GFLOPs: 55.7040. Time: 28.6376 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #17: GFLOPs: 2.6834. Time: 594.4717 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #18: GFLOPs: 63.2035. Time: 25.2396 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #19: GFLOPs: 27.7378. Time: 57.5110 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #20: GFLOPs: 11.1190. Time: 143.4686 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #21: GFLOPs: 17.1018. Time: 93.2785 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #22: GFLOPs: 49.1179. Time: 32.4776 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #23: GFLOPs: 34.8740. Time: 45.7427 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 13, 13, 4), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 256, 13, 13, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 128, 15, 15, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 256, 13, 13, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(26, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2 in T.grid(1, 128, 15):
                for ax3_ax4_fused in T.vectorized(12):
                    with T.block("data_pad"):
                        i0, i1, i2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        i3 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 26 // 2 + ax3_ax4_fused // 4)
                        i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 14 and 1 <= i3 and i3 < 14, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i1_2_init, i4_2_init, i1_3_init, i2_3_init in T.grid(8, 2, 32, 13):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i1_2_init * 32 + i1_3_init)
                    oh = T.axis.spatial(13, i2_3_init)
                    ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [256, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 3, 3, 1, 8, 1, 1, 2, 2, 1, 1, 1, 32, 13, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(256, i1_2 * 32 + i1_3)
                    oh = T.axis.spatial(13, i2_3)
                    ow = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                    ic = T.axis.reduce(512, i5_0 * 2 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 13, 13, 4], "float32"], ["TENSOR", [256, 128, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(13):
                for i3_i4_fused in T.vectorized(52):
                    with T.block("T_leaky_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2 = T.axis.remap("SS", [i0_i1_fused, i2])
                        ax3 = T.axis.spatial(13, i3_i4_fused // 4)
                        ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_leaky_relu[ax0, ax1, ax2, ax3, ax4])
                        T_leaky_relu[ax0, ax1, ax2, ax3, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4], (conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 8, 32])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 13])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[13, 1, 1, 1])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[256, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b67)
l85 = sch.fuse(l70, l71, l72, l73, l74, l75, l76, l77, l78, l79)
sch.parallel(loop=l85)
l86 = sch.fuse(l83, l84)
sch.vectorize(loop=l86)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b68)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108 = sch.get_loops(block=b69)
l109 = sch.fuse(l104, l105)
sch.parallel(loop=l109)
l110 = sch.fuse(l107, l108)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
b129 = sch.decompose_reduction(block=b111, loop=l113)
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #25: GFLOPs: 23.9304. Time: 66.6613 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #26: GFLOPs: 54.6797. Time: 29.1741 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #27: GFLOPs: 10.3276. Time: 154.4632 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #28: GFLOPs: 33.2170. Time: 48.0245 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #29: GFLOPs: 10.5442. Time: 151.2892 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #30: GFLOPs: 33.9838. Time: 46.9409 ms. Best GFLOPs: 148.2293
[19:43:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"] Trial #31: GFLOPs: 34.6753. Time: 46.0048 ms. Best GFLOPs: 148.2293
[19:43:22] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |            N/A |          N/A |                   N/A |      0 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 800
Total latency (us): 135521

[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #0: GFLOPs: 10.2803. Time: 17.2546 ms. Best GFLOPs: 10.2803
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #1: GFLOPs: 22.6637. Time: 7.8267 ms. Best GFLOPs: 22.6637
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #2: GFLOPs: 35.9319. Time: 4.9366 ms. Best GFLOPs: 35.9319
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #3: GFLOPs: 23.2015. Time: 7.6453 ms. Best GFLOPs: 35.9319
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #4: GFLOPs: 16.8555. Time: 10.5237 ms. Best GFLOPs: 35.9319
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #5: GFLOPs: 42.9790. Time: 4.1272 ms. Best GFLOPs: 42.9790
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #6: GFLOPs: 24.1823. Time: 7.3352 ms. Best GFLOPs: 42.9790
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #7: GFLOPs: 32.7568. Time: 5.4151 ms. Best GFLOPs: 42.9790
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #8: GFLOPs: 46.0472. Time: 3.8522 ms. Best GFLOPs: 46.0472
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #9: GFLOPs: 17.1586. Time: 10.3378 ms. Best GFLOPs: 46.0472
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #10: GFLOPs: 45.7542. Time: 3.8769 ms. Best GFLOPs: 46.0472
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #11: GFLOPs: 22.2697. Time: 7.9652 ms. Best GFLOPs: 46.0472
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #12: GFLOPs: 33.8292. Time: 5.2435 ms. Best GFLOPs: 46.0472
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #13: GFLOPs: 16.8387. Time: 10.5342 ms. Best GFLOPs: 46.0472
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #14: GFLOPs: 41.2592. Time: 4.2992 ms. Best GFLOPs: 46.0472
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #15: GFLOPs: 30.8442. Time: 5.7509 ms. Best GFLOPs: 46.0472
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #16: GFLOPs: 28.6495. Time: 6.1915 ms. Best GFLOPs: 46.0472
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #17: GFLOPs: 43.4455. Time: 4.0829 ms. Best GFLOPs: 46.0472
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #18: GFLOPs: 37.9654. Time: 4.6722 ms. Best GFLOPs: 46.0472
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #19: GFLOPs: 36.5979. Time: 4.8468 ms. Best GFLOPs: 46.0472
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #20: GFLOPs: 50.2308. Time: 3.5313 ms. Best GFLOPs: 50.2308
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #21: GFLOPs: 18.8813. Time: 9.3946 ms. Best GFLOPs: 50.2308
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #22: GFLOPs: 32.9234. Time: 5.3877 ms. Best GFLOPs: 50.2308
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #23: GFLOPs: 17.9713. Time: 9.8703 ms. Best GFLOPs: 50.2308
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #24: GFLOPs: 20.0162. Time: 8.8619 ms. Best GFLOPs: 50.2308
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #25: GFLOPs: 48.3354. Time: 3.6698 ms. Best GFLOPs: 50.2308
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #26: GFLOPs: 22.6025. Time: 7.8479 ms. Best GFLOPs: 50.2308
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #27: GFLOPs: 19.4296. Time: 9.1295 ms. Best GFLOPs: 50.2308
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #28: GFLOPs: 7.6928. Time: 23.0582 ms. Best GFLOPs: 50.2308
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #29: GFLOPs: 8.3624. Time: 21.2118 ms. Best GFLOPs: 50.2308
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #30: GFLOPs: 30.9703. Time: 5.7275 ms. Best GFLOPs: 50.2308
[19:43:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"] Trial #31: GFLOPs: 5.0501. Time: 35.1244 ms. Best GFLOPs: 50.2308
[19:43:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 832
Total latency (us): 156709

[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #0: GFLOPs: 11.0063. Time: 4.0331 ms. Best GFLOPs: 11.0063
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #1: GFLOPs: 33.1973. Time: 1.3371 ms. Best GFLOPs: 33.1973
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #2: GFLOPs: 31.7595. Time: 1.3977 ms. Best GFLOPs: 33.1973
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #3: GFLOPs: 6.9764. Time: 6.3627 ms. Best GFLOPs: 33.1973
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #4: GFLOPs: 29.9480. Time: 1.4822 ms. Best GFLOPs: 33.1973
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #5: GFLOPs: 29.8849. Time: 1.4853 ms. Best GFLOPs: 33.1973
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #6: GFLOPs: 53.8910. Time: 0.8237 ms. Best GFLOPs: 53.8910
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #7: GFLOPs: 77.0173. Time: 0.5763 ms. Best GFLOPs: 77.0173
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #8: GFLOPs: 14.1858. Time: 3.1291 ms. Best GFLOPs: 77.0173
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #9: GFLOPs: 56.4810. Time: 0.7859 ms. Best GFLOPs: 77.0173
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #10: GFLOPs: 23.0406. Time: 1.9265 ms. Best GFLOPs: 77.0173
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #11: GFLOPs: 52.6156. Time: 0.8436 ms. Best GFLOPs: 77.0173
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #12: GFLOPs: 26.0343. Time: 1.7050 ms. Best GFLOPs: 77.0173
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #13: GFLOPs: 20.8280. Time: 2.1312 ms. Best GFLOPs: 77.0173
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #14: GFLOPs: 18.3702. Time: 2.4163 ms. Best GFLOPs: 77.0173
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #15: GFLOPs: 43.0955. Time: 1.0300 ms. Best GFLOPs: 77.0173
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #16: GFLOPs: 87.4216. Time: 0.5078 ms. Best GFLOPs: 87.4216
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #17: GFLOPs: 57.8328. Time: 0.7675 ms. Best GFLOPs: 87.4216
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #18: GFLOPs: 73.0243. Time: 0.6079 ms. Best GFLOPs: 87.4216
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #19: GFLOPs: 97.4293. Time: 0.4556 ms. Best GFLOPs: 97.4293
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #20: GFLOPs: 27.9464. Time: 1.5884 ms. Best GFLOPs: 97.4293
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #21: GFLOPs: 45.0482. Time: 0.9854 ms. Best GFLOPs: 97.4293
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #22: GFLOPs: 16.5559. Time: 2.6811 ms. Best GFLOPs: 97.4293
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #23: GFLOPs: 60.6374. Time: 0.7320 ms. Best GFLOPs: 97.4293
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #24: GFLOPs: 8.4275. Time: 5.2671 ms. Best GFLOPs: 97.4293
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #25: GFLOPs: 12.7127. Time: 3.4917 ms. Best GFLOPs: 97.4293
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #26: GFLOPs: 104.3850. Time: 0.4252 ms. Best GFLOPs: 104.3850
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #27: GFLOPs: 63.1182. Time: 0.7033 ms. Best GFLOPs: 104.3850
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #28: GFLOPs: 4.7484. Time: 9.3481 ms. Best GFLOPs: 104.3850
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #29: GFLOPs: 31.9202. Time: 1.3906 ms. Best GFLOPs: 104.3850
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #30: GFLOPs: 30.7502. Time: 1.4435 ms. Best GFLOPs: 104.3850
[19:43:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"] Trial #31: GFLOPs: 14.1714. Time: 3.1323 ms. Best GFLOPs: 104.3850
[19:43:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 864
Total latency (us): 157134

[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #0: GFLOPs: 19.1969. Time: 83.0982 ms. Best GFLOPs: 19.1969
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #1: GFLOPs: 14.5163. Time: 109.8921 ms. Best GFLOPs: 19.1969
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #2: GFLOPs: 20.1746. Time: 79.0713 ms. Best GFLOPs: 20.1746
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #3: GFLOPs: 12.5236. Time: 127.3780 ms. Best GFLOPs: 20.1746
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #4: GFLOPs: 18.6341. Time: 85.6080 ms. Best GFLOPs: 20.1746
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #5: GFLOPs: 137.8990. Time: 11.5681 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #6: GFLOPs: 17.2252. Time: 92.6104 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #7: GFLOPs: 78.0611. Time: 20.4357 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #8: GFLOPs: 24.3280. Time: 65.5717 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #9: GFLOPs: 50.2790. Time: 31.7276 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #10: GFLOPs: 25.1668. Time: 63.3863 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #11: GFLOPs: 20.1732. Time: 79.0766 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #12: GFLOPs: 21.6076. Time: 73.8272 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #13: GFLOPs: 16.7995. Time: 94.9570 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #14: GFLOPs: 47.0000. Time: 33.9411 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #15: GFLOPs: 41.3844. Time: 38.5466 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #16: GFLOPs: 13.0352. Time: 122.3789 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #17: GFLOPs: 41.6009. Time: 38.3461 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #18: GFLOPs: 45.9096. Time: 34.7472 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #19: GFLOPs: 40.4121. Time: 39.4740 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #20: GFLOPs: 68.7758. Time: 23.1946 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #21: GFLOPs: 80.6396. Time: 19.7822 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #22: GFLOPs: 35.8638. Time: 44.4803 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #23: GFLOPs: 29.9012. Time: 53.3501 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #24: GFLOPs: 4.5877. Time: 347.7200 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #25: GFLOPs: 32.6326. Time: 48.8846 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #26: GFLOPs: 10.8875. Time: 146.5195 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #27: GFLOPs: 41.3314. Time: 38.5960 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 53, 53, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(3392, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(53):
                for i4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(64, i0_i1_i2_fused // 53)
                        i2 = T.axis.spatial(53, i0_i1_i2_fused % 53)
                        i3_1, i4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3_1 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3_1, i4])
                        data_pad[i0, i1, i2, i3_1, i4] = T.if_then_else(1 <= i2 and i2 < 53 and 1 <= i3_1 and i3_1 < 53, placeholder[i0, i1, i2 - 1, i3_1 - 1, i4], T.float32(0), dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i3_2_init, i2_3_init, i3_3_init in T.grid(16, 2, 13, 13):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 64 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 16 + i1_2_init)
                        oh = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 13 + i2_3_init)
                        ow = T.axis.spatial(26, i3_2_init * 13 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(16, 3, 1, 1, 16, 1, 2, 1, 16, 1, 3, 1, 1, 13, 13):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 64 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 16 + i1_2)
                        oh = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 13 + i2_3)
                        ow = T.axis.spatial(26, i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(256, i5_0 * 16 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(3328, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(26):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(128, i0_i1_i2_fused // 26)
                        ax2 = T.axis.spatial(26, i0_i1_i2_fused % 26)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 16, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 16])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67, b68 = sch.get_child_blocks(b65)
l69, l70, l71, l72, l73 = sch.get_loops(block=b66)
l74 = sch.fuse(l69, l70, l71)
sch.parallel(loop=l74)
l75 = sch.fuse(l73)
sch.vectorize(loop=l75)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l76, l77, l78, l79, l80, l81, l82, l83, l84, l85)
sch.parallel(loop=l102)
l103 = sch.fuse(l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108 = sch.get_loops(block=b68)
l109 = sch.fuse(l104, l105, l106)
sch.parallel(loop=l109)
l110 = sch.fuse(l108)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l109, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l109, ann_key="pragma_unroll_explicit", ann_val=1)
b111 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
b129 = sch.decompose_reduction(block=b111, loop=l113)
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #29: GFLOPs: 23.1255. Time: 68.9815 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #30: GFLOPs: 33.4071. Time: 47.7513 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"] Trial #31: GFLOPs: 43.0389. Time: 37.0648 ms. Best GFLOPs: 137.8990
[19:43:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |            N/A |          N/A |                   N/A |      0 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 896
Total latency (us): 168702

[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #0: GFLOPs: 29.7934. Time: 5.9538 ms. Best GFLOPs: 29.7934
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #1: GFLOPs: 105.4720. Time: 1.6818 ms. Best GFLOPs: 105.4720
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #2: GFLOPs: 29.6873. Time: 5.9750 ms. Best GFLOPs: 105.4720
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #3: GFLOPs: 64.0717. Time: 2.7685 ms. Best GFLOPs: 105.4720
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #4: GFLOPs: 46.9151. Time: 3.7809 ms. Best GFLOPs: 105.4720
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #5: GFLOPs: 42.8090. Time: 4.1436 ms. Best GFLOPs: 105.4720
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #6: GFLOPs: 52.7030. Time: 3.3657 ms. Best GFLOPs: 105.4720
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #7: GFLOPs: 117.1641. Time: 1.5140 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #8: GFLOPs: 79.0191. Time: 2.2448 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #9: GFLOPs: 33.3698. Time: 5.3157 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #10: GFLOPs: 54.0808. Time: 3.2800 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #11: GFLOPs: 97.1105. Time: 1.8266 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #12: GFLOPs: 14.0280. Time: 12.6449 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #13: GFLOPs: 28.0941. Time: 6.3139 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #14: GFLOPs: 7.6069. Time: 23.3186 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #15: GFLOPs: 26.8170. Time: 6.6145 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #16: GFLOPs: 42.4467. Time: 4.1789 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #17: GFLOPs: 26.7805. Time: 6.6236 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #18: GFLOPs: 13.9208. Time: 12.7422 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #19: GFLOPs: 3.9783. Time: 44.5878 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #20: GFLOPs: 34.2831. Time: 5.1740 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #21: GFLOPs: 34.3030. Time: 5.1710 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #22: GFLOPs: 39.2306. Time: 4.5215 ms. Best GFLOPs: 117.1641
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #23: GFLOPs: 159.5811. Time: 1.1115 ms. Best GFLOPs: 159.5811
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #24: GFLOPs: 81.9039. Time: 2.1657 ms. Best GFLOPs: 159.5811
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #25: GFLOPs: 5.2526. Time: 33.7705 ms. Best GFLOPs: 159.5811
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #26: GFLOPs: 11.2068. Time: 15.8281 ms. Best GFLOPs: 159.5811
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #27: GFLOPs: 43.0634. Time: 4.1191 ms. Best GFLOPs: 159.5811
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #28: GFLOPs: 170.6403. Time: 1.0395 ms. Best GFLOPs: 170.6403
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i3_2_init, i2_3_init, i3_3_init in T.grid(32, 2, 26, 13):
                for i4_3_fused_init in T.vectorized(4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 32 + i1_2_init)
                        oh = T.axis.spatial(26, i2_3_init)
                        ow = T.axis.spatial(26, i3_2_init * 13 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(32, 1, 1, 1, 32, 1, 2, 1, 16, 1, 1, 1, 1, 26, 13):
                for i4_3_fused in T.vectorized(4):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused * 32 + i1_2)
                        oh = T.axis.spatial(26, i2_3)
                        ow = T.axis.spatial(26, i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_3_fused)
                        ic = T.axis.reduce(512, i5_0 * 16 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(26):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(64, i0_i1_i2_fused // 26)
                        ax2 = T.axis.spatial(26, i0_i1_i2_fused % 26)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 2, 32, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 26])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
l93 = sch.fuse(l91)
sch.vectorize(loop=l93)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b65)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #30: GFLOPs: 14.5090. Time: 12.2257 ms. Best GFLOPs: 170.6403
[19:43:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"] Trial #31: GFLOPs: 20.1846. Time: 8.7880 ms. Best GFLOPs: 170.6403
[19:43:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |            N/A |          N/A |                   N/A |      0 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 928
Total latency (us): 170781

[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #0: GFLOPs: 1.9410. Time: 0.1783 ms. Best GFLOPs: 1.9410
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #1: GFLOPs: 1.7366. Time: 0.1993 ms. Best GFLOPs: 1.9410
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #2: GFLOPs: 2.1121. Time: 0.1639 ms. Best GFLOPs: 2.1121
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #3: GFLOPs: 1.9928. Time: 0.1737 ms. Best GFLOPs: 2.1121
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #4: GFLOPs: 2.0219. Time: 0.1712 ms. Best GFLOPs: 2.1121
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #5: GFLOPs: 2.0194. Time: 0.1714 ms. Best GFLOPs: 2.1121
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #6: GFLOPs: 1.6596. Time: 0.2086 ms. Best GFLOPs: 2.1121
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #7: GFLOPs: 2.0223. Time: 0.1712 ms. Best GFLOPs: 2.1121
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #8: GFLOPs: 2.0282. Time: 0.1706 ms. Best GFLOPs: 2.1121
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #9: GFLOPs: 1.9951. Time: 0.1735 ms. Best GFLOPs: 2.1121
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #10: GFLOPs: 2.1280. Time: 0.1626 ms. Best GFLOPs: 2.1280
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #11: GFLOPs: 1.6685. Time: 0.2074 ms. Best GFLOPs: 2.1280
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #12: GFLOPs: 1.9932. Time: 0.1736 ms. Best GFLOPs: 2.1280
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #13: GFLOPs: 1.9655. Time: 0.1761 ms. Best GFLOPs: 2.1280
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #14: GFLOPs: 1.6372. Time: 0.2114 ms. Best GFLOPs: 2.1280
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #15: GFLOPs: 2.0248. Time: 0.1709 ms. Best GFLOPs: 2.1280
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #16: GFLOPs: 2.0231. Time: 0.1711 ms. Best GFLOPs: 2.1280
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #17: GFLOPs: 2.1361. Time: 0.1620 ms. Best GFLOPs: 2.1361
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #18: GFLOPs: 2.2233. Time: 0.1557 ms. Best GFLOPs: 2.2233
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #19: GFLOPs: 1.8543. Time: 0.1867 ms. Best GFLOPs: 2.2233
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #20: GFLOPs: 1.5569. Time: 0.2223 ms. Best GFLOPs: 2.2233
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #21: GFLOPs: 2.1563. Time: 0.1605 ms. Best GFLOPs: 2.2233
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #22: GFLOPs: 2.0338. Time: 0.1702 ms. Best GFLOPs: 2.2233
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #23: GFLOPs: 2.1044. Time: 0.1645 ms. Best GFLOPs: 2.2233
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #24: GFLOPs: 1.7206. Time: 0.2012 ms. Best GFLOPs: 2.2233
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #25: GFLOPs: 1.7909. Time: 0.1933 ms. Best GFLOPs: 2.2233
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #26: GFLOPs: 1.8523. Time: 0.1869 ms. Best GFLOPs: 2.2233
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #27: GFLOPs: 2.0366. Time: 0.1699 ms. Best GFLOPs: 2.2233
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #28: GFLOPs: 2.1045. Time: 0.1645 ms. Best GFLOPs: 2.2233
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #29: GFLOPs: 1.7829. Time: 0.1941 ms. Best GFLOPs: 2.2233
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #30: GFLOPs: 2.1998. Time: 0.1573 ms. Best GFLOPs: 2.2233
[19:43:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"] Trial #31: GFLOPs: 1.9014. Time: 0.1820 ms. Best GFLOPs: 2.2233
[19:43:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |            N/A |          N/A |                   N/A |      0 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 960
Total latency (us): 172182

[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #0: GFLOPs: 60.4936. Time: 13.1851 ms. Best GFLOPs: 60.4936
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #1: GFLOPs: 45.6776. Time: 17.4618 ms. Best GFLOPs: 60.4936
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #2: GFLOPs: 92.6211. Time: 8.6116 ms. Best GFLOPs: 92.6211
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #3: GFLOPs: 69.1865. Time: 11.5285 ms. Best GFLOPs: 92.6211
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #4: GFLOPs: 17.7256. Time: 44.9980 ms. Best GFLOPs: 92.6211
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #5: GFLOPs: 29.0663. Time: 27.4412 ms. Best GFLOPs: 92.6211
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #6: GFLOPs: 68.4801. Time: 11.6474 ms. Best GFLOPs: 92.6211
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(104, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 16, 13, 2):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 26 // 13 * 32 + i1_2_init * 16 + i1_3_init)
                    oh = T.axis.spatial(26, i2_2_init * 13 + i2_3_init)
                    ow = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 13 * 2 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 26)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 3, 3, 1, 2, 2, 1, 1, 64, 1, 1, 1, 16, 13, 2, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 26 // 13 * 32 + i1_2 * 16 + i1_3)
                    oh = T.axis.spatial(26, i2_2 * 13 + i2_3)
                    ow = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 13 * 2 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 26)
                    ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                    kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [64, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh + kh and oh + kh < 27 and 1 <= ow + kw and ow + kw < 27, placeholder[n, ic // 4, oh + kh - 1, ow + kw - 1, ic % 4], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 32, 26, 2, 1):
                with T.block("T_add"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 26 // 13 * 32 + ax1)
                    ax2_1 = T.axis.spatial(26, ax2)
                    ax3_1 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 13 * 2 + ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 26)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 2, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 13, 1, 2])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68 = sch.get_child_blocks(b66)
l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94 = sch.get_loops(block=b67)
l95 = sch.fuse(l69, l70, l71, l72, l73, l74, l75, l76, l77, l78)
sch.parallel(loop=l95)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b68)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #8: GFLOPs: 36.2084. Time: 22.0284 ms. Best GFLOPs: 92.6211
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #9: GFLOPs: 32.0963. Time: 24.8507 ms. Best GFLOPs: 92.6211
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #10: GFLOPs: 67.2329. Time: 11.8635 ms. Best GFLOPs: 92.6211
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #11: GFLOPs: 4.4282. Time: 180.1203 ms. Best GFLOPs: 92.6211
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #12: GFLOPs: 191.2175. Time: 4.1712 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #13: GFLOPs: 157.6952. Time: 5.0580 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #14: GFLOPs: 130.5096. Time: 6.1115 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #15: GFLOPs: 147.3104. Time: 5.4145 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #16: GFLOPs: 122.6322. Time: 6.5041 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #17: GFLOPs: 23.7413. Time: 33.5961 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #18: GFLOPs: 139.0721. Time: 5.7353 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #19: GFLOPs: 55.3175. Time: 14.4189 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #20: GFLOPs: 176.5260. Time: 4.5184 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #21: GFLOPs: 32.1074. Time: 24.8421 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #22: GFLOPs: 112.1790. Time: 7.1102 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #23: GFLOPs: 94.0996. Time: 8.4763 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #24: GFLOPs: 109.7968. Time: 7.2645 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #25: GFLOPs: 32.0310. Time: 24.9013 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #26: GFLOPs: 35.5007. Time: 22.4676 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #27: GFLOPs: 22.0212. Time: 36.2203 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #28: GFLOPs: 51.0958. Time: 15.6102 ms. Best GFLOPs: 191.2175
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #29: GFLOPs: 250.8098. Time: 3.1802 ms. Best GFLOPs: 250.8098
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #30: GFLOPs: 72.3896. Time: 11.0184 ms. Best GFLOPs: 250.8098
[19:43:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"] Trial #31: GFLOPs: 77.1010. Time: 10.3451 ms. Best GFLOPs: 250.8098
[19:43:29] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |            N/A |          N/A |                   N/A |      0 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 992
Total latency (us): 197623

[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #0: GFLOPs: 3.3161. Time: 0.1566 ms. Best GFLOPs: 3.3161
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #1: GFLOPs: 2.1414. Time: 0.2424 ms. Best GFLOPs: 3.3161
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #2: GFLOPs: 3.3072. Time: 0.1570 ms. Best GFLOPs: 3.3161
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #3: GFLOPs: 2.9891. Time: 0.1737 ms. Best GFLOPs: 3.3161
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #4: GFLOPs: 2.9605. Time: 0.1754 ms. Best GFLOPs: 3.3161
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #5: GFLOPs: 3.0028. Time: 0.1729 ms. Best GFLOPs: 3.3161
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #6: GFLOPs: 3.1914. Time: 0.1627 ms. Best GFLOPs: 3.3161
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #7: GFLOPs: 3.3420. Time: 0.1553 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #8: GFLOPs: 2.8031. Time: 0.1852 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #9: GFLOPs: 2.8107. Time: 0.1847 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #10: GFLOPs: 3.1150. Time: 0.1667 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #11: GFLOPs: 2.9861. Time: 0.1739 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #12: GFLOPs: 2.4937. Time: 0.2082 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #13: GFLOPs: 3.3201. Time: 0.1564 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #14: GFLOPs: 2.2557. Time: 0.2302 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #15: GFLOPs: 3.0897. Time: 0.1680 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #16: GFLOPs: 2.9388. Time: 0.1767 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #17: GFLOPs: 2.5322. Time: 0.2050 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #18: GFLOPs: 2.6308. Time: 0.1973 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #19: GFLOPs: 2.9395. Time: 0.1766 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #20: GFLOPs: 2.9062. Time: 0.1786 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #21: GFLOPs: 2.9870. Time: 0.1738 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #22: GFLOPs: 2.6252. Time: 0.1978 ms. Best GFLOPs: 3.3420
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #23: GFLOPs: 3.4074. Time: 0.1524 ms. Best GFLOPs: 3.4074
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #24: GFLOPs: 2.9520. Time: 0.1759 ms. Best GFLOPs: 3.4074
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #25: GFLOPs: 2.9298. Time: 0.1772 ms. Best GFLOPs: 3.4074
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #26: GFLOPs: 2.9281. Time: 0.1773 ms. Best GFLOPs: 3.4074
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #27: GFLOPs: 3.2246. Time: 0.1610 ms. Best GFLOPs: 3.4074
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #28: GFLOPs: 2.6290. Time: 0.1975 ms. Best GFLOPs: 3.4074
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #29: GFLOPs: 2.9936. Time: 0.1734 ms. Best GFLOPs: 3.4074
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #30: GFLOPs: 3.2568. Time: 0.1594 ms. Best GFLOPs: 3.4074
[19:43:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"] Trial #31: GFLOPs: 3.1517. Time: 0.1647 ms. Best GFLOPs: 3.4074
[19:43:30] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |            N/A |          N/A |                   N/A |      0 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1024
Total latency (us): 198842

[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #0: GFLOPs: 178.7553. Time: 0.4966 ms. Best GFLOPs: 178.7553
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #1: GFLOPs: 215.5950. Time: 0.4118 ms. Best GFLOPs: 215.5950
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #2: GFLOPs: 181.4478. Time: 0.4893 ms. Best GFLOPs: 215.5950
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #3: GFLOPs: 86.5922. Time: 1.0252 ms. Best GFLOPs: 215.5950
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #4: GFLOPs: 55.9176. Time: 1.5877 ms. Best GFLOPs: 215.5950
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #5: GFLOPs: 82.6710. Time: 1.0739 ms. Best GFLOPs: 215.5950
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #6: GFLOPs: 132.2472. Time: 0.6713 ms. Best GFLOPs: 215.5950
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #7: GFLOPs: 362.9019. Time: 0.2446 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #8: GFLOPs: 44.9315. Time: 1.9758 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #9: GFLOPs: 38.3111. Time: 2.3173 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #10: GFLOPs: 117.2616. Time: 0.7571 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #11: GFLOPs: 145.2495. Time: 0.6112 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #12: GFLOPs: 61.3229. Time: 1.4477 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #13: GFLOPs: 114.2419. Time: 0.7771 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #14: GFLOPs: 59.5934. Time: 1.4897 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #15: GFLOPs: 11.6185. Time: 7.6410 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #16: GFLOPs: 133.2140. Time: 0.6664 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #17: GFLOPs: 92.7282. Time: 0.9574 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #18: GFLOPs: 28.3273. Time: 3.1340 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #19: GFLOPs: 92.5544. Time: 0.9592 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #20: GFLOPs: 90.2350. Time: 0.9839 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #21: GFLOPs: 89.8682. Time: 0.9879 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #22: GFLOPs: 111.7242. Time: 0.7946 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #23: GFLOPs: 150.0877. Time: 0.5915 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #24: GFLOPs: 196.3908. Time: 0.4520 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 2, 13, 13):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 4 + i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 13 + i2_3_init)
                        ow = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 13 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(32, 1, 1, 1, 2, 1, 1, 2, 8, 1, 1, 1, 2, 13, 13):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 4 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 13 + i2_3)
                        ow = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2 in T.grid(1, 4, 13):
                for ax3_ax4_fused in T.vectorized(52):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 8 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 4 + ax1)
                        ax2_1 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 13 + ax2)
                        ax3 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 13 + ax3_ax4_fused // 4)
                        ax4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 2, 2, 2])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 1, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
l94 = sch.fuse(l92)
sch.vectorize(loop=l94)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b66)
l101 = sch.fuse(l99, l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b102)
b120 = sch.decompose_reduction(block=b102, loop=l104)
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #26: GFLOPs: 156.6835. Time: 0.5666 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #27: GFLOPs: 131.0935. Time: 0.6772 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #28: GFLOPs: 172.8256. Time: 0.5137 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #29: GFLOPs: 109.1083. Time: 0.8137 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #30: GFLOPs: 56.2587. Time: 1.5780 ms. Best GFLOPs: 362.9019
[19:43:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"] Trial #31: GFLOPs: 38.7488. Time: 2.2911 ms. Best GFLOPs: 362.9019
[19:43:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1056
Total latency (us): 201044

[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #0: GFLOPs: 1.3341. Time: 0.5189 ms. Best GFLOPs: 1.3341
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #1: GFLOPs: 1.6763. Time: 0.4130 ms. Best GFLOPs: 1.6763
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #2: GFLOPs: 1.7696. Time: 0.3912 ms. Best GFLOPs: 1.7696
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #3: GFLOPs: 1.2418. Time: 0.5574 ms. Best GFLOPs: 1.7696
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #4: GFLOPs: 1.2470. Time: 0.5551 ms. Best GFLOPs: 1.7696
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #5: GFLOPs: 1.7901. Time: 0.3867 ms. Best GFLOPs: 1.7901
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #6: GFLOPs: 1.6623. Time: 0.4164 ms. Best GFLOPs: 1.7901
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #7: GFLOPs: 1.5057. Time: 0.4597 ms. Best GFLOPs: 1.7901
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #8: GFLOPs: 1.4074. Time: 0.4918 ms. Best GFLOPs: 1.7901
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #9: GFLOPs: 1.7666. Time: 0.3918 ms. Best GFLOPs: 1.7901
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #10: GFLOPs: 1.7283. Time: 0.4005 ms. Best GFLOPs: 1.7901
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #11: GFLOPs: 1.3437. Time: 0.5151 ms. Best GFLOPs: 1.7901
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #12: GFLOPs: 1.8752. Time: 0.3691 ms. Best GFLOPs: 1.8752
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #13: GFLOPs: 1.3723. Time: 0.5044 ms. Best GFLOPs: 1.8752
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #14: GFLOPs: 2.1464. Time: 0.3225 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #15: GFLOPs: 1.8558. Time: 0.3730 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #16: GFLOPs: 1.3320. Time: 0.5197 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #17: GFLOPs: 1.3079. Time: 0.5293 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #18: GFLOPs: 1.8223. Time: 0.3799 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #19: GFLOPs: 1.6958. Time: 0.4082 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #20: GFLOPs: 1.2859. Time: 0.5383 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #21: GFLOPs: 1.3506. Time: 0.5125 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #22: GFLOPs: 1.9322. Time: 0.3583 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #23: GFLOPs: 1.6987. Time: 0.4075 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #24: GFLOPs: 1.9586. Time: 0.3534 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #25: GFLOPs: 1.2967. Time: 0.5338 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #26: GFLOPs: 1.7158. Time: 0.4034 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #27: GFLOPs: 1.5112. Time: 0.4581 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #28: GFLOPs: 1.7427. Time: 0.3972 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #29: GFLOPs: 1.7483. Time: 0.3959 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #30: GFLOPs: 1.3430. Time: 0.5154 ms. Best GFLOPs: 2.1464
[19:43:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"] Trial #31: GFLOPs: 1.2395. Time: 0.5585 ms. Best GFLOPs: 2.1464
[19:43:33] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1088
Total latency (us): 201367

[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #0: GFLOPs: 89.4180. Time: 3.9675 ms. Best GFLOPs: 89.4180
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #1: GFLOPs: 16.6404. Time: 21.3195 ms. Best GFLOPs: 89.4180
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #2: GFLOPs: 55.3644. Time: 6.4078 ms. Best GFLOPs: 89.4180
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #3: GFLOPs: 65.0072. Time: 5.4573 ms. Best GFLOPs: 89.4180
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #4: GFLOPs: 20.5122. Time: 17.2953 ms. Best GFLOPs: 89.4180
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #5: GFLOPs: 69.8082. Time: 5.0820 ms. Best GFLOPs: 89.4180
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #6: GFLOPs: 9.0775. Time: 39.0818 ms. Best GFLOPs: 89.4180
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #7: GFLOPs: 98.7007. Time: 3.5944 ms. Best GFLOPs: 98.7007
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #8: GFLOPs: 91.4062. Time: 3.8812 ms. Best GFLOPs: 98.7007
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #9: GFLOPs: 79.3300. Time: 4.4720 ms. Best GFLOPs: 98.7007
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #10: GFLOPs: 179.0390. Time: 1.9815 ms. Best GFLOPs: 179.0390
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #11: GFLOPs: 151.4319. Time: 2.3427 ms. Best GFLOPs: 179.0390
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #12: GFLOPs: 116.8900. Time: 3.0350 ms. Best GFLOPs: 179.0390
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #13: GFLOPs: 63.2980. Time: 5.6047 ms. Best GFLOPs: 179.0390
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #14: GFLOPs: 94.1402. Time: 3.7685 ms. Best GFLOPs: 179.0390
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #15: GFLOPs: 50.5790. Time: 7.0141 ms. Best GFLOPs: 179.0390
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #16: GFLOPs: 137.2834. Time: 2.5842 ms. Best GFLOPs: 179.0390
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #17: GFLOPs: 65.8847. Time: 5.3846 ms. Best GFLOPs: 179.0390
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #18: GFLOPs: 113.0789. Time: 3.1373 ms. Best GFLOPs: 179.0390
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #19: GFLOPs: 27.0359. Time: 13.1220 ms. Best GFLOPs: 179.0390
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #20: GFLOPs: 46.3019. Time: 7.6620 ms. Best GFLOPs: 179.0390
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #21: GFLOPs: 151.4976. Time: 2.3417 ms. Best GFLOPs: 179.0390
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #22: GFLOPs: 237.2906. Time: 1.4951 ms. Best GFLOPs: 237.2906
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #23: GFLOPs: 45.8939. Time: 7.7301 ms. Best GFLOPs: 237.2906
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #24: GFLOPs: 428.6387. Time: 0.8277 ms. Best GFLOPs: 428.6387
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #25: GFLOPs: 169.3023. Time: 2.0955 ms. Best GFLOPs: 428.6387
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #26: GFLOPs: 100.2122. Time: 3.5401 ms. Best GFLOPs: 428.6387
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #27: GFLOPs: 80.1794. Time: 4.4246 ms. Best GFLOPs: 428.6387
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #28: GFLOPs: 182.5756. Time: 1.9431 ms. Best GFLOPs: 428.6387
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #29: GFLOPs: 29.1777. Time: 12.1588 ms. Best GFLOPs: 428.6387
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #30: GFLOPs: 112.3403. Time: 3.1579 ms. Best GFLOPs: 428.6387
[19:43:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"] Trial #31: GFLOPs: 124.3352. Time: 2.8533 ms. Best GFLOPs: 428.6387
[19:43:35] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |            N/A |          N/A |                   N/A |      0 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1120
Total latency (us): 202194

[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #0: GFLOPs: 2.2237. Time: 0.3113 ms. Best GFLOPs: 2.2237
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #1: GFLOPs: 2.0764. Time: 0.3334 ms. Best GFLOPs: 2.2237
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #2: GFLOPs: 2.1194. Time: 0.3266 ms. Best GFLOPs: 2.2237
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #3: GFLOPs: 2.1997. Time: 0.3147 ms. Best GFLOPs: 2.2237
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #4: GFLOPs: 2.0768. Time: 0.3333 ms. Best GFLOPs: 2.2237
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #5: GFLOPs: 2.2490. Time: 0.3078 ms. Best GFLOPs: 2.2490
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #6: GFLOPs: 1.8924. Time: 0.3658 ms. Best GFLOPs: 2.2490
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #7: GFLOPs: 1.9565. Time: 0.3538 ms. Best GFLOPs: 2.2490
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #8: GFLOPs: 2.0352. Time: 0.3401 ms. Best GFLOPs: 2.2490
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #9: GFLOPs: 1.9913. Time: 0.3476 ms. Best GFLOPs: 2.2490
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #10: GFLOPs: 2.2779. Time: 0.3039 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #11: GFLOPs: 2.0674. Time: 0.3348 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #12: GFLOPs: 2.0871. Time: 0.3317 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #13: GFLOPs: 2.2183. Time: 0.3121 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #14: GFLOPs: 2.1388. Time: 0.3236 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #15: GFLOPs: 2.1855. Time: 0.3167 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #16: GFLOPs: 2.0054. Time: 0.3452 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #17: GFLOPs: 2.2064. Time: 0.3137 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #18: GFLOPs: 2.0272. Time: 0.3415 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #19: GFLOPs: 1.9512. Time: 0.3548 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #20: GFLOPs: 2.0035. Time: 0.3455 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #21: GFLOPs: 2.0488. Time: 0.3379 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #22: GFLOPs: 2.2006. Time: 0.3146 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #23: GFLOPs: 1.8318. Time: 0.3779 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #24: GFLOPs: 2.1921. Time: 0.3158 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #25: GFLOPs: 1.7855. Time: 0.3877 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #26: GFLOPs: 2.1991. Time: 0.3148 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #27: GFLOPs: 2.0709. Time: 0.3343 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #28: GFLOPs: 2.1989. Time: 0.3148 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #29: GFLOPs: 2.0059. Time: 0.3451 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #30: GFLOPs: 2.0633. Time: 0.3355 ms. Best GFLOPs: 2.2779
[19:43:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"] Trial #31: GFLOPs: 2.0826. Time: 0.3324 ms. Best GFLOPs: 2.2779
[19:43:36] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1152
Total latency (us): 202802

[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #0: GFLOPs: 0.0000. Time: 0.0193 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #1: GFLOPs: 0.0000. Time: 1.1761 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #2: GFLOPs: 0.0000. Time: 1.1755 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #3: GFLOPs: 0.0000. Time: 1.3635 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #4: GFLOPs: 0.0000. Time: 0.0354 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #5: GFLOPs: 0.0000. Time: 0.1585 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #6: GFLOPs: 0.0000. Time: 0.1823 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #7: GFLOPs: 0.0000. Time: 0.1101 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #8: GFLOPs: 0.0000. Time: 0.0316 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #9: GFLOPs: 0.0000. Time: 0.0758 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #10: GFLOPs: 0.0000. Time: 0.1643 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #11: GFLOPs: 0.0000. Time: 0.1624 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #12: GFLOPs: 0.0000. Time: 0.0542 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #13: GFLOPs: 0.0000. Time: 0.0623 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #14: GFLOPs: 0.0000. Time: 0.0282 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #15: GFLOPs: 0.0000. Time: 1.1656 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #16: GFLOPs: 0.0000. Time: 0.0376 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #17: GFLOPs: 0.0000. Time: 0.0244 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #18: GFLOPs: 0.0000. Time: 0.0316 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #19: GFLOPs: 0.0000. Time: 1.2805 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #20: GFLOPs: 0.0000. Time: 1.1632 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #21: GFLOPs: 0.0000. Time: 0.0256 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #22: GFLOPs: 0.0000. Time: 1.2836 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #23: GFLOPs: 0.0000. Time: 1.2035 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #24: GFLOPs: 0.0000. Time: 1.1837 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #25: GFLOPs: 0.0000. Time: 0.0799 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #26: GFLOPs: 0.0000. Time: 0.0695 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #27: GFLOPs: 0.0000. Time: 0.0411 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #28: GFLOPs: 0.0000. Time: 0.0327 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #29: GFLOPs: 0.0000. Time: 0.0863 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #30: GFLOPs: 0.0000. Time: 1.1790 ms. Best GFLOPs: 0.0000
[19:43:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"] Trial #31: GFLOPs: 0.0000. Time: 0.0254 ms. Best GFLOPs: 0.0000
[19:43:37] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1184
Total latency (us): 202821

[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #0: GFLOPs: 27.3245. Time: 58.3936 ms. Best GFLOPs: 27.3245
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #1: GFLOPs: 29.7969. Time: 53.5484 ms. Best GFLOPs: 29.7969
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #2: GFLOPs: 55.9250. Time: 28.5307 ms. Best GFLOPs: 55.9250
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #3: GFLOPs: 46.7427. Time: 34.1353 ms. Best GFLOPs: 55.9250
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #4: GFLOPs: 16.8376. Time: 94.7625 ms. Best GFLOPs: 55.9250
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #5: GFLOPs: 46.3635. Time: 34.4145 ms. Best GFLOPs: 55.9250
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #6: GFLOPs: 155.4300. Time: 10.2656 ms. Best GFLOPs: 155.4300
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #7: GFLOPs: 25.2371. Time: 63.2235 ms. Best GFLOPs: 155.4300
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #8: GFLOPs: 91.6635. Time: 17.4069 ms. Best GFLOPs: 155.4300
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #9: GFLOPs: 8.6723. Time: 183.9851 ms. Best GFLOPs: 155.4300
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #10: GFLOPs: 74.2390. Time: 21.4924 ms. Best GFLOPs: 155.4300
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #11: GFLOPs: 99.4168. Time: 16.0494 ms. Best GFLOPs: 155.4300
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #12: GFLOPs: 82.5465. Time: 19.3294 ms. Best GFLOPs: 155.4300
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #13: GFLOPs: 41.9071. Time: 38.0741 ms. Best GFLOPs: 155.4300
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #14: GFLOPs: 94.3413. Time: 16.9128 ms. Best GFLOPs: 155.4300
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #15: GFLOPs: 69.6948. Time: 22.8938 ms. Best GFLOPs: 155.4300
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #16: GFLOPs: 20.7429. Time: 76.9217 ms. Best GFLOPs: 155.4300
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #17: GFLOPs: 189.8062. Time: 8.4063 ms. Best GFLOPs: 189.8062
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 128, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 64, 28, 28, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 128, 26, 26, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 1, 13, 2):
                for ax0, ax1, ax2 in T.grid(1, 64, 15):
                    for ax3_ax4_fused in T.vectorized(16):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 13 + ax2)
                            i3 = T.axis.spatial(28, i3_1 * 2 + ax3_ax4_fused // 4)
                            i4 = T.axis.spatial(4, ax3_ax4_fused % 4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 27 and 1 <= i3 and i3 < 27, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i1_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(4, 4, 13, 2):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_1 * 16 + i1_2_init * 4 + i1_3_init)
                            oh = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 13 + i2_3_init)
                            ow = T.axis.spatial(26, i3_1 * 2 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(4, 1, 1, 1, 4, 1, 1, 1, 64, 3, 3, 1, 4, 13, 2):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(128, i1_1 * 16 + i1_2 * 4 + i1_3)
                            oh = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 13 + i2_3)
                            ow = T.axis.spatial(26, i3_1 * 2 + i3_3)
                            oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3_fused)
                            ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                            kh, kw = T.axis.remap("RR", [i6_1, i7_1])
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [128, 64, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 128, 13, 26):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(128, ax1)
                        ax2_1 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 13 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15])
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 8, 4, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23])
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 13])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31])
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 13, 1, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39])
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47])
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[4, 64])
l54, l55 = sch.split(loop=l9, factors=[v52, v53])
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l58, l59 = sch.split(loop=l10, factors=[v56, v57])
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l62, l63 = sch.split(loop=l11, factors=[v60, v61])
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
sch.enter_postproc()
b67 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.unroll_explicit")
b68, b69, b70 = sch.get_child_blocks(b67)
l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b68)
l86 = sch.fuse(l71, l72, l73, l74, l75)
sch.parallel(loop=l86)
l87 = sch.fuse(l84, l85)
sch.vectorize(loop=l87)
sch.annotate(block_or_loop=l86, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l86, ann_key="pragma_unroll_explicit", ann_val=1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b69)
l110 = sch.fuse(l109)
sch.vectorize(loop=l110)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b70)
l117 = sch.fuse(l116)
sch.vectorize(loop=l117)
sch.annotate(block_or_loop=l111, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l111, ann_key="pragma_unroll_explicit", ann_val=1)
b118 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b118)
b141 = sch.decompose_reduction(block=b118, loop=l125)
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #19: GFLOPs: 191.2651. Time: 8.3422 ms. Best GFLOPs: 191.2651
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #20: GFLOPs: 55.9233. Time: 28.5315 ms. Best GFLOPs: 191.2651
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #21: GFLOPs: 15.1034. Time: 105.6435 ms. Best GFLOPs: 191.2651
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #22: GFLOPs: 315.4886. Time: 5.0575 ms. Best GFLOPs: 315.4886
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #23: GFLOPs: 343.4176. Time: 4.6462 ms. Best GFLOPs: 343.4176
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #24: GFLOPs: 203.3782. Time: 7.8454 ms. Best GFLOPs: 343.4176
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #25: GFLOPs: 67.4482. Time: 23.6563 ms. Best GFLOPs: 343.4176
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #26: GFLOPs: 51.2996. Time: 31.1031 ms. Best GFLOPs: 343.4176
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #27: GFLOPs: 10.4898. Time: 152.1078 ms. Best GFLOPs: 343.4176
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #28: GFLOPs: 3.9721. Time: 401.6926 ms. Best GFLOPs: 343.4176
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #29: GFLOPs: 54.3175. Time: 29.3750 ms. Best GFLOPs: 343.4176
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #30: GFLOPs: 20.9317. Time: 76.2276 ms. Best GFLOPs: 343.4176
[19:43:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"] Trial #31: GFLOPs: 5.0495. Time: 315.9841 ms. Best GFLOPs: 343.4176
[19:43:39] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1216
Total latency (us): 226052

[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 13, 2, 4):
                for i1_2_init, i3_2_init, i1_3_init in T.grid(2, 13, 16):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_1 * 32 + i1_2_init * 16 + i1_3_init)
                        oh = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 13 + i2_1)
                        ow = T.axis.spatial(26, i3_1 * 13 + i3_2_init)
                        oc_block = T.axis.spatial(4, i4_1)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 2, 1, 13, 1, 32, 1, 1, 1, 16, 1, 1, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_1 * 32 + i1_2 * 16 + i1_3)
                        oh = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 13 + i2_1)
                        ow = T.axis.spatial(26, i3_1 * 13 + i3_2)
                        oc_block = T.axis.spatial(4, i4_1)
                        ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 64, 13, 26):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, ax1)
                        ax2_1 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 13 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 2, 16])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 13, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
l101 = sch.fuse(l100)
sch.vectorize(loop=l101)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124 = sch.get_loops(block=b102)
b125 = sch.decompose_reduction(block=b102, loop=l109)
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #1: GFLOPs: 67.0111. Time: 2.6496 ms. Best GFLOPs: 67.0111
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #2: GFLOPs: 56.0184. Time: 3.1696 ms. Best GFLOPs: 67.0111
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #3: GFLOPs: 103.8613. Time: 1.7095 ms. Best GFLOPs: 103.8613
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #4: GFLOPs: 29.6947. Time: 5.9794 ms. Best GFLOPs: 103.8613
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #5: GFLOPs: 129.1383. Time: 1.3749 ms. Best GFLOPs: 129.1383
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #6: GFLOPs: 24.2034. Time: 7.3360 ms. Best GFLOPs: 129.1383
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #7: GFLOPs: 82.2474. Time: 2.1588 ms. Best GFLOPs: 129.1383
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #8: GFLOPs: 108.9090. Time: 1.6303 ms. Best GFLOPs: 129.1383
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #9: GFLOPs: 4.7756. Time: 37.1801 ms. Best GFLOPs: 129.1383
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #10: GFLOPs: 123.4546. Time: 1.4382 ms. Best GFLOPs: 129.1383
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #11: GFLOPs: 89.2351. Time: 1.9897 ms. Best GFLOPs: 129.1383
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #12: GFLOPs: 78.0008. Time: 2.2763 ms. Best GFLOPs: 129.1383
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #13: GFLOPs: 80.2531. Time: 2.2124 ms. Best GFLOPs: 129.1383
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #14: GFLOPs: 62.4148. Time: 2.8448 ms. Best GFLOPs: 129.1383
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #15: GFLOPs: 232.2114. Time: 0.7646 ms. Best GFLOPs: 232.2114
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #16: GFLOPs: 337.5153. Time: 0.5261 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #17: GFLOPs: 154.8362. Time: 1.1467 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #18: GFLOPs: 97.1303. Time: 1.8280 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #19: GFLOPs: 4.6284. Time: 38.3620 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #20: GFLOPs: 152.1507. Time: 1.1670 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #21: GFLOPs: 53.1364. Time: 3.3415 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #22: GFLOPs: 55.7151. Time: 3.1868 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #23: GFLOPs: 321.0511. Time: 0.5530 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #24: GFLOPs: 31.8795. Time: 5.5696 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #25: GFLOPs: 154.8850. Time: 1.1464 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #26: GFLOPs: 24.1264. Time: 7.3594 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #27: GFLOPs: 120.9989. Time: 1.4674 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #28: GFLOPs: 79.9556. Time: 2.2207 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #29: GFLOPs: 103.7592. Time: 1.7112 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #30: GFLOPs: 5.0289. Time: 35.3073 ms. Best GFLOPs: 337.5153
[19:43:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(64, 128, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 64, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 26, 26, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(32, 13, 2, 2, 13):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(26, i2_2_init * 2 + i2_3_init)
                    ow = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 13 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 1, 1, 1, 32, 13, 1, 1, 32, 1, 1, 1, 2, 2, 13, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(64, i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(26, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    ic = T.axis.reduce(512, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 128, 26, 26, 4], "float32"], ["TENSOR", [64, 128, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 64, 26, 13, 1):
                with T.block("T_leaky_relu"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1, ax2_1 = T.axis.remap("SS", [ax1, ax2])
                    ax3_1 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 13 + ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 32, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[19:43:41] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |            N/A |          N/A |                   N/A |      0 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1248
Total latency (us): 229735

[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 26, 26, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 32, 26, 26, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 26, 26, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i1_3_init, i2_3_init in T.grid(2, 13, 13, 2, 2):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 4 + i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(26, i2_2_init * 2 + i2_3_init)
                    ow = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 13 + i3_2_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 2, 13, 13, 1, 32, 1, 1, 1, 2, 2, 1, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 4 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(26, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 13 + i3_2)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    ic = T.axis.reduce(256, i5_0 * 32 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 26, 26, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 26, 13, 1):
                with T.block("T_leaky_relu"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 4 + ax1)
                    ax2_1 = T.axis.spatial(26, ax2)
                    ax3_1 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 13 + ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[8, 1, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 13, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 13, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[8, 32])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b67)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #1: GFLOPs: 240.1283. Time: 0.1852 ms. Best GFLOPs: 240.1283
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #2: GFLOPs: 82.9280. Time: 0.5363 ms. Best GFLOPs: 240.1283
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #3: GFLOPs: 70.0893. Time: 0.6346 ms. Best GFLOPs: 240.1283
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #4: GFLOPs: 40.3983. Time: 1.1009 ms. Best GFLOPs: 240.1283
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #5: GFLOPs: 53.0561. Time: 0.8383 ms. Best GFLOPs: 240.1283
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #6: GFLOPs: 149.3740. Time: 0.2977 ms. Best GFLOPs: 240.1283
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #7: GFLOPs: 142.1232. Time: 0.3129 ms. Best GFLOPs: 240.1283
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #8: GFLOPs: 182.0633. Time: 0.2443 ms. Best GFLOPs: 240.1283
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #9: GFLOPs: 294.0662. Time: 0.1512 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #10: GFLOPs: 33.3461. Time: 1.3338 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #11: GFLOPs: 196.7877. Time: 0.2260 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #12: GFLOPs: 116.9692. Time: 0.3802 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #13: GFLOPs: 99.3090. Time: 0.4478 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #14: GFLOPs: 48.2006. Time: 0.9227 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #15: GFLOPs: 112.7344. Time: 0.3945 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #16: GFLOPs: 52.4434. Time: 0.8481 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #17: GFLOPs: 28.7662. Time: 1.5461 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #18: GFLOPs: 61.3951. Time: 0.7244 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #19: GFLOPs: 130.3233. Time: 0.3413 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #20: GFLOPs: 75.4932. Time: 0.5891 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #21: GFLOPs: 185.3899. Time: 0.2399 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #22: GFLOPs: 78.4947. Time: 0.5666 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #23: GFLOPs: 55.9586. Time: 0.7948 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #24: GFLOPs: 138.4400. Time: 0.3213 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #25: GFLOPs: 42.6720. Time: 1.0423 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #26: GFLOPs: 73.0953. Time: 0.6085 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #27: GFLOPs: 163.5813. Time: 0.2719 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #28: GFLOPs: 7.2029. Time: 6.1747 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #29: GFLOPs: 77.0251. Time: 0.5774 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #30: GFLOPs: 93.4400. Time: 0.4760 ms. Best GFLOPs: 294.0662
[19:43:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"] Trial #31: GFLOPs: 80.3211. Time: 0.5537 ms. Best GFLOPs: 294.0662
[19:43:43] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1280
Total latency (us): 229886

[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #0: GFLOPs: 0.0000. Time: 0.1127 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #1: GFLOPs: 0.0000. Time: 0.0621 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #2: GFLOPs: 0.0000. Time: 0.0105 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #3: GFLOPs: 0.0000. Time: 0.0604 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #4: GFLOPs: 0.0000. Time: 0.0101 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #5: GFLOPs: 0.0000. Time: 0.0101 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #6: GFLOPs: 0.0000. Time: 0.0406 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #7: GFLOPs: 0.0000. Time: 0.0106 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #8: GFLOPs: 0.0000. Time: 0.0102 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #9: GFLOPs: 0.0000. Time: 0.0137 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #10: GFLOPs: 0.0000. Time: 0.0100 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #11: GFLOPs: 0.0000. Time: 0.0100 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #12: GFLOPs: 0.0000. Time: 0.0102 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #13: GFLOPs: 0.0000. Time: 0.0103 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #14: GFLOPs: 0.0000. Time: 0.0616 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #15: GFLOPs: 0.0000. Time: 0.0138 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #16: GFLOPs: 0.0000. Time: 0.0102 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #17: GFLOPs: 0.0000. Time: 0.0102 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #18: GFLOPs: 0.0000. Time: 0.0103 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #19: GFLOPs: 0.0000. Time: 0.0101 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #20: GFLOPs: 0.0000. Time: 0.0137 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #21: GFLOPs: 0.0000. Time: 0.0137 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #22: GFLOPs: 0.0000. Time: 0.0393 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #23: GFLOPs: 0.0000. Time: 0.0108 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #24: GFLOPs: 0.0000. Time: 0.0102 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #25: GFLOPs: 0.0000. Time: 0.0614 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #26: GFLOPs: 0.0000. Time: 0.0387 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #27: GFLOPs: 0.0000. Time: 0.0404 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #28: GFLOPs: 0.0000. Time: 0.0137 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #29: GFLOPs: 0.0000. Time: 0.0602 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #30: GFLOPs: 0.0000. Time: 0.0389 ms. Best GFLOPs: 0.0000
[19:43:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #40: "fused_transpose_layout_transform"] Trial #31: GFLOPs: 0.0000. Time: 0.0101 ms. Best GFLOPs: 0.0000
[19:43:44] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #40: "fused_transpose_layout_transform"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |            N/A |          N/A |                   N/A |      0 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1312
Total latency (us): 229896

[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #0: GFLOPs: 114.0064. Time: 2.6716 ms. Best GFLOPs: 114.0064
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #1: GFLOPs: 60.3881. Time: 5.0437 ms. Best GFLOPs: 114.0064
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #2: GFLOPs: 50.4517. Time: 6.0370 ms. Best GFLOPs: 114.0064
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #3: GFLOPs: 121.1198. Time: 2.5147 ms. Best GFLOPs: 121.1198
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #4: GFLOPs: 45.6292. Time: 6.6751 ms. Best GFLOPs: 121.1198
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #5: GFLOPs: 33.8386. Time: 9.0009 ms. Best GFLOPs: 121.1198
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #6: GFLOPs: 82.3051. Time: 3.7006 ms. Best GFLOPs: 121.1198
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #7: GFLOPs: 77.1105. Time: 3.9499 ms. Best GFLOPs: 121.1198
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #8: GFLOPs: 45.1397. Time: 6.7475 ms. Best GFLOPs: 121.1198
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #9: GFLOPs: 73.8447. Time: 4.1246 ms. Best GFLOPs: 121.1198
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #10: GFLOPs: 66.4271. Time: 4.5852 ms. Best GFLOPs: 121.1198
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #11: GFLOPs: 78.7990. Time: 3.8653 ms. Best GFLOPs: 121.1198
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #12: GFLOPs: 63.6874. Time: 4.7824 ms. Best GFLOPs: 121.1198
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #13: GFLOPs: 62.1740. Time: 4.8988 ms. Best GFLOPs: 121.1198
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #14: GFLOPs: 80.1205. Time: 3.8015 ms. Best GFLOPs: 121.1198
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #15: GFLOPs: 84.6490. Time: 3.5981 ms. Best GFLOPs: 121.1198
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #16: GFLOPs: 90.2963. Time: 3.3731 ms. Best GFLOPs: 121.1198
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 416, 416, 3), "float32"], placeholder_1: T.Buffer[(8, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 8, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 8, 416, 416, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 418, 418, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 8, 416, 416, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_1, i3_1, i4_1 in T.grid(4, 1, 1):
                for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(2, 2, 2, 2, 13, 13):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 2 + i1_2_init)
                            oh = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 64 * 104 + i2_1 * 26 + i2_2_init * 13 + i2_3_init)
                            ow = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 // 4 * 26 + i3_2_init * 13 + i3_3_init)
                            oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0 in T.serial(3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 28, 28, 1):
                        with T.block("data_pad"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 256 // 64 * 104 + i2_1 * 26 + ax2)
                            i3 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 // 4 * 26 + ax3)
                            i4 = T.axis.spatial(3, i5_0 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(data_pad[i0, i1, i2, i3, i4])
                            data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 417 and 1 <= i3 and i3 < 417, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 1, 1, 2, 2, 2, 2, 1, 1, 3, 1, 1, 13, 13):
                        for i4_3_fused in T.vectorized(2):
                            with T.block("conv2d_NCHWc_update"):
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(8, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 4 * 2 + i1_2)
                                oh = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused // 64 * 104 + i2_1 * 26 + i2_2 * 13 + i2_3)
                                ow = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_fused % 64 // 4 * 26 + i3_2 * 13 + i3_3)
                                oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                                ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_1])
                                T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 416, 416, 3], "float32"], ["TENSOR", [8, 1, 3, 3, 3, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh + kh, ow + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0_i1_i2_fused in T.parallel(3328, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(416):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(8, i0_i1_i2_fused // 416)
                        ax2 = T.axis.spatial(416, i0_i1_i2_fused % 416)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 4, 2, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[16, 1, 2, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
l64 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l64, preserve_unit_loops=True)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67, b68 = sch.get_child_blocks(b65)
l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b66)
l85 = sch.fuse(l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l85)
sch.annotate(block_or_loop=l85, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l85, ann_key="pragma_unroll_explicit", ann_val=1)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b67)
l106 = sch.fuse(l105)
sch.vectorize(loop=l106)
sch.annotate(block_or_loop=l86, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l86, ann_key="pragma_unroll_explicit", ann_val=1)
l107, l108, l109, l110, l111 = sch.get_loops(block=b68)
l112 = sch.fuse(l107, l108, l109)
sch.parallel(loop=l112)
l113 = sch.fuse(l111)
sch.vectorize(loop=l113)
sch.annotate(block_or_loop=l112, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l112, ann_key="pragma_unroll_explicit", ann_val=1)
b114 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b114)
b135 = sch.decompose_reduction(block=b114, loop=l119)
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #18: GFLOPs: 231.3046. Time: 1.3168 ms. Best GFLOPs: 231.3046
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #19: GFLOPs: 42.1777. Time: 7.2213 ms. Best GFLOPs: 231.3046
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #20: GFLOPs: 86.6052. Time: 3.5169 ms. Best GFLOPs: 231.3046
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #21: GFLOPs: 157.9496. Time: 1.9283 ms. Best GFLOPs: 231.3046
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #22: GFLOPs: 143.8059. Time: 2.1180 ms. Best GFLOPs: 231.3046
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #23: GFLOPs: 106.1350. Time: 2.8697 ms. Best GFLOPs: 231.3046
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #24: GFLOPs: 72.9888. Time: 4.1729 ms. Best GFLOPs: 231.3046
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #25: GFLOPs: 163.1355. Time: 1.8670 ms. Best GFLOPs: 231.3046
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #26: GFLOPs: 58.6089. Time: 5.1968 ms. Best GFLOPs: 231.3046
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #27: GFLOPs: 50.9183. Time: 5.9817 ms. Best GFLOPs: 231.3046
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #28: GFLOPs: 75.5356. Time: 4.0323 ms. Best GFLOPs: 231.3046
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #29: GFLOPs: 77.5859. Time: 3.9257 ms. Best GFLOPs: 231.3046
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #30: GFLOPs: 54.9757. Time: 5.5402 ms. Best GFLOPs: 231.3046
[19:43:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"] Trial #31: GFLOPs: 23.2933. Time: 13.0758 ms. Best GFLOPs: 231.3046
[19:43:46] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1344
Total latency (us): 231213

[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #0: GFLOPs: 1.4067. Time: 7.8735 ms. Best GFLOPs: 1.4067
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #1: GFLOPs: 0.9778. Time: 11.3273 ms. Best GFLOPs: 1.4067
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #2: GFLOPs: 1.2398. Time: 8.9334 ms. Best GFLOPs: 1.4067
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #3: GFLOPs: 1.5742. Time: 7.0359 ms. Best GFLOPs: 1.5742
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #4: GFLOPs: 1.9728. Time: 5.6141 ms. Best GFLOPs: 1.9728
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #5: GFLOPs: 1.3095. Time: 8.4581 ms. Best GFLOPs: 1.9728
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #6: GFLOPs: 1.5154. Time: 7.3087 ms. Best GFLOPs: 1.9728
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #7: GFLOPs: 1.5071. Time: 7.3488 ms. Best GFLOPs: 1.9728
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #8: GFLOPs: 2.1225. Time: 5.2182 ms. Best GFLOPs: 2.1225
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #9: GFLOPs: 2.3044. Time: 4.8063 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #10: GFLOPs: 1.2080. Time: 9.1689 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #11: GFLOPs: 1.2555. Time: 8.8219 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #12: GFLOPs: 2.1898. Time: 5.0579 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #13: GFLOPs: 1.4003. Time: 7.9092 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #14: GFLOPs: 1.4077. Time: 7.8681 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #15: GFLOPs: 1.2643. Time: 8.7601 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #16: GFLOPs: 1.8053. Time: 6.1350 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #17: GFLOPs: 1.8197. Time: 6.0865 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #18: GFLOPs: 1.7633. Time: 6.2811 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #19: GFLOPs: 2.1615. Time: 5.1241 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 416, 416, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 8, 416, 416, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 8, 416, 416, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 32, 416, 416], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 8, 416, 416, 4], dtype="float32")
        for i0_i1_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 416, 416):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_exp"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3_1, ax4 = T.axis.remap("SSSS", [i0_i1_fused, ax2, ax3, ax4_fused])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T.writes(T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.exp(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4], dtype="float32")
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 416, 416):
                for ax0_2, ax1_2, ax2_2, ax3_2 in T.grid(1, 4, 1, 1):
                    with T.block("T_layout_trans"):
                        ax0_3 = T.axis.spatial(1, 0)
                        ax1_3 = T.axis.spatial(32, i0_i1_fused * 4 + ax1_2)
                        ax2_3, ax3_3 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(T_exp[ax0_3, ax1_3 // 4, ax2_3, ax3_3, ax1_3 % 4])
                        T.writes(T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3])
                        T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3] = T.if_then_else(ax0_3 < 1 and ax1_3 < 32 and ax2_3 < 416 and ax3_3 < 416, T_exp[ax0_3, ax1_3 // 4, ax2_3, ax3_3, ax1_3 % 4], T.float32(0), dtype="float32")
                for ax4 in T.serial(4):
                    with T.block("T_layout_trans_1"):
                        ax0_4 = T.axis.spatial(1, 0)
                        ax1_4, ax2_4, ax3_4, ax4_1 = T.axis.remap("SSSS", [i0_i1_fused, ax2, ax3, ax4])
                        T.reads(placeholder_1[ax0_4, 0, 0, 0], T_layout_trans[ax0_4, ax1_4 * 4 + ax4_1, ax2_4, ax3_4])
                        T.writes(T_layout_trans_1[ax0_4, ax1_4, ax2_4, ax3_4, ax4_1])
                        T_layout_trans_1[ax0_4, ax1_4, ax2_4, ax3_4, ax4_1] = T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4_1 < 32 and ax2_4 < 416 and ax3_4 < 416, T.tanh(T.log(placeholder_1[ax0_4, 0, 0, 0] + T_layout_trans[ax0_4, ax1_4 * 4 + ax4_1, ax2_4, ax3_4], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
            for i2, i3 in T.grid(416, 416):
                for i4_fused in T.vectorized(4):
                    with T.block("T_multiply"):
                        ax0_5 = T.axis.spatial(1, 0)
                        ax1_5, ax2_5, ax3_5, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4_fused])
                        T.reads(placeholder[ax0_5, ax1_5, ax2_5, ax3_5, ax4], T_layout_trans_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4])
                        T.writes(T_multiply[ax0_5, ax1_5, ax2_5, ax3_5, ax4])
                        T_multiply[ax0_5, ax1_5, ax2_5, ax3_5, ax4] = placeholder[ax0_5, ax1_5, ax2_5, ax3_5, ax4] * T_layout_trans_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4]
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=1)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=5)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
sch.enter_postproc()
b11 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b11, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b11, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b11, ann_key="meta_schedule.unroll_explicit")
b12, b13, b14, b15 = sch.get_child_blocks(b11)
l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b12)
l23 = sch.fuse(l16, l17)
sch.parallel(loop=l23)
l24 = sch.fuse(l22)
sch.vectorize(loop=l24)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26, l27, l28, l29, l30, l31, l32, l33 = sch.get_loops(block=b13)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l34, l35, l36, l37, l38, l39 = sch.get_loops(block=b14)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l40, l41, l42, l43 = sch.get_loops(block=b15)
l44 = sch.fuse(l43)
sch.vectorize(loop=l44)
sch.annotate(block_or_loop=l40, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l40, ann_key="pragma_unroll_explicit", ann_val=1)
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #21: GFLOPs: 1.3992. Time: 7.9158 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 416, 416, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 8, 416, 416, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 8, 416, 416, 4], dtype="float32")
        for i0_i1_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 416, 416):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_exp"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3_1, ax4 = T.axis.remap("SSSS", [i0_i1_fused, ax2, ax3, ax4_fused])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T.writes(T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.exp(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4], dtype="float32")
            for i2, i3 in T.grid(416, 416):
                for i4_fused in T.vectorized(4):
                    with T.block("T_multiply"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4_fused])
                        T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, 0, 0, 0], T_exp[ax0, ax4 // 4 + ax1, ax2, ax3, ax4 % 4])
                        T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                        T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 32 and ax2 < 416 and ax3 < 416, T.tanh(T.log(placeholder_1[ax0, 0, 0, 0] + T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 32 and ax2 < 416 and ax3 < 416, T_exp[ax0, (ax1 * 4 + ax4) // 4, ax2, ax3, (ax1 * 4 + ax4) % 4], T.float32(0), dtype="float32"), dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=-2)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
sch.enter_postproc()
b11 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b11, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b11, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b11, ann_key="meta_schedule.unroll_explicit")
b12, b13 = sch.get_child_blocks(b11)
l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b12)
l21 = sch.fuse(l14, l15)
sch.parallel(loop=l21)
l22 = sch.fuse(l20)
sch.vectorize(loop=l22)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l23, l24, l25, l26 = sch.get_loops(block=b13)
l27 = sch.fuse(l26)
sch.vectorize(loop=l27)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #23: GFLOPs: 1.2956. Time: 8.5488 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #24: GFLOPs: 2.2877. Time: 4.8414 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #25: GFLOPs: 1.1868. Time: 9.3321 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #26: GFLOPs: 1.3010. Time: 8.5134 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #27: GFLOPs: 1.8504. Time: 5.9856 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #28: GFLOPs: 1.2552. Time: 8.8237 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #29: GFLOPs: 1.0564. Time: 10.4843 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #30: GFLOPs: 1.5781. Time: 7.0183 ms. Best GFLOPs: 2.3044
[19:43:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"] Trial #31: GFLOPs: 1.0790. Time: 10.2650 ms. Best GFLOPs: 2.3044
[19:43:47] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1376
Total latency (us): 236019

[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #0: GFLOPs: 94.9552. Time: 16.8253 ms. Best GFLOPs: 94.9552
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #1: GFLOPs: 193.2118. Time: 8.2689 ms. Best GFLOPs: 193.2118
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #2: GFLOPs: 171.6357. Time: 9.3084 ms. Best GFLOPs: 193.2118
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #3: GFLOPs: 264.5293. Time: 6.0396 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #4: GFLOPs: 129.1250. Time: 12.3729 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #5: GFLOPs: 133.1396. Time: 11.9998 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #6: GFLOPs: 71.9418. Time: 22.2076 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #7: GFLOPs: 80.4085. Time: 19.8692 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #8: GFLOPs: 176.2239. Time: 9.0660 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #9: GFLOPs: 50.4327. Time: 31.6789 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #10: GFLOPs: 0.5638. Time: 2833.7085 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #11: GFLOPs: 43.0542. Time: 37.1079 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #12: GFLOPs: 55.6027. Time: 28.7334 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #13: GFLOPs: 162.8585. Time: 9.8101 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #14: GFLOPs: 53.8847. Time: 29.6495 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #15: GFLOPs: 8.3910. Time: 190.4000 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #16: GFLOPs: 27.5317. Time: 58.0296 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #17: GFLOPs: 87.7086. Time: 18.2155 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #18: GFLOPs: 243.0604. Time: 6.5731 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #19: GFLOPs: 203.7139. Time: 7.8426 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #20: GFLOPs: 74.6554. Time: 21.4004 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #21: GFLOPs: 18.0219. Time: 88.6507 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #22: GFLOPs: 124.1096. Time: 12.8729 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #23: GFLOPs: 148.2565. Time: 10.7763 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #24: GFLOPs: 33.4447. Time: 47.7700 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 416, 416, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 8, 417, 417, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        for i0_0_i1_0_i2_0_fused in T.parallel(26, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 17, 417):
                for ax4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(417, i0_0_i1_0_i2_0_fused % 26 * 16 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 417 and 1 <= i3 and i3 < 417, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 1, 2, 2):
                for i1_2_init, i3_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(8, 2, 2, 8, 52):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_2_init)
                        oh = T.axis.spatial(208, i0_0_i1_0_i2_0_fused * 8 + i2_3_init)
                        ow = T.axis.spatial(208, i3_1 * 104 + i3_2_init * 52 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 416, 416, 4], "float32"], ["TENSOR", [16, 8, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(16, 3, 3, 1, 8, 1, 2, 2, 2, 1, 1, 1, 1, 8, 52, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i1_1 * 8 + i1_2)
                        oh = T.axis.spatial(208, i0_0_i1_0_i2_0_fused * 8 + i2_3)
                        ow = T.axis.spatial(208, i3_1 * 104 + i3_2 * 52 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(32, i5_0 * 2 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_0])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 416, 416, 4], "float32"], ["TENSOR", [16, 8, 3, 3, 4, 4], "float32"], [2, 2], [1, 1, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh * 2 + kh, ow * 2 + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(1, 8, 8, 104):
                    for ax4_fused in T.vectorized(2):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(16, i1_1 * 8 + ax1)
                            ax2_1 = T.axis.spatial(208, i0_0_i1_0_i2_0_fused * 8 + ax2)
                            ax3_1 = T.axis.spatial(208, i3_1 * 104 + ax3)
                            ax4 = T.axis.spatial(4, i4_1 * 2 + ax4_fused)
                            T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 8, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[26, 1, 1, 8])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 52])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[16, 2])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b67)
l78 = sch.fuse(l70, l71, l72)
sch.parallel(loop=l78)
l79 = sch.fuse(l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b68)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b69)
l117 = sch.fuse(l116)
sch.vectorize(loop=l117)
sch.annotate(block_or_loop=l104, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l104, ann_key="pragma_unroll_explicit", ann_val=1)
b118 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b118)
b143 = sch.decompose_reduction(block=b118, loop=l127)
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #26: GFLOPs: 88.0540. Time: 18.1440 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #27: GFLOPs: 100.9345. Time: 15.8286 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #28: GFLOPs: 104.4597. Time: 15.2944 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #29: GFLOPs: 99.5876. Time: 16.0427 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #30: GFLOPs: 91.9908. Time: 17.3675 ms. Best GFLOPs: 264.5293
[19:43:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"] Trial #31: GFLOPs: 206.1680. Time: 7.7493 ms. Best GFLOPs: 264.5293
[19:43:49] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |            N/A |          N/A |                   N/A |      0 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1408
Total latency (us): 242059

[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #0: GFLOPs: 78.0016. Time: 2.2896 ms. Best GFLOPs: 78.0016
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #1: GFLOPs: 71.4299. Time: 2.5003 ms. Best GFLOPs: 78.0016
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #2: GFLOPs: 86.5025. Time: 2.0646 ms. Best GFLOPs: 86.5025
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #3: GFLOPs: 148.7253. Time: 1.2008 ms. Best GFLOPs: 148.7253
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #4: GFLOPs: 200.9220. Time: 0.8889 ms. Best GFLOPs: 200.9220
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #5: GFLOPs: 68.7371. Time: 2.5982 ms. Best GFLOPs: 200.9220
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #6: GFLOPs: 120.7605. Time: 1.4789 ms. Best GFLOPs: 200.9220
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #7: GFLOPs: 243.0437. Time: 0.7348 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #8: GFLOPs: 139.6705. Time: 1.2787 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #9: GFLOPs: 44.7363. Time: 3.9921 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #10: GFLOPs: 17.1842. Time: 10.3929 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #11: GFLOPs: 178.4687. Time: 1.0007 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #12: GFLOPs: 26.7866. Time: 6.6673 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #13: GFLOPs: 107.6382. Time: 1.6592 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #14: GFLOPs: 108.6744. Time: 1.6434 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #15: GFLOPs: 219.1782. Time: 0.8148 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #16: GFLOPs: 19.9227. Time: 8.9644 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #17: GFLOPs: 181.9409. Time: 0.9816 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #18: GFLOPs: 81.4721. Time: 2.1921 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #19: GFLOPs: 37.0723. Time: 4.8174 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #20: GFLOPs: 126.6633. Time: 1.4100 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #21: GFLOPs: 150.8814. Time: 1.1837 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #22: GFLOPs: 154.3094. Time: 1.1574 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #23: GFLOPs: 57.0940. Time: 3.1281 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #24: GFLOPs: 196.6889. Time: 0.9080 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #25: GFLOPs: 8.5956. Time: 20.7773 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #26: GFLOPs: 95.5882. Time: 1.8684 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #27: GFLOPs: 44.4871. Time: 4.0145 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #28: GFLOPs: 96.9332. Time: 1.8424 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #29: GFLOPs: 68.4480. Time: 2.6092 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #30: GFLOPs: 56.0176. Time: 3.1882 ms. Best GFLOPs: 243.0437
[19:43:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"] Trial #31: GFLOPs: 27.9139. Time: 6.3980 ms. Best GFLOPs: 243.0437
[19:43:51] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1440
Total latency (us): 242793

[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #0: GFLOPs: 1.4870. Time: 1.8621 ms. Best GFLOPs: 1.4870
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #1: GFLOPs: 1.9330. Time: 1.4324 ms. Best GFLOPs: 1.9330
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #2: GFLOPs: 1.4310. Time: 1.9350 ms. Best GFLOPs: 1.9330
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #3: GFLOPs: 1.2908. Time: 2.1450 ms. Best GFLOPs: 1.9330
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #4: GFLOPs: 2.0592. Time: 1.3447 ms. Best GFLOPs: 2.0592
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #5: GFLOPs: 2.1448. Time: 1.2910 ms. Best GFLOPs: 2.1448
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #6: GFLOPs: 1.9940. Time: 1.3886 ms. Best GFLOPs: 2.1448
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #7: GFLOPs: 1.3892. Time: 1.9931 ms. Best GFLOPs: 2.1448
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #8: GFLOPs: 1.7693. Time: 1.5650 ms. Best GFLOPs: 2.1448
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #9: GFLOPs: 1.2812. Time: 2.1611 ms. Best GFLOPs: 2.1448
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #10: GFLOPs: 1.9408. Time: 1.4267 ms. Best GFLOPs: 2.1448
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #11: GFLOPs: 2.0629. Time: 1.3422 ms. Best GFLOPs: 2.1448
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #12: GFLOPs: 2.2812. Time: 1.2138 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #13: GFLOPs: 1.9067. Time: 1.4522 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #14: GFLOPs: 2.1230. Time: 1.3042 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #15: GFLOPs: 2.1309. Time: 1.2994 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #16: GFLOPs: 1.8491. Time: 1.4974 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #17: GFLOPs: 1.3616. Time: 2.0335 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #18: GFLOPs: 2.0246. Time: 1.3676 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #19: GFLOPs: 1.3852. Time: 1.9989 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #20: GFLOPs: 1.3843. Time: 2.0002 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #21: GFLOPs: 2.1596. Time: 1.2822 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #22: GFLOPs: 1.9603. Time: 1.4125 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #23: GFLOPs: 1.7308. Time: 1.5997 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #24: GFLOPs: 1.2571. Time: 2.2027 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #25: GFLOPs: 1.3839. Time: 2.0007 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #26: GFLOPs: 1.3726. Time: 2.0172 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #27: GFLOPs: 2.0382. Time: 1.3585 ms. Best GFLOPs: 2.2812
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #28: GFLOPs: 2.2860. Time: 1.2112 ms. Best GFLOPs: 2.2860
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #29: GFLOPs: 2.1444. Time: 1.2912 ms. Best GFLOPs: 2.2860
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #30: GFLOPs: 2.0433. Time: 1.3551 ms. Best GFLOPs: 2.2860
[19:43:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"] Trial #31: GFLOPs: 1.7361. Time: 1.5949 ms. Best GFLOPs: 2.2860
[19:43:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |            N/A |          N/A |                   N/A |      0 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1472
Total latency (us): 244005

[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #0: GFLOPs: 24.3861. Time: 65.5149 ms. Best GFLOPs: 24.3861
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #1: GFLOPs: 306.7114. Time: 5.2090 ms. Best GFLOPs: 306.7114
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #2: GFLOPs: 124.7596. Time: 12.8058 ms. Best GFLOPs: 306.7114
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #3: GFLOPs: 7.5573. Time: 211.4056 ms. Best GFLOPs: 306.7114
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #4: GFLOPs: 132.4998. Time: 12.0578 ms. Best GFLOPs: 306.7114
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #5: GFLOPs: 32.4375. Time: 49.2533 ms. Best GFLOPs: 306.7114
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #6: GFLOPs: 75.4995. Time: 21.1611 ms. Best GFLOPs: 306.7114
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #7: GFLOPs: 172.5376. Time: 9.2597 ms. Best GFLOPs: 306.7114
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #8: GFLOPs: 23.0292. Time: 69.3752 ms. Best GFLOPs: 306.7114
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #9: GFLOPs: 145.7196. Time: 10.9639 ms. Best GFLOPs: 306.7114
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #10: GFLOPs: 125.5031. Time: 12.7300 ms. Best GFLOPs: 306.7114
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #11: GFLOPs: 56.6338. Time: 28.2102 ms. Best GFLOPs: 306.7114
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #12: GFLOPs: 370.3680. Time: 4.3137 ms. Best GFLOPs: 370.3680
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #13: GFLOPs: 92.4702. Time: 17.2775 ms. Best GFLOPs: 370.3680
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #14: GFLOPs: 42.5447. Time: 37.5523 ms. Best GFLOPs: 370.3680
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #15: GFLOPs: 76.4934. Time: 20.8862 ms. Best GFLOPs: 370.3680
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #16: GFLOPs: 135.0711. Time: 11.8282 ms. Best GFLOPs: 370.3680
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 8, 208, 208, 4), "float32"], placeholder_1: T.Buffer[(16, 8, 3, 3, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 208, 208, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 8, 210, 210, 4], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 208, 208, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 8, 210, 210):
                for ax4_fused in T.vectorized(4):
                    with T.block("data_pad"):
                        i0 = T.axis.spatial(1, 0)
                        i1, i2, i3, i4 = T.axis.remap("SSSS", [ax1, ax2, ax3, ax4_fused])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 209 and 1 <= i3 and i3 < 209, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 4, 2, 1):
                for i1_2_init, i2_2_init, i3_2_init, i2_3_init, i3_3_init in T.grid(2, 2, 8, 26, 13):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 8 + i1_1 * 2 + i1_2_init)
                        oh = T.axis.spatial(208, i2_1 * 52 + i2_2_init * 26 + i2_3_init)
                        ow = T.axis.spatial(208, i3_1 * 104 + i3_2_init * 13 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 208, 208, 4], "float32"], ["TENSOR", [16, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 3, 1, 1, 2, 2, 8, 1, 8, 1, 3, 1, 1, 26, 13, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 8 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(208, i2_1 * 52 + i2_2 * 26 + i2_3)
                        ow = T.axis.spatial(208, i3_1 * 104 + i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        ic = T.axis.reduce(32, i5_0 * 8 + i5_1)
                        kh, kw = T.axis.remap("RR", [i6_0, i7_1])
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 8, 208, 208, 4], "float32"], ["TENSOR", [16, 8, 3, 3, 4, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 52, 104, 1):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 8 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(208, i2_1 * 52 + ax2)
                        ax3_1 = T.axis.spatial(208, i3_1 * 104 + ax3)
                        ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 2, 26])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 8, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
l65 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l65, preserve_unit_loops=True)
sch.enter_postproc()
b66 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.unroll_explicit")
b67, b68, b69 = sch.get_child_blocks(b66)
l70, l71, l72, l73, l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b67)
l80 = sch.fuse(l70, l71, l72, l73, l74)
sch.parallel(loop=l80)
l81 = sch.fuse(l79)
sch.vectorize(loop=l81)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103 = sch.get_loops(block=b68)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b69)
sch.annotate(block_or_loop=l104, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l104, ann_key="pragma_unroll_explicit", ann_val=1)
b115 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137 = sch.get_loops(block=b115)
b138 = sch.decompose_reduction(block=b115, loop=l122)
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #18: GFLOPs: 73.2029. Time: 21.8250 ms. Best GFLOPs: 370.3680
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #19: GFLOPs: 85.1308. Time: 18.7670 ms. Best GFLOPs: 370.3680
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #20: GFLOPs: 12.5151. Time: 127.6581 ms. Best GFLOPs: 370.3680
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #21: GFLOPs: 115.9024. Time: 13.7845 ms. Best GFLOPs: 370.3680
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #22: GFLOPs: 79.6171. Time: 20.0667 ms. Best GFLOPs: 370.3680
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #23: GFLOPs: 122.6725. Time: 13.0237 ms. Best GFLOPs: 370.3680
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #24: GFLOPs: 61.2768. Time: 26.0727 ms. Best GFLOPs: 370.3680
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #25: GFLOPs: 435.7001. Time: 3.6669 ms. Best GFLOPs: 435.7001
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #26: GFLOPs: 54.5014. Time: 29.3140 ms. Best GFLOPs: 435.7001
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #27: GFLOPs: 101.0252. Time: 15.8144 ms. Best GFLOPs: 435.7001
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #28: GFLOPs: 105.5885. Time: 15.1309 ms. Best GFLOPs: 435.7001
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #29: GFLOPs: 169.3135. Time: 9.4361 ms. Best GFLOPs: 435.7001
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #30: GFLOPs: 90.4319. Time: 17.6669 ms. Best GFLOPs: 435.7001
[19:43:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"] Trial #31: GFLOPs: 12.5862. Time: 126.9370 ms. Best GFLOPs: 435.7001
[19:43:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |            N/A |          N/A |                   N/A |      0 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1504
Total latency (us): 247671

[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #0: GFLOPs: 3.0730. Time: 2.7031 ms. Best GFLOPs: 3.0730
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #1: GFLOPs: 2.2282. Time: 3.7280 ms. Best GFLOPs: 3.0730
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #2: GFLOPs: 2.2088. Time: 3.7607 ms. Best GFLOPs: 3.0730
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #3: GFLOPs: 2.2037. Time: 3.7694 ms. Best GFLOPs: 3.0730
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #4: GFLOPs: 2.9764. Time: 2.7909 ms. Best GFLOPs: 3.0730
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #5: GFLOPs: 2.1119. Time: 3.9333 ms. Best GFLOPs: 3.0730
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #6: GFLOPs: 2.2129. Time: 3.7538 ms. Best GFLOPs: 3.0730
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #7: GFLOPs: 3.1189. Time: 2.6633 ms. Best GFLOPs: 3.1189
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #8: GFLOPs: 2.2024. Time: 3.7717 ms. Best GFLOPs: 3.1189
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #9: GFLOPs: 2.0766. Time: 4.0001 ms. Best GFLOPs: 3.1189
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #10: GFLOPs: 3.0329. Time: 2.7389 ms. Best GFLOPs: 3.1189
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #11: GFLOPs: 3.0893. Time: 2.6889 ms. Best GFLOPs: 3.1189
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #12: GFLOPs: 1.9874. Time: 4.1797 ms. Best GFLOPs: 3.1189
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #13: GFLOPs: 2.1551. Time: 3.8544 ms. Best GFLOPs: 3.1189
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #14: GFLOPs: 3.2672. Time: 2.5425 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #15: GFLOPs: 2.1954. Time: 3.7837 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #16: GFLOPs: 3.1854. Time: 2.6077 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #17: GFLOPs: 1.8842. Time: 4.4087 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #18: GFLOPs: 2.8815. Time: 2.8828 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #19: GFLOPs: 2.2451. Time: 3.6999 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #20: GFLOPs: 2.3106. Time: 3.5950 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #21: GFLOPs: 2.2087. Time: 3.7609 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #22: GFLOPs: 2.7434. Time: 3.0279 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #23: GFLOPs: 3.0755. Time: 2.7010 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #24: GFLOPs: 2.2799. Time: 3.6434 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #25: GFLOPs: 3.2219. Time: 2.5782 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #26: GFLOPs: 3.2657. Time: 2.5436 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #27: GFLOPs: 2.0121. Time: 4.1285 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #28: GFLOPs: 2.3567. Time: 3.5246 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #29: GFLOPs: 1.9722. Time: 4.2118 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #30: GFLOPs: 2.0774. Time: 3.9987 ms. Best GFLOPs: 3.2672
[19:43:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"] Trial #31: GFLOPs: 2.0309. Time: 4.0901 ms. Best GFLOPs: 3.2672
[19:43:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |            N/A |          N/A |                   N/A |      0 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1536
Total latency (us): 250214

[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #0: GFLOPs: 250.7005. Time: 1.4248 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #1: GFLOPs: 194.1227. Time: 1.8400 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #2: GFLOPs: 79.8001. Time: 4.4760 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #3: GFLOPs: 40.4269. Time: 8.8354 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #4: GFLOPs: 33.4012. Time: 10.6939 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #5: GFLOPs: 77.2788. Time: 4.6221 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #6: GFLOPs: 54.2146. Time: 6.5884 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #7: GFLOPs: 13.4447. Time: 26.5672 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #8: GFLOPs: 70.8588. Time: 5.0408 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #9: GFLOPs: 115.0549. Time: 3.1045 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #10: GFLOPs: 178.2235. Time: 2.0042 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #11: GFLOPs: 50.7958. Time: 7.0318 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #12: GFLOPs: 164.6711. Time: 2.1691 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #13: GFLOPs: 123.8344. Time: 2.8844 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #14: GFLOPs: 102.0419. Time: 3.5004 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #15: GFLOPs: 58.6950. Time: 6.0855 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #16: GFLOPs: 90.4194. Time: 3.9503 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #17: GFLOPs: 172.6976. Time: 2.0683 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #18: GFLOPs: 25.1580. Time: 14.1978 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #19: GFLOPs: 46.0205. Time: 7.7615 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #20: GFLOPs: 54.4911. Time: 6.5550 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #21: GFLOPs: 237.4762. Time: 1.5041 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #22: GFLOPs: 59.0449. Time: 6.0494 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #23: GFLOPs: 35.5000. Time: 10.0616 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #24: GFLOPs: 28.5440. Time: 12.5136 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #25: GFLOPs: 96.7889. Time: 3.6904 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #26: GFLOPs: 66.5744. Time: 5.3652 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #27: GFLOPs: 70.5785. Time: 5.0609 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #28: GFLOPs: 71.4403. Time: 4.9998 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #29: GFLOPs: 233.6430. Time: 1.5288 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #30: GFLOPs: 70.5730. Time: 5.0612 ms. Best GFLOPs: 250.7005
[19:43:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"] Trial #31: GFLOPs: 96.2600. Time: 3.7107 ms. Best GFLOPs: 250.7005
[19:43:58] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |            N/A |          N/A |                   N/A |      0 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1568
Total latency (us): 254488

[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #0: GFLOPs: 1.7756. Time: 6.2376 ms. Best GFLOPs: 1.7756
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #1: GFLOPs: 1.8352. Time: 6.0352 ms. Best GFLOPs: 1.8352
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #2: GFLOPs: 1.4778. Time: 7.4948 ms. Best GFLOPs: 1.8352
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #3: GFLOPs: 1.7056. Time: 6.4937 ms. Best GFLOPs: 1.8352
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #4: GFLOPs: 1.7976. Time: 6.1613 ms. Best GFLOPs: 1.8352
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #5: GFLOPs: 1.7580. Time: 6.3000 ms. Best GFLOPs: 1.8352
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #6: GFLOPs: 1.5820. Time: 7.0012 ms. Best GFLOPs: 1.8352
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #7: GFLOPs: 1.5946. Time: 6.9457 ms. Best GFLOPs: 1.8352
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #8: GFLOPs: 1.8643. Time: 5.9409 ms. Best GFLOPs: 1.8643
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #9: GFLOPs: 1.6497. Time: 6.7136 ms. Best GFLOPs: 1.8643
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #10: GFLOPs: 1.3289. Time: 8.3341 ms. Best GFLOPs: 1.8643
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #11: GFLOPs: 1.1350. Time: 9.7579 ms. Best GFLOPs: 1.8643
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #12: GFLOPs: 1.6690. Time: 6.6362 ms. Best GFLOPs: 1.8643
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #13: GFLOPs: 1.1987. Time: 9.2399 ms. Best GFLOPs: 1.8643
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #14: GFLOPs: 1.8350. Time: 6.0356 ms. Best GFLOPs: 1.8643
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #15: GFLOPs: 1.7143. Time: 6.4609 ms. Best GFLOPs: 1.8643
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #16: GFLOPs: 1.8628. Time: 5.9456 ms. Best GFLOPs: 1.8643
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #17: GFLOPs: 1.9906. Time: 5.5640 ms. Best GFLOPs: 1.9906
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #18: GFLOPs: 1.6078. Time: 6.8885 ms. Best GFLOPs: 1.9906
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #19: GFLOPs: 1.8084. Time: 6.1245 ms. Best GFLOPs: 1.9906
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #20: GFLOPs: 1.7311. Time: 6.3982 ms. Best GFLOPs: 1.9906
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #21: GFLOPs: 2.0015. Time: 5.5336 ms. Best GFLOPs: 2.0015
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #22: GFLOPs: 2.0144. Time: 5.4982 ms. Best GFLOPs: 2.0144
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #23: GFLOPs: 1.6798. Time: 6.5933 ms. Best GFLOPs: 2.0144
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #24: GFLOPs: 1.5815. Time: 7.0032 ms. Best GFLOPs: 2.0144
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #25: GFLOPs: 1.3848. Time: 7.9977 ms. Best GFLOPs: 2.0144
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #26: GFLOPs: 1.2764. Time: 8.6769 ms. Best GFLOPs: 2.0144
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #27: GFLOPs: 1.6467. Time: 6.7260 ms. Best GFLOPs: 2.0144
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #28: GFLOPs: 1.6399. Time: 6.7537 ms. Best GFLOPs: 2.0144
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #29: GFLOPs: 1.2758. Time: 8.6816 ms. Best GFLOPs: 2.0144
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #30: GFLOPs: 1.3493. Time: 8.2084 ms. Best GFLOPs: 2.0144
[19:43:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"] Trial #31: GFLOPs: 1.3020. Time: 8.5068 ms. Best GFLOPs: 2.0144
[19:44:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |            N/A |          N/A |                   N/A |      0 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1600
Total latency (us): 259986

[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #0: GFLOPs: 254.1051. Time: 2.8004 ms. Best GFLOPs: 254.1051
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #1: GFLOPs: 114.7119. Time: 6.2034 ms. Best GFLOPs: 254.1051
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #2: GFLOPs: 63.1068. Time: 11.2762 ms. Best GFLOPs: 254.1051
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #3: GFLOPs: 290.4949. Time: 2.4496 ms. Best GFLOPs: 290.4949
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #4: GFLOPs: 235.3797. Time: 3.0232 ms. Best GFLOPs: 290.4949
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #5: GFLOPs: 41.6035. Time: 17.1045 ms. Best GFLOPs: 290.4949
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #6: GFLOPs: 65.3673. Time: 10.8863 ms. Best GFLOPs: 290.4949
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #7: GFLOPs: 52.1950. Time: 13.6336 ms. Best GFLOPs: 290.4949
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #8: GFLOPs: 314.8462. Time: 2.2602 ms. Best GFLOPs: 314.8462
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #9: GFLOPs: 106.7450. Time: 6.6664 ms. Best GFLOPs: 314.8462
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #10: GFLOPs: 62.2978. Time: 11.4226 ms. Best GFLOPs: 314.8462
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #11: GFLOPs: 110.9238. Time: 6.4153 ms. Best GFLOPs: 314.8462
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #12: GFLOPs: 48.2079. Time: 14.7612 ms. Best GFLOPs: 314.8462
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #13: GFLOPs: 83.9556. Time: 8.4760 ms. Best GFLOPs: 314.8462
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #14: GFLOPs: 338.5455. Time: 2.1020 ms. Best GFLOPs: 338.5455
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #15: GFLOPs: 111.2161. Time: 6.3984 ms. Best GFLOPs: 338.5455
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #16: GFLOPs: 110.4441. Time: 6.4431 ms. Best GFLOPs: 338.5455
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #17: GFLOPs: 367.5954. Time: 1.9358 ms. Best GFLOPs: 367.5954
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #18: GFLOPs: 92.0527. Time: 7.7304 ms. Best GFLOPs: 367.5954
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #19: GFLOPs: 82.3945. Time: 8.6366 ms. Best GFLOPs: 367.5954
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #20: GFLOPs: 71.9866. Time: 9.8853 ms. Best GFLOPs: 367.5954
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #21: GFLOPs: 102.5923. Time: 6.9363 ms. Best GFLOPs: 367.5954
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #22: GFLOPs: 51.7856. Time: 13.7414 ms. Best GFLOPs: 367.5954
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #23: GFLOPs: 120.8943. Time: 5.8862 ms. Best GFLOPs: 367.5954
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #24: GFLOPs: 73.3617. Time: 9.7000 ms. Best GFLOPs: 367.5954
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #25: GFLOPs: 10.4309. Time: 68.2210 ms. Best GFLOPs: 367.5954
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #26: GFLOPs: 240.4964. Time: 2.9589 ms. Best GFLOPs: 367.5954
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #27: GFLOPs: 67.3437. Time: 10.5668 ms. Best GFLOPs: 367.5954
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #28: GFLOPs: 278.0848. Time: 2.5590 ms. Best GFLOPs: 367.5954
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #29: GFLOPs: 184.5596. Time: 3.8557 ms. Best GFLOPs: 367.5954
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #30: GFLOPs: 89.4458. Time: 7.9557 ms. Best GFLOPs: 367.5954
[19:44:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"] Trial #31: GFLOPs: 102.3579. Time: 6.9521 ms. Best GFLOPs: 367.5954
[19:44:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |            N/A |          N/A |                   N/A |      0 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1632
Total latency (us): 261922

[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #0: GFLOPs: 1.7901. Time: 3.0935 ms. Best GFLOPs: 1.7901
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #1: GFLOPs: 2.0162. Time: 2.7467 ms. Best GFLOPs: 2.0162
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #2: GFLOPs: 2.1321. Time: 2.5973 ms. Best GFLOPs: 2.1321
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #3: GFLOPs: 1.2775. Time: 4.3350 ms. Best GFLOPs: 2.1321
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #4: GFLOPs: 1.4539. Time: 3.8090 ms. Best GFLOPs: 2.1321
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #5: GFLOPs: 2.1746. Time: 2.5466 ms. Best GFLOPs: 2.1746
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #6: GFLOPs: 1.4111. Time: 3.9245 ms. Best GFLOPs: 2.1746
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #7: GFLOPs: 2.0986. Time: 2.6388 ms. Best GFLOPs: 2.1746
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #8: GFLOPs: 1.4894. Time: 3.7182 ms. Best GFLOPs: 2.1746
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #9: GFLOPs: 2.2066. Time: 2.5096 ms. Best GFLOPs: 2.2066
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #10: GFLOPs: 1.4543. Time: 3.8080 ms. Best GFLOPs: 2.2066
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #11: GFLOPs: 1.5104. Time: 3.6665 ms. Best GFLOPs: 2.2066
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #12: GFLOPs: 1.3972. Time: 3.9636 ms. Best GFLOPs: 2.2066
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #13: GFLOPs: 1.4626. Time: 3.7862 ms. Best GFLOPs: 2.2066
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #14: GFLOPs: 1.3846. Time: 3.9996 ms. Best GFLOPs: 2.2066
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #15: GFLOPs: 1.5170. Time: 3.6504 ms. Best GFLOPs: 2.2066
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #16: GFLOPs: 2.2231. Time: 2.4911 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #17: GFLOPs: 1.5243. Time: 3.6330 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #18: GFLOPs: 1.2660. Time: 4.3743 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #19: GFLOPs: 1.9823. Time: 2.7936 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #20: GFLOPs: 2.1804. Time: 2.5397 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #21: GFLOPs: 1.9720. Time: 2.8082 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #22: GFLOPs: 2.0505. Time: 2.7007 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #23: GFLOPs: 1.4121. Time: 3.9216 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #24: GFLOPs: 1.3844. Time: 4.0000 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #25: GFLOPs: 2.0575. Time: 2.6915 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #26: GFLOPs: 1.2428. Time: 4.4558 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #27: GFLOPs: 1.5103. Time: 3.6666 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #28: GFLOPs: 1.5710. Time: 3.5251 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #29: GFLOPs: 2.1931. Time: 2.5251 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #30: GFLOPs: 2.0388. Time: 2.7162 ms. Best GFLOPs: 2.2231
[19:44:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"] Trial #31: GFLOPs: 1.2394. Time: 4.4683 ms. Best GFLOPs: 2.2231
[19:44:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |            N/A |          N/A |                   N/A |      0 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1664
Total latency (us): 269395

[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #0: GFLOPs: 56.1265. Time: 28.4406 ms. Best GFLOPs: 56.1265
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #1: GFLOPs: 50.4799. Time: 31.6219 ms. Best GFLOPs: 56.1265
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #2: GFLOPs: 144.7940. Time: 11.0244 ms. Best GFLOPs: 144.7940
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #3: GFLOPs: 89.3565. Time: 17.8640 ms. Best GFLOPs: 144.7940
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #4: GFLOPs: 125.2500. Time: 12.7447 ms. Best GFLOPs: 144.7940
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #5: GFLOPs: 116.7248. Time: 13.6755 ms. Best GFLOPs: 144.7940
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #6: GFLOPs: 106.3428. Time: 15.0106 ms. Best GFLOPs: 144.7940
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #7: GFLOPs: 165.0799. Time: 9.6697 ms. Best GFLOPs: 165.0799
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #8: GFLOPs: 71.8721. Time: 22.2099 ms. Best GFLOPs: 165.0799
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #9: GFLOPs: 4.4769. Time: 356.5574 ms. Best GFLOPs: 165.0799
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #10: GFLOPs: 152.1964. Time: 10.4882 ms. Best GFLOPs: 165.0799
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #11: GFLOPs: 11.8711. Time: 134.4663 ms. Best GFLOPs: 165.0799
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #12: GFLOPs: 35.1311. Time: 45.4375 ms. Best GFLOPs: 165.0799
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #13: GFLOPs: 176.4249. Time: 9.0479 ms. Best GFLOPs: 176.4249
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #14: GFLOPs: 122.8613. Time: 12.9924 ms. Best GFLOPs: 176.4249
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #15: GFLOPs: 221.2860. Time: 7.2136 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #16: GFLOPs: 177.2623. Time: 9.0051 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #17: GFLOPs: 208.6188. Time: 7.6516 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #18: GFLOPs: 189.7949. Time: 8.4105 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #19: GFLOPs: 39.7851. Time: 40.1223 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #20: GFLOPs: 19.2210. Time: 83.0483 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #21: GFLOPs: 36.7857. Time: 43.3937 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #22: GFLOPs: 34.0294. Time: 46.9086 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #23: GFLOPs: 80.4979. Time: 19.8299 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #24: GFLOPs: 111.6926. Time: 14.2916 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #25: GFLOPs: 10.2916. Time: 155.1037 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #26: GFLOPs: 56.1277. Time: 28.4400 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #27: GFLOPs: 26.6296. Time: 59.9434 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #28: GFLOPs: 134.5965. Time: 11.8597 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #29: GFLOPs: 120.7892. Time: 13.2153 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #30: GFLOPs: 65.4940. Time: 24.3728 ms. Best GFLOPs: 221.2860
[19:44:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"] Trial #31: GFLOPs: 8.9957. Time: 177.4487 ms. Best GFLOPs: 221.2860
[19:44:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |            N/A |          N/A |                   N/A |      0 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1696
Total latency (us): 276609

[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #0: GFLOPs: 222.4025. Time: 0.7999 ms. Best GFLOPs: 222.4025
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #1: GFLOPs: 160.1227. Time: 1.1110 ms. Best GFLOPs: 222.4025
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #2: GFLOPs: 286.5021. Time: 0.6209 ms. Best GFLOPs: 286.5021
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #3: GFLOPs: 59.9989. Time: 2.9651 ms. Best GFLOPs: 286.5021
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #4: GFLOPs: 230.2721. Time: 0.7726 ms. Best GFLOPs: 286.5021
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #5: GFLOPs: 314.4453. Time: 0.5658 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #6: GFLOPs: 162.5047. Time: 1.0947 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #7: GFLOPs: 83.4508. Time: 2.1318 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #8: GFLOPs: 303.2792. Time: 0.5866 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #9: GFLOPs: 98.9308. Time: 1.7982 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #10: GFLOPs: 186.9434. Time: 0.9516 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #11: GFLOPs: 47.7785. Time: 3.7235 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #12: GFLOPs: 266.8677. Time: 0.6666 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #13: GFLOPs: 304.7186. Time: 0.5838 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #14: GFLOPs: 74.6649. Time: 2.3827 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i3_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(8, 2, 2, 13, 26):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 8 + i1_2_init)
                        oh = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 52 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 13 + i2_3_init)
                        ow = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 52 + i3_2_init * 26 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [16, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(128, 1, 1, 1, 8, 1, 2, 2, 1, 1, 1, 1, 1, 13, 26):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 8 + i1_2)
                        oh = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 52 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 13 + i2_3)
                        ow = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 52 + i3_2 * 26 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(128, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [16, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(104):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 104)
                        ax2 = T.axis.spatial(104, i0_i1_i2_fused % 104)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 2, 8, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 26])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
l93 = sch.fuse(l91)
sch.vectorize(loop=l93)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b65)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #16: GFLOPs: 11.8775. Time: 14.9781 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #17: GFLOPs: 62.4843. Time: 2.8471 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #18: GFLOPs: 191.1971. Time: 0.9305 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #19: GFLOPs: 105.7689. Time: 1.6820 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #20: GFLOPs: 82.2556. Time: 2.1628 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #21: GFLOPs: 118.4172. Time: 1.5023 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #22: GFLOPs: 248.7774. Time: 0.7151 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #23: GFLOPs: 145.5578. Time: 1.2222 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #24: GFLOPs: 43.5099. Time: 4.0888 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #25: GFLOPs: 300.2696. Time: 0.5925 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #26: GFLOPs: 37.5476. Time: 4.7380 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #27: GFLOPs: 45.7823. Time: 3.8858 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #28: GFLOPs: 84.0737. Time: 2.1160 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #29: GFLOPs: 61.9673. Time: 2.8709 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #30: GFLOPs: 125.4375. Time: 1.4182 ms. Best GFLOPs: 314.4453
[19:44:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"] Trial #31: GFLOPs: 106.0040. Time: 1.6783 ms. Best GFLOPs: 314.4453
[19:44:09] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |            N/A |          N/A |                   N/A |      0 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1728
Total latency (us): 277741

[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #0: GFLOPs: 1.4565. Time: 0.9505 ms. Best GFLOPs: 1.4565
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #1: GFLOPs: 2.0411. Time: 0.6783 ms. Best GFLOPs: 2.0411
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #2: GFLOPs: 2.1704. Time: 0.6379 ms. Best GFLOPs: 2.1704
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #3: GFLOPs: 2.2014. Time: 0.6289 ms. Best GFLOPs: 2.2014
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #4: GFLOPs: 1.9743. Time: 0.7013 ms. Best GFLOPs: 2.2014
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #5: GFLOPs: 2.2536. Time: 0.6143 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #6: GFLOPs: 2.0369. Time: 0.6797 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #7: GFLOPs: 1.4976. Time: 0.9244 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #8: GFLOPs: 1.5002. Time: 0.9228 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #9: GFLOPs: 1.9590. Time: 0.7067 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #10: GFLOPs: 2.1202. Time: 0.6530 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #11: GFLOPs: 1.4999. Time: 0.9230 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #12: GFLOPs: 2.0558. Time: 0.6734 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #13: GFLOPs: 2.1369. Time: 0.6479 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #14: GFLOPs: 2.0381. Time: 0.6793 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #15: GFLOPs: 1.9867. Time: 0.6968 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #16: GFLOPs: 1.7092. Time: 0.8100 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #17: GFLOPs: 1.4562. Time: 0.9507 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #18: GFLOPs: 1.9268. Time: 0.7185 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #19: GFLOPs: 1.3859. Time: 0.9990 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #20: GFLOPs: 2.0206. Time: 0.6851 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #21: GFLOPs: 1.9933. Time: 0.6946 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #22: GFLOPs: 2.0356. Time: 0.6801 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #23: GFLOPs: 2.1632. Time: 0.6400 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #24: GFLOPs: 1.9524. Time: 0.7091 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #25: GFLOPs: 1.8653. Time: 0.7422 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #26: GFLOPs: 1.9387. Time: 0.7141 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #27: GFLOPs: 1.4028. Time: 0.9869 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #28: GFLOPs: 1.5242. Time: 0.9083 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #29: GFLOPs: 2.2152. Time: 0.6250 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #30: GFLOPs: 1.4946. Time: 0.9263 ms. Best GFLOPs: 2.2536
[19:44:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"] Trial #31: GFLOPs: 2.2742. Time: 0.6088 ms. Best GFLOPs: 2.2742
[19:44:11] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |            N/A |          N/A |                   N/A |      0 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1760
Total latency (us): 279567

[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #0: GFLOPs: 8.7573. Time: 91.1388 ms. Best GFLOPs: 8.7573
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #1: GFLOPs: 114.0945. Time: 6.9954 ms. Best GFLOPs: 114.0945
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #2: GFLOPs: 46.3737. Time: 17.2109 ms. Best GFLOPs: 114.0945
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #3: GFLOPs: 7.0714. Time: 112.8681 ms. Best GFLOPs: 114.0945
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #4: GFLOPs: 68.0194. Time: 11.7339 ms. Best GFLOPs: 114.0945
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #5: GFLOPs: 170.9614. Time: 4.6685 ms. Best GFLOPs: 170.9614
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #6: GFLOPs: 68.4041. Time: 11.6679 ms. Best GFLOPs: 170.9614
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #7: GFLOPs: 72.8311. Time: 10.9587 ms. Best GFLOPs: 170.9614
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #8: GFLOPs: 22.5883. Time: 35.3339 ms. Best GFLOPs: 170.9614
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #9: GFLOPs: 16.6046. Time: 48.0670 ms. Best GFLOPs: 170.9614
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #10: GFLOPs: 204.7129. Time: 3.8988 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #11: GFLOPs: 179.1629. Time: 4.4548 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #12: GFLOPs: 119.2734. Time: 6.6916 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #13: GFLOPs: 11.4809. Time: 69.5183 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #14: GFLOPs: 155.6632. Time: 5.1273 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #15: GFLOPs: 153.0231. Time: 5.2158 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #16: GFLOPs: 165.3513. Time: 4.8269 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #17: GFLOPs: 17.3331. Time: 46.0469 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #18: GFLOPs: 110.9145. Time: 7.1959 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #19: GFLOPs: 79.4425. Time: 10.0467 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #20: GFLOPs: 33.8945. Time: 23.5476 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #21: GFLOPs: 71.3324. Time: 11.1889 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #22: GFLOPs: 78.3044. Time: 10.1927 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #23: GFLOPs: 181.7393. Time: 4.3916 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #24: GFLOPs: 159.5245. Time: 5.0032 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #25: GFLOPs: 106.1150. Time: 7.5214 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #26: GFLOPs: 139.2206. Time: 5.7329 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #27: GFLOPs: 77.0653. Time: 10.3566 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #28: GFLOPs: 107.0914. Time: 7.4528 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #29: GFLOPs: 86.3131. Time: 9.2470 ms. Best GFLOPs: 204.7129
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #30: GFLOPs: 391.8432. Time: 2.0369 ms. Best GFLOPs: 391.8432
[19:44:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"] Trial #31: GFLOPs: 67.2692. Time: 11.8648 ms. Best GFLOPs: 391.8432
[19:44:13] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |            N/A |          N/A |                   N/A |      0 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1792
Total latency (us): 283641

[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #0: GFLOPs: 2.4167. Time: 0.8593 ms. Best GFLOPs: 2.4167
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #1: GFLOPs: 2.8555. Time: 0.7273 ms. Best GFLOPs: 2.8555
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #2: GFLOPs: 2.8272. Time: 0.7345 ms. Best GFLOPs: 2.8555
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #3: GFLOPs: 3.0634. Time: 0.6779 ms. Best GFLOPs: 3.0634
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #4: GFLOPs: 1.6448. Time: 1.2626 ms. Best GFLOPs: 3.0634
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #5: GFLOPs: 2.9120. Time: 0.7132 ms. Best GFLOPs: 3.0634
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #6: GFLOPs: 3.0792. Time: 0.6744 ms. Best GFLOPs: 3.0792
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #7: GFLOPs: 2.8796. Time: 0.7212 ms. Best GFLOPs: 3.0792
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #8: GFLOPs: 1.9716. Time: 1.0533 ms. Best GFLOPs: 3.0792
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #9: GFLOPs: 3.2133. Time: 0.6463 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #10: GFLOPs: 2.9369. Time: 0.7071 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #11: GFLOPs: 2.7529. Time: 0.7543 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #12: GFLOPs: 3.1151. Time: 0.6667 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #13: GFLOPs: 3.0836. Time: 0.6735 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #14: GFLOPs: 2.6693. Time: 0.7780 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #15: GFLOPs: 2.9187. Time: 0.7115 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 104, 104, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(104):
                for i4_fused in T.vectorized(4):
                    with T.block("T_exp"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 104)
                        ax2 = T.axis.spatial(104, i0_i1_i2_fused % 104)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                        T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(6656, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(104):
                with T.block("T_layout_trans"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(64, i0_i1_i2_fused // 104)
                    ax2 = T.axis.spatial(104, i0_i1_i2_fused % 104)
                    ax3 = T.axis.spatial(104, i3)
                    T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 104 and ax3 < 104, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0_i1_i2_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3, i4 in T.grid(104, 4):
                with T.block("T_add_1"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(16, i0_i1_i2_fused // 104)
                    ax2 = T.axis.spatial(104, i0_i1_i2_fused % 104)
                    ax3, ax4 = T.axis.remap("SS", [i3, i4])
                    T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4], placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1 * 4 + ax4, ax2, ax3])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = placeholder_2[ax0, ax1, ax2, ax3, ax4] + placeholder[ax0, ax1, ax2, ax3, ax4] * T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 64 and ax2 < 104 and ax3 < 104, T.tanh(T.log(placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1 * 4 + ax4, ax2, ax3], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v8 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v8)
l9 = sch.sample_compute_location(block=b5, decision=-2)
sch.compute_at(block=b5, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l10, preserve_unit_loops=True)
l11 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l11, preserve_unit_loops=True)
sch.enter_postproc()
b12 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b12, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b12, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b12, ann_key="meta_schedule.unroll_explicit")
b13, b14, b15 = sch.get_child_blocks(b12)
l16, l17, l18, l19, l20 = sch.get_loops(block=b13)
l21 = sch.fuse(l16, l17, l18)
sch.parallel(loop=l21)
l22 = sch.fuse(l20)
sch.vectorize(loop=l22)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l23, l24, l25, l26 = sch.get_loops(block=b14)
l27 = sch.fuse(l23, l24, l25)
sch.parallel(loop=l27)
sch.annotate(block_or_loop=l27, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l27, ann_key="pragma_unroll_explicit", ann_val=1)
l28, l29, l30, l31, l32 = sch.get_loops(block=b15)
l33 = sch.fuse(l28, l29, l30)
sch.parallel(loop=l33)
sch.annotate(block_or_loop=l33, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l33, ann_key="pragma_unroll_explicit", ann_val=1)
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #17: GFLOPs: 2.9177. Time: 0.7117 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #18: GFLOPs: 3.1014. Time: 0.6696 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 104, 104, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        for i0_i1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 104, 104):
                for ax0_1, ax1_1, ax2_1, ax3_1, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_exp"):
                        ax0_2 = T.axis.spatial(1, 0)
                        ax1_2, ax2_2, ax3_2, ax4_1 = T.axis.remap("SSSS", [i0_i1_fused, ax2, ax3, ax1])
                        T.reads(placeholder[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1])
                        T.writes(T_exp[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1])
                        T_exp[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1] = T.exp(placeholder[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1], dtype="float32")
                with T.block("T_layout_trans"):
                    ax0_3 = T.axis.spatial(1, 0)
                    ax1_3 = T.axis.spatial(64, i0_i1_fused * 4 + ax1)
                    ax2_3, ax3_3 = T.axis.remap("SS", [ax2, ax3])
                    T.reads(T_exp[ax0_3, ax1_3 // 4, ax2_3, ax3_3, ax1_3 % 4])
                    T.writes(T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3])
                    T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3] = T.if_then_else(ax0_3 < 1 and ax1_3 < 64 and ax2_3 < 104 and ax3_3 < 104, T_exp[ax0_3, ax1_3 // 4, ax2_3, ax3_3, ax1_3 % 4], T.float32(0), dtype="float32")
            for i2, i3, i4 in T.grid(104, 104, 4):
                with T.block("T_add_1"):
                    ax0_4 = T.axis.spatial(1, 0)
                    ax1_4, ax2_4, ax3_4, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads(placeholder_2[ax0_4, ax1_4, ax2_4, ax3_4, ax4], placeholder[ax0_4, ax1_4, ax2_4, ax3_4, ax4], placeholder_1[ax0_4, 0, 0, 0], T_layout_trans[ax0_4, ax1_4 * 4 + ax4, ax2_4, ax3_4])
                    T.writes(T_add[ax0_4, ax1_4, ax2_4, ax3_4, ax4])
                    T_add[ax0_4, ax1_4, ax2_4, ax3_4, ax4] = placeholder_2[ax0_4, ax1_4, ax2_4, ax3_4, ax4] + placeholder[ax0_4, ax1_4, ax2_4, ax3_4, ax4] * T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4 < 64 and ax2_4 < 104 and ax3_4 < 104, T.tanh(T.log(placeholder_1[ax0_4, 0, 0, 0] + T_layout_trans[ax0_4, ax1_4 * 4 + ax4, ax2_4, ax3_4], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v8 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v8)
l9 = sch.sample_compute_location(block=b5, decision=-2)
sch.compute_at(block=b5, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l10, preserve_unit_loops=True)
l11 = sch.sample_compute_location(block=b0, decision=5)
sch.compute_at(block=b0, loop=l11, preserve_unit_loops=True)
sch.enter_postproc()
b12 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b12, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b12, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b12, ann_key="meta_schedule.unroll_explicit")
b13, b14, b15 = sch.get_child_blocks(b12)
l16, l17, l18, l19, l20, l21, l22, l23, l24, l25, l26 = sch.get_loops(block=b13)
l27 = sch.fuse(l16, l17)
sch.parallel(loop=l27)
sch.annotate(block_or_loop=l27, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l27, ann_key="pragma_unroll_explicit", ann_val=1)
l28, l29, l30, l31, l32 = sch.get_loops(block=b14)
sch.annotate(block_or_loop=l28, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l28, ann_key="pragma_unroll_explicit", ann_val=1)
l33, l34, l35, l36 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l33, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l33, ann_key="pragma_unroll_explicit", ann_val=1)
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #20: GFLOPs: 1.7035. Time: 1.2191 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #21: GFLOPs: 2.2046. Time: 0.9420 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #22: GFLOPs: 2.8551. Time: 0.7274 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #23: GFLOPs: 2.9074. Time: 0.7143 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 104, 104, 4), "float32"], T_add: T.Buffer[(1, 16, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        for i0_i1_fused in T.parallel(16, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 104, 104):
                for ax0_1, ax1_1, ax2_1, ax3_1, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_exp"):
                        ax0_2 = T.axis.spatial(1, 0)
                        ax1_2, ax2_2, ax3_2, ax4_1 = T.axis.remap("SSSS", [i0_i1_fused, ax2, ax3, ax1])
                        T.reads(placeholder[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1])
                        T.writes(T_exp[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1])
                        T_exp[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1] = T.exp(placeholder[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1], dtype="float32")
                with T.block("T_layout_trans"):
                    ax0_3 = T.axis.spatial(1, 0)
                    ax1_3 = T.axis.spatial(64, i0_i1_fused * 4 + ax1)
                    ax2_3, ax3_3 = T.axis.remap("SS", [ax2, ax3])
                    T.reads(T_exp[ax0_3, ax1_3 // 4, ax2_3, ax3_3, ax1_3 % 4])
                    T.writes(T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3])
                    T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3] = T.if_then_else(ax0_3 < 1 and ax1_3 < 64 and ax2_3 < 104 and ax3_3 < 104, T_exp[ax0_3, ax1_3 // 4, ax2_3, ax3_3, ax1_3 % 4], T.float32(0), dtype="float32")
            for i2, i3, i4 in T.grid(104, 104, 4):
                with T.block("T_add_1"):
                    ax0_4 = T.axis.spatial(1, 0)
                    ax1_4, ax2_4, ax3_4, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads(placeholder_2[ax0_4, ax1_4, ax2_4, ax3_4, ax4], placeholder[ax0_4, ax1_4, ax2_4, ax3_4, ax4], placeholder_1[ax0_4, 0, 0, 0], T_layout_trans[ax0_4, ax1_4 * 4 + ax4, ax2_4, ax3_4])
                    T.writes(T_add[ax0_4, ax1_4, ax2_4, ax3_4, ax4])
                    T_add[ax0_4, ax1_4, ax2_4, ax3_4, ax4] = placeholder_2[ax0_4, ax1_4, ax2_4, ax3_4, ax4] + placeholder[ax0_4, ax1_4, ax2_4, ax3_4, ax4] * T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4 < 64 and ax2_4 < 104 and ax3_4 < 104, T.tanh(T.log(placeholder_1[ax0_4, 0, 0, 0] + T_layout_trans[ax0_4, ax1_4 * 4 + ax4, ax2_4, ax3_4], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v8 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v8)
l9 = sch.sample_compute_location(block=b5, decision=-2)
sch.compute_at(block=b5, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l10, preserve_unit_loops=True)
l11 = sch.sample_compute_location(block=b0, decision=5)
sch.compute_at(block=b0, loop=l11, preserve_unit_loops=True)
sch.enter_postproc()
b12 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b12, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b12, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b12, ann_key="meta_schedule.unroll_explicit")
b13, b14, b15 = sch.get_child_blocks(b12)
l16, l17, l18, l19, l20, l21, l22, l23, l24, l25, l26 = sch.get_loops(block=b13)
l27 = sch.fuse(l16, l17)
sch.parallel(loop=l27)
sch.annotate(block_or_loop=l27, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l27, ann_key="pragma_unroll_explicit", ann_val=1)
l28, l29, l30, l31, l32 = sch.get_loops(block=b14)
sch.annotate(block_or_loop=l28, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l28, ann_key="pragma_unroll_explicit", ann_val=1)
l33, l34, l35, l36 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l33, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l33, ann_key="pragma_unroll_explicit", ann_val=1)
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #25: GFLOPs: 3.0218. Time: 0.6872 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #26: GFLOPs: 2.9045. Time: 0.7150 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #27: GFLOPs: 2.9863. Time: 0.6954 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #28: GFLOPs: 3.0743. Time: 0.6755 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #29: GFLOPs: 2.5413. Time: 0.8172 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #30: GFLOPs: 2.2143. Time: 0.9379 ms. Best GFLOPs: 3.2133
[19:44:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"] Trial #31: GFLOPs: 1.1806. Time: 1.7590 ms. Best GFLOPs: 3.2133
[19:44:16] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |            N/A |          N/A |                   N/A |      0 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1824
Total latency (us): 284933

[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #0: GFLOPs: 82.7608. Time: 1.0790 ms. Best GFLOPs: 82.7608
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #1: GFLOPs: 344.0658. Time: 0.2595 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #2: GFLOPs: 65.7580. Time: 1.3580 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #3: GFLOPs: 134.3499. Time: 0.6647 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #4: GFLOPs: 93.3742. Time: 0.9563 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #5: GFLOPs: 49.5201. Time: 1.8032 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #6: GFLOPs: 261.4163. Time: 0.3416 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #7: GFLOPs: 59.0733. Time: 1.5116 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #8: GFLOPs: 61.2102. Time: 1.4589 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #9: GFLOPs: 63.0116. Time: 1.4172 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #10: GFLOPs: 74.5243. Time: 1.1982 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #11: GFLOPs: 78.0434. Time: 1.1442 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #12: GFLOPs: 160.7874. Time: 0.5554 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #13: GFLOPs: 68.5855. Time: 1.3020 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #14: GFLOPs: 187.5832. Time: 0.4760 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #15: GFLOPs: 50.9478. Time: 1.7527 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #16: GFLOPs: 138.5223. Time: 0.6446 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #17: GFLOPs: 136.9953. Time: 0.6518 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #18: GFLOPs: 88.1425. Time: 1.0131 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #19: GFLOPs: 87.8766. Time: 1.0162 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #20: GFLOPs: 68.1276. Time: 1.3107 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #21: GFLOPs: 64.0343. Time: 1.3945 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #22: GFLOPs: 101.0964. Time: 0.8833 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #23: GFLOPs: 157.3823. Time: 0.5674 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #24: GFLOPs: 63.2420. Time: 1.4120 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #25: GFLOPs: 107.4050. Time: 0.8314 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #26: GFLOPs: 176.6794. Time: 0.5054 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #27: GFLOPs: 238.6597. Time: 0.3742 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #28: GFLOPs: 157.0956. Time: 0.5684 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #29: GFLOPs: 103.9013. Time: 0.8594 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #30: GFLOPs: 45.6632. Time: 1.9556 ms. Best GFLOPs: 344.0658
[19:44:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"] Trial #31: GFLOPs: 187.2276. Time: 0.4769 ms. Best GFLOPs: 344.0658
[19:44:18] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1856
Total latency (us): 285712

[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #0: GFLOPs: 1.6298. Time: 1.6989 ms. Best GFLOPs: 1.6298
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #1: GFLOPs: 1.7266. Time: 1.6036 ms. Best GFLOPs: 1.7266
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #2: GFLOPs: 1.0148. Time: 2.7284 ms. Best GFLOPs: 1.7266
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #3: GFLOPs: 1.6288. Time: 1.7000 ms. Best GFLOPs: 1.7266
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #4: GFLOPs: 1.3319. Time: 2.0790 ms. Best GFLOPs: 1.7266
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #5: GFLOPs: 1.2861. Time: 2.1530 ms. Best GFLOPs: 1.7266
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #6: GFLOPs: 1.6852. Time: 1.6431 ms. Best GFLOPs: 1.7266
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #7: GFLOPs: 1.6307. Time: 1.6980 ms. Best GFLOPs: 1.7266
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #8: GFLOPs: 1.6401. Time: 1.6883 ms. Best GFLOPs: 1.7266
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #9: GFLOPs: 1.6206. Time: 1.7086 ms. Best GFLOPs: 1.7266
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #10: GFLOPs: 1.6034. Time: 1.7268 ms. Best GFLOPs: 1.7266
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #11: GFLOPs: 1.7559. Time: 1.5769 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #12: GFLOPs: 1.5816. Time: 1.7507 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #13: GFLOPs: 1.6187. Time: 1.7106 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #14: GFLOPs: 1.4509. Time: 1.9084 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #15: GFLOPs: 1.2595. Time: 2.1984 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #16: GFLOPs: 1.6568. Time: 1.6712 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #17: GFLOPs: 1.7384. Time: 1.5928 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #18: GFLOPs: 1.6106. Time: 1.7191 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #19: GFLOPs: 1.4131. Time: 1.9595 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #20: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 104, 104, 4), "float32"], T_concat: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(104):
                for i4_fused in T.vectorized(4):
                    with T.block("T_exp"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(16, i0_i1_i2_fused // 104)
                        ax2 = T.axis.spatial(104, i0_i1_i2_fused % 104)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                        T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder_2[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(3328, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 104):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_layout_trans_3"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(16, i0_i1_i2_fused // 104)
                        ax2_1 = T.axis.spatial(104, i0_i1_i2_fused % 104)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.where(i0_i1_i2_fused % 3328 // 104 < 16)
                        T.reads(placeholder_1[ax0_1, 0, 0, 0], placeholder[ax0_1, ax4 // 4 + ax1_1, ax2_1, ax3_1, ax4 % 4])
                        T.writes(T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_layout_trans_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.if_then_else(ax0_1 < 1 and ax1_1 * 4 + ax4 < 64 and ax2_1 < 104 and ax3_1 < 104, T.tanh(T.log(placeholder_1[ax0_1, 0, 0, 0] + T.if_then_else(ax0_1 < 1 and ax1_1 * 4 + ax4 < 64 and ax2_1 < 104 and ax3_1 < 104, T.exp(placeholder[ax0_1, (ax1_1 * 4 + ax4) // 4, ax2_1, ax3_1, (ax1_1 * 4 + ax4) % 4], dtype="float32"), T.float32(0), dtype="float32"), dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 104):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_layout_trans_1"):
                        ax0_2 = T.axis.spatial(1, 0)
                        ax1_2 = T.axis.spatial(16, i0_i1_i2_fused // 104 + -16)
                        ax2_2 = T.axis.spatial(104, i0_i1_i2_fused % 104)
                        ax3_2, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.where(16 <= i0_i1_i2_fused % 3328 // 104)
                        T.reads(placeholder_1[ax0_2, 0, 0, 0], T_exp[ax0_2, ax4 // 4 + ax1_2, ax2_2, ax3_2, ax4 % 4])
                        T.writes(T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_2, ax4])
                        T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_2, ax4] = T.if_then_else(ax0_2 < 1 and ax1_2 * 4 + ax4 < 64 and ax2_2 < 104 and ax3_2 < 104, T.tanh(T.log(placeholder_1[ax0_2, 0, 0, 0] + T.if_then_else(ax0_2 < 1 and ax1_2 * 4 + ax4 < 64 and ax2_2 < 104 and ax3_2 < 104, T_exp[ax0_2, (ax1_2 * 4 + ax4) // 4, ax2_2, ax3_2, (ax1_2 * 4 + ax4) % 4], T.float32(0), dtype="float32"), dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
            for i3 in T.serial(104):
                for i4_fused in T.vectorized(4):
                    with T.block("T_concat"):
                        ax0_3 = T.axis.spatial(1, 0)
                        ax1_3 = T.axis.spatial(32, i0_i1_i2_fused // 104)
                        ax2_3 = T.axis.spatial(104, i0_i1_i2_fused % 104)
                        ax3_3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder_2[ax0_3, ax1_3 - 16, ax2_3, ax3_3, ax4], T_layout_trans[ax0_3, ax1_3 - 16, ax2_3, ax3_3, ax4], placeholder[ax0_3, ax1_3, ax2_3, ax3_3, ax4], T_layout_trans_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4])
                        T.writes(T_concat[ax0_3, ax1_3, ax2_3, ax3_3, ax4])
                        T_concat[ax0_3, ax1_3, ax2_3, ax3_3, ax4] = T.if_then_else(16 <= ax1_3, placeholder_2[ax0_3, ax1_3 - 16, ax2_3, ax3_3, ax4] * T_layout_trans[ax0_3, ax1_3 - 16, ax2_3, ax3_3, ax4], placeholder[ax0_3, ax1_3, ax2_3, ax3_3, ax4] * T_layout_trans_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4], dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="T_exp_1", func_name="main")
b8 = sch.get_block(name="T_layout_trans_2", func_name="main")
b9 = sch.get_block(name="T_add_1", func_name="main")
b10 = sch.get_block(name="T_log_1", func_name="main")
b11 = sch.get_block(name="T_tanh_1", func_name="main")
b12 = sch.get_block(name="T_layout_trans_3", func_name="main")
b13 = sch.get_block(name="T_multiply_1", func_name="main")
b14 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b13)
sch.compute_inline(block=b11)
sch.compute_inline(block=b10)
sch.compute_inline(block=b9)
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
l16 = sch.sample_compute_location(block=b12, decision=2)
sch.compute_at(block=b12, loop=l16, preserve_unit_loops=True)
l17 = sch.sample_compute_location(block=b8, decision=-2)
sch.compute_at(block=b8, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b7, decision=-2)
sch.compute_at(block=b7, loop=l18, preserve_unit_loops=True)
l19 = sch.sample_compute_location(block=b5, decision=2)
sch.compute_at(block=b5, loop=l19, preserve_unit_loops=True)
l20 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l21, preserve_unit_loops=True)
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24, b25, b26 = sch.get_child_blocks(b22)
l27, l28, l29, l30, l31 = sch.get_loops(block=b23)
l32 = sch.fuse(l27, l28, l29)
sch.parallel(loop=l32)
l33 = sch.fuse(l31)
sch.vectorize(loop=l33)
sch.annotate(block_or_loop=l32, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l32, ann_key="pragma_unroll_explicit", ann_val=1)
l34, l35, l36, l37, l38, l39, l40, l41 = sch.get_loops(block=b24)
l42 = sch.fuse(l34, l35, l36)
sch.parallel(loop=l42)
l43 = sch.fuse(l41)
sch.vectorize(loop=l43)
sch.annotate(block_or_loop=l42, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l42, ann_key="pragma_unroll_explicit", ann_val=1)
l44, l45, l46, l47, l48, l49 = sch.get_loops(block=b25)
l50 = sch.fuse(l49)
sch.vectorize(loop=l50)
sch.annotate(block_or_loop=l44, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l44, ann_key="pragma_unroll_explicit", ann_val=1)
l51, l52, l53 = sch.get_loops(block=b26)
l54 = sch.fuse(l53)
sch.vectorize(loop=l54)
sch.annotate(block_or_loop=l51, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l51, ann_key="pragma_unroll_explicit", ann_val=1)
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #21: GFLOPs: 1.7471. Time: 1.5849 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #22: GFLOPs: 1.2214. Time: 2.2669 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #23: GFLOPs: 1.5340. Time: 1.8050 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #24: GFLOPs: 1.6379. Time: 1.6906 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #25: GFLOPs: 1.3634. Time: 2.0309 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #26: GFLOPs: 1.6130. Time: 1.7166 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #27: GFLOPs: 1.1867. Time: 2.3333 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #28: GFLOPs: 1.7343. Time: 1.5965 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #29: GFLOPs: 1.6613. Time: 1.6667 ms. Best GFLOPs: 1.7559
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 104, 104, 4), "float32"], T_concat: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_exp_1 = T.alloc_buffer([1, 16, 104, 104, 4], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        for i0_i1_fused in T.parallel(64, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 104, 104, 1):
                with T.block("T_exp_1"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(16, i0_i1_fused // 4)
                    ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                    ax4_1 = T.axis.spatial(4, i0_i1_fused % 4)
                    T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T.writes(T_exp_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_exp_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.exp(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], dtype="float32")
            for i2, i3 in T.grid(104, 104):
                with T.block("T_layout_trans_2"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3 = T.axis.remap("SSS", [i0_i1_fused, i2, i3])
                    T.reads(T_exp_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3])
                    T_layout_trans_1[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 64 and ax2 < 104 and ax3 < 104, T_exp_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0_i1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 104, 104):
                for ax0_2, ax1_2, ax2_2 in T.grid(1, 1, 1):
                    for ax3_ax4_fused in T.vectorized(4):
                        with T.block("T_exp"):
                            ax0_3 = T.axis.spatial(1, 0)
                            ax1_3 = T.axis.spatial(16, i0_i1_fused + -16)
                            ax2_3, ax3_2, ax4 = T.axis.remap("SSS", [ax2, ax3, ax3_ax4_fused])
                            T.where(16 <= i0_i1_fused % 32)
                            T.reads(placeholder_2[ax0_3, ax1_3, ax2_3, ax3_2, ax4])
                            T.writes(T_exp[ax0_3, ax1_3, ax2_3, ax3_2, ax4])
                            T_exp[ax0_3, ax1_3, ax2_3, ax3_2, ax4] = T.exp(placeholder_2[ax0_3, ax1_3, ax2_3, ax3_2, ax4], dtype="float32")
                for ax4_fused in T.vectorized(4):
                    with T.block("T_layout_trans_1"):
                        ax0_4 = T.axis.spatial(1, 0)
                        ax1_4 = T.axis.spatial(16, i0_i1_fused + -16)
                        ax2_4, ax3_3, ax4 = T.axis.remap("SSS", [ax2, ax3, ax4_fused])
                        T.where(16 <= i0_i1_fused % 32)
                        T.reads(placeholder_1[ax0_4, 0, 0, 0], T_exp[ax0_4, ax4 // 4 + ax1_4, ax2_4, ax3_3, ax4 % 4])
                        T.writes(T_layout_trans[ax0_4, ax1_4, ax2_4, ax3_3, ax4])
                        T_layout_trans[ax0_4, ax1_4, ax2_4, ax3_3, ax4] = T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4 < 64 and ax2_4 < 104 and ax3_3 < 104, T.tanh(T.log(placeholder_1[ax0_4, 0, 0, 0] + T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4 < 64 and ax2_4 < 104 and ax3_3 < 104, T_exp[ax0_4, (ax1_4 * 4 + ax4) // 4, ax2_4, ax3_3, (ax1_4 * 4 + ax4) % 4], T.float32(0), dtype="float32"), dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
            for i2, i3, i4 in T.grid(104, 104, 4):
                with T.block("T_concat"):
                    ax0_5 = T.axis.spatial(1, 0)
                    ax1_5, ax2_5, ax3_4, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads(placeholder_2[ax0_5, ax1_5 - 16, ax2_5, ax3_4, ax4], T_layout_trans[ax0_5, ax1_5 - 16, ax2_5, ax3_4, ax4], placeholder[ax0_5, ax1_5, ax2_5, ax3_4, ax4], placeholder_1[ax0_5, 0, 0, 0], T_layout_trans_1[ax0_5, ax1_5 * 4 + ax4, ax2_5, ax3_4])
                    T.writes(T_concat[ax0_5, ax1_5, ax2_5, ax3_4, ax4])
                    T_concat[ax0_5, ax1_5, ax2_5, ax3_4, ax4] = T.if_then_else(16 <= ax1_5, placeholder_2[ax0_5, ax1_5 - 16, ax2_5, ax3_4, ax4] * T_layout_trans[ax0_5, ax1_5 - 16, ax2_5, ax3_4, ax4], placeholder[ax0_5, ax1_5, ax2_5, ax3_4, ax4] * T.if_then_else(ax0_5 < 1 and ax1_5 * 4 + ax4 < 64 and ax2_5 < 104 and ax3_4 < 104, T.tanh(T.log(placeholder_1[ax0_5, 0, 0, 0] + T_layout_trans_1[ax0_5, ax1_5 * 4 + ax4, ax2_5, ax3_4], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32"), dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="T_exp_1", func_name="main")
b8 = sch.get_block(name="T_layout_trans_2", func_name="main")
b9 = sch.get_block(name="T_add_1", func_name="main")
b10 = sch.get_block(name="T_log_1", func_name="main")
b11 = sch.get_block(name="T_tanh_1", func_name="main")
b12 = sch.get_block(name="T_layout_trans_3", func_name="main")
b13 = sch.get_block(name="T_multiply_1", func_name="main")
b14 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b13)
sch.compute_inline(block=b11)
sch.compute_inline(block=b10)
sch.compute_inline(block=b9)
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
l16 = sch.sample_compute_location(block=b12, decision=-2)
sch.compute_at(block=b12, loop=l16, preserve_unit_loops=True)
l17 = sch.sample_compute_location(block=b8, decision=-1)
sch.compute_at(block=b8, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b7, decision=1)
sch.compute_at(block=b7, loop=l18, preserve_unit_loops=True)
l19 = sch.sample_compute_location(block=b5, decision=1)
sch.compute_at(block=b5, loop=l19, preserve_unit_loops=True)
l20 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b0, decision=5)
sch.compute_at(block=b0, loop=l21, preserve_unit_loops=True)
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24, b25, b26, b27 = sch.get_child_blocks(b22)
l28, l29, l30, l31, l32, l33, l34 = sch.get_loops(block=b23)
l35 = sch.fuse(l28, l29)
sch.parallel(loop=l35)
sch.annotate(block_or_loop=l35, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l35, ann_key="pragma_unroll_explicit", ann_val=1)
l36, l37, l38 = sch.get_loops(block=b24)
sch.annotate(block_or_loop=l36, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l36, ann_key="pragma_unroll_explicit", ann_val=1)
l39, l40, l41, l42, l43, l44, l45, l46, l47, l48, l49 = sch.get_loops(block=b25)
l50 = sch.fuse(l39, l40)
sch.parallel(loop=l50)
l51 = sch.fuse(l48, l49)
sch.vectorize(loop=l51)
sch.annotate(block_or_loop=l50, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l50, ann_key="pragma_unroll_explicit", ann_val=1)
l52, l53, l54, l55, l56, l57 = sch.get_loops(block=b26)
l58 = sch.fuse(l57)
sch.vectorize(loop=l58)
sch.annotate(block_or_loop=l52, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l52, ann_key="pragma_unroll_explicit", ann_val=1)
l59, l60, l61, l62 = sch.get_loops(block=b27)
sch.annotate(block_or_loop=l59, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l59, ann_key="pragma_unroll_explicit", ann_val=1)
[19:44:18] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"] Trial #31: GFLOPs: 1.7625. Time: 1.5710 ms. Best GFLOPs: 1.7625
[19:44:20] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1888
Total latency (us): 287283

[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #0: GFLOPs: 94.2871. Time: 3.7736 ms. Best GFLOPs: 94.2871
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #1: GFLOPs: 355.8567. Time: 0.9998 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #2: GFLOPs: 196.6049. Time: 1.8097 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #3: GFLOPs: 119.0231. Time: 2.9894 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #4: GFLOPs: 140.2082. Time: 2.5377 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #5: GFLOPs: 177.0704. Time: 2.0094 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #6: GFLOPs: 161.9789. Time: 2.1966 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #7: GFLOPs: 15.1173. Time: 23.5362 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #8: GFLOPs: 24.9620. Time: 14.2538 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(8, 2, 2, 2, 26, 13):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 8 + i1_2_init)
                        oh = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 52 + i2_2_init * 26 + i2_3_init)
                        ow = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 26 + i3_2_init * 13 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(4, 1, 1, 1, 8, 2, 2, 2, 32, 1, 1, 1, 1, 26, 13):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 16 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 8 * 8 + i1_2)
                        oh = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 52 + i2_2 * 26 + i2_3)
                        ow = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 26 + i3_2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(128, i5_0 * 32 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(3328, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(104):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(32, i0_i1_i2_fused // 104)
                        ax2 = T.axis.spatial(104, i0_i1_i2_fused % 104)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[2, 2, 8, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 2, 26])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 2, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[4, 32])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
l93 = sch.fuse(l91)
sch.vectorize(loop=l93)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b65)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #10: GFLOPs: 52.2856. Time: 6.8050 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #11: GFLOPs: 117.0062. Time: 3.0409 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #12: GFLOPs: 111.8968. Time: 3.1797 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #13: GFLOPs: 230.1208. Time: 1.5462 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #14: GFLOPs: 69.8308. Time: 5.0952 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #15: GFLOPs: 230.5534. Time: 1.5433 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #16: GFLOPs: 119.1015. Time: 2.9874 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #17: GFLOPs: 24.4200. Time: 14.5702 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #18: GFLOPs: 142.0236. Time: 2.5052 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #19: GFLOPs: 84.1628. Time: 4.2276 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #20: GFLOPs: 114.0308. Time: 3.1202 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #21: GFLOPs: 168.0264. Time: 2.1175 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #22: GFLOPs: 137.5279. Time: 2.5871 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #23: GFLOPs: 67.3472. Time: 5.2831 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i2_2_init, i3_2_init, i2_3_init, i3_3_init in T.grid(32, 4, 4, 26, 13):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_2_init)
                    oh = T.axis.spatial(104, i2_2_init * 26 + i2_3_init)
                    ow = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 52 + i3_2_init * 13 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 32, 4, 4, 1, 4, 1, 1, 1, 1, 26, 13, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_2)
                    oh = T.axis.spatial(104, i2_2 * 26 + i2_3)
                    ow = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 52 + i3_2 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 2 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2)
                    ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 104, 104, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(3328, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(104):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(32, i0_i1_i2_fused // 104)
                        ax2 = T.axis.spatial(104, i0_i1_i2_fused % 104)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 32, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 4, 26])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 4, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97 = sch.get_loops(block=b65)
l98 = sch.fuse(l93, l94, l95)
sch.parallel(loop=l98)
l99 = sch.fuse(l97)
sch.vectorize(loop=l99)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b100)
b118 = sch.decompose_reduction(block=b100, loop=l102)
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #25: GFLOPs: 174.3026. Time: 2.0413 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #26: GFLOPs: 120.1252. Time: 2.9619 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #27: GFLOPs: 18.0902. Time: 19.6683 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #28: GFLOPs: 103.1783. Time: 3.4484 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #29: GFLOPs: 43.9733. Time: 8.0913 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #30: GFLOPs: 229.6003. Time: 1.5497 ms. Best GFLOPs: 355.8567
[19:44:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"] Trial #31: GFLOPs: 65.0644. Time: 5.4685 ms. Best GFLOPs: 355.8567
[19:44:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |            N/A |          N/A |                   N/A |      0 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1920
Total latency (us): 288283

[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #0: GFLOPs: 2.0399. Time: 1.3573 ms. Best GFLOPs: 2.0399
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #1: GFLOPs: 1.8878. Time: 1.4668 ms. Best GFLOPs: 2.0399
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #2: GFLOPs: 1.9296. Time: 1.4349 ms. Best GFLOPs: 2.0399
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #3: GFLOPs: 1.5923. Time: 1.7389 ms. Best GFLOPs: 2.0399
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #4: GFLOPs: 1.6880. Time: 1.6403 ms. Best GFLOPs: 2.0399
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #5: GFLOPs: 2.2821. Time: 1.2133 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #6: GFLOPs: 1.7616. Time: 1.5718 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #7: GFLOPs: 1.9157. Time: 1.4454 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #8: GFLOPs: 2.0127. Time: 1.3757 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #9: GFLOPs: 1.8140. Time: 1.5264 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #10: GFLOPs: 2.0278. Time: 1.3655 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #11: GFLOPs: 1.9141. Time: 1.4466 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #12: GFLOPs: 2.0488. Time: 1.3515 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #13: GFLOPs: 2.1384. Time: 1.2948 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #14: GFLOPs: 2.2134. Time: 1.2510 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #15: GFLOPs: 2.1444. Time: 1.2912 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #16: GFLOPs: 1.9261. Time: 1.4376 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #17: GFLOPs: 1.6332. Time: 1.6954 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 128, 104, 104], dtype="float32")
        for i0_i1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 104, 104):
                with T.block("T_layout_trans"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(128, i0_i1_fused * 4 + ax1)
                    ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                    T.reads(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4])
                    T.writes(T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1])
                    T_layout_trans[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(ax0_1 < 1 and ax1_1 < 128 and ax2_1 < 104 and ax3_1 < 104, T.exp(placeholder[ax0_1, ax1_1 // 4, ax2_1, ax3_1, ax1_1 % 4], dtype="float32"), T.float32(0), dtype="float32")
            for i2, i3, i4 in T.grid(104, 104, 4):
                with T.block("T_multiply"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1 * 4 + ax4, ax2, ax3])
                    T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                    T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 128 and ax2 < 104 and ax3 < 104, T.tanh(T.log(placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1 * 4 + ax4, ax2, ax3], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=-2)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
sch.enter_postproc()
b11 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b11, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b11, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b11, ann_key="meta_schedule.unroll_explicit")
b12, b13 = sch.get_child_blocks(b11)
l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b12)
l20 = sch.fuse(l14, l15)
sch.parallel(loop=l20)
sch.annotate(block_or_loop=l20, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l20, ann_key="pragma_unroll_explicit", ann_val=1)
l21, l22, l23, l24 = sch.get_loops(block=b13)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #19: GFLOPs: 2.2219. Time: 1.2462 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #20: GFLOPs: 1.9236. Time: 1.4394 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #21: GFLOPs: 1.9889. Time: 1.3922 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #22: GFLOPs: 1.9601. Time: 1.4126 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #23: GFLOPs: 1.9180. Time: 1.4436 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #24: GFLOPs: 1.9492. Time: 1.4206 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #25: GFLOPs: 2.1382. Time: 1.2950 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #26: GFLOPs: 1.9337. Time: 1.4319 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #27: GFLOPs: 2.0919. Time: 1.3236 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #28: GFLOPs: 1.8705. Time: 1.4803 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #29: GFLOPs: 2.0500. Time: 1.3507 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #30: GFLOPs: 1.9686. Time: 1.4066 ms. Best GFLOPs: 2.2821
[19:44:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], T_multiply: T.Buffer[(1, 32, 104, 104, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 32, 104, 104, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 128, 104, 104], dtype="float32")
        for i0_i1_i2_fused in T.parallel(13312, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(104):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_exp"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, i0_i1_i2_fused // 416)
                        ax2_1 = T.axis.spatial(104, i0_i1_i2_fused % 104)
                        ax3_1 = T.axis.spatial(104, i3)
                        ax4_1 = T.axis.spatial(4, i0_i1_i2_fused % 416 // 104)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T.writes(T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_exp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.exp(placeholder[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], dtype="float32")
                with T.block("T_layout_trans"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(128, i0_i1_i2_fused // 104)
                    ax2 = T.axis.spatial(104, i0_i1_i2_fused % 104)
                    ax3 = T.axis.spatial(104, i3)
                    T.reads(T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 128 and ax2 < 104 and ax3 < 104, T_exp[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0_i1_i2_fused in T.parallel(3328, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3, i4 in T.grid(104, 4):
                with T.block("T_multiply"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(32, i0_i1_i2_fused // 104)
                    ax2 = T.axis.spatial(104, i0_i1_i2_fused % 104)
                    ax3, ax4 = T.axis.remap("SS", [i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2, ax3, ax4], placeholder_1[ax0, 0, 0, 0], T_layout_trans[ax0, ax1 * 4 + ax4, ax2, ax3])
                    T.writes(T_multiply[ax0, ax1, ax2, ax3, ax4])
                    T_multiply[ax0, ax1, ax2, ax3, ax4] = placeholder[ax0, ax1, ax2, ax3, ax4] * T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 128 and ax2 < 104 and ax3 < 104, T.tanh(T.log(placeholder_1[ax0, 0, 0, 0] + T_layout_trans[ax0, ax1 * 4 + ax4, ax2, ax3], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
l8 = sch.sample_compute_location(block=b5, decision=-2)
sch.compute_at(block=b5, loop=l8, preserve_unit_loops=True)
l9 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l9, preserve_unit_loops=True)
l10 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l10, preserve_unit_loops=True)
sch.enter_postproc()
b11 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b11, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b11, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b11, ann_key="meta_schedule.unroll_explicit")
b12, b13, b14 = sch.get_child_blocks(b11)
l15, l16, l17, l18, l19, l20, l21, l22, l23 = sch.get_loops(block=b12)
l24 = sch.fuse(l15, l16, l17)
sch.parallel(loop=l24)
sch.annotate(block_or_loop=l24, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l24, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26 = sch.get_loops(block=b13)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l27, l28, l29, l30, l31 = sch.get_loops(block=b14)
l32 = sch.fuse(l27, l28, l29)
sch.parallel(loop=l32)
sch.annotate(block_or_loop=l32, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l32, ann_key="pragma_unroll_explicit", ann_val=1)
[19:44:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |            N/A |          N/A |                   N/A |      0 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1952
Total latency (us): 290709

[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #0: GFLOPs: 16.2867. Time: 97.9682 ms. Best GFLOPs: 16.2867
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #1: GFLOPs: 14.9897. Time: 106.4445 ms. Best GFLOPs: 16.2867
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #2: GFLOPs: 115.0977. Time: 13.8628 ms. Best GFLOPs: 115.0977
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #3: GFLOPs: 65.1627. Time: 24.4860 ms. Best GFLOPs: 115.0977
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #4: GFLOPs: 40.7248. Time: 39.1794 ms. Best GFLOPs: 115.0977
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #5: GFLOPs: 179.9979. Time: 8.8644 ms. Best GFLOPs: 179.9979
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #6: GFLOPs: 202.4086. Time: 7.8829 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #7: GFLOPs: 66.9359. Time: 23.8374 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #8: GFLOPs: 11.9454. Time: 133.5722 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #9: GFLOPs: 71.9623. Time: 22.1724 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #10: GFLOPs: 107.3746. Time: 14.8599 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #11: GFLOPs: 50.0765. Time: 31.8628 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #12: GFLOPs: 137.4975. Time: 11.6044 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #13: GFLOPs: 78.6252. Time: 20.2934 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #14: GFLOPs: 133.8269. Time: 11.9227 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #15: GFLOPs: 158.8606. Time: 10.0439 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #16: GFLOPs: 44.5085. Time: 35.8488 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #17: GFLOPs: 168.2593. Time: 9.4828 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #18: GFLOPs: 164.0959. Time: 9.7234 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #19: GFLOPs: 78.4565. Time: 20.3371 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #20: GFLOPs: 68.7854. Time: 23.1964 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #21: GFLOPs: 160.1762. Time: 9.9614 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #22: GFLOPs: 22.2761. Time: 71.6272 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #23: GFLOPs: 50.4747. Time: 31.6114 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #24: GFLOPs: 34.5260. Time: 46.2138 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #25: GFLOPs: 81.7671. Time: 19.5137 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #26: GFLOPs: 72.1189. Time: 22.1243 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #27: GFLOPs: 73.5455. Time: 21.6951 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #28: GFLOPs: 42.8381. Time: 37.2467 ms. Best GFLOPs: 202.4086
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #29: GFLOPs: 234.2467. Time: 6.8115 ms. Best GFLOPs: 234.2467
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #30: GFLOPs: 1.1562. Time: 1379.9886 ms. Best GFLOPs: 234.2467
[19:44:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"] Trial #31: GFLOPs: 6.0716. Time: 262.7950 ms. Best GFLOPs: 234.2467
[19:44:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |            N/A |          N/A |                   N/A |      0 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1984
Total latency (us): 297521

[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #0: GFLOPs: 8.1109. Time: 21.8910 ms. Best GFLOPs: 8.1109
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #1: GFLOPs: 184.2813. Time: 0.9635 ms. Best GFLOPs: 184.2813
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #2: GFLOPs: 301.4543. Time: 0.5890 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #3: GFLOPs: 27.3786. Time: 6.4852 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #4: GFLOPs: 85.7199. Time: 2.0713 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #5: GFLOPs: 273.0403. Time: 0.6503 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #6: GFLOPs: 105.0275. Time: 1.6906 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #7: GFLOPs: 209.8847. Time: 0.8460 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #8: GFLOPs: 58.1217. Time: 3.0549 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #9: GFLOPs: 133.5625. Time: 1.3294 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #10: GFLOPs: 59.0994. Time: 3.0044 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #11: GFLOPs: 49.3533. Time: 3.5976 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #12: GFLOPs: 85.3573. Time: 2.0801 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #13: GFLOPs: 63.2779. Time: 2.8060 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #14: GFLOPs: 15.3672. Time: 11.5542 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #15: GFLOPs: 63.4526. Time: 2.7982 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #16: GFLOPs: 83.1791. Time: 2.1346 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #17: GFLOPs: 157.7044. Time: 1.1259 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #18: GFLOPs: 9.8398. Time: 18.0446 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #19: GFLOPs: 136.6893. Time: 1.2990 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #20: GFLOPs: 151.5697. Time: 1.1714 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #21: GFLOPs: 59.9665. Time: 2.9609 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #22: GFLOPs: 59.2216. Time: 2.9982 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #23: GFLOPs: 48.3593. Time: 3.6716 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #24: GFLOPs: 43.8146. Time: 4.0524 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #25: GFLOPs: 118.2192. Time: 1.5019 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #26: GFLOPs: 192.7399. Time: 0.9212 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(2, 2, 13, 26):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 2 + i1_2_init)
                    oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 8 * 13 + i2_3_init)
                    ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 26 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 2, 1, 1, 2, 8, 1, 1, 1, 1, 13, 26, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 2 + i1_2)
                    oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 8 * 13 + i2_3)
                    ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 26 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_2)
                    ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 26):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 4 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 4 * 2 + ax1)
                        ax2_1 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 8 * 13 + ax2)
                        ax3_1 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 26 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[8, 2, 2, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[4, 1, 1, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 1, 26])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 8])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71, l72, l73, l74, l75, l76)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l99)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #28: GFLOPs: 95.6671. Time: 1.8560 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #29: GFLOPs: 64.2058. Time: 2.7654 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #30: GFLOPs: 28.2478. Time: 6.2856 ms. Best GFLOPs: 301.4543
[19:44:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"] Trial #31: GFLOPs: 81.5901. Time: 2.1762 ms. Best GFLOPs: 301.4543
[19:44:30] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |       301.4543 |     588.9962 |             1177.9924 |     32 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |            N/A |          N/A |                   N/A |      0 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 2016
Total latency (us): 298699

[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #0: GFLOPs: 1.6666. Time: 0.4153 ms. Best GFLOPs: 1.6666
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #1: GFLOPs: 1.9934. Time: 0.3473 ms. Best GFLOPs: 1.9934
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #2: GFLOPs: 2.2226. Time: 0.3114 ms. Best GFLOPs: 2.2226
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #3: GFLOPs: 2.2265. Time: 0.3109 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #4: GFLOPs: 1.8338. Time: 0.3775 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #5: GFLOPs: 2.1414. Time: 0.3233 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #6: GFLOPs: 1.8185. Time: 0.3807 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #7: GFLOPs: 2.0436. Time: 0.3387 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #8: GFLOPs: 1.9903. Time: 0.3478 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #9: GFLOPs: 1.7724. Time: 0.3906 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #10: GFLOPs: 2.1688. Time: 0.3192 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #11: GFLOPs: 2.2217. Time: 0.3116 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #12: GFLOPs: 1.7384. Time: 0.3982 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #13: GFLOPs: 2.1843. Time: 0.3169 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #14: GFLOPs: 1.7439. Time: 0.3969 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #15: GFLOPs: 1.7628. Time: 0.3927 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #16: GFLOPs: 2.2047. Time: 0.3140 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #17: GFLOPs: 2.0027. Time: 0.3456 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #18: GFLOPs: 2.0235. Time: 0.3421 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #19: GFLOPs: 1.8355. Time: 0.3771 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #20: GFLOPs: 2.0975. Time: 0.3300 ms. Best GFLOPs: 2.2265
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #21: GFLOPs: 2.2831. Time: 0.3032 ms. Best GFLOPs: 2.2831
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #22: GFLOPs: 2.2139. Time: 0.3127 ms. Best GFLOPs: 2.2831
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #23: GFLOPs: 2.1920. Time: 0.3158 ms. Best GFLOPs: 2.2831
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #24: GFLOPs: 1.6878. Time: 0.4101 ms. Best GFLOPs: 2.2831
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #25: GFLOPs: 2.2419. Time: 0.3088 ms. Best GFLOPs: 2.2831
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #26: GFLOPs: 1.2538. Time: 0.5521 ms. Best GFLOPs: 2.2831
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #27: GFLOPs: 2.0214. Time: 0.3425 ms. Best GFLOPs: 2.2831
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #28: GFLOPs: 2.2466. Time: 0.3081 ms. Best GFLOPs: 2.2831
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #29: GFLOPs: 2.0106. Time: 0.3443 ms. Best GFLOPs: 2.2831
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #30: GFLOPs: 1.5404. Time: 0.4494 ms. Best GFLOPs: 2.2831
[19:44:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"] Trial #31: GFLOPs: 1.9028. Time: 0.3638 ms. Best GFLOPs: 2.2831
[19:44:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |       301.4543 |     588.9962 |             1177.9924 |     32 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |         2.2831 |     303.1980 |             2728.7824 |     32 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |            N/A |          N/A |                   N/A |      0 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 2048
Total latency (us): 301427

[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #0: GFLOPs: 261.3719. Time: 3.0523 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #1: GFLOPs: 128.2531. Time: 6.2204 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #2: GFLOPs: 38.6987. Time: 20.6153 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #3: GFLOPs: 140.4726. Time: 5.6793 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #4: GFLOPs: 13.6828. Time: 58.3061 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #5: GFLOPs: 16.9537. Time: 47.0569 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #6: GFLOPs: 62.3423. Time: 12.7969 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #7: GFLOPs: 49.8230. Time: 16.0125 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #8: GFLOPs: 27.6554. Time: 28.8474 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #9: GFLOPs: 176.2600. Time: 4.5262 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #10: GFLOPs: 51.9744. Time: 15.3496 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #11: GFLOPs: 10.4295. Time: 76.4932 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #12: GFLOPs: 53.3842. Time: 14.9443 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #13: GFLOPs: 185.7259. Time: 4.2955 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #14: GFLOPs: 173.5310. Time: 4.5974 ms. Best GFLOPs: 261.3719
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #15: GFLOPs: 356.7222. Time: 2.2364 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #16: GFLOPs: 81.4170. Time: 9.7988 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #17: GFLOPs: 67.5274. Time: 11.8143 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #18: GFLOPs: 110.6856. Time: 7.2077 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #19: GFLOPs: 69.0509. Time: 11.5536 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #20: GFLOPs: 71.3018. Time: 11.1889 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #21: GFLOPs: 23.3600. Time: 34.1519 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #22: GFLOPs: 134.4953. Time: 5.9317 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #23: GFLOPs: 73.2359. Time: 10.8934 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #24: GFLOPs: 286.1867. Time: 2.7876 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #25: GFLOPs: 175.9503. Time: 4.5342 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #26: GFLOPs: 196.0809. Time: 4.0687 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #27: GFLOPs: 115.7956. Time: 6.8896 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #28: GFLOPs: 35.8109. Time: 22.2778 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #29: GFLOPs: 47.7275. Time: 16.7155 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #30: GFLOPs: 99.7129. Time: 8.0009 ms. Best GFLOPs: 356.7222
[19:44:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"] Trial #31: GFLOPs: 110.5350. Time: 7.2175 ms. Best GFLOPs: 356.7222
[19:44:35] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |       301.4543 |     588.9962 |             1177.9924 |     32 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |         2.2831 |     303.1980 |             2728.7824 |     32 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |       356.7222 |    2236.4412 |            17891.5298 |     32 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |            N/A |          N/A |                   N/A |      0 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 2080
Total latency (us): 319319

[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #0: GFLOPs: 3.0861. Time: 0.3365 ms. Best GFLOPs: 3.0861
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #1: GFLOPs: 3.3570. Time: 0.3093 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #2: GFLOPs: 2.6671. Time: 0.3893 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #3: GFLOPs: 3.2381. Time: 0.3207 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #4: GFLOPs: 3.0246. Time: 0.3433 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #5: GFLOPs: 3.1156. Time: 0.3333 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #6: GFLOPs: 2.8616. Time: 0.3629 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #7: GFLOPs: 3.3110. Time: 0.3136 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #8: GFLOPs: 2.9895. Time: 0.3473 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #9: GFLOPs: 3.0181. Time: 0.3440 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #10: GFLOPs: 3.0310. Time: 0.3426 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #11: GFLOPs: 3.2977. Time: 0.3149 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #12: GFLOPs: 3.2766. Time: 0.3169 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #13: GFLOPs: 2.3737. Time: 0.4374 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #14: GFLOPs: 3.0212. Time: 0.3437 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #15: GFLOPs: 3.1859. Time: 0.3259 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #16: GFLOPs: 3.0746. Time: 0.3377 ms. Best GFLOPs: 3.3570
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #17: GFLOPs: 3.3629. Time: 0.3088 ms. Best GFLOPs: 3.3629
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #18: GFLOPs: 2.7802. Time: 0.3735 ms. Best GFLOPs: 3.3629
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #19: GFLOPs: 2.7829. Time: 0.3731 ms. Best GFLOPs: 3.3629
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #20: GFLOPs: 2.7008. Time: 0.3845 ms. Best GFLOPs: 3.3629
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #21: GFLOPs: 2.9345. Time: 0.3538 ms. Best GFLOPs: 3.3629
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #22: GFLOPs: 2.8274. Time: 0.3672 ms. Best GFLOPs: 3.3629
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #23: GFLOPs: 2.4828. Time: 0.4182 ms. Best GFLOPs: 3.3629
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #24: GFLOPs: 3.0767. Time: 0.3375 ms. Best GFLOPs: 3.3629
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #25: GFLOPs: 3.0308. Time: 0.3426 ms. Best GFLOPs: 3.3629
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #26: GFLOPs: 2.7997. Time: 0.3709 ms. Best GFLOPs: 3.3629
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #27: GFLOPs: 2.9606. Time: 0.3507 ms. Best GFLOPs: 3.3629
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #28: GFLOPs: 3.0641. Time: 0.3389 ms. Best GFLOPs: 3.3629
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #29: GFLOPs: 3.1220. Time: 0.3326 ms. Best GFLOPs: 3.3629
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #30: GFLOPs: 3.3834. Time: 0.3069 ms. Best GFLOPs: 3.3834
[19:44:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"] Trial #31: GFLOPs: 2.6772. Time: 0.3878 ms. Best GFLOPs: 3.3834
[19:44:38] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |       301.4543 |     588.9962 |             1177.9924 |     32 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |         2.2831 |     303.1980 |             2728.7824 |     32 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |       356.7222 |    2236.4412 |            17891.5298 |     32 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |         3.3834 |     306.8890 |             2455.1122 |     32 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |            N/A |          N/A |                   N/A |      0 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 2112
Total latency (us): 321774

[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #0: GFLOPs: 32.5275. Time: 2.7346 ms. Best GFLOPs: 32.5275
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #1: GFLOPs: 49.2909. Time: 1.8046 ms. Best GFLOPs: 49.2909
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #2: GFLOPs: 202.5105. Time: 0.4392 ms. Best GFLOPs: 202.5105
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #3: GFLOPs: 326.6489. Time: 0.2723 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #4: GFLOPs: 25.5730. Time: 3.4783 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #5: GFLOPs: 166.2977. Time: 0.5349 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #6: GFLOPs: 88.4142. Time: 1.0061 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #7: GFLOPs: 73.7047. Time: 1.2069 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(32, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 8, 13, 26):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 8 + i1_3_init)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 13 + i2_3_init)
                        ow = T.axis.spatial(52, i3_2_init * 26 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(32, 1, 1, 1, 1, 1, 2, 1, 4, 1, 1, 1, 8, 13, 26):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 8 * 8 + i1_3)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 8 // 2 * 13 + i2_3)
                        ow = T.axis.spatial(52, i3_2 * 26 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(128, i5_0 * 4 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(52):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(32, i0_i1_i2_fused // 52)
                        ax2 = T.axis.spatial(52, i0_i1_i2_fused % 52)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 4, 1, 8])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 4, 1, 13])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 26])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[32, 4])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
l93 = sch.fuse(l91)
sch.vectorize(loop=l93)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b65)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #9: GFLOPs: 137.9660. Time: 0.6447 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i4_2_init, i2_3_init, i3_3_init in T.grid(32, 4, 26, 13):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_2_init)
                    oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 26 + i2_3_init)
                    ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 13 + i3_3_init)
                    oc_block = T.axis.spatial(4, i4_2_init)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(8, 1, 1, 1, 32, 1, 1, 4, 16, 1, 1, 1, 1, 26, 13, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(32, i1_2)
                    oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 26 + i2_3)
                    ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 * 13 + i3_3)
                    oc_block = T.axis.spatial(4, i4_2)
                    ic = T.axis.reduce(128, i5_0 * 16 + i5_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 32, 52, 52, 4], "float32"], ["TENSOR", [32, 32, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(52):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(32, i0_i1_i2_fused // 52)
                        ax2 = T.axis.spatial(52, i0_i1_i2_fused % 52)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 32, 1])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 26])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 4, 1, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[8, 16])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97 = sch.get_loops(block=b65)
l98 = sch.fuse(l93, l94, l95)
sch.parallel(loop=l98)
l99 = sch.fuse(l97)
sch.vectorize(loop=l99)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b100 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b100)
b118 = sch.decompose_reduction(block=b100, loop=l102)
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #11: GFLOPs: 178.3069. Time: 0.4989 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #12: GFLOPs: 29.0097. Time: 3.0662 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #13: GFLOPs: 39.9000. Time: 2.2293 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #14: GFLOPs: 78.1048. Time: 1.1389 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #15: GFLOPs: 202.0524. Time: 0.4402 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #16: GFLOPs: 231.8230. Time: 0.3837 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #17: GFLOPs: 93.0891. Time: 0.9555 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #18: GFLOPs: 46.4754. Time: 1.9139 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #19: GFLOPs: 125.1504. Time: 0.7108 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #20: GFLOPs: 99.8839. Time: 0.8905 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #21: GFLOPs: 74.6822. Time: 1.1911 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #22: GFLOPs: 191.4591. Time: 0.4646 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #23: GFLOPs: 98.7299. Time: 0.9010 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #24: GFLOPs: 208.8371. Time: 0.4259 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #25: GFLOPs: 97.5608. Time: 0.9117 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #26: GFLOPs: 57.9971. Time: 1.5337 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #27: GFLOPs: 12.6362. Time: 7.0394 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #28: GFLOPs: 49.7428. Time: 1.7882 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #29: GFLOPs: 187.4841. Time: 0.4744 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #30: GFLOPs: 69.9382. Time: 1.2718 ms. Best GFLOPs: 326.6489
[19:44:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"] Trial #31: GFLOPs: 113.0469. Time: 0.7868 ms. Best GFLOPs: 326.6489
[19:44:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |       301.4543 |     588.9962 |             1177.9924 |     32 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |         2.2831 |     303.1980 |             2728.7824 |     32 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |       356.7222 |    2236.4412 |            17891.5298 |     32 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |         3.3834 |     306.8890 |             2455.1122 |     32 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |       326.6489 |     272.3131 |             2450.8182 |     32 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 2144
Total latency (us): 324225

[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #0: GFLOPs: 1.6957. Time: 0.8164 ms. Best GFLOPs: 1.6957
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #1: GFLOPs: 1.7186. Time: 0.8055 ms. Best GFLOPs: 1.7186
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #2: GFLOPs: 1.2874. Time: 1.0754 ms. Best GFLOPs: 1.7186
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #3: GFLOPs: 1.6967. Time: 0.8160 ms. Best GFLOPs: 1.7186
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #4: GFLOPs: 1.8101. Time: 0.7648 ms. Best GFLOPs: 1.8101
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #5: GFLOPs: 1.7956. Time: 0.7710 ms. Best GFLOPs: 1.8101
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #6: GFLOPs: 1.3232. Time: 1.0463 ms. Best GFLOPs: 1.8101
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #7: GFLOPs: 1.5823. Time: 0.8750 ms. Best GFLOPs: 1.8101
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #8: GFLOPs: 1.6089. Time: 0.8605 ms. Best GFLOPs: 1.8101
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #9: GFLOPs: 1.6044. Time: 0.8629 ms. Best GFLOPs: 1.8101
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #10: GFLOPs: 1.3317. Time: 1.0396 ms. Best GFLOPs: 1.8101
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #11: GFLOPs: 1.2311. Time: 1.1245 ms. Best GFLOPs: 1.8101
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #12: GFLOPs: 2.0072. Time: 0.6898 ms. Best GFLOPs: 2.0072
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #13: GFLOPs: 1.7006. Time: 0.8141 ms. Best GFLOPs: 2.0072
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #14: GFLOPs: 1.3733. Time: 1.0081 ms. Best GFLOPs: 2.0072
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #15: GFLOPs: 1.8441. Time: 0.7507 ms. Best GFLOPs: 2.0072
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #16: GFLOPs: 1.2173. Time: 1.1373 ms. Best GFLOPs: 2.0072
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #17: GFLOPs: 1.6014. Time: 0.8645 ms. Best GFLOPs: 2.0072
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #18: GFLOPs: 1.9194. Time: 0.7213 ms. Best GFLOPs: 2.0072
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #19: GFLOPs: 1.9700. Time: 0.7028 ms. Best GFLOPs: 2.0072
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #20: GFLOPs: 1.2761. Time: 1.0849 ms. Best GFLOPs: 2.0072
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #21: GFLOPs: 1.7798. Time: 0.7779 ms. Best GFLOPs: 2.0072
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #22: GFLOPs: 1.3385. Time: 1.0343 ms. Best GFLOPs: 2.0072
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #23: GFLOPs: 2.0132. Time: 0.6877 ms. Best GFLOPs: 2.0132
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #24: GFLOPs: 1.6325. Time: 0.8481 ms. Best GFLOPs: 2.0132
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #25: GFLOPs: 1.1776. Time: 1.1757 ms. Best GFLOPs: 2.0132
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #26: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 52, 52, 4), "float32"], T_concat: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_exp = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_layout_trans = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_exp_1 = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        T_layout_trans_1 = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_layout_trans_2 = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(52):
                for i4_fused in T.vectorized(4):
                    with T.block("T_exp"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(32, i0_i1_i2_fused // 52)
                        ax2 = T.axis.spatial(52, i0_i1_i2_fused % 52)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder_2[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_exp[ax0, ax1, ax2, ax3, ax4])
                        T_exp[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder_2[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1664, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(52):
                for i4_fused in T.vectorized(4):
                    with T.block("T_exp_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(32, i0_i1_i2_fused // 52)
                        ax2 = T.axis.spatial(52, i0_i1_i2_fused % 52)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[ax0, ax1, ax2, ax3, ax4])
                        T.writes(T_exp_1[ax0, ax1, ax2, ax3, ax4])
                        T_exp_1[ax0, ax1, ax2, ax3, ax4] = T.exp(placeholder[ax0, ax1, ax2, ax3, ax4], dtype="float32")
        for i0_i1_i2_fused in T.parallel(6656, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(52):
                with T.block("T_layout_trans_2"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(128, i0_i1_i2_fused // 52)
                    ax2 = T.axis.spatial(52, i0_i1_i2_fused % 52)
                    ax3 = T.axis.spatial(52, i3)
                    T.reads(T_exp_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans_1[ax0, ax1, ax2, ax3])
                    T_layout_trans_1[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 128 and ax2 < 52 and ax3 < 52, T_exp_1[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0_i1_i2_fused in T.parallel(3328, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 52, 4):
                with T.block("T_layout_trans_3"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(32, i0_i1_i2_fused // 52)
                    ax2_1 = T.axis.spatial(52, i0_i1_i2_fused % 52)
                    ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                    T.where(i0_i1_i2_fused % 3328 // 52 < 32)
                    T.reads(placeholder_1[ax0_1, 0, 0, 0], T_layout_trans_1[ax0_1, ax1_1 * 4 + ax4_1, ax2_1, ax3_1])
                    T.writes(T_layout_trans_2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_layout_trans_2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(ax0_1 < 1 and ax1_1 * 4 + ax4_1 < 128 and ax2_1 < 52 and ax3_1 < 52, T.tanh(T.log(placeholder_1[ax0_1, 0, 0, 0] + T_layout_trans_1[ax0_1, ax1_1 * 4 + ax4_1, ax2_1, ax3_1], dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
            for i3, i4 in T.grid(52, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_layout_trans_1"):
                        ax0_2 = T.axis.spatial(1, 0)
                        ax1_2 = T.axis.spatial(32, i0_i1_i2_fused // 52 + -32)
                        ax2_2 = T.axis.spatial(52, i0_i1_i2_fused % 52)
                        ax3_2, ax4_2 = T.axis.remap("SS", [i3, i4])
                        T.where(32 <= i0_i1_i2_fused % 3328 // 52)
                        T.reads(placeholder_1[ax0_2, 0, 0, 0], T_exp[ax0_2, ax4_2 // 4 + ax1_2, ax2_2, ax3_2, ax4_2 % 4])
                        T.writes(T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2])
                        T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] = T.if_then_else(ax0_2 < 1 and ax1_2 * 4 + ax4_2 < 128 and ax2_2 < 52 and ax3_2 < 52, T.tanh(T.log(placeholder_1[ax0_2, 0, 0, 0] + T.if_then_else(ax0_2 < 1 and ax1_2 * 4 + ax4_2 < 128 and ax2_2 < 52 and ax3_2 < 52, T_exp[ax0_2, (ax1_2 * 4 + ax4_2) // 4, ax2_2, ax3_2, (ax1_2 * 4 + ax4_2) % 4], T.float32(0), dtype="float32"), dtype="float32"), dtype="float32"), T.float32(0), dtype="float32")
                with T.block("T_concat"):
                    ax0_3 = T.axis.spatial(1, 0)
                    ax1_3 = T.axis.spatial(64, i0_i1_i2_fused // 52)
                    ax2_3 = T.axis.spatial(52, i0_i1_i2_fused % 52)
                    ax3_3, ax4_3 = T.axis.remap("SS", [i3, i4])
                    T.reads(placeholder_2[ax0_3, ax1_3 - 32, ax2_3, ax3_3, ax4_3], T_layout_trans[ax0_3, ax1_3 - 32, ax2_3, ax3_3, ax4_3], placeholder[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3], T_layout_trans_2[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                    T.writes(T_concat[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                    T_concat[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] = T.if_then_else(32 <= ax1_3, placeholder_2[ax0_3, ax1_3 - 32, ax2_3, ax3_3, ax4_3] * T_layout_trans[ax0_3, ax1_3 - 32, ax2_3, ax3_3, ax4_3], placeholder[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] * T_layout_trans_2[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3], dtype="float32")
    

b0 = sch.get_block(name="T_exp", func_name="main")
b1 = sch.get_block(name="T_layout_trans", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_log", func_name="main")
b4 = sch.get_block(name="T_tanh", func_name="main")
b5 = sch.get_block(name="T_layout_trans_1", func_name="main")
b6 = sch.get_block(name="T_multiply", func_name="main")
b7 = sch.get_block(name="T_exp_1", func_name="main")
b8 = sch.get_block(name="T_layout_trans_2", func_name="main")
b9 = sch.get_block(name="T_add_1", func_name="main")
b10 = sch.get_block(name="T_log_1", func_name="main")
b11 = sch.get_block(name="T_tanh_1", func_name="main")
b12 = sch.get_block(name="T_layout_trans_3", func_name="main")
b13 = sch.get_block(name="T_multiply_1", func_name="main")
b14 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b13)
sch.compute_inline(block=b11)
sch.compute_inline(block=b10)
sch.compute_inline(block=b9)
sch.compute_inline(block=b6)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
l16 = sch.sample_compute_location(block=b12, decision=2)
sch.compute_at(block=b12, loop=l16, preserve_unit_loops=True)
l17 = sch.sample_compute_location(block=b8, decision=-1)
sch.compute_at(block=b8, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b7, decision=-1)
sch.compute_at(block=b7, loop=l18, preserve_unit_loops=True)
l19 = sch.sample_compute_location(block=b5, decision=4)
sch.compute_at(block=b5, loop=l19, preserve_unit_loops=True)
l20 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l21, preserve_unit_loops=True)
sch.enter_postproc()
b22 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b22, ann_key="meta_schedule.unroll_explicit")
b23, b24, b25, b26, b27, b28 = sch.get_child_blocks(b22)
l29, l30, l31, l32, l33 = sch.get_loops(block=b23)
l34 = sch.fuse(l29, l30, l31)
sch.parallel(loop=l34)
l35 = sch.fuse(l33)
sch.vectorize(loop=l35)
sch.annotate(block_or_loop=l34, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l34, ann_key="pragma_unroll_explicit", ann_val=1)
l36, l37, l38, l39, l40 = sch.get_loops(block=b24)
l41 = sch.fuse(l36, l37, l38)
sch.parallel(loop=l41)
l42 = sch.fuse(l40)
sch.vectorize(loop=l42)
sch.annotate(block_or_loop=l41, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l41, ann_key="pragma_unroll_explicit", ann_val=1)
l43, l44, l45, l46 = sch.get_loops(block=b25)
l47 = sch.fuse(l43, l44, l45)
sch.parallel(loop=l47)
sch.annotate(block_or_loop=l47, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l47, ann_key="pragma_unroll_explicit", ann_val=1)
l48, l49, l50, l51, l52, l53, l54, l55 = sch.get_loops(block=b26)
l56 = sch.fuse(l48, l49, l50)
sch.parallel(loop=l56)
sch.annotate(block_or_loop=l56, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l56, ann_key="pragma_unroll_explicit", ann_val=1)
l57, l58, l59, l60, l61, l62, l63, l64 = sch.get_loops(block=b27)
sch.annotate(block_or_loop=l57, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l57, ann_key="pragma_unroll_explicit", ann_val=1)
l65, l66, l67 = sch.get_loops(block=b28)
sch.annotate(block_or_loop=l65, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l65, ann_key="pragma_unroll_explicit", ann_val=1)
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #27: GFLOPs: 1.3017. Time: 1.0636 ms. Best GFLOPs: 2.0132
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #28: GFLOPs: 2.0019. Time: 0.6916 ms. Best GFLOPs: 2.0132
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #29: GFLOPs: 1.7623. Time: 0.7856 ms. Best GFLOPs: 2.0132
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #30: GFLOPs: 1.9709. Time: 0.7024 ms. Best GFLOPs: 2.0132
[19:44:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"] Trial #31: GFLOPs: 1.7913. Time: 0.7729 ms. Best GFLOPs: 2.0132
[19:44:43] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |       301.4543 |     588.9962 |             1177.9924 |     32 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |         2.2831 |     303.1980 |             2728.7824 |     32 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |       356.7222 |    2236.4412 |            17891.5298 |     32 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |         3.3834 |     306.8890 |             2455.1122 |     32 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |       326.6489 |     272.3131 |             2450.8182 |     32 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |         2.0132 |     687.6941 |              687.6941 |     32 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |            N/A |          N/A |                   N/A |      0 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 2176
Total latency (us): 324913

[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #0: GFLOPs: 87.1295. Time: 4.0757 ms. Best GFLOPs: 87.1295
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #1: GFLOPs: 75.0970. Time: 4.7287 ms. Best GFLOPs: 87.1295
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #2: GFLOPs: 226.0415. Time: 1.5710 ms. Best GFLOPs: 226.0415
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_2_init, i4_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(4, 2, 16, 26, 13):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_2_init * 16 + i1_3_init)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 26 + i2_3_init)
                        ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 26 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 13 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_2_init * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(128, 1, 1, 1, 4, 1, 1, 2, 2, 1, 1, 1, 16, 26, 13):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_2 * 16 + i1_3)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 4 * 26 + i2_3)
                        ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 26 + i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i4_2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(256, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0_i1_i2_fused in T.parallel(3328, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(52):
                for i4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(64, i0_i1_i2_fused // 52)
                        ax2 = T.axis.spatial(52, i0_i1_i2_fused % 52)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                        T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                        T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 4, 16])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 1, 26])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 2, 1, 13])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[128, 2])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v62 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v62)
sch.enter_postproc()
b63 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b63, ann_key="meta_schedule.unroll_explicit")
b64, b65 = sch.get_child_blocks(b63)
l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91 = sch.get_loops(block=b64)
l92 = sch.fuse(l66, l67, l68, l69, l70, l71, l72, l73, l74, l75)
sch.parallel(loop=l92)
l93 = sch.fuse(l91)
sch.vectorize(loop=l93)
sch.annotate(block_or_loop=l92, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l92, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98 = sch.get_loops(block=b65)
l99 = sch.fuse(l94, l95, l96)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b101)
b119 = sch.decompose_reduction(block=b101, loop=l103)
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #4: GFLOPs: 68.9664. Time: 5.1490 ms. Best GFLOPs: 226.0415
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #5: GFLOPs: 68.6327. Time: 5.1741 ms. Best GFLOPs: 226.0415
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #6: GFLOPs: 177.5748. Time: 1.9998 ms. Best GFLOPs: 226.0415
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #7: GFLOPs: 119.8810. Time: 2.9622 ms. Best GFLOPs: 226.0415
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #8: GFLOPs: 40.3113. Time: 8.8092 ms. Best GFLOPs: 226.0415
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #9: GFLOPs: 66.5966. Time: 5.3323 ms. Best GFLOPs: 226.0415
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #10: GFLOPs: 145.2292. Time: 2.4452 ms. Best GFLOPs: 226.0415
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #11: GFLOPs: 361.1469. Time: 0.9833 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #12: GFLOPs: 69.6436. Time: 5.0990 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #13: GFLOPs: 108.0052. Time: 3.2879 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #14: GFLOPs: 27.7617. Time: 12.7914 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(64, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 64, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 64, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(13, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 4, 13, 2):
                for i1_2_init, i4_2_init, i1_3_init, i3_3_init in T.grid(4, 2, 16, 4):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_2_init * 16 + i1_3_init)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + i2_1)
                        ow = T.axis.spatial(52, i3_1 * 4 + i3_3_init)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 4, 1, 1, 2, 1, 1, 1, 1, 16, 1, 4, 1):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(64, i1_2 * 16 + i1_3)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + i2_1)
                        ow = T.axis.spatial(52, i3_1 * 4 + i3_3)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_2)
                        ic = T.axis.reduce(256, i5_0)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [64, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 64, 4, 52):
                for ax4_fused in T.vectorized(4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(64, ax1)
                        ax2_1 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + ax2)
                        ax3_1, ax4 = T.axis.remap("SS", [ax3, ax4_fused])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b0)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l2, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 4, 16])
l22, l23, l24, l25 = sch.split(loop=l3, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[13, 4, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l4, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 13, 1, 4])
l38, l39, l40, l41 = sch.split(loop=l5, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l6, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l7, n=2, max_innermost_factor=64, decision=[256, 1])
l52, l53 = sch.split(loop=l7, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 1])
l56, l57 = sch.split(loop=l8, factors=[v54, v55])
v58, v59 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l60, l61 = sch.split(loop=l9, factors=[v58, v59])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l60, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49)
b62, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b62, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
sch.enter_postproc()
b64 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b64, ann_key="meta_schedule.unroll_explicit")
b65, b66 = sch.get_child_blocks(b64)
l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b65)
l93 = sch.fuse(l67, l68, l69, l70, l71)
sch.parallel(loop=l93)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b66)
l100 = sch.fuse(l99)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
b101 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b101)
b124 = sch.decompose_reduction(block=b101, loop=l108)
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #16: GFLOPs: 122.8959. Time: 2.8895 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #17: GFLOPs: 142.0146. Time: 2.5005 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #18: GFLOPs: 60.3101. Time: 5.8881 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #19: GFLOPs: 295.5893. Time: 1.2014 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #20: GFLOPs: 60.1203. Time: 5.9067 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #21: GFLOPs: 150.1108. Time: 2.3657 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #22: GFLOPs: 110.5857. Time: 3.2112 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #23: GFLOPs: 27.8576. Time: 12.7474 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #24: GFLOPs: 92.2989. Time: 3.8474 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #25: GFLOPs: 71.9789. Time: 4.9335 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #26: GFLOPs: 74.6420. Time: 4.7575 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #27: GFLOPs: 64.2228. Time: 5.5294 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #28: GFLOPs: 169.6077. Time: 2.0937 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #29: GFLOPs: 32.2945. Time: 10.9960 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #30: GFLOPs: 37.8834. Time: 9.3738 ms. Best GFLOPs: 361.1469
[19:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"] Trial #31: GFLOPs: 252.3465. Time: 1.4072 ms. Best GFLOPs: 361.1469
[19:44:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |       301.4543 |     588.9962 |             1177.9924 |     32 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |         2.2831 |     303.1980 |             2728.7824 |     32 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |       356.7222 |    2236.4412 |            17891.5298 |     32 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |         3.3834 |     306.8890 |             2455.1122 |     32 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |       326.6489 |     272.3131 |             2450.8182 |     32 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |         2.0132 |     687.6941 |              687.6941 |     32 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |       361.1469 |     983.2867 |              983.2867 |     32 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |            N/A |          N/A |                   N/A |      0 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 2208
Total latency (us): 325896

[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #0: GFLOPs: 2.1765. Time: 0.6361 ms. Best GFLOPs: 2.1765
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #1: GFLOPs: 1.7241. Time: 0.8030 ms. Best GFLOPs: 2.1765
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #2: GFLOPs: 2.0633. Time: 0.6710 ms. Best GFLOPs: 2.1765
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #3: GFLOPs: 1.6293. Time: 0.8497 ms. Best GFLOPs: 2.1765
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #4: GFLOPs: 1.8488. Time: 0.7488 ms. Best GFLOPs: 2.1765
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #5: GFLOPs: 1.9799. Time: 0.6993 ms. Best GFLOPs: 2.1765
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #6: GFLOPs: 2.0466. Time: 0.6765 ms. Best GFLOPs: 2.1765
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #7: GFLOPs: 2.2212. Time: 0.6233 ms. Best GFLOPs: 2.2212
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #8: GFLOPs: 2.2094. Time: 0.6266 ms. Best GFLOPs: 2.2212
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #9: GFLOPs: 2.2321. Time: 0.6202 ms. Best GFLOPs: 2.2321
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #10: GFLOPs: 2.0439. Time: 0.6773 ms. Best GFLOPs: 2.2321
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #11: GFLOPs: 1.7112. Time: 0.8090 ms. Best GFLOPs: 2.2321
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #12: GFLOPs: 2.1447. Time: 0.6455 ms. Best GFLOPs: 2.2321
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #13: GFLOPs: 1.8279. Time: 0.7574 ms. Best GFLOPs: 2.2321
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #14: GFLOPs: 1.9886. Time: 0.6962 ms. Best GFLOPs: 2.2321
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #15: GFLOPs: 1.7508. Time: 0.7907 ms. Best GFLOPs: 2.2321
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #16: GFLOPs: 2.3025. Time: 0.6013 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #17: GFLOPs: 1.9219. Time: 0.7203 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #18: GFLOPs: 1.7147. Time: 0.8074 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #19: GFLOPs: 1.8615. Time: 0.7437 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #20: GFLOPs: 2.0356. Time: 0.6801 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #21: GFLOPs: 1.9564. Time: 0.7077 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #22: GFLOPs: 1.9037. Time: 0.7273 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #23: GFLOPs: 2.0212. Time: 0.6850 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #24: GFLOPs: 1.8894. Time: 0.7328 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #25: GFLOPs: 2.2869. Time: 0.6054 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #26: GFLOPs: 1.8964. Time: 0.7300 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #27: GFLOPs: 1.8413. Time: 0.7519 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #28: GFLOPs: 1.9727. Time: 0.7018 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #29: GFLOPs: 2.2471. Time: 0.6161 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #30: GFLOPs: 2.0727. Time: 0.6679 ms. Best GFLOPs: 2.3025
[19:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"] Trial #31: GFLOPs: 2.0486. Time: 0.6758 ms. Best GFLOPs: 2.3025
[19:44:48] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |       301.4543 |     588.9962 |             1177.9924 |     32 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |         2.2831 |     303.1980 |             2728.7824 |     32 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |       356.7222 |    2236.4412 |            17891.5298 |     32 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |         3.3834 |     306.8890 |             2455.1122 |     32 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |       326.6489 |     272.3131 |             2450.8182 |     32 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |         2.0132 |     687.6941 |              687.6941 |     32 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |       361.1469 |     983.2867 |              983.2867 |     32 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |         2.3025 |     601.2840 |             1202.5680 |     32 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 2240
Total latency (us): 327099

[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #0: GFLOPs: 0.0000. Time: 0.0530 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #1: GFLOPs: 0.0000. Time: 9.2206 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #2: GFLOPs: 0.0000. Time: 9.2546 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #3: GFLOPs: 0.0000. Time: 0.0729 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #4: GFLOPs: 0.0000. Time: 0.1282 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #5: GFLOPs: 0.0000. Time: 0.0428 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #6: GFLOPs: 0.0000. Time: 0.1048 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #7: GFLOPs: 0.0000. Time: 9.2935 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #8: GFLOPs: 0.0000. Time: 10.3081 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #9: GFLOPs: 0.0000. Time: 0.0627 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #10: GFLOPs: 0.0000. Time: 10.1034 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #11: GFLOPs: 0.0000. Time: 10.2642 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #12: GFLOPs: 0.0000. Time: 0.1250 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #13: GFLOPs: 0.0000. Time: 10.1805 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #14: GFLOPs: 0.0000. Time: 0.3590 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #15: GFLOPs: 0.0000. Time: 0.3045 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #16: GFLOPs: 0.0000. Time: 0.1004 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #17: GFLOPs: 0.0000. Time: 0.0672 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #18: GFLOPs: 0.0000. Time: 0.0611 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #19: GFLOPs: 0.0000. Time: 0.0436 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #20: GFLOPs: 0.0000. Time: 0.0576 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #21: GFLOPs: 0.0000. Time: 9.0140 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #22: GFLOPs: 0.0000. Time: 0.0974 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #23: GFLOPs: 0.0000. Time: 10.1719 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #24: GFLOPs: 0.0000. Time: 0.2094 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #25: GFLOPs: 0.0000. Time: 9.0395 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #26: GFLOPs: 0.0000. Time: 9.4608 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #27: GFLOPs: 0.0000. Time: 0.3141 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #28: GFLOPs: 0.0000. Time: 10.1719 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #29: GFLOPs: 0.0000. Time: 9.6949 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #30: GFLOPs: 0.0000. Time: 0.0370 ms. Best GFLOPs: 0.0000
[19:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"] Trial #31: GFLOPs: 0.0000. Time: 0.2514 ms. Best GFLOPs: 0.0000
[19:44:50] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |       301.4543 |     588.9962 |             1177.9924 |     32 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |         2.2831 |     303.1980 |             2728.7824 |     32 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |       356.7222 |    2236.4412 |            17891.5298 |     32 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |         3.3834 |     306.8890 |             2455.1122 |     32 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |       326.6489 |     272.3131 |             2450.8182 |     32 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |         2.0132 |     687.6941 |              687.6941 |     32 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |       361.1469 |     983.2867 |              983.2867 |     32 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |         2.3025 |     601.2840 |             1202.5680 |     32 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |         0.0000 |      37.0297 |               37.0297 |     32 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |            N/A |          N/A |                   N/A |      0 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 2272
Total latency (us): 327136

[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #0: GFLOPs: 71.9728. Time: 2.4718 ms. Best GFLOPs: 71.9728
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #1: GFLOPs: 98.6792. Time: 1.8028 ms. Best GFLOPs: 98.6792
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #2: GFLOPs: 15.4463. Time: 11.5174 ms. Best GFLOPs: 98.6792
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #3: GFLOPs: 80.8370. Time: 2.2007 ms. Best GFLOPs: 98.6792
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #4: GFLOPs: 78.0644. Time: 2.2789 ms. Best GFLOPs: 98.6792
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #5: GFLOPs: 96.8632. Time: 1.8366 ms. Best GFLOPs: 98.6792
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 13, 13):
                for i4_3_fused_init in T.vectorized(2):
                    with T.block("conv2d_NCHWc_init"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 2 + i1_3_init)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 26 + i2_2_init * 13 + i2_3_init)
                        ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 4 * 13 + i3_3_init)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused_init)
                        T.reads()
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(32, 1, 1, 1, 1, 2, 1, 1, 8, 1, 1, 1, 2, 13, 13):
                for i4_3_fused in T.vectorized(2):
                    with T.block("conv2d_NCHWc_update"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 2 + i1_3)
                        oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 26 + i2_2 * 13 + i2_3)
                        ow = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 4 * 13 + i3_3)
                        oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + i4_3_fused)
                        ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 2, 26, 13):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 16 * 2 + ax1)
                        ax2_1 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4 // 2 * 26 + ax2)
                        ax3_1 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 16 // 4 * 13 + ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[16, 1, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 2, 2, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 13])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72, l73, l74, l75, l76, l77)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b103)
b121 = sch.decompose_reduction(block=b103, loop=l105)
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #7: GFLOPs: 83.1530. Time: 2.1394 ms. Best GFLOPs: 98.6792
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52, 4), "float32"], placeholder_1: T.Buffer[(32, 64, 1, 1, 4, 4), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1, 4), "float32"], T_leaky_relu: T.Buffer[(1, 32, 52, 52, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 32, 52, 52, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(8, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 4, 1, 2, 1):
                for i1_2_init, i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 2, 13, 26):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("conv2d_NCHWc_init"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 16 + i1_1 * 4 + i1_2_init * 2 + i1_3_init)
                            oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 26 + i2_2_init * 13 + i2_3_init)
                            ow = T.axis.spatial(52, i3_1 * 26 + i3_3_init)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3 in T.grid(32, 1, 1, 1, 2, 2, 1, 1, 8, 1, 1, 1, 2, 13, 26):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("conv2d_NCHWc_update"):
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 16 + i1_1 * 4 + i1_2 * 2 + i1_3)
                            oh = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 26 + i2_2 * 13 + i2_3)
                            ow = T.axis.spatial(52, i3_1 * 26 + i3_3)
                            oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + i4_3_fused)
                            ic = T.axis.reduce(256, i5_0 * 8 + i5_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4], placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 64, 52, 52, 4], "float32"], ["TENSOR", [32, 64, 1, 1, 4, 4], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + placeholder[n, ic // 4, oh + kh, ow + kw, ic % 4] * placeholder_1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(1, 16, 26, 52):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_leaky_relu"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_i4_0_fused // 4 * 16 + ax1)
                        ax2_1 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 // 2 * 26 + ax2)
                        ax3_1 = T.axis.spatial(52, ax3)
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_leaky_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = T.Select(T.float32(0) < conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4], (conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14])
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 2, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22])
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 13])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30])
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 26])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38])
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46])
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 8])
l53, l54 = sch.split(loop=l8, factors=[v51, v52])
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56])
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60])
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
sch.enter_postproc()
b65 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b65, ann_key="meta_schedule.unroll_explicit")
b66, b67 = sch.get_child_blocks(b65)
l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b66)
l94 = sch.fuse(l68, l69, l70, l71, l72)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b67)
l102 = sch.fuse(l101)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l96, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l96, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b103)
b126 = sch.decompose_reduction(block=b103, loop=l110)
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #9: GFLOPs: 71.9139. Time: 2.4738 ms. Best GFLOPs: 98.6792
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #10: GFLOPs: 45.6477. Time: 3.8973 ms. Best GFLOPs: 98.6792
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #11: GFLOPs: 26.3430. Time: 6.7533 ms. Best GFLOPs: 98.6792
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #12: GFLOPs: 6.9203. Time: 25.7073 ms. Best GFLOPs: 98.6792
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #13: GFLOPs: 11.5225. Time: 15.4395 ms. Best GFLOPs: 98.6792
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #14: GFLOPs: 161.0082. Time: 1.1049 ms. Best GFLOPs: 161.0082
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #15: GFLOPs: 182.1202. Time: 0.9768 ms. Best GFLOPs: 182.1202
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #16: GFLOPs: 128.9431. Time: 1.3797 ms. Best GFLOPs: 182.1202
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #17: GFLOPs: 143.0480. Time: 1.2436 ms. Best GFLOPs: 182.1202
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #18: GFLOPs: 65.7610. Time: 2.7053 ms. Best GFLOPs: 182.1202
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #19: GFLOPs: 433.2827. Time: 0.4106 ms. Best GFLOPs: 433.2827
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #20: GFLOPs: 12.4957. Time: 14.2370 ms. Best GFLOPs: 433.2827
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #21: GFLOPs: 53.2480. Time: 3.3410 ms. Best GFLOPs: 433.2827
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #22: GFLOPs: 69.3698. Time: 2.5645 ms. Best GFLOPs: 433.2827
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #23: GFLOPs: 54.2442. Time: 3.2796 ms. Best GFLOPs: 433.2827
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #24: GFLOPs: 133.3192. Time: 1.3344 ms. Best GFLOPs: 433.2827
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #25: GFLOPs: 51.3681. Time: 3.4633 ms. Best GFLOPs: 433.2827
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #26: GFLOPs: 24.7464. Time: 7.1890 ms. Best GFLOPs: 433.2827
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #27: GFLOPs: 104.2074. Time: 1.7072 ms. Best GFLOPs: 433.2827
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #28: GFLOPs: 17.6707. Time: 10.0676 ms. Best GFLOPs: 433.2827
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #29: GFLOPs: 215.9736. Time: 0.8237 ms. Best GFLOPs: 433.2827
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #30: GFLOPs: 136.2184. Time: 1.3060 ms. Best GFLOPs: 433.2827
[19:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"] Trial #31: GFLOPs: 77.9642. Time: 2.2818 ms. Best GFLOPs: 433.2827
[19:44:53] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |       301.4543 |     588.9962 |             1177.9924 |     32 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |         2.2831 |     303.1980 |             2728.7824 |     32 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |       356.7222 |    2236.4412 |            17891.5298 |     32 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |         3.3834 |     306.8890 |             2455.1122 |     32 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |       326.6489 |     272.3131 |             2450.8182 |     32 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |         2.0132 |     687.6941 |              687.6941 |     32 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |       361.1469 |     983.2867 |              983.2867 |     32 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |         2.3025 |     601.2840 |             1202.5680 |     32 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |         0.0000 |      37.0297 |               37.0297 |     32 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |       433.2827 |     410.5901 |             1642.3603 |     32 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |            N/A |          N/A |                   N/A |      0 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 2304
Total latency (us): 328778

[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #0: GFLOPs: 411.9103. Time: 3.8753 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #1: GFLOPs: 287.0873. Time: 5.5602 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #2: GFLOPs: 8.9137. Time: 179.0813 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #3: GFLOPs: 132.5280. Time: 12.0448 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #4: GFLOPs: 56.8487. Time: 28.0792 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #5: GFLOPs: 46.7940. Time: 34.1127 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #6: GFLOPs: 103.7662. Time: 15.3833 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #7: GFLOPs: 45.0998. Time: 35.3941 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #8: GFLOPs: 22.7259. Time: 70.2402 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #9: GFLOPs: 181.4396. Time: 8.7978 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #10: GFLOPs: 60.5297. Time: 26.3717 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #11: GFLOPs: 54.0824. Time: 29.5155 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #12: GFLOPs: 234.1604. Time: 6.8170 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #13: GFLOPs: 100.8876. Time: 15.8222 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #14: GFLOPs: 55.7382. Time: 28.6387 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #15: GFLOPs: 33.7494. Time: 47.2976 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #16: GFLOPs: 295.4669. Time: 5.4025 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #17: GFLOPs: 18.7933. Time: 84.9382 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #18: GFLOPs: 56.8292. Time: 28.0889 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #19: GFLOPs: 87.5856. Time: 18.2252 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #20: GFLOPs: 224.5937. Time: 7.1074 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #21: GFLOPs: 41.7788. Time: 38.2076 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #22: GFLOPs: 179.3653. Time: 8.8995 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #23: GFLOPs: 58.7670. Time: 27.1627 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #24: GFLOPs: 46.3608. Time: 34.4315 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #25: GFLOPs: 40.5964. Time: 39.3204 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #26: GFLOPs: 165.1864. Time: 9.6634 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #27: GFLOPs: 207.4399. Time: 7.6951 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #28: GFLOPs: 297.1274. Time: 5.3723 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #29: GFLOPs: 241.7619. Time: 6.6026 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #30: GFLOPs: 92.3256. Time: 17.2896 ms. Best GFLOPs: 411.9103
[19:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"] Trial #31: GFLOPs: 219.5847. Time: 7.2695 ms. Best GFLOPs: 411.9103
[19:44:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |       301.4543 |     588.9962 |             1177.9924 |     32 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |         2.2831 |     303.1980 |             2728.7824 |     32 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |       356.7222 |    2236.4412 |            17891.5298 |     32 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |         3.3834 |     306.8890 |             2455.1122 |     32 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |       326.6489 |     272.3131 |             2450.8182 |     32 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |         2.0132 |     687.6941 |              687.6941 |     32 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |       361.1469 |     983.2867 |              983.2867 |     32 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |         2.3025 |     601.2840 |             1202.5680 |     32 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |         0.0000 |      37.0297 |               37.0297 |     32 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |       433.2827 |     410.5901 |             1642.3603 |     32 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |       411.9103 |    3875.2817 |            11625.8451 |     32 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 2336
Total latency (us): 340404

[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #0: GFLOPs: 0.0000. Time: 0.0146 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #1: GFLOPs: 0.0000. Time: 0.0143 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #2: GFLOPs: 0.0000. Time: 0.0115 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #3: GFLOPs: 0.0000. Time: 0.0143 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #4: GFLOPs: 0.0000. Time: 0.0143 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #5: GFLOPs: 0.0000. Time: 0.0145 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #6: GFLOPs: 0.0000. Time: 0.0114 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #7: GFLOPs: 0.0000. Time: 0.0142 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #8: GFLOPs: 0.0000. Time: 0.0143 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #9: GFLOPs: 0.0000. Time: 0.0116 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #10: GFLOPs: 0.0000. Time: 0.0143 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #11: GFLOPs: 0.0000. Time: 0.0143 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #12: GFLOPs: 0.0000. Time: 0.0145 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #13: GFLOPs: 0.0000. Time: 0.0113 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #14: GFLOPs: 0.0000. Time: 0.0142 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #15: GFLOPs: 0.0000. Time: 0.0143 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #16: GFLOPs: 0.0000. Time: 0.0143 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #17: GFLOPs: 0.0000. Time: 0.0115 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #18: GFLOPs: 0.0000. Time: 0.0143 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #19: GFLOPs: 0.0000. Time: 0.0143 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #20: GFLOPs: 0.0000. Time: 0.0114 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #21: GFLOPs: 0.0000. Time: 0.0115 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #22: GFLOPs: 0.0000. Time: 0.0114 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #23: GFLOPs: 0.0000. Time: 0.0143 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #24: GFLOPs: 0.0000. Time: 0.0145 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #25: GFLOPs: 0.0000. Time: 0.0118 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #26: GFLOPs: 0.0000. Time: 0.0117 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #27: GFLOPs: 0.0000. Time: 0.0123 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #28: GFLOPs: 0.0000. Time: 0.0149 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #29: GFLOPs: 0.0000. Time: 0.0147 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #30: GFLOPs: 0.0000. Time: 0.0146 ms. Best GFLOPs: 0.0000
[19:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #73: "fused_layout_transform_2"] Trial #31: GFLOPs: 0.0000. Time: 0.0119 ms. Best GFLOPs: 0.0000
[19:44:58] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #73: "fused_layout_transform_2"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |       301.4543 |     588.9962 |             1177.9924 |     32 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |         2.2831 |     303.1980 |             2728.7824 |     32 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |       356.7222 |    2236.4412 |            17891.5298 |     32 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |         3.3834 |     306.8890 |             2455.1122 |     32 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |       326.6489 |     272.3131 |             2450.8182 |     32 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |         2.0132 |     687.6941 |              687.6941 |     32 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |       361.1469 |     983.2867 |              983.2867 |     32 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |         2.3025 |     601.2840 |             1202.5680 |     32 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |         0.0000 |      37.0297 |               37.0297 |     32 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |       433.2827 |     410.5901 |             1642.3603 |     32 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |       411.9103 |    3875.2817 |            11625.8451 |     32 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |         0.0001 |      11.3017 |               11.3017 |     32 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 2368
Total latency (us): 340415

[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #0: GFLOPs: 52.6241. Time: 6.7217 ms. Best GFLOPs: 52.6241
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #1: GFLOPs: 22.2820. Time: 15.8749 ms. Best GFLOPs: 52.6241
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #2: GFLOPs: 82.0507. Time: 4.3110 ms. Best GFLOPs: 82.0507
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #3: GFLOPs: 47.6928. Time: 7.4167 ms. Best GFLOPs: 82.0507
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #4: GFLOPs: 7.2754. Time: 48.6192 ms. Best GFLOPs: 82.0507
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #5: GFLOPs: 82.4137. Time: 4.2920 ms. Best GFLOPs: 82.4137
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #6: GFLOPs: 53.8706. Time: 6.5662 ms. Best GFLOPs: 82.4137
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #7: GFLOPs: 44.7854. Time: 7.8982 ms. Best GFLOPs: 82.4137
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #8: GFLOPs: 47.2186. Time: 7.4912 ms. Best GFLOPs: 82.4137
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #9: GFLOPs: 157.9180. Time: 2.2399 ms. Best GFLOPs: 157.9180
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #10: GFLOPs: 60.9135. Time: 5.8070 ms. Best GFLOPs: 157.9180
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #11: GFLOPs: 147.5025. Time: 2.3981 ms. Best GFLOPs: 157.9180
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #12: GFLOPs: 14.1864. Time: 24.9340 ms. Best GFLOPs: 157.9180
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #13: GFLOPs: 142.6523. Time: 2.4796 ms. Best GFLOPs: 157.9180
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #14: GFLOPs: 42.7401. Time: 8.2762 ms. Best GFLOPs: 157.9180
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #15: GFLOPs: 66.4195. Time: 5.3256 ms. Best GFLOPs: 157.9180
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #16: GFLOPs: 120.4981. Time: 2.9355 ms. Best GFLOPs: 157.9180
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #17: GFLOPs: 115.3076. Time: 3.0677 ms. Best GFLOPs: 157.9180
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #18: GFLOPs: 14.4616. Time: 24.4595 ms. Best GFLOPs: 157.9180
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #19: GFLOPs: 243.2147. Time: 1.4544 ms. Best GFLOPs: 243.2147
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #20: GFLOPs: 51.0703. Time: 6.9262 ms. Best GFLOPs: 243.2147
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #21: GFLOPs: 17.0175. Time: 20.7859 ms. Best GFLOPs: 243.2147
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #22: GFLOPs: 218.6557. Time: 1.6177 ms. Best GFLOPs: 243.2147
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #23: GFLOPs: 99.0729. Time: 3.5703 ms. Best GFLOPs: 243.2147
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #24: GFLOPs: 61.5960. Time: 5.7426 ms. Best GFLOPs: 243.2147
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #25: GFLOPs: 144.0327. Time: 2.4559 ms. Best GFLOPs: 243.2147
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #26: GFLOPs: 61.5017. Time: 5.7514 ms. Best GFLOPs: 243.2147
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #27: GFLOPs: 169.8318. Time: 2.0828 ms. Best GFLOPs: 243.2147
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #28: GFLOPs: 34.5226. Time: 10.2461 ms. Best GFLOPs: 243.2147
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #29: GFLOPs: 89.1824. Time: 3.9663 ms. Best GFLOPs: 243.2147
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #30: GFLOPs: 52.8511. Time: 6.6928 ms. Best GFLOPs: 243.2147
[19:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"] Trial #31: GFLOPs: 16.1274. Time: 21.9331 ms. Best GFLOPs: 243.2147
[19:45:01] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |       301.4543 |     588.9962 |             1177.9924 |     32 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |         2.2831 |     303.1980 |             2728.7824 |     32 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |       356.7222 |    2236.4412 |            17891.5298 |     32 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |         3.3834 |     306.8890 |             2455.1122 |     32 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |       326.6489 |     272.3131 |             2450.8182 |     32 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |         2.0132 |     687.6941 |              687.6941 |     32 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |       361.1469 |     983.2867 |              983.2867 |     32 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |         2.3025 |     601.2840 |             1202.5680 |     32 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |         0.0000 |      37.0297 |               37.0297 |     32 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |       433.2827 |     410.5901 |             1642.3603 |     32 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |       411.9103 |    3875.2817 |            11625.8451 |     32 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |         0.0001 |      11.3017 |               11.3017 |     32 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |       243.2147 |    1454.3684 |             1454.3684 |     32 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 2400
Total latency (us): 341869

[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #0: GFLOPs: 0.0000. Time: 0.2328 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #1: GFLOPs: 0.0000. Time: 4.3181 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #2: GFLOPs: 0.0000. Time: 3.9026 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #3: GFLOPs: 0.0000. Time: 0.1710 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #4: GFLOPs: 0.0000. Time: 2.3345 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #5: GFLOPs: 0.0000. Time: 0.2266 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #6: GFLOPs: 0.0000. Time: 0.2728 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #7: GFLOPs: 0.0000. Time: 1.5051 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #8: GFLOPs: 0.0000. Time: 4.0031 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #9: GFLOPs: 0.0000. Time: 0.2098 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #10: GFLOPs: 0.0000. Time: 0.1944 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #11: GFLOPs: 0.0000. Time: 0.1944 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #12: GFLOPs: 0.0000. Time: 0.2221 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #13: GFLOPs: 0.0000. Time: 0.1616 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #14: GFLOPs: 0.0000. Time: 2.5795 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #15: GFLOPs: 0.0000. Time: 0.2252 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #16: GFLOPs: 0.0000. Time: 0.2044 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #17: GFLOPs: 0.0000. Time: 0.2501 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #18: GFLOPs: 0.0000. Time: 0.2701 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #19: GFLOPs: 0.0000. Time: 2.2610 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #20: GFLOPs: 0.0000. Time: 0.1666 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #21: GFLOPs: 0.0000. Time: 2.3022 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #22: GFLOPs: 0.0000. Time: 0.2611 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #23: GFLOPs: 0.0000. Time: 0.2091 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #24: GFLOPs: 0.0000. Time: 4.0950 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #25: GFLOPs: 0.0000. Time: 0.2072 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #26: GFLOPs: 0.0000. Time: 0.2252 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #27: GFLOPs: 0.0000. Time: 0.1760 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #28: GFLOPs: 0.0000. Time: 0.2830 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #29: GFLOPs: 0.0000. Time: 1.5235 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #30: GFLOPs: 0.0000. Time: 0.1693 ms. Best GFLOPs: 0.0000
[19:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"] Trial #31: GFLOPs: 0.0000. Time: 0.2782 ms. Best GFLOPs: 0.0000
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"
 ID |                                                                                                 Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                                      fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu |  398894080 |      1 |        69.4381 |    5744.6037 |             5744.6037 |     32 |            
  1 |                                                                                    fused_concatenate |          1 |      1 |         0.0001 |      14.8321 |               14.8321 |     32 |            
  2 |                                                                               fused_layout_transform |          1 |      1 |         0.0001 |      17.5001 |               17.5001 |     32 |            
  3 |                                                                    fused_nn_contrib_conv2d_NCHWc_add |   88301655 |      1 |        64.4650 |    1369.7602 |             1369.7602 |     32 |            
  4 |                           fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate |          1 |      1 |         0.0000 |      43.6440 |               43.6440 |     32 |            
  5 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1 |  399067136 |      1 |        92.9105 |    4295.1796 |             4295.1796 |     32 |            
  6 |                                                                                  fused_concatenate_1 |          1 |      1 |         0.0000 |      27.1122 |               27.1122 |     32 |            
  7 |                                                                             fused_layout_transform_1 |          1 |      1 |         0.0000 |      43.9884 |               43.9884 |     32 |            
  8 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_1 |  176689500 |      1 |        42.5448 |    4153.0226 |             4153.0226 |     32 |            
  9 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1 |          1 |      1 |         0.0000 |     169.3747 |              169.3747 |     32 |            
 10 |                                                                                  fused_nn_max_pool2d |    2163200 |      1 |        19.1105 |     113.1943 |              113.1943 |     32 |            
 11 |                                                                                fused_nn_max_pool2d_1 |    7008768 |      1 |        17.9495 |     390.4721 |              390.4721 |     32 |            
 12 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_2 | 1595057152 |      1 |        93.6486 |   17032.3591 |            17032.3591 |     32 |            
 13 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_3 |  177295872 |      2 |       177.3662 |     999.6036 |             1999.2072 |     32 |            
 14 |                                    fused_exp_layout_transform_add_log_tanh_layout_transform_multiply |     173056 |      5 |         1.1611 |     149.0404 |              745.2021 |     32 |            
 15 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_4 |  797528576 |      4 |        91.2437 |    8740.6383 |            34962.5532 |     32 |            
 16 |                                fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add |     259584 |      4 |         0.9552 |     271.7722 |             1087.0890 |     32 |            
 17 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_5 |   88691200 |      5 |       134.2602 |     660.5919 |             3302.9593 |     32 |            
 18 |   fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_ |     346112 |      1 |         1.0197 |     339.4267 |              339.4267 |     32 |            
 19 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_6 |  354591744 |      1 |       155.3409 |    2282.6685 |             2282.6685 |     32 |            
 20 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1 |     346112 |      2 |         1.1523 |     300.3598 |              600.7197 |     32 |            
 21 |                                                                                fused_nn_max_pool2d_2 |   14623232 |      1 |        27.8106 |     525.8148 |              525.8148 |     32 |            
 22 |                                                                                  fused_concatenate_2 |          1 |      1 |         0.0001 |      10.3095 |               10.3095 |     32 |            
 23 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2 |  354591744 |      1 |       145.3149 |    2440.1604 |             2440.1604 |     32 |            
 24 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3 | 1595230208 |      5 |       148.2293 |   10761.9082 |            53809.5409 |     32 |            
 25 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4 |  177382400 |      6 |        50.2308 |    3531.3469 |            21188.0817 |     32 |            
 26 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5 |   44388864 |      1 |       104.3850 |     425.2417 |              425.2417 |     32 |            
 27 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_7 | 1595230208 |      1 |       137.8990 |   11568.1027 |            11568.1027 |     32 |            
 28 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_8 |  177382400 |      2 |       170.6403 |    1039.5106 |             2079.0212 |     32 |            
 29 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2 |     346112 |      9 |         2.2233 |     155.6738 |             1401.0643 |     32 |            
 30 |                                                                  fused_nn_contrib_conv2d_NCHWc_add_9 |  797615104 |      8 |       250.8098 |    3180.1593 |            25441.2747 |     32 |            
 31 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1 |     519168 |      8 |         3.4074 |     152.3635 |             1218.9079 |     32 |            
 32 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_10 |   88777728 |      9 |       362.9019 |     244.6328 |             2201.6956 |     32 |            
 33 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1 |     692224 |      1 |         2.1464 |     322.5041 |              322.5041 |     32 |            
 34 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_11 |  354764800 |      1 |       428.6387 |     827.6547 |              827.6547 |     32 |            
 35 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3 |     692224 |      2 |         2.2779 |     303.8847 |              607.7695 |     32 |            
 36 |   fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_ |          1 |      1 |         0.0001 |      19.2886 |               19.2886 |     32 |            
 37 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6 | 1595576320 |      5 |       343.4176 |    4646.1688 |            23230.8442 |     32 |            
 38 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7 |  177555456 |      7 |       337.5153 |     526.0664 |             3682.4648 |     32 |            
 39 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8 |   44475392 |      1 |       294.0662 |     151.2428 |              151.2428 |     32 |            
 40 |                                                                     fused_transpose_layout_transform |          1 |      1 |         0.0001 |      10.0099 |               10.0099 |     32 |            
 41 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_12 |  304578560 |      1 |       231.3046 |    1316.7854 |             1316.7854 |     32 |            
 42 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4 |   11075584 |      1 |         2.3044 |    4806.2766 |             4806.2766 |     32 |            
 43 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_13 | 1597652992 |      1 |       264.5293 |    6039.6068 |             6039.6068 |     32 |            
 44 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_14 |  178593792 |      1 |       243.0437 |     734.8218 |              734.8218 |     32 |            
 45 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5 |    2768896 |      1 |         2.2860 |    1211.2376 |             1211.2376 |     32 |            
 46 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_15 | 1597652992 |      1 |       435.7001 |    3666.8641 |             3666.8641 |     32 |            
 47 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2 |    8306688 |      1 |         3.2672 |    2542.4762 |             2542.4762 |     32 |            
 48 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_16 |  357187584 |      3 |       250.7005 |    1424.7581 |             4274.2744 |     32 |            
 49 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2 |   11075584 |      1 |         2.0144 |    5498.1731 |             5498.1731 |     32 |            
 50 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_17 |  711606272 |      1 |       367.5954 |    1935.8411 |             1935.8411 |     32 |            
 51 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6 |    5537792 |      3 |         2.2231 |    2491.0524 |             7473.1572 |     32 |            
 52 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_18 | 1596268544 |      1 |       221.2860 |    7213.6005 |             7213.6005 |     32 |            
 53 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_19 |  177901568 |      2 |       314.4453 |     565.7632 |             1131.5264 |     32 |            
 54 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7 |    1384448 |      3 |         2.2742 |     608.7646 |             1826.2939 |     32 |            
 55 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_20 |  798134272 |      2 |       391.8432 |    2036.8714 |             4073.7428 |     32 |            
 56 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3 |    2076672 |      2 |         3.2133 |     646.2803 |             1292.5606 |     32 |            
 57 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_21 |   89296896 |      3 |       344.0658 |     259.5343 |              778.6030 |     32 |            
 58 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3 |    2768896 |      1 |         1.7625 |    1571.0319 |             1571.0319 |     32 |            
 59 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_22 |  355803136 |      1 |       355.8567 |     999.8495 |              999.8495 |     32 |            
 60 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8 |    2768896 |      2 |         2.2821 |    1213.3050 |             2426.6101 |     32 |            
 61 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_23 | 1595576320 |      1 |       234.2467 |    6811.5219 |             6811.5219 |     32 |            
 62 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_24 |  177555456 |      2 |       301.4543 |     588.9962 |             1177.9924 |     32 |            
 63 |                                  fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9 |     692224 |      9 |         2.2831 |     303.1980 |             2728.7824 |     32 |            
 64 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_25 |  797788160 |      8 |       356.7222 |    2236.4412 |            17891.5298 |     32 |            
 65 |                              fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4 |    1038336 |      8 |         3.3834 |     306.8890 |             2455.1122 |     32 |            
 66 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_26 |   88950784 |      9 |       326.6489 |     272.3131 |             2450.8182 |     32 |            
 67 | fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4 |    1384448 |      1 |         2.0132 |     687.6941 |              687.6941 |     32 |            
 68 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_27 |  355110912 |      1 |       361.1469 |     983.2867 |              983.2867 |     32 |            
 69 |                                 fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10 |    1384448 |      2 |         2.3025 |     601.2840 |             1202.5680 |     32 |            
 70 | fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1 |          1 |      1 |         0.0000 |      37.0297 |               37.0297 |     32 |            
 71 |                                                    fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9 |  177901568 |      4 |       433.2827 |     410.5901 |             1642.3603 |     32 |            
 72 |                                                   fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10 | 1596268544 |      3 |       411.9103 |    3875.2817 |            11625.8451 |     32 |            
 73 |                                                                             fused_layout_transform_2 |          1 |      1 |         0.0001 |      11.3017 |               11.3017 |     32 |            
 74 |                                                                 fused_nn_contrib_conv2d_NCHWc_add_28 |  353723760 |      1 |       243.2147 |    1454.3684 |             1454.3684 |     32 |            
 75 |                         fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2 |          1 |      1 |         0.0000 |     161.5624 |              161.5624 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 2432
Total latency (us): 342031

[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #24: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_3"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #24 has finished. Remaining task(s): 75
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #15: "fused_nn_contrib_conv2d_NCHWc_add_4"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #15 has finished. Remaining task(s): 74
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #30: "fused_nn_contrib_conv2d_NCHWc_add_9"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #30 has finished. Remaining task(s): 73
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #37: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_6"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #37 has finished. Remaining task(s): 72
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #25: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_4"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #25 has finished. Remaining task(s): 71
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #64: "fused_nn_contrib_conv2d_NCHWc_add_25"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #64 has finished. Remaining task(s): 70
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_nn_contrib_conv2d_NCHWc_add_2"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #12 has finished. Remaining task(s): 69
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #72: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_10"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #72 has finished. Remaining task(s): 68
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #27: "fused_nn_contrib_conv2d_NCHWc_add_7"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #27 has finished. Remaining task(s): 67
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #51: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_6"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #51 has finished. Remaining task(s): 66
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #52: "fused_nn_contrib_conv2d_NCHWc_add_18"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #52 has finished. Remaining task(s): 65
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #61: "fused_nn_contrib_conv2d_NCHWc_add_23"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #61 has finished. Remaining task(s): 64
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #43: "fused_nn_contrib_conv2d_NCHWc_add_13"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #43 has finished. Remaining task(s): 63
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #0 has finished. Remaining task(s): 62
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #49: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__2"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #49 has finished. Remaining task(s): 61
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #42: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_4"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #42 has finished. Remaining task(s): 60
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_1"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #5 has finished. Remaining task(s): 59
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #48: "fused_nn_contrib_conv2d_NCHWc_add_16"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #48 has finished. Remaining task(s): 58
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_nn_contrib_conv2d_NCHWc_add_1"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #8 has finished. Remaining task(s): 57
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #55: "fused_nn_contrib_conv2d_NCHWc_add_20"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #55 has finished. Remaining task(s): 56
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #38: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_7"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #38 has finished. Remaining task(s): 55
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #46: "fused_nn_contrib_conv2d_NCHWc_add_15"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #46 has finished. Remaining task(s): 54
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #17: "fused_nn_contrib_conv2d_NCHWc_add_5"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #17 has finished. Remaining task(s): 53
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #63: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_9"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #63 has finished. Remaining task(s): 52
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #47: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_2"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #47 has finished. Remaining task(s): 51
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #65: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_4"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #65 has finished. Remaining task(s): 50
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #66: "fused_nn_contrib_conv2d_NCHWc_add_26"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #66 has finished. Remaining task(s): 49
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #23: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_2"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #23 has finished. Remaining task(s): 48
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #60: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_8"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #60 has finished. Remaining task(s): 47
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #19: "fused_nn_contrib_conv2d_NCHWc_add_6"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #19 has finished. Remaining task(s): 46
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #32: "fused_nn_contrib_conv2d_NCHWc_add_10"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #32 has finished. Remaining task(s): 45
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #28: "fused_nn_contrib_conv2d_NCHWc_add_8"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #28 has finished. Remaining task(s): 44
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_nn_contrib_conv2d_NCHWc_add_3"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #13 has finished. Remaining task(s): 43
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #50: "fused_nn_contrib_conv2d_NCHWc_add_17"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #50 has finished. Remaining task(s): 42
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #54: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_7"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #54 has finished. Remaining task(s): 41
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #71: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_9"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #71 has finished. Remaining task(s): 40
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #58: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__3"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #58 has finished. Remaining task(s): 39
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #74: "fused_nn_contrib_conv2d_NCHWc_add_28"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #74 has finished. Remaining task(s): 38
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #29: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_2"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #29 has finished. Remaining task(s): 37
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_nn_contrib_conv2d_NCHWc_add"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #3 has finished. Remaining task(s): 36
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #41: "fused_nn_contrib_conv2d_NCHWc_add_12"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #41 has finished. Remaining task(s): 35
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #56: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_3"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #56 has finished. Remaining task(s): 34
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #31: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add_1"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #31 has finished. Remaining task(s): 33
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #45: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_5"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #45 has finished. Remaining task(s): 32
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #69: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_10"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #69 has finished. Remaining task(s): 31
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #62: "fused_nn_contrib_conv2d_NCHWc_add_24"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #62 has finished. Remaining task(s): 30
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #53: "fused_nn_contrib_conv2d_NCHWc_add_19"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #53 has finished. Remaining task(s): 29
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #16: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_add"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #16 has finished. Remaining task(s): 28
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #59: "fused_nn_contrib_conv2d_NCHWc_add_22"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #59 has finished. Remaining task(s): 27
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #68: "fused_nn_contrib_conv2d_NCHWc_add_27"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #68 has finished. Remaining task(s): 26
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #34: "fused_nn_contrib_conv2d_NCHWc_add_11"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #34 has finished. Remaining task(s): 25
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #57: "fused_nn_contrib_conv2d_NCHWc_add_21"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #57 has finished. Remaining task(s): 24
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #14: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #14 has finished. Remaining task(s): 23
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #44: "fused_nn_contrib_conv2d_NCHWc_add_14"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #44 has finished. Remaining task(s): 22
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #67: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__4"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #67 has finished. Remaining task(s): 21
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #35: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_3"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #35 has finished. Remaining task(s): 20
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #20: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_1"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #20 has finished. Remaining task(s): 19
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #21: "fused_nn_max_pool2d_2"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #21 has finished. Remaining task(s): 18
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #26: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_5"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #26 has finished. Remaining task(s): 17
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_nn_max_pool2d_1"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #11 has finished. Remaining task(s): 16
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #18: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202_"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #18 has finished. Remaining task(s): 15
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #33: "fused_exp_layout_transform_add_log_tanh_layout_transform_multiply_exp_layout_tra_8b9474d6fb655202__1"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #33 has finished. Remaining task(s): 14
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_1"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #9 has finished. Remaining task(s): 13
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #75: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate_2"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #75 has finished. Remaining task(s): 12
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #39: "fused_nn_contrib_conv2d_NCHWc_add_nn_leaky_relu_8"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #39 has finished. Remaining task(s): 11
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_nn_max_pool2d"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #10 has finished. Remaining task(s): 10
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_layout_transform_1"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #7 has finished. Remaining task(s): 9
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_layout_transform_transpose_reshape_split_sigmoid_sigmoid_concatenate"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #4 has finished. Remaining task(s): 8
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #70: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2__1"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #70 has finished. Remaining task(s): 7
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_concatenate_1"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #6 has finished. Remaining task(s): 6
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #36: "fused_layout_transform_layout_transform_image_resize2d_concatenate_layout_transf_9e799fdbaa8536b2_"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #36 has finished. Remaining task(s): 5
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_layout_transform"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #2 has finished. Remaining task(s): 4
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_concatenate"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #1 has finished. Remaining task(s): 3
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #73: "fused_layout_transform_2"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #73 has finished. Remaining task(s): 2
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #22: "fused_concatenate_2"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #22 has finished. Remaining task(s): 1
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #40: "fused_transpose_layout_transform"
[19:45:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #40 has finished. Remaining task(s): 0
[[[[[-1.15011603e-01 -2.53103912e-01 -5.44018328e-01 ...
      3.75848991e-04  1.41891389e-04  2.64593808e-04]
    [ 7.12434769e-01  6.97403848e-01 -3.59071136e-01 ...
      5.68801945e-04  1.44919861e-04  2.48427328e-04]
    [ 1.52983880e+00  8.07204604e-01 -5.25524437e-01 ...
      7.16879091e-04  1.42338715e-04  1.82303149e-04]]

   [[ 2.42047645e-02 -3.55607927e-01  4.21934932e-01 ...
      2.34160820e-04  1.00615609e-04  8.30609351e-05]
    [-1.62358657e-01  2.85818249e-01  9.84940976e-02 ...
      2.21181952e-04  7.62742275e-05  7.56023728e-05]
    [-2.88756877e-01  3.85440767e-01 -4.41337585e-01 ...
      2.86790921e-04  6.40686194e-05  6.44070678e-05]]

   [[ 4.65826184e-01 -4.66439068e-01  2.04610825e-02 ...
      1.68509374e-04  8.40643115e-05  9.34183045e-05]
    [-1.97605774e-01  3.46761137e-01  5.06270975e-02 ...
      1.76064583e-04  6.39495265e-05  8.09588382e-05]
    [-5.20186305e-01  2.66039878e-01 -1.46361589e-01 ...
      2.33409868e-04  5.50392833e-05  6.19482162e-05]]

   ...

   [[ 1.98679611e-01 -5.55714250e-01 -1.43196046e-01 ...
      1.35751005e-04  6.70553418e-05  5.55661863e-05]
    [-2.48353213e-01  3.86467576e-01 -7.46031106e-03 ...
      1.35296505e-04  4.40347649e-05  4.63115903e-05]
    [-4.63421524e-01  3.54369700e-01 -7.63826966e-02 ...
      1.61761141e-04  3.54608601e-05  3.23674612e-05]]

   [[ 5.43076433e-02 -4.93035674e-01 -1.66601002e-01 ...
      1.76135625e-04  8.34162201e-05  7.74170912e-05]
    [-9.38076712e-03  4.40817803e-01 -1.57088980e-01 ...
      1.93885309e-04  6.03312510e-05  6.82460814e-05]
    [-1.75529450e-01  4.91802245e-01 -3.07507217e-01 ...
      2.44344381e-04  5.17907101e-05  4.98496920e-05]]

   [[ 2.53480636e-02 -1.47970811e-01 -5.08241415e-01 ...
      3.45908484e-04  1.14260569e-04  1.45345271e-04]
    [-5.72777390e-01  9.03449833e-01 -5.67935884e-01 ...
      4.83147858e-04  1.04294850e-04  1.28447020e-04]
    [-1.28416646e+00  9.16818023e-01 -7.53184438e-01 ...
      5.75980346e-04  1.03622144e-04  9.53600684e-05]]]


  [[[-2.68284351e-01  8.29394311e-02 -5.00410318e-01 ...
      2.41916874e-04  9.50885660e-05  8.39688873e-05]
    [ 5.59014082e-01 -1.96863472e-01 -5.93530297e-01 ...
      2.97102088e-04  6.45535329e-05  8.60847504e-05]
    [ 1.52947521e+00 -1.65270403e-01 -1.05867839e+00 ...
      2.98277650e-04  5.85031048e-05  6.94202099e-05]]

   [[-2.40731910e-01  1.63846403e-01  5.30536354e-01 ...
      9.82065685e-05  8.62734378e-05  3.05255671e-05]
    [-2.76128441e-01 -8.14899802e-03  1.58538967e-01 ...
      8.60240252e-05  4.22017511e-05  2.65851686e-05]
    [-3.18865597e-01  1.52910650e-02 -5.69780231e-01 ...
      8.72963865e-05  3.21013758e-05  2.31529411e-05]]

   [[-6.86689764e-02  6.44007862e-01 -2.04850912e-01 ...
      5.95674319e-05  6.03615772e-05  4.48982682e-05]
    [-3.53131294e-01  2.51228273e-01  1.22164197e-01 ...
      6.74520052e-05  3.07471128e-05  2.98694358e-05]
    [-4.20113087e-01  2.18934089e-01 -1.88042700e-01 ...
      7.22529658e-05  2.31129216e-05  2.06367022e-05]]

   ...

   [[-4.27095860e-01  5.62961996e-01 -1.83581173e-01 ...
      5.62296700e-05  5.12637635e-05  1.91988765e-05]
    [-2.26196244e-01  2.15779305e-01 -8.92005414e-02 ...
      5.24599200e-05  2.06069089e-05  1.43040716e-05]
    [-2.96924450e-02  2.72801310e-01 -4.42456663e-01 ...
      4.99169582e-05  1.46581151e-05  9.92022069e-06]]

   [[ 1.76391333e-01  2.21061349e-01 -2.69523680e-01 ...
      7.63843564e-05  6.23715678e-05  2.68480253e-05]
    [ 1.85772300e-01 -4.48028371e-02 -3.05541277e-01 ...
      7.36828370e-05  2.71983208e-05  2.14498777e-05]
    [ 3.86923216e-02  3.22301760e-02 -5.82798243e-01 ...
      7.51069092e-05  2.05252400e-05  1.53302008e-05]]

   [[ 4.25392687e-01 -5.48881590e-01 -5.75017869e-01 ...
      1.80059753e-04  9.82155616e-05  7.83030700e-05]
    [-4.52754170e-01 -3.24432850e-01 -6.03174806e-01 ...
      2.16999382e-04  6.77363932e-05  7.00428136e-05]
    [-1.37253058e+00 -6.28110468e-02 -9.87479210e-01 ...
      2.29333687e-04  6.28608977e-05  5.36517400e-05]]]


  [[[-2.33051568e-01 -2.47829095e-01 -4.59792256e-01 ...
      1.57575196e-04  1.07572130e-04  1.13948838e-04]
    [ 5.78260839e-01 -2.99338400e-01 -5.37969768e-01 ...
      2.02592331e-04  8.39971617e-05  1.09805551e-04]
    [ 1.60936821e+00 -9.75577533e-02 -9.75124836e-01 ...
      2.09354039e-04  7.78294561e-05  8.55656326e-05]]

   [[ 6.73617795e-03  1.67120799e-01  2.35076264e-01 ...
      6.92140457e-05  6.68547873e-05  3.23115783e-05]
    [-2.89723694e-01 -2.03071415e-01  7.19908029e-02 ...
      6.77072094e-05  3.61661005e-05  2.57806951e-05]
    [-3.51656526e-01 -1.02454633e-01 -5.28575838e-01 ...
      6.96247444e-05  2.80231252e-05  2.03638447e-05]]

   [[ 6.35522008e-02  1.98673606e-02 -2.01995492e-01 ...
      5.13488958e-05  5.74819096e-05  4.78575348e-05]
    [-4.39831167e-01 -2.80831724e-01  6.63548335e-02 ...
      5.87341929e-05  3.19256505e-05  3.07387272e-05]
    [-4.86884713e-01  7.70481862e-03 -2.23862022e-01 ...
      6.33294258e-05  2.47780663e-05  2.01844396e-05]]

   ...

   [[-8.01455751e-02  4.81647491e-01 -9.47602987e-02 ...
      4.43638710e-05  4.94492342e-05  1.77598104e-05]
    [-1.49301752e-01 -1.03934295e-02 -1.09464660e-01 ...
      4.20825127e-05  2.01511102e-05  1.29122354e-05]
    [-3.00993770e-02  7.68766403e-02 -5.13843834e-01 ...
      3.90301029e-05  1.43664120e-05  8.90551473e-06]]

   [[ 5.18298686e-01  5.45781910e-01 -4.41406131e-01 ...
      5.12487059e-05  5.51304256e-05  2.77337131e-05]
    [ 3.25398654e-01 -1.67705491e-02 -2.09485814e-01 ...
      6.13583979e-05  2.65003964e-05  2.17197157e-05]
    [ 1.87603123e-02 -8.79032463e-02 -3.99812281e-01 ...
      6.30391878e-05  2.04734533e-05  1.43687803e-05]]

   [[-5.37221730e-01  4.76511180e-01 -3.17459762e-01 ...
      1.21442834e-04  8.72903984e-05  5.50510922e-05]
    [-7.00004756e-01  1.60587877e-01 -4.82496977e-01 ...
      1.38417119e-04  5.50104778e-05  4.72135216e-05]
    [-1.32257354e+00  1.09808981e-01 -9.85718012e-01 ...
      1.37208815e-04  4.82971664e-05  3.76437347e-05]]]


  ...


  [[[-3.45685363e-01  1.47176757e-01 -6.41862094e-01 ...
      1.81437295e-04  1.21252378e-04  1.24401267e-04]
    [ 3.71810913e-01 -6.96136728e-02 -5.77610791e-01 ...
      2.69420590e-04  9.83947975e-05  1.07468586e-04]
    [ 1.30825043e+00 -6.23629987e-03 -9.20722246e-01 ...
      2.81773362e-04  9.19830127e-05  8.08573022e-05]]

   [[-1.01039924e-01  6.05673790e-01 -4.33365762e-01 ...
      6.56213306e-05  8.22750590e-05  5.39051871e-05]
    [-3.78305554e-01  1.61390483e-01  5.97739071e-02 ...
      9.59169265e-05  5.60373883e-05  2.96484486e-05]
    [-5.29330194e-01  1.49665356e-01 -2.56209522e-01 ...
      1.01440164e-04  4.41959746e-05  1.88515478e-05]]

   [[-9.31136757e-02  7.53848404e-02 -5.36222517e-01 ...
      5.11940452e-05  6.84185579e-05  5.05183416e-05]
    [-2.68832207e-01 -6.31660596e-02  9.10860151e-02 ...
      7.23885460e-05  4.25311882e-05  2.60905763e-05]
    [-2.26047814e-01  1.90958735e-02 -6.69566989e-02 ...
      7.58011738e-05  3.29954455e-05  1.59078609e-05]]

   ...

   [[ 2.24910662e-01  5.79197764e-01 -5.06229520e-01 ...
      5.81013010e-05  6.68392313e-05  3.82397593e-05]
    [-2.32691988e-02  1.13108680e-01 -1.17811427e-01 ...
      7.53839486e-05  3.91362883e-05  2.30972346e-05]
    [-1.06671616e-01  3.36510688e-02 -2.67731309e-01 ...
      8.06331591e-05  3.16909245e-05  1.50990099e-05]]

   [[-8.80521536e-02  4.13341880e-01 -3.13784599e-01 ...
      7.97690227e-05  8.04568917e-05  4.56381276e-05]
    [-7.74482340e-02  2.98977457e-02 -1.98777899e-01 ...
      9.83386126e-05  4.91339197e-05  2.95282152e-05]
    [-2.22046196e-01  2.09955908e-02 -4.55955774e-01 ...
      1.04912586e-04  4.03061531e-05  2.06924542e-05]]

   [[ 2.08815679e-01 -4.20800328e-01 -5.13007581e-01 ...
      2.11970211e-04  1.25079154e-04  1.18090917e-04]
    [-4.98422146e-01  3.79711576e-02 -5.66863835e-01 ...
      3.04991641e-04  1.06643529e-04  9.03254258e-05]
    [-1.36499989e+00  2.19102949e-01 -9.34497476e-01 ...
      3.25983885e-04  1.04059509e-04  6.94401460e-05]]]


  [[[-2.77468115e-01 -1.94206938e-01 -6.40814364e-01 ...
      2.95997161e-04  1.27969251e-04  1.05539810e-04]
    [ 3.33281249e-01  2.40298480e-01 -6.01606131e-01 ...
      4.22666169e-04  1.05343039e-04  8.90033189e-05]
    [ 1.25243795e+00  3.60670149e-01 -9.23388004e-01 ...
      4.34613728e-04  9.87831663e-05  6.64793843e-05]]

   [[ 7.19704032e-01 -1.74609795e-01 -5.80083072e-01 ...
      1.15255491e-04  1.11867819e-04  7.25529608e-05]
    [-2.00625196e-01  3.61485839e-01  1.95967108e-02 ...
      1.82048490e-04  9.55855794e-05  4.17812480e-05]
    [-6.32219255e-01  4.47534382e-01 -1.65502638e-01 ...
      2.02064504e-04  8.21143040e-05  2.70479868e-05]]

   [[ 4.54074144e-03 -1.00342169e-01 -5.90906680e-01 ...
      7.65741279e-05  9.22315812e-05  6.14912497e-05]
    [-1.81259811e-01  3.23664486e-01  2.11284116e-01 ...
      1.10719033e-04  6.83256512e-05  3.09936368e-05]
    [-9.98772085e-02  3.94723117e-01  8.37428868e-02 ...
      1.16213298e-04  5.53810823e-05  1.89551320e-05]]

   ...

   [[ 1.05534956e-01 -5.41759789e-01 -5.11982203e-01 ...
      9.88004031e-05  8.72696764e-05  6.04372617e-05]
    [-1.93513781e-02  1.48129910e-02 -2.87039578e-03 ...
      1.33903319e-04  6.31383373e-05  3.68845213e-05]
    [-9.08656269e-02  1.62540272e-01 -6.26987219e-02 ...
      1.52096400e-04  5.18815687e-05  2.44616713e-05]]

   [[-2.21453279e-01 -5.15830517e-01 -4.79025304e-01 ...
      1.14938630e-04  9.65179934e-05  6.21490399e-05]
    [-1.29728094e-01 -2.90565565e-02 -5.86250126e-02 ...
      1.64114084e-04  7.54111243e-05  4.17835981e-05]
    [-1.53730363e-01  1.69080392e-01 -2.56980032e-01 ...
      1.90287727e-04  6.39297141e-05  2.95935315e-05]]

   [[ 9.29571241e-02  5.62551796e-01 -4.63988721e-01 ...
      3.43800610e-04  1.36121598e-04  1.12234527e-04]
    [-4.44705546e-01  5.40963888e-01 -5.66132188e-01 ...
      4.86209261e-04  1.22012527e-04  8.99294755e-05]
    [-1.21574271e+00  4.13381547e-01 -9.36604381e-01 ...
      5.04424621e-04  1.17241550e-04  7.09917731e-05]]]


  [[[ 1.41731933e-01 -3.61499041e-02 -5.33501685e-01 ...
      6.08632574e-04  1.74026674e-04  2.19723835e-04]
    [ 6.83811843e-01 -7.64137328e-01 -4.87897038e-01 ...
      9.43988794e-04  1.95473491e-04  2.05831093e-04]
    [ 1.21260905e+00 -8.02047610e-01 -6.00913405e-01 ...
      1.10168720e-03  1.88243997e-04  1.55321162e-04]]

   [[ 4.79009837e-01  8.41192603e-02 -5.18653929e-01 ...
      2.64528237e-04  1.56106908e-04  1.28563915e-04]
    [-1.35344431e-01 -6.63860381e-01 -2.66352952e-01 ...
      3.74705589e-04  1.77708702e-04  1.07542590e-04]
    [-5.70028186e-01 -7.53103256e-01 -2.30733648e-01 ...
      4.61072632e-04  1.67215781e-04  7.50678082e-05]]

   [[ 1.64623901e-01  1.35887325e-01 -3.68444681e-01 ...
      1.70383719e-04  1.37605690e-04  9.59488607e-05]
    [-1.55155994e-02 -5.49035072e-01 -1.73518226e-01 ...
      2.20655988e-04  1.37171141e-04  7.50805557e-05]
    [-1.52629152e-01 -5.68277419e-01 -1.43152356e-01 ...
      2.70043180e-04  1.23747712e-04  5.19729329e-05]]

   ...

   [[ 3.34069163e-01  2.50912428e-01 -5.46497166e-01 ...
      1.72784450e-04  1.35772760e-04  1.19389260e-04]
    [-3.70029211e-02 -7.13244379e-01 -1.47026494e-01 ...
      2.40047302e-04  1.42163597e-04  9.39934835e-05]
    [-3.16304207e-01 -7.05533326e-01 -1.68485343e-02 ...
      2.90691940e-04  1.28586107e-04  6.52859599e-05]]

   [[ 1.27252594e-01  3.10121179e-01 -2.76713252e-01 ...
      1.85057506e-04  1.17939780e-04  9.95013834e-05]
    [ 1.24920920e-01 -6.09691203e-01 -1.13427714e-01 ...
      2.49125151e-04  1.20622957e-04  8.29604032e-05]
    [-4.21137027e-02 -7.99894154e-01 -1.82836235e-01 ...
      3.01956054e-04  1.08165230e-04  6.23656742e-05]]

   [[ 1.09190680e-02  1.14602908e-01 -5.13058364e-01 ...
      2.89942400e-04  1.44496662e-04  1.65173609e-04]
    [-5.72071075e-01 -5.55777311e-01 -5.22369266e-01 ...
      4.42065473e-04  1.58876152e-04  1.54810899e-04]
    [-1.27363396e+00 -6.55373573e-01 -7.41131485e-01 ...
      5.16947126e-04  1.65655612e-04  1.21321886e-04]]]]]
[[[[[-1.15012132e-01 -2.53103852e-01 -5.44018567e-01 ...
      3.75848613e-04  1.41891258e-04  2.64593808e-04]
    [ 7.12434173e-01  6.97403610e-01 -3.59071314e-01 ...
      5.68801654e-04  1.44919861e-04  2.48427328e-04]
    [ 1.52983963e+00  8.07204783e-01 -5.25524318e-01 ...
      7.16878683e-04  1.42338715e-04  1.82303149e-04]]

   [[ 2.42051072e-02 -3.55608046e-01  4.21934634e-01 ...
      2.34160820e-04  1.00615514e-04  8.30609351e-05]
    [-1.62358850e-01  2.85818160e-01  9.84940678e-02 ...
      2.21181952e-04  7.62742275e-05  7.56023728e-05]
    [-2.88757473e-01  3.85440528e-01 -4.41337705e-01 ...
      2.86790921e-04  6.40686194e-05  6.44070678e-05]]

   [[ 4.65826839e-01 -4.66439664e-01  2.04601884e-02 ...
      1.68509374e-04  8.40643115e-05  9.34183918e-05]
    [-1.97605416e-01  3.46761495e-01  5.06268144e-02 ...
      1.76064917e-04  6.39495265e-05  8.09588382e-05]
    [-5.20186901e-01  2.66040266e-01 -1.46361053e-01 ...
      2.33410319e-04  5.50393343e-05  6.19482162e-05]]

   ...

   [[ 1.98679850e-01 -5.55714190e-01 -1.43196285e-01 ...
      1.35750874e-04  6.70552763e-05  5.55661863e-05]
    [-2.48353705e-01  3.86467576e-01 -7.46020675e-03 ...
      1.35296388e-04  4.40347649e-05  4.63115903e-05]
    [-4.63422358e-01  3.54369760e-01 -7.63824582e-02 ...
      1.61760981e-04  3.54608601e-05  3.23674612e-05]]

   [[ 5.43081276e-02 -4.93035257e-01 -1.66601121e-01 ...
      1.76135451e-04  8.34162201e-05  7.74171567e-05]
    [-9.38018039e-03  4.40818012e-01 -1.57089069e-01 ...
      1.93885106e-04  6.03311928e-05  6.82461396e-05]
    [-1.75528720e-01  4.91802841e-01 -3.07507426e-01 ...
      2.44344148e-04  5.17906628e-05  4.98497393e-05]]

   [[ 2.53479965e-02 -1.47969857e-01 -5.08241355e-01 ...
      3.45908484e-04  1.14260569e-04  1.45345271e-04]
    [-5.72777510e-01  9.03450072e-01 -5.67936122e-01 ...
      4.83147858e-04  1.04294850e-04  1.28447136e-04]
    [-1.28416634e+00  9.16818321e-01 -7.53184497e-01 ...
      5.75980346e-04  1.03622144e-04  9.53600684e-05]]]


  [[[-2.68283606e-01  8.29393640e-02 -5.00409782e-01 ...
      2.41916874e-04  9.50885660e-05  8.39689674e-05]
    [ 5.59014499e-01 -1.96863592e-01 -5.93530357e-01 ...
      2.97102379e-04  6.45534747e-05  8.60847504e-05]
    [ 1.52947521e+00 -1.65270343e-01 -1.05867839e+00 ...
      2.98277650e-04  5.85030466e-05  6.94202099e-05]]

   [[-2.40730658e-01  1.63847119e-01  5.30535579e-01 ...
      9.82066631e-05  8.62733505e-05  3.05255671e-05]
    [-2.76127845e-01 -8.14817846e-03  1.58538893e-01 ...
      8.60240252e-05  4.22016710e-05  2.65851413e-05]
    [-3.18865150e-01  1.52920112e-02 -5.69780052e-01 ...
      8.72963865e-05  3.21013467e-05  2.31529411e-05]]

   [[-6.86690360e-02  6.44008636e-01 -2.04851449e-01 ...
      5.95675410e-05  6.03615772e-05  4.48982682e-05]
    [-3.53131026e-01  2.51229644e-01  1.22162722e-01 ...
      6.74520052e-05  3.07471128e-05  2.98694358e-05]
    [-4.20112997e-01  2.18935207e-01 -1.88043624e-01 ...
      7.22531040e-05  2.31129216e-05  2.06367222e-05]]

   ...

   [[-4.27096874e-01  5.62961876e-01 -1.83580995e-01 ...
      5.62296154e-05  5.12637089e-05  1.91988765e-05]
    [-2.26196975e-01  2.15779155e-01 -8.92003924e-02 ...
      5.24598181e-05  2.06069089e-05  1.43040716e-05]
    [-2.96928994e-02  2.72801131e-01 -4.42456782e-01 ...
      4.99169582e-05  1.46581024e-05  9.92022069e-06]]

   [[ 1.76391840e-01  2.21061036e-01 -2.69523919e-01 ...
      7.63842836e-05  6.23715023e-05  2.68480508e-05]
    [ 1.85772792e-01 -4.48030904e-02 -3.05540979e-01 ...
      7.36827642e-05  2.71982935e-05  2.14499196e-05]
    [ 3.86927165e-02  3.22299823e-02 -5.82798123e-01 ...
      7.51068364e-05  2.05252400e-05  1.53302153e-05]]

   [[ 4.25392985e-01 -5.48881829e-01 -5.75018108e-01 ...
      1.80059753e-04  9.82155616e-05  7.83030700e-05]
    [-4.52754349e-01 -3.24432701e-01 -6.03175104e-01 ...
      2.16999382e-04  6.77363932e-05  7.00428136e-05]
    [-1.37253106e+00 -6.28109723e-02 -9.87478971e-01 ...
      2.29333687e-04  6.28608395e-05  5.36517400e-05]]]


  [[[-2.33051330e-01 -2.47829184e-01 -4.59791601e-01 ...
      1.57575341e-04  1.07572130e-04  1.13948838e-04]
    [ 5.78260958e-01 -2.99338251e-01 -5.37969410e-01 ...
      2.02592331e-04  8.39971617e-05  1.09805442e-04]
    [ 1.60936809e+00 -9.75579172e-02 -9.75124836e-01 ...
      2.09354039e-04  7.78295362e-05  8.55656326e-05]]

   [[ 6.73645362e-03  1.67120546e-01  2.35076427e-01 ...
      6.92140457e-05  6.68547873e-05  3.23115491e-05]
    [-2.89723456e-01 -2.03070790e-01  7.19908699e-02 ...
      6.77073331e-05  3.61661005e-05  2.57806951e-05]
    [-3.51656079e-01 -1.02454305e-01 -5.28575718e-01 ...
      6.96248098e-05  2.80231252e-05  2.03638447e-05]]

   [[ 6.35515749e-02  1.98661163e-02 -2.01995850e-01 ...
      5.13489431e-05  5.74819096e-05  4.78574912e-05]
    [-4.39831316e-01 -2.80832231e-01  6.63541406e-02 ...
      5.87341929e-05  3.19256178e-05  3.07386690e-05]
    [-4.86884415e-01  7.70436414e-03 -2.23862380e-01 ...
      6.33293675e-05  2.47780663e-05  2.01844014e-05]]

   ...

   [[-8.01454633e-02  4.81646061e-01 -9.47600603e-02 ...
      4.43638710e-05  4.94491869e-05  1.77597940e-05]
    [-1.49301887e-01 -1.03946254e-02 -1.09464422e-01 ...
      4.20825127e-05  2.01511102e-05  1.29122354e-05]
    [-3.00997756e-02  7.68760443e-02 -5.13844073e-01 ...
      3.90301029e-05  1.43664120e-05  8.90551473e-06]]

   [[ 5.18299162e-01  5.45781732e-01 -4.41406131e-01 ...
      5.12487059e-05  5.51304256e-05  2.77337658e-05]
    [ 3.25399131e-01 -1.67711750e-02 -2.09485188e-01 ...
      6.13585144e-05  2.65004201e-05  2.17197357e-05]
    [ 1.87610202e-02 -8.79040062e-02 -3.99811864e-01 ...
      6.30392533e-05  2.04734733e-05  1.43687948e-05]]

   [[-5.37221193e-01  4.76510406e-01 -3.17459702e-01 ...
      1.21442834e-04  8.72903984e-05  5.50510922e-05]
    [-7.00004637e-01  1.60586804e-01 -4.82496917e-01 ...
      1.38417250e-04  5.50104778e-05  4.72135653e-05]
    [-1.32257378e+00  1.09807953e-01 -9.85717773e-01 ...
      1.37208946e-04  4.82971191e-05  3.76438074e-05]]]


  ...


  [[[-3.45685482e-01  1.47177055e-01 -6.41862452e-01 ...
      1.81437470e-04  1.21252378e-04  1.24401398e-04]
    [ 3.71810317e-01 -6.96135461e-02 -5.77611089e-01 ...
      2.69420852e-04  9.83947975e-05  1.07468586e-04]
    [ 1.30824935e+00 -6.23643398e-03 -9.20721889e-01 ...
      2.81773653e-04  9.19831000e-05  8.08573022e-05]]

   [[-1.01039663e-01  6.05673492e-01 -4.33366001e-01 ...
      6.56213961e-05  8.22751390e-05  5.39051871e-05]
    [-3.78304482e-01  1.61390662e-01  5.97744435e-02 ...
      9.59171157e-05  5.60373883e-05  2.96484486e-05]
    [-5.29329181e-01  1.49665743e-01 -2.56208748e-01 ...
      1.01440353e-04  4.41959746e-05  1.88515296e-05]]

   [[-9.31132808e-02  7.53843635e-02 -5.36221921e-01 ...
      5.11940016e-05  6.84184997e-05  5.05182397e-05]
    [-2.68832058e-01 -6.31669238e-02  9.10857469e-02 ...
      7.23884805e-05  4.25311046e-05  2.60905272e-05]
    [-2.26046994e-01  1.90952793e-02 -6.69570565e-02 ...
      7.58011738e-05  3.29954128e-05  1.59078154e-05]]

   ...

   [[ 2.24910155e-01  5.79197645e-01 -5.06229162e-01 ...
      5.81012428e-05  6.68392313e-05  3.82397229e-05]
    [-2.32689455e-02  1.13108419e-01 -1.17811248e-01 ...
      7.53840213e-05  3.91362883e-05  2.30971909e-05]
    [-1.06671095e-01  3.36509496e-02 -2.67731518e-01 ...
      8.06332318e-05  3.16909245e-05  1.50989954e-05]]

   [[-8.80527869e-02  4.13342357e-01 -3.13784778e-01 ...
      7.97689499e-05  8.04568917e-05  4.56380403e-05]
    [-7.74489790e-02  2.98975632e-02 -1.98778048e-01 ...
      9.83385180e-05  4.91338724e-05  2.95281570e-05]
    [-2.22046733e-01  2.09954418e-02 -4.55955893e-01 ...
      1.04912382e-04  4.03061167e-05  2.06924142e-05]]

   [[ 2.08816752e-01 -4.20800805e-01 -5.13008177e-01 ...
      2.11969818e-04  1.25079023e-04  1.18090684e-04]
    [-4.98421967e-01  3.79706435e-02 -5.66863775e-01 ...
      3.04991641e-04  1.06643427e-04  9.03253458e-05]
    [-1.36500049e+00  2.19102487e-01 -9.34497237e-01 ...
      3.25983594e-04  1.04059312e-04  6.94400151e-05]]]


  [[[-2.77468145e-01 -1.94208100e-01 -6.40814841e-01 ...
      2.95997161e-04  1.27969382e-04  1.05539919e-04]
    [ 3.33280921e-01  2.40298659e-01 -6.01605892e-01 ...
      4.22666577e-04  1.05343250e-04  8.90034062e-05]
    [ 1.25243771e+00  3.60670537e-01 -9.23387527e-01 ...
      4.34614136e-04  9.87832609e-05  6.64794497e-05]]

   [[ 7.19703972e-01 -1.74609646e-01 -5.80082476e-01 ...
      1.15255600e-04  1.11867819e-04  7.25530263e-05]
    [-2.00625062e-01  3.61486614e-01  1.95969045e-02 ...
      1.82048490e-04  9.55856667e-05  4.17812480e-05]
    [-6.32218778e-01  4.47534859e-01 -1.65502399e-01 ...
      2.02064504e-04  8.21143040e-05  2.70479613e-05]]

   [[ 4.54159081e-03 -1.00341752e-01 -5.90906203e-01 ...
      7.65741279e-05  9.22315812e-05  6.14913079e-05]
    [-1.81259796e-01  3.23664725e-01  2.11284369e-01 ...
      1.10719127e-04  6.83255857e-05  3.09936659e-05]
    [-9.98771340e-02  3.94723296e-01  8.37430656e-02 ...
      1.16213414e-04  5.53810278e-05  1.89551320e-05]]

   ...

   [[ 1.05533674e-01 -5.41760206e-01 -5.11982083e-01 ...
      9.88005922e-05  8.72696764e-05  6.04372035e-05]
    [-1.93522051e-02  1.48128606e-02 -2.87063420e-03 ...
      1.33903435e-04  6.31383955e-05  3.68844885e-05]
    [-9.08655524e-02  1.62539855e-01 -6.26996160e-02 ...
      1.52096691e-04  5.18816632e-05  2.44616713e-05]]

   [[-2.21453726e-01 -5.15831649e-01 -4.79025781e-01 ...
      1.14938528e-04  9.65179934e-05  6.21490981e-05]
    [-1.29728407e-01 -2.90570632e-02 -5.86245656e-02 ...
      1.64114244e-04  7.54111243e-05  4.17835981e-05]
    [-1.53730869e-01  1.69079870e-01 -2.56979287e-01 ...
      1.90287916e-04  6.39297141e-05  2.95935042e-05]]

   [[ 9.29574519e-02  5.62551141e-01 -4.63988960e-01 ...
      3.43800784e-04  1.36121453e-04  1.12234418e-04]
    [-4.44705397e-01  5.40964067e-01 -5.66132605e-01 ...
      4.86209261e-04  1.22012294e-04  8.99293955e-05]
    [-1.21574247e+00  4.13381964e-01 -9.36604142e-01 ...
      5.04424621e-04  1.17241441e-04  7.09917076e-05]]]


  [[[ 1.41731575e-01 -3.61494720e-02 -5.33501923e-01 ...
      6.08633738e-04  1.74026674e-04  2.19723835e-04]
    [ 6.83812439e-01 -7.64137030e-01 -4.87896860e-01 ...
      9.43990657e-04  1.95473491e-04  2.05831093e-04]
    [ 1.21260941e+00 -8.02047253e-01 -6.00912929e-01 ...
      1.10168930e-03  1.88243997e-04  1.55321162e-04]]

   [[ 4.79008883e-01  8.41188356e-02 -5.18652618e-01 ...
      2.64528469e-04  1.56107038e-04  1.28563915e-04]
    [-1.35344625e-01 -6.63860261e-01 -2.66352534e-01 ...
      3.74705764e-04  1.77708876e-04  1.07542590e-04]
    [-5.70027471e-01 -7.53103256e-01 -2.30733544e-01 ...
      4.61073098e-04  1.67215941e-04  7.50678082e-05]]

   [[ 1.64624780e-01  1.35885790e-01 -3.68444681e-01 ...
      1.70383719e-04  1.37605821e-04  9.59487588e-05]
    [-1.55154057e-02 -5.49035549e-01 -1.73518375e-01 ...
      2.20655988e-04  1.37171271e-04  7.50805557e-05]
    [-1.52629763e-01 -5.68277597e-01 -1.43152386e-01 ...
      2.70043442e-04  1.23747712e-04  5.19729329e-05]]

   ...

   [[ 3.34069729e-01  2.50912905e-01 -5.46497166e-01 ...
      1.72784610e-04  1.35772760e-04  1.19389144e-04]
    [-3.70030925e-02 -7.13244081e-01 -1.47026226e-01 ...
      2.40047753e-04  1.42163743e-04  9.39933961e-05]
    [-3.16304415e-01 -7.05533504e-01 -1.68483555e-02 ...
      2.90692464e-04  1.28586238e-04  6.52858434e-05]]

   [[ 1.27251342e-01  3.10121179e-01 -2.76713669e-01 ...
      1.85057506e-04  1.17939780e-04  9.95013834e-05]
    [ 1.24920338e-01 -6.09690845e-01 -1.13427922e-01 ...
      2.49125151e-04  1.20623074e-04  8.29604032e-05]
    [-4.21134010e-02 -7.99894154e-01 -1.82836264e-01 ...
      3.01956345e-04  1.08165332e-04  6.23656742e-05]]

   [[ 1.09195150e-02  1.14602797e-01 -5.13058543e-01 ...
      2.89942109e-04  1.44496516e-04  1.65173449e-04]
    [-5.72070718e-01 -5.55777133e-01 -5.22369444e-01 ...
      4.42064833e-04  1.58876006e-04  1.54810739e-04]
    [-1.27363360e+00 -6.55373573e-01 -7.41131663e-01 ...
      5.16946660e-04  1.65655467e-04  1.21321769e-04]]]]]
