Starting to build with relay.
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
[16:16:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #0: "fused_nn_avg_pool2d"
[16:16:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], tensor: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 272, 16, 16], dtype="float32")
        tensor_1 = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 272, 16, 16):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                T.writes(pad_temp[ax0, ax1, ax2, ax3])
                pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 15 and 1 <= ax3 and ax3 < 15, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 272, 7, 7, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 * 2 + 1, 13) + 2 - T.max(1 - ax2 * 2, 0) - ax2 * 2) * (T.min(ax3 * 2 + 1, 13) + 2 - T.max(1 - ax3 * 2, 0) - ax3 * 2), 1), "float32")
    

[16:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], tensor: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            tensor_1 = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 272, 7, 7, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + T.if_then_else(1 <= ax2 * 2 + rv0 and ax2 * 2 + rv0 < 15 and 1 <= ax3 * 2 + rv1 and ax3 * 2 + rv1 < 15, placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1], T.float32(0), dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 * 2 + 1, 13) + 2 - T.max(1 - ax2 * 2, 0) - ax2 * 2) * (T.min(ax3 * 2 + 1, 13) + 2 - T.max(1 - ax3 * 2, 0) - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[16:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #1: "fused_nn_avg_pool2d_1"
[16:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], tensor: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 136, 30, 30], dtype="float32")
        tensor_1 = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 136, 30, 30):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                T.writes(pad_temp[ax0, ax1, ax2, ax3])
                pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 29 and 1 <= ax3 and ax3 < 29, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 136, 14, 14, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 * 2 + 1, 27) + 2 - T.max(1 - ax2 * 2, 0) - ax2 * 2) * (T.min(ax3 * 2 + 1, 27) + 2 - T.max(1 - ax3 * 2, 0) - ax3 * 2), 1), "float32")
    

[16:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], tensor: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            tensor_1 = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 136, 14, 14, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + T.if_then_else(1 <= ax2 * 2 + rv0 and ax2 * 2 + rv0 < 29 and 1 <= ax3 * 2 + rv1 and ax3 * 2 + rv1 < 29, placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1], T.float32(0), dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 * 2 + 1, 27) + 2 - T.max(1 - ax2 * 2, 0) - ax2 * 2) * (T.min(ax3 * 2 + 1, 27) + 2 - T.max(1 - ax3 * 2, 0) - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[16:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #2: "fused_nn_avg_pool2d_2"
[16:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], tensor: T.Buffer[(1, 24, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 24, 58, 58], dtype="float32")
        tensor_1 = T.alloc_buffer([1, 24, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 24, 58, 58):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                T.writes(pad_temp[ax0, ax1, ax2, ax3])
                pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 57 and 1 <= ax3 and ax3 < 57, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 24, 28, 28, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 24, 28, 28):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 * 2 + 1, 55) + 2 - T.max(1 - ax2 * 2, 0) - ax2 * 2) * (T.min(ax3 * 2 + 1, 55) + 2 - T.max(1 - ax3 * 2, 0) - ax3 * 2), 1), "float32")
    

[16:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], tensor: T.Buffer[(1, 24, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            tensor_1 = T.alloc_buffer([1, 24, 28, 28], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 24, 28, 28, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + T.if_then_else(1 <= ax2 * 2 + rv0 and ax2 * 2 + rv0 < 57 and 1 <= ax3 * 2 + rv1 and ax3 * 2 + rv1 < 57, placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1], T.float32(0), dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 24, 28, 28):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 * 2 + 1, 55) + 2 - T.max(1 - ax2 * 2, 0) - ax2 * 2) * (T.min(ax3 * 2 + 1, 55) + 2 - T.max(1 - ax3 * 2, 0) - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[16:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #3: "fused_nn_conv2d_add_add_nn_relu"
[16:16:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(24, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 24, 1, 1), "float32"], T_relu: T.Buffer[(1, 24, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 3, 226, 226], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 24, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 24, 112, 112], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 24, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 226, 226):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 225 and 1 <= i3_1 and i3_1 < 225, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 24, 112, 112, 3, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [24, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 24, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 24, 112, 112):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 24, 112, 112):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:16:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(24, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 24, 1, 1), "float32"], T_relu: T.Buffer[(1, 24, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([24, 3, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(28, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(3, 3, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3345):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(226, i5_0 + ax0_ax1_ax2_ax3_fused % 3345 // 15)
                                    v3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 14 * 16 + i6_0 + ax0_ax1_ax2_ax3_fused % 15)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(12):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 14 * 12 + ax0_ax1_ax2_ax3_fused)
                                    v1, v2, v3 = T.axis.remap("SSS", [i4_0, i5_0, i6_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 12, 8, 1, 1, 1, 1, 1, 1, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 14 * 12 + i1_3)
                                    yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused // 4 * 16 + i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_0, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [24, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 12, 16, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 14 * 12 + ax1)
                                v2 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused // 4 * 16 + ax2)
                                v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 12, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 8, 2])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[14, 2, 4, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
[16:16:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #4: "fused_nn_max_pool2d"
[16:16:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112), "float32"], tensor: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 24, 114, 114], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 24, 114, 114):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                T.writes(pad_temp[ax0, ax1, ax2, ax3])
                pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 113 and 1 <= ax3 and ax3 < 113, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 24, 56, 56, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[16:16:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112), "float32"], tensor: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 24, 56, 56, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], T.if_then_else(1 <= ax2 * 2 + rv0 and ax2 * 2 + rv0 < 113 and 1 <= ax3 * 2 + rv1 and ax3 * 2 + rv1 < 113, placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1], T.float32(-3.4028234663852886e+38), dtype="float32"))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[16:16:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #5: "fused_nn_conv2d_multiply_add_nn_relu"
[16:16:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(112, 6, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1), "float32"], T_relu: T.Buffer[(1, 112, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 24, 56, 56], dtype="float32")
        group_conv2d_nchw = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
        T_multiply = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 24, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 112, 56, 56, 6, 1, 1):
            with T.block("group_conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, ff // 28 * 6 + rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(group_conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [112, 6, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                with T.init():
                    group_conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                group_conv2d_nchw[nn, ff, yy, xx] = group_conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, ff // 28 * 6 + rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 112, 56, 56):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(group_conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = group_conv2d_nchw[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 112, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 112, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:16:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(112, 6, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1), "float32"], T_relu: T.Buffer[(1, 112, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            group_conv2d_nchw_local = T.alloc_buffer([1, 112, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([112, 6, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(9408):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(24, i4_0 * 3 + ax0_ax1_ax2_ax3_fused % 9408 // 448)
                                    v2 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused % 448 // 8)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(336):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(112, ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(6, i4_0 * 3 + ax0_ax1_ax2_ax3_fused % 3)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 7, 4, 1, 1, 1, 1, 2, 4, 1):
                                with T.block("group_conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                    yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + i2_3 * 4 + i2_4)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i3_3)
                                    rc = T.axis.reduce(6, i4_0 * 3 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, ff // 28 * 6 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [112, 6, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                    with T.init():
                                        group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 28 * 6 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 28, 4):
                            with T.block("group_conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                                v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + ax3)
                                T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 56, 1, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 7, 4])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[7, 2, 1, 4, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[2, 3, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
[16:16:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #6: "fused_reshape_transpose_reshape"
[16:16:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 56, 56), "float32"], T_reshape: T.Buffer[(1, 112, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([1, 4, 28, 56, 56], dtype="float32")
        T_transpose = T.alloc_buffer([1, 28, 4, 56, 56], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 28, 56, 56):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, (ax1 * 28 + (ax4 // 56 + ax3) // 56 + ax2) % 112, (ax4 // 56 + ax3) % 56, ax4 % 56])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3, ax4])
                T_reshape_1[ax0, ax1, ax2, ax3, ax4] = placeholder[0, (ax1 * 28 + (ax4 // 56 + ax3) // 56 + ax2) % 112, (ax4 // 56 + ax3) % 56, ax4 % 56]
        for i0, i1, i2, i3, i4 in T.grid(1, 28, 4, 56, 56):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax2, ax1, ax3, ax4])
                T.writes(T_transpose[ax0, ax1, ax2, ax3, ax4])
                T_transpose[ax0, ax1, ax2, ax3, ax4] = T_reshape_1[ax0, ax2, ax1, ax3, ax4]
        for i0, i1, i2, i3 in T.grid(1, 112, 56, 56):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_transpose[0, ((ax3 // 56 + ax2) // 56 + ax1) % 112 // 4, ((ax3 // 56 + ax2) // 56 + ax1) % 4, (ax3 // 56 + ax2) % 56, ax3 % 56])
                T.writes(T_reshape[ax0, ax1, ax2, ax3])
                T_reshape[ax0, ax1, ax2, ax3] = T_transpose[0, ((ax3 // 56 + ax2) // 56 + ax1) % 112 // 4, ((ax3 // 56 + ax2) // 56 + ax1) % 4, (ax3 // 56 + ax2) % 56, ax3 % 56]
    

[16:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 56, 56), "float32"], T_reshape: T.Buffer[(1, 112, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 112, 56, 56):
                with T.block("T_reshape_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[0, ((ax3 // 56 + ax2) // 56 + ax1) % 4 * 28 + ((ax3 // 56 + ax2) // 56 + ax1) % 112 // 4, (ax3 // 56 + ax2) % 56, ax3 % 56])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3])
                    T_reshape[ax0, ax1, ax2, ax3] = placeholder[0, (((ax3 // 56 + ax2) // 56 + ax1) % 4 * 28 + (ax3 % 56 // 56 + (ax3 // 56 + ax2) % 56) // 56 + ((ax3 // 56 + ax2) // 56 + ax1) % 112 // 4) % 112, (ax3 % 56 // 56 + (ax3 // 56 + ax2) % 56) % 56, ax3 % 56 % 56]
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #7: "fused_nn_conv2d_add"
[16:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 56, 56), "float32"], placeholder_1: T.Buffer[(112, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], T_add: T.Buffer[(1, 112, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 112, 58, 58], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 112, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 112, 58, 58):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 112, 28, 28, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 112, 56, 56], "float32"], ["TENSOR", [112, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 112, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[16:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 56, 56), "float32"], placeholder_1: T.Buffer[(112, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], T_add: T.Buffer[(1, 112, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            DepthwiseConv2d_local = T.alloc_buffer([1, 112, 28, 28], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 112, 58, 58], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([112, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(196, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(2088):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + ax0_ax1_ax2_ax3_fused % 2088 // 261)
                                    v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + ax0_ax1_ax2_ax3_fused % 261 // 29)
                                    v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax0_ax1_ax2_ax3_fused % 29)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(72):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 14, 3, 3, 1, 2, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_3)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 112, 56, 56], "float32"], ["TENSOR", [112, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 14):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l4, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[14, 1, 4, 1, 2])
l25, l26, l27, l28, l29 = sch.split(loop=l5, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 1, 2, 2, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l6, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 14, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l7, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l53, l54, l55 = sch.split(loop=l8, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l59, l60, l61 = sch.split(loop=l9, factors=[v56, v57, v58])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l54, l60, l18, l28, l38, l48, l55, l61, l19, l29, l39, l49)
l62 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l62, thread_axis="blockIdx.x")
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="vthread.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b65 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b65, loop=l64, preserve_unit_loops=True)
b66 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b66, loop=l59, preserve_unit_loops=True)
l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b66)
l76 = sch.fuse(l72, l73, l74, l75)
v77 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b66, ann_key="meta_schedule.cooperative_fetch", ann_val=v77)
b78 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b78, loop=l59, preserve_unit_loops=True)
l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b78)
l88 = sch.fuse(l84, l85, l86, l87)
v89 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b78, ann_key="meta_schedule.cooperative_fetch", ann_val=v89)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v90 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v90)
[16:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #8: "fused_nn_conv2d_multiply_add"
[16:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28), "float32"], placeholder_1: T.Buffer[(112, 28, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1), "float32"], T_add: T.Buffer[(1, 112, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 112, 28, 28], dtype="float32")
        group_conv2d_nchw = T.alloc_buffer([1, 112, 28, 28], dtype="float32")
        T_multiply = T.alloc_buffer([1, 112, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 112, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 112, 28, 28, 28, 1, 1):
            with T.block("group_conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, ff // 28 * 28 + rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(group_conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 112, 28, 28], "float32"], ["TENSOR", [112, 28, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                with T.init():
                    group_conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                group_conv2d_nchw[nn, ff, yy, xx] = group_conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, ff // 28 * 28 + rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 112, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(group_conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = group_conv2d_nchw[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 112, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
    

[16:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28), "float32"], placeholder_1: T.Buffer[(112, 28, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1), "float32"], T_add: T.Buffer[(1, 112, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            group_conv2d_nchw_local = T.alloc_buffer([1, 112, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 112, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([112, 28, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(196, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(87808):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(112, ax0_ax1_ax2_ax3_fused // 784)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 784 // 28)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(3136):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(112, ax0_ax1_ax2_ax3_fused // 28)
                                    v1 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 28)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(7, 1, 1, 1, 2, 2, 1, 4, 1, 1, 1, 2, 2, 2):
                                with T.block("group_conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4)
                                    rc = T.axis.reduce(28, i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, ff // 28 * 28 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 112, 28, 28], "float32"], ["TENSOR", [112, 28, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                    with T.init():
                                        group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 28 * 28 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 2):
                            with T.block("group_conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + ax1)
                                v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax2)
                                v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                                T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 28, 1, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 2, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 7, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[16:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #9: "fused_concatenate_nn_relu"
[16:16:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 24, 28, 28), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_concat = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 112, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(112 <= ax1, placeholder_1[ax0, ax1 - 112, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_concat[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_concat[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:16:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28), "float32"], placeholder_1: T.Buffer[(1, 24, 28, 28), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 112, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(T.if_then_else(112 <= ax1, placeholder_1[ax0, ax1 - 112, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32"), T.float32(0))
    

b0 = sch.get_block(name="T_relu", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[16:16:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #10: "fused_nn_conv2d_add_1"
[16:16:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], T_add: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 136, 30, 30], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 136, 30, 30):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 136, 28, 28, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[16:16:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], T_add: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            DepthwiseConv2d_local = T.alloc_buffer([1, 136, 28, 28], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 136, 30, 30], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([136, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(119, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(122400):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(136, ax0_ax1_ax2_ax3_fused // 900)
                                    v2 = T.axis.spatial(30, ax0_ax1_ax2_ax3_fused % 900 // 30)
                                    v3 = T.axis.spatial(30, ax0_ax1_ax2_ax3_fused % 30)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1224):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(136, ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 2, 7, 3, 3, 1, 4, 2, 2):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(136, i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3 * 4 + i1_4)
                                    i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3 * 2 + i2_4)
                                    j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i3_3 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 14):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(136, i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + ax1)
                                v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax2)
                                v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l4, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 17, 2, 4])
l25, l26, l27, l28, l29 = sch.split(loop=l5, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 2])
l35, l36, l37, l38, l39 = sch.split(loop=l6, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 2])
l45, l46, l47, l48, l49 = sch.split(loop=l7, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l53, l54, l55 = sch.split(loop=l8, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l59, l60, l61 = sch.split(loop=l9, factors=[v56, v57, v58])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l54, l60, l18, l28, l38, l48, l55, l61, l19, l29, l39, l49)
l62 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l62, thread_axis="blockIdx.x")
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="vthread.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b65 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b65, loop=l64, preserve_unit_loops=True)
b66 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b66, loop=l59, preserve_unit_loops=True)
l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b66)
l76 = sch.fuse(l72, l73, l74, l75)
v77 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b66, ann_key="meta_schedule.cooperative_fetch", ann_val=v77)
b78 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b78, loop=l59, preserve_unit_loops=True)
l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b78)
l88 = sch.fuse(l84, l85, l86, l87)
v89 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b78, ann_key="meta_schedule.cooperative_fetch", ann_val=v89)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v90 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v90)
[16:16:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"
[16:16:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 136, 28, 28), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        group_conv2d_nchw = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        T_multiply = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 136, 28, 28, 34, 1, 1):
            with T.block("group_conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, ff // 34 * 34 + rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(group_conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                with T.init():
                    group_conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                group_conv2d_nchw[nn, ff, yy, xx] = group_conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, ff // 34 * 34 + rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(group_conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = group_conv2d_nchw[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:16:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 136, 28, 28), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            group_conv2d_nchw_local = T.alloc_buffer([1, 136, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 136, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([136, 34, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(119, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(34, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial((i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 % 34 + 7) // 34 * 3808 + 112):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(136, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 // 34 * 34 + i4_0 + ax0_ax1_ax2_ax3_fused % (112 * ((i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + 7) // 34 * 34 + 1 - i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 // 34 * 34)) // 112)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 112 // 4)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(8):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(136, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + ax0_ax1_ax2_ax3_fused)
                                    v1 = T.axis.spatial(34, i4_0)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 4, 2, 2):
                                with T.block("group_conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(136, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i3_4)
                                    rc = T.axis.reduce(34, i4_0)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, ff // 34 * 34 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                    with T.init():
                                        group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 34 * 34 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 2):
                            with T.block("group_conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(136, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + ax1)
                                v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax3)
                                T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0], placeholder_4[v0, v1, v2, v3])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0] + placeholder_4[v0, v1, v2, v3], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l19, l20, l21, l22, l23 = sch.split(loop=l7, factors=[v14, v15, v16, v17, v18])
v24, v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[17, 1, 1, 2, 4])
l29, l30, l31, l32, l33 = sch.split(loop=l8, factors=[v24, v25, v26, v27, v28])
v34, v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 2])
l39, l40, l41, l42, l43 = sch.split(loop=l9, factors=[v34, v35, v36, v37, v38])
v44, v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l10, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 2])
l49, l50, l51, l52, l53 = sch.split(loop=l10, factors=[v44, v45, v46, v47, v48])
v54, v55, v56 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[34, 1, 1])
l57, l58, l59 = sch.split(loop=l11, factors=[v54, v55, v56])
v60, v61, v62 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l63, l64, l65 = sch.split(loop=l12, factors=[v60, v61, v62])
v66, v67, v68 = sch.sample_perfect_tile(loop=l13, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l69, l70, l71 = sch.split(loop=l13, factors=[v66, v67, v68])
sch.reorder(l19, l29, l39, l49, l20, l30, l40, l50, l21, l31, l41, l51, l57, l63, l69, l58, l64, l70, l22, l32, l42, l52, l59, l65, l71, l23, l33, l43, l53)
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="vthread.x")
l74 = sch.fuse(l21, l31, l41, l51)
sch.bind(loop=l74, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b75 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b75, loop=l74, preserve_unit_loops=True)
b76 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b76, loop=l69, preserve_unit_loops=True)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b76)
l87 = sch.fuse(l83, l84, l85, l86)
v88 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch", ann_val=v88)
b89 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b89, loop=l69, preserve_unit_loops=True)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b89)
l100 = sch.fuse(l96, l97, l98, l99)
v101 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
[16:16:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"
[16:16:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        group_conv2d_nchw = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        T_multiply = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 136, 28, 28, 34, 1, 1):
            with T.block("group_conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, ff // 34 * 34 + rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(group_conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                with T.init():
                    group_conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                group_conv2d_nchw[nn, ff, yy, xx] = group_conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, ff // 34 * 34 + rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(group_conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = group_conv2d_nchw[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:16:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            group_conv2d_nchw_local = T.alloc_buffer([1, 136, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 136, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([136, 34, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(952, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(7, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(17, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(14112):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(136, i0_0_i1_0_i2_0_i3_0_fused // 2 * 68 + i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 14112 // 392)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 392 // 14)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(136):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(136, i0_0_i1_0_i2_0_i3_0_fused // 2 * 68 + ax0_ax1_ax2_ax3_fused // 2)
                                    v1 = T.axis.spatial(34, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1):
                                with T.block("group_conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(136, i0_0_i1_0_i2_0_i3_0_fused // 2 * 68 + i0_1_i1_1_i2_1_i3_1_fused // 56 * 4 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 56 // 14 * 7 + i0_2_i1_2_i2_2_i3_2_fused)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14)
                                    rc = T.axis.reduce(34, i4_0 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, ff // 34 * 34 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                    with T.init():
                                        group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 34 * 34 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                            with T.block("group_conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(136, i0_0_i1_0_i2_0_i3_0_fused // 2 * 68 + i0_1_i1_1_i2_1_i3_1_fused // 56 * 4 + ax1)
                                v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 56 // 14 * 7 + i0_2_i1_2_i2_2_i3_2_fused + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14 + ax3)
                                T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 17, 1, 2, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 7, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[2, 14, 1, 1, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[17, 1, 2])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
[16:16:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #13: "fused_reshape_transpose_reshape_1"
[16:16:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], T_reshape: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([1, 4, 34, 28, 28], dtype="float32")
        T_transpose = T.alloc_buffer([1, 34, 4, 28, 28], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 34, 28, 28):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, (ax1 * 34 + (ax4 // 28 + ax3) // 28 + ax2) % 136, (ax4 // 28 + ax3) % 28, ax4 % 28])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3, ax4])
                T_reshape_1[ax0, ax1, ax2, ax3, ax4] = placeholder[0, (ax1 * 34 + (ax4 // 28 + ax3) // 28 + ax2) % 136, (ax4 // 28 + ax3) % 28, ax4 % 28]
        for i0, i1, i2, i3, i4 in T.grid(1, 34, 4, 28, 28):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax2, ax1, ax3, ax4])
                T.writes(T_transpose[ax0, ax1, ax2, ax3, ax4])
                T_transpose[ax0, ax1, ax2, ax3, ax4] = T_reshape_1[ax0, ax2, ax1, ax3, ax4]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_transpose[0, ((ax3 // 28 + ax2) // 28 + ax1) % 136 // 4, ((ax3 // 28 + ax2) // 28 + ax1) % 4, (ax3 // 28 + ax2) % 28, ax3 % 28])
                T.writes(T_reshape[ax0, ax1, ax2, ax3])
                T_reshape[ax0, ax1, ax2, ax3] = T_transpose[0, ((ax3 // 28 + ax2) // 28 + ax1) % 136 // 4, ((ax3 // 28 + ax2) // 28 + ax1) % 4, (ax3 // 28 + ax2) % 28, ax3 % 28]
    

[16:16:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], T_reshape: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
                with T.block("T_reshape_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[0, ((ax3 // 28 + ax2) // 28 + ax1) % 4 * 34 + ((ax3 // 28 + ax2) // 28 + ax1) % 136 // 4, (ax3 // 28 + ax2) % 28, ax3 % 28])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3])
                    T_reshape[ax0, ax1, ax2, ax3] = placeholder[0, (((ax3 // 28 + ax2) // 28 + ax1) % 4 * 34 + (ax3 % 28 // 28 + (ax3 // 28 + ax2) % 28) // 28 + ((ax3 // 28 + ax2) // 28 + ax1) % 136 // 4) % 136, (ax3 % 28 // 28 + (ax3 // 28 + ax2) % 28) % 28, ax3 % 28 % 28]
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:16:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #14: "fused_nn_conv2d_add_2"
[16:16:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], T_add: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 136, 30, 30], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 136, 30, 30):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 136, 14, 14, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[16:16:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], T_add: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            DepthwiseConv2d_local = T.alloc_buffer([1, 136, 14, 14], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 136, 30, 30], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([136, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(18360):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(136, ax0_ax1_ax2_ax3_fused % 18360 // 135)
                                    v2 = T.axis.spatial(30, i4_0 + ax0_ax1_ax2_ax3_fused % 135 // 5)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax0_ax1_ax2_ax3_fused % 5)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(408):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(136, ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 4, 1, 1, 1, 1, 1, 17, 7, 2):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(136, i0_1_i1_1_i2_1_i3_1_fused * 68 + i1_3 * 17 + i1_4)
                                    i = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused * 7 + i2_4)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 68, 7, 2):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(136, i0_1_i1_1_i2_1_i3_1_fused * 68 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused * 7 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l4, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 1, 4, 17])
l25, l26, l27, l28, l29 = sch.split(loop=l5, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l35, l36, l37, l38, l39 = sch.split(loop=l6, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l45, l46, l47, l48, l49 = sch.split(loop=l7, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l53, l54, l55 = sch.split(loop=l8, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l59, l60, l61 = sch.split(loop=l9, factors=[v56, v57, v58])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l54, l60, l18, l28, l38, l48, l55, l61, l19, l29, l39, l49)
l62 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l62, thread_axis="blockIdx.x")
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="vthread.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b65 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b65, loop=l64, preserve_unit_loops=True)
b66 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b66, loop=l59, preserve_unit_loops=True)
l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b66)
l76 = sch.fuse(l72, l73, l74, l75)
v77 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b66, ann_key="meta_schedule.cooperative_fetch", ann_val=v77)
b78 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b78, loop=l59, preserve_unit_loops=True)
l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b78)
l88 = sch.fuse(l84, l85, l86, l87)
v89 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b78, ann_key="meta_schedule.cooperative_fetch", ann_val=v89)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v90 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v90)
[16:16:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #15: "fused_nn_conv2d_multiply_add_1"
[16:16:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], T_add: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
        group_conv2d_nchw = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
        T_multiply = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 136, 14, 14, 34, 1, 1):
            with T.block("group_conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, ff // 34 * 34 + rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(group_conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 136, 14, 14], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                with T.init():
                    group_conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                group_conv2d_nchw[nn, ff, yy, xx] = group_conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, ff // 34 * 34 + rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(group_conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = group_conv2d_nchw[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
    

[16:16:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], T_add: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            group_conv2d_nchw_local = T.alloc_buffer([1, 136, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 136, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([136, 34, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(952, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial((i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 % 34 + 3) // 34 * 238 + 238):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(136, i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 // 34 * 34 + ax0_ax1_ax2_ax3_fused % (7 * ((i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 % 34 + 3) // 34 * 34 + 34)) // 7)
                                    v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 7 + ax0_ax1_ax2_ax3_fused % 7)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + 0)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(136):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(136, i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 + ax0_ax1_ax2_ax3_fused // 34)
                                    v1 = T.axis.spatial(34, ax0_ax1_ax2_ax3_fused % 34)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(34, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 7, 1):
                                with T.block("group_conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(136, i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 7 + i2_4)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                    rc = T.axis.reduce(34, i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, ff // 34 * 34 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 136, 14, 14], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                    with T.init():
                                        group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 34 * 34 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 1):
                            with T.block("group_conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(136, i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 + ax1)
                                v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 7 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + ax3)
                                T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[34, 1, 1, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 34, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[16:16:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #16: "fused_concatenate_nn_relu_1"
[16:16:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 136, 14, 14), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_concat = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 136, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(136 <= ax1, placeholder_1[ax0, ax1 - 136, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_concat[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_concat[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:16:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 136, 14, 14), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 136, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(T.if_then_else(136 <= ax1, placeholder_1[ax0, ax1 - 136, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32"), T.float32(0))
    

b0 = sch.get_block(name="T_relu", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[16:16:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #17: "fused_nn_conv2d_add_3"
[16:16:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 272, 16, 16], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 272, 16, 16):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 272, 14, 14, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[16:16:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            DepthwiseConv2d_local = T.alloc_buffer([1, 272, 14, 14], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 272, 16, 16], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([272, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(272, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(60928):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(272, ax0_ax1_ax2_ax3_fused % 60928 // 224)
                                    v2 = T.axis.spatial(16, i4_0 + ax0_ax1_ax2_ax3_fused % 224 // 16)
                                    v3 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused % 16)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(816):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(272, ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 7, 7, 1, 3, 1, 2, 1, 2):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i1_4)
                                    i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i2_3)
                                    j = T.axis.spatial(14, i3_3 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 14):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax2)
                                v3 = T.axis.spatial(14, ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l4, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 136, 1, 1, 2])
l25, l26, l27, l28, l29 = sch.split(loop=l5, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l6, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 2])
l45, l46, l47, l48, l49 = sch.split(loop=l7, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l53, l54, l55 = sch.split(loop=l8, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l59, l60, l61 = sch.split(loop=l9, factors=[v56, v57, v58])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l54, l60, l18, l28, l38, l48, l55, l61, l19, l29, l39, l49)
l62 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l62, thread_axis="blockIdx.x")
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="vthread.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b65 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b65, loop=l64, preserve_unit_loops=True)
b66 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b66, loop=l59, preserve_unit_loops=True)
l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b66)
l76 = sch.fuse(l72, l73, l74, l75)
v77 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b66, ann_key="meta_schedule.cooperative_fetch", ann_val=v77)
b78 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b78, loop=l59, preserve_unit_loops=True)
l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b78)
l88 = sch.fuse(l84, l85, l86, l87)
v89 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b78, ann_key="meta_schedule.cooperative_fetch", ann_val=v89)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v90 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v90)
[16:16:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"
[16:16:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 272, 14, 14), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        group_conv2d_nchw = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        T_multiply = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 272, 14, 14, 68, 1, 1):
            with T.block("group_conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, ff // 68 * 68 + rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(group_conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                with T.init():
                    group_conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                group_conv2d_nchw[nn, ff, yy, xx] = group_conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, ff // 68 * 68 + rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(group_conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = group_conv2d_nchw[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:16:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 272, 14, 14), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            group_conv2d_nchw_local = T.alloc_buffer([1, 272, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 272, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([272, 68, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(68, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(68, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(20090):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(272, i4_0 + ax0_ax1_ax2_ax3_fused % 20090 // 98)
                                    v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + ax0_ax1_ax2_ax3_fused % 98 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(272):
                                with T.block("placeholder_shared"):
                                    v0, v1 = T.axis.remap("SS", [ax0_ax1_ax2_ax3_fused, i4_0])
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 7, 14, 1, 1, 1, 1, 1, 1, 1):
                                with T.block("group_conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused * 68 + i0_2_i1_2_i2_2_i3_2_fused)
                                    yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + i2_3)
                                    xx, rc = T.axis.remap("SR", [i3_3, i4_0])
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                    with T.init():
                                        group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 14):
                            with T.block("group_conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused * 68 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                                v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 7 + ax2)
                                v3 = T.axis.spatial(14, ax3)
                                T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0], placeholder_4[v0, v1, v2, v3])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0] + placeholder_4[v0, v1, v2, v3], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l19, l20, l21, l22, l23 = sch.split(loop=l7, factors=[v14, v15, v16, v17, v18])
v24, v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 68, 1, 1])
l29, l30, l31, l32, l33 = sch.split(loop=l8, factors=[v24, v25, v26, v27, v28])
v34, v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 1])
l39, l40, l41, l42, l43 = sch.split(loop=l9, factors=[v34, v35, v36, v37, v38])
v44, v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l10, n=5, max_innermost_factor=64, decision=[1, 1, 1, 14, 1])
l49, l50, l51, l52, l53 = sch.split(loop=l10, factors=[v44, v45, v46, v47, v48])
v54, v55, v56 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[68, 1, 1])
l57, l58, l59 = sch.split(loop=l11, factors=[v54, v55, v56])
v60, v61, v62 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l63, l64, l65 = sch.split(loop=l12, factors=[v60, v61, v62])
v66, v67, v68 = sch.sample_perfect_tile(loop=l13, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l69, l70, l71 = sch.split(loop=l13, factors=[v66, v67, v68])
sch.reorder(l19, l29, l39, l49, l20, l30, l40, l50, l21, l31, l41, l51, l57, l63, l69, l58, l64, l70, l22, l32, l42, l52, l59, l65, l71, l23, l33, l43, l53)
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="vthread.x")
l74 = sch.fuse(l21, l31, l41, l51)
sch.bind(loop=l74, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b75 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b75, loop=l74, preserve_unit_loops=True)
b76 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b76, loop=l69, preserve_unit_loops=True)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b76)
l87 = sch.fuse(l83, l84, l85, l86)
v88 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch", ann_val=v88)
b89 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b89, loop=l69, preserve_unit_loops=True)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b89)
l100 = sch.fuse(l96, l97, l98, l99)
v101 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
[16:16:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"
[16:16:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        group_conv2d_nchw = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        T_multiply = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 272, 14, 14, 68, 1, 1):
            with T.block("group_conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, ff // 68 * 68 + rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(group_conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                with T.init():
                    group_conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                group_conv2d_nchw[nn, ff, yy, xx] = group_conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, ff // 68 * 68 + rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(group_conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = group_conv2d_nchw[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:16:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            group_conv2d_nchw_local = T.alloc_buffer([1, 272, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 272, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([272, 68, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(17, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(17, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(40768):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(272, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 40768 // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1088):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(272, ax0_ax1_ax2_ax3_fused // 4)
                                    v1 = T.axis.spatial(68, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 7, 2, 4, 1, 1, 1, 1, 1, 7):
                                with T.block("group_conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused // 2 * 68 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                    yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i2_3)
                                    xx = T.axis.spatial(14, i3_3 * 7 + i3_4)
                                    rc = T.axis.reduce(68, i4_0 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                    with T.init():
                                        group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 14):
                            with T.block("group_conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused // 2 * 68 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax2)
                                v3 = T.axis.spatial(14, ax3)
                                T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 17, 4, 1])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 7])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[17, 1, 4])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
[16:16:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #20: "fused_reshape_transpose_reshape_2"
[16:16:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], T_reshape: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([1, 4, 68, 14, 14], dtype="float32")
        T_transpose = T.alloc_buffer([1, 68, 4, 14, 14], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 68, 14, 14):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, (ax1 * 68 + (ax4 // 14 + ax3) // 14 + ax2) % 272, (ax4 // 14 + ax3) % 14, ax4 % 14])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3, ax4])
                T_reshape_1[ax0, ax1, ax2, ax3, ax4] = placeholder[0, (ax1 * 68 + (ax4 // 14 + ax3) // 14 + ax2) % 272, (ax4 // 14 + ax3) % 14, ax4 % 14]
        for i0, i1, i2, i3, i4 in T.grid(1, 68, 4, 14, 14):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax2, ax1, ax3, ax4])
                T.writes(T_transpose[ax0, ax1, ax2, ax3, ax4])
                T_transpose[ax0, ax1, ax2, ax3, ax4] = T_reshape_1[ax0, ax2, ax1, ax3, ax4]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_transpose[0, ((ax3 // 14 + ax2) // 14 + ax1) % 272 // 4, ((ax3 // 14 + ax2) // 14 + ax1) % 4, (ax3 // 14 + ax2) % 14, ax3 % 14])
                T.writes(T_reshape[ax0, ax1, ax2, ax3])
                T_reshape[ax0, ax1, ax2, ax3] = T_transpose[0, ((ax3 // 14 + ax2) // 14 + ax1) % 272 // 4, ((ax3 // 14 + ax2) // 14 + ax1) % 4, (ax3 // 14 + ax2) % 14, ax3 % 14]
    

[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], T_reshape: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
                with T.block("T_reshape_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[0, ((ax3 // 14 + ax2) // 14 + ax1) % 4 * 68 + ((ax3 // 14 + ax2) // 14 + ax1) % 272 // 4, (ax3 // 14 + ax2) % 14, ax3 % 14])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3])
                    T_reshape[ax0, ax1, ax2, ax3] = placeholder[0, (((ax3 // 14 + ax2) // 14 + ax1) % 4 * 68 + (ax3 % 14 // 14 + (ax3 // 14 + ax2) % 14) // 14 + ((ax3 // 14 + ax2) // 14 + ax1) % 272 // 4) % 272, (ax3 % 14 // 14 + (ax3 // 14 + ax2) % 14) % 14, ax3 % 14 % 14]
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #21: "fused_nn_conv2d_add_4"
[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 272, 16, 16], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 272, 16, 16):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 272, 7, 7, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            DepthwiseConv2d_local = T.alloc_buffer([1, 272, 7, 7], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 272, 16, 16], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([272, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(61200):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(272, ax0_ax1_ax2_ax3_fused // 225)
                                    v2 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused % 225 // 15)
                                    v3 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused % 15)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(2448):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(272, ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1, 1, 1, 1, 17, 7, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused // 7 * 136 + i0_2_i1_2_i2_2_i3_2_fused * 17 + i1_4)
                                    i = T.axis.spatial(7, i2_4)
                                    j = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 17, 7, 1):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused // 7 * 136 + i0_2_i1_2_i2_2_i3_2_fused * 17 + ax1)
                                v2 = T.axis.spatial(7, ax2)
                                v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l4, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 17])
l25, l26, l27, l28, l29 = sch.split(loop=l5, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l35, l36, l37, l38, l39 = sch.split(loop=l6, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l7, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l53, l54, l55 = sch.split(loop=l8, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l59, l60, l61 = sch.split(loop=l9, factors=[v56, v57, v58])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l54, l60, l18, l28, l38, l48, l55, l61, l19, l29, l39, l49)
l62 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l62, thread_axis="blockIdx.x")
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="vthread.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b65 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b65, loop=l64, preserve_unit_loops=True)
b66 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b66, loop=l59, preserve_unit_loops=True)
l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b66)
l76 = sch.fuse(l72, l73, l74, l75)
v77 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b66, ann_key="meta_schedule.cooperative_fetch", ann_val=v77)
b78 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b78, loop=l59, preserve_unit_loops=True)
l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b78)
l88 = sch.fuse(l84, l85, l86, l87)
v89 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b78, ann_key="meta_schedule.cooperative_fetch", ann_val=v89)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v90 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v90)
[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #22: "fused_nn_conv2d_multiply_add_2"
[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
        group_conv2d_nchw = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
        T_multiply = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 272, 7, 7, 68, 1, 1):
            with T.block("group_conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, ff // 68 * 68 + rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(group_conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 7, 7], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                with T.init():
                    group_conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                group_conv2d_nchw[nn, ff, yy, xx] = group_conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, ff // 68 * 68 + rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(group_conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = group_conv2d_nchw[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
    

[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            group_conv2d_nchw_local = T.alloc_buffer([1, 272, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 272, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([272, 68, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(119, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(11662):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(272, i4_0 * 34 + ax0_ax1_ax2_ax3_fused % 11662 // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(9248):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(272, ax0_ax1_ax2_ax3_fused // 34)
                                    v1 = T.axis.spatial(68, i4_0 * 34 + ax0_ax1_ax2_ax3_fused % 34)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 17, 1, 1, 1, 2, 1, 7):
                                with T.block("group_conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                    yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    xx = T.axis.spatial(7, i3_4)
                                    rc = T.axis.reduce(68, i4_0 * 34 + i4_1 * 17 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 7, 7], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                    with T.init():
                                        group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 7):
                            with T.block("group_conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, ax3)
                                T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 17, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 2, 17])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #23: "fused_concatenate_nn_relu_2"
[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 272, 7, 7), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_concat = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 272, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(272 <= ax1, placeholder_1[ax0, ax1 - 272, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_concat[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_concat[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 272, 7, 7), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
                with T.block("T_concat"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 272, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(T.if_then_else(272 <= ax1, placeholder_1[ax0, ax1 - 272, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32"), T.float32(0))
    

b0 = sch.get_block(name="T_relu", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"
[16:16:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        group_conv2d_nchw = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        T_multiply = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 544, 7, 7, 136, 1, 1):
            with T.block("group_conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, ff // 136 * 136 + rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(group_conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                with T.init():
                    group_conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                group_conv2d_nchw[nn, ff, yy, xx] = group_conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, ff // 136 * 136 + rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(group_conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = group_conv2d_nchw[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:16:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            group_conv2d_nchw_local = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([544, 136, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3808):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(544, ax0_ax1_ax2_ax3_fused // 7)
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(73984):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(544, ax0_ax1_ax2_ax3_fused // 136)
                                    v1 = T.axis.spatial(136, ax0_ax1_ax2_ax3_fused % 136)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(136, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 17, 1, 7):
                                with T.block("group_conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(544, i0_1_i1_1_i2_1_i3_1_fused * 136 + i0_2_i1_2_i2_2_i3_2_fused * 17 + i1_4)
                                    yy, xx, rc = T.axis.remap("SSR", [i0_0_i1_0_i2_0_i3_0_fused, i3_4, i4_1])
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, ff // 136 * 136 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                    with T.init():
                                        group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 136 * 136 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 17, 1, 7):
                            with T.block("group_conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(544, i0_1_i1_1_i2_1_i3_1_fused * 136 + i0_2_i1_2_i2_2_i3_2_fused * 17 + ax1)
                                v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused + ax2)
                                v3 = T.axis.spatial(7, ax3)
                                T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 8, 1, 17])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 136, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
[16:16:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #25: "fused_reshape_transpose_reshape_3"
[16:16:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], T_reshape: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([1, 4, 136, 7, 7], dtype="float32")
        T_transpose = T.alloc_buffer([1, 136, 4, 7, 7], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 136, 7, 7):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, (ax1 * 136 + (ax4 // 7 + ax3) // 7 + ax2) % 544, (ax4 // 7 + ax3) % 7, ax4 % 7])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3, ax4])
                T_reshape_1[ax0, ax1, ax2, ax3, ax4] = placeholder[0, (ax1 * 136 + (ax4 // 7 + ax3) // 7 + ax2) % 544, (ax4 // 7 + ax3) % 7, ax4 % 7]
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 4, 7, 7):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax2, ax1, ax3, ax4])
                T.writes(T_transpose[ax0, ax1, ax2, ax3, ax4])
                T_transpose[ax0, ax1, ax2, ax3, ax4] = T_reshape_1[ax0, ax2, ax1, ax3, ax4]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_transpose[0, ((ax3 // 7 + ax2) // 7 + ax1) % 544 // 4, ((ax3 // 7 + ax2) // 7 + ax1) % 4, (ax3 // 7 + ax2) % 7, ax3 % 7])
                T.writes(T_reshape[ax0, ax1, ax2, ax3])
                T_reshape[ax0, ax1, ax2, ax3] = T_transpose[0, ((ax3 // 7 + ax2) // 7 + ax1) % 544 // 4, ((ax3 // 7 + ax2) // 7 + ax1) % 4, (ax3 // 7 + ax2) % 7, ax3 % 7]
    

[16:16:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], T_reshape: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
                with T.block("T_reshape_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[0, ((ax3 // 7 + ax2) // 7 + ax1) % 4 * 136 + ((ax3 // 7 + ax2) // 7 + ax1) % 544 // 4, (ax3 // 7 + ax2) % 7, ax3 % 7])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3])
                    T_reshape[ax0, ax1, ax2, ax3] = placeholder[0, (((ax3 // 7 + ax2) // 7 + ax1) % 4 * 136 + (ax3 % 7 // 7 + (ax3 // 7 + ax2) % 7) // 7 + ((ax3 // 7 + ax2) // 7 + ax1) % 544 // 4) % 544, (ax3 % 7 // 7 + (ax3 // 7 + ax2) % 7) % 7, ax3 % 7 % 7]
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[16:16:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #26: "fused_nn_conv2d_add_5"
[16:16:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], T_add: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 544, 9, 9], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 544, 9, 9):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 544, 7, 7, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[16:16:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], T_add: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            DepthwiseConv2d_local = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 544, 9, 9], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([544, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(476, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(17136):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + ax0_ax1_ax2_ax3_fused % 17136 // 63)
                                    v2 = T.axis.spatial(9, ax0_ax1_ax2_ax3_fused % 63 // 7)
                                    v3 = T.axis.spatial(9, i5_0 + ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(816):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i5_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 7):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i1_4)
                                    i = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    j, di, dj = T.axis.remap("SRR", [i3_4, i4_1, i5_0])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 7):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l4, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 68, 1, 1, 4])
l25, l26, l27, l28, l29 = sch.split(loop=l5, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l6, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l45, l46, l47, l48, l49 = sch.split(loop=l7, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l53, l54, l55 = sch.split(loop=l8, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l59, l60, l61 = sch.split(loop=l9, factors=[v56, v57, v58])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l54, l60, l18, l28, l38, l48, l55, l61, l19, l29, l39, l49)
l62 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l62, thread_axis="blockIdx.x")
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="vthread.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b65 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b65, loop=l64, preserve_unit_loops=True)
b66 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b66, loop=l59, preserve_unit_loops=True)
l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b66)
l76 = sch.fuse(l72, l73, l74, l75)
v77 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b66, ann_key="meta_schedule.cooperative_fetch", ann_val=v77)
b78 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b78, loop=l59, preserve_unit_loops=True)
l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b78)
l88 = sch.fuse(l84, l85, l86, l87)
v89 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b78, ann_key="meta_schedule.cooperative_fetch", ann_val=v89)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v90 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v90)
[16:16:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"
[16:16:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 544, 7, 7), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        group_conv2d_nchw = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        T_multiply = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 544, 7, 7, 136, 1, 1):
            with T.block("group_conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, ff // 136 * 136 + rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(group_conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                with T.init():
                    group_conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                group_conv2d_nchw[nn, ff, yy, xx] = group_conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, ff // 136 * 136 + rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(group_conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = group_conv2d_nchw[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 544, 7, 7), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            group_conv2d_nchw_local = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([544, 136, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1666, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(8330):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i4_0 * 34 + ax0_ax1_ax2_ax3_fused % 8330 // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(9248):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + ax0_ax1_ax2_ax3_fused // 34)
                                    v1 = T.axis.spatial(136, i4_0 * 34 + ax0_ax1_ax2_ax3_fused % 34)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(17, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 4, 1, 1):
                                with T.block("group_conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 8 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7)
                                    xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    rc = T.axis.reduce(136, i4_0 * 34 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, ff // 136 * 136 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                    with T.init():
                                        group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 136 * 136 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                            with T.block("group_conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 8 + ax1)
                                v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 + ax2)
                                v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                                T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0], placeholder_4[v0, v1, v2, v3])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0] + placeholder_4[v0, v1, v2, v3], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l19, l20, l21, l22, l23 = sch.split(loop=l7, factors=[v14, v15, v16, v17, v18])
v24, v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 34, 2, 4])
l29, l30, l31, l32, l33 = sch.split(loop=l8, factors=[v24, v25, v26, v27, v28])
v34, v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l39, l40, l41, l42, l43 = sch.split(loop=l9, factors=[v34, v35, v36, v37, v38])
v44, v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l10, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l49, l50, l51, l52, l53 = sch.split(loop=l10, factors=[v44, v45, v46, v47, v48])
v54, v55, v56 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[4, 17, 2])
l57, l58, l59 = sch.split(loop=l11, factors=[v54, v55, v56])
v60, v61, v62 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l63, l64, l65 = sch.split(loop=l12, factors=[v60, v61, v62])
v66, v67, v68 = sch.sample_perfect_tile(loop=l13, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l69, l70, l71 = sch.split(loop=l13, factors=[v66, v67, v68])
sch.reorder(l19, l29, l39, l49, l20, l30, l40, l50, l21, l31, l41, l51, l57, l63, l69, l58, l64, l70, l22, l32, l42, l52, l59, l65, l71, l23, l33, l43, l53)
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="vthread.x")
l74 = sch.fuse(l21, l31, l41, l51)
sch.bind(loop=l74, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b75 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b75, loop=l74, preserve_unit_loops=True)
b76 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b76, loop=l69, preserve_unit_loops=True)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b76)
l87 = sch.fuse(l83, l84, l85, l86)
v88 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch", ann_val=v88)
b89 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b89, loop=l69, preserve_unit_loops=True)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b89)
l100 = sch.fuse(l96, l97, l98, l99)
v101 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #28: "fused_nn_avg_pool2d_3"
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], tensor: T.Buffer[(1, 544, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 544, 1, 1], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 544, 1, 1, 7, 7):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 544, 1, 1):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 + 6, 6) + 1 - T.max(0, ax2)) * (T.min(ax3 + 6, 6) + 1 - T.max(0, ax3)), 1), "float32")
    

[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 2 design space(s) generated
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], tensor: T.Buffer[(1, 544, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            tensor_shared = T.alloc_buffer([1, 544, 1, 1], dtype="float32", scope="shared")
            for i0, i1, i2, i3_0 in T.grid(1, 544, 1, 1):
                for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(1, 1, 1, 1, 1):
                    for ax4_ax5_fused_1 in T.thread_binding(512, thread="threadIdx.x"):
                        with T.block("tensor"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(544, i1)
                            ax2_1 = T.axis.spatial(1, 0)
                            ax3_1 = T.axis.spatial(1, 0)
                            rv0 = T.axis.reduce(7, ax4_ax5_fused_1 // 7)
                            rv1 = T.axis.reduce(7, ax4_ax5_fused_1 % 7)
                            T.where(ax4_ax5_fused_1 < 49)
                            T.reads(placeholder[ax0_1, ax1_1, ax2_1 + rv0, ax3_1 + rv1])
                            T.writes(tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1])
                            with T.init():
                                tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] = T.float32(0)
                            tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] = tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] + placeholder[ax0_1, ax1_1, ax2_1 + rv0, ax3_1 + rv1]
                for i3_1 in T.thread_binding(512, thread="threadIdx.x"):
                    with T.block("tensor_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(544, i1)
                        ax2 = T.axis.spatial(1, 0)
                        ax3 = T.axis.spatial(1, 0)
                        T.where(i3_1 < 1)
                        T.reads(tensor_shared[ax0, ax1, ax2, ax3])
                        T.writes(tensor[ax0, ax1, ax2, ax3])
                        tensor[ax0, ax1, ax2, ax3] = tensor_shared[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 + 6, 6) + 1 - T.max(0, ax2)) * (T.min(ax3 + 6, 6) + 1 - T.max(0, ax3)), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
b2, = sch.get_consumers(block=b0)
l3, l4, l5, l6 = sch.get_loops(block=b2)
v7 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l8, l9 = sch.split(loop=l6, factors=[None, v7])
sch.bind(loop=l9, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l8, preserve_unit_loops=True)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l10, l11, l12, l13, l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b0)
l20 = sch.fuse(l18, l19)
l21, l22 = sch.split(loop=l20, factors=[None, v7])
sch.bind(loop=l22, thread_axis="threadIdx.x")
v23 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v23)
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], tensor: T.Buffer[(1, 544, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            tensor_1 = T.alloc_buffer([1, 544, 1, 1], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 544, 1, 1, 7, 7):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1]
            for i0, i1, i2, i3 in T.grid(1, 544, 1, 1):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 + 6, 6) + 1 - T.max(0, ax2)) * (T.min(ax3 + 6, 6) + 1 - T.max(0, ax3)), 1), "float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #29: "fused_reshape"
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 1, 1), "float32"], T_reshape: T.Buffer[(1, 544), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(1, 544):
            with T.block("T_reshape"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[0, ax1 % 544, 0, 0])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = placeholder[0, ax1 % 544, 0, 0]
    

[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 1, 1), "float32"], T_reshape: T.Buffer[(1, 544), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1 in T.grid(1, 544):
                with T.block("T_reshape"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[0, ax1 % 544, 0, 0])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = placeholder[0, ax1 % 544, 0, 0]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #30: "fused_nn_dense_add"
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544), "float32"], placeholder_1: T.Buffer[(1000, 544), "float32"], placeholder_2: T.Buffer[(1, 1000), "float32"], T_add: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 1000], dtype="float32")
        for i0, i1, i2 in T.grid(1, 1000, 544):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1], "workload":["dense_small_batch.gpu", ["TENSOR", [1, 544], "float32"], ["TENSOR", [1000, 544], "float32"], None, "float32"]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544), "float32"], placeholder_1: T.Buffer[(1000, 544), "float32"], placeholder_2: T.Buffer[(1, 1000), "float32"], T_add: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            T_matmul_NT_local = T.alloc_buffer([1, 1000], dtype="float32", scope="local")
            placeholder_shared = T.alloc_buffer([1, 544], dtype="float32", scope="shared")
            placeholder_shared_1 = T.alloc_buffer([1000, 544], dtype="float32", scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(5, thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i2_0 in T.serial(34):
                            for ax0_ax1_fused in T.serial(16):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(544, i2_0 * 16 + ax0_ax1_fused)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                            for ax0_ax1_fused in T.serial(3200):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1000, i0_0_i1_0_fused * 200 + ax0_ax1_fused // 16)
                                    v1 = T.axis.spatial(544, i2_0 * 16 + ax0_ax1_fused % 16)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                            for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(8, 1, 10, 2, 1, 5):
                                with T.block("T_matmul_NT"):
                                    i = T.axis.spatial(1, 0)
                                    j = T.axis.spatial(1000, i0_0_i1_0_fused * 200 + i0_1_i1_1_fused * 50 + i1_3 * 5 + i1_4)
                                    k = T.axis.reduce(544, i2_0 * 16 + i2_1 * 2 + i2_2)
                                    T.reads(placeholder_shared[i, k], placeholder_shared_1[j, k])
                                    T.writes(T_matmul_NT_local[i, j])
                                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 544], "float32"], ["TENSOR", [1000, 544], "float32"], None, "float32"]})
                                    with T.init():
                                        T_matmul_NT_local[i, j] = T.float32(0)
                                    T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                        for ax0, ax1 in T.grid(1, 50):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1000, i0_0_i1_0_fused * 200 + i0_1_i1_1_fused * 50 + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                                T.writes(T_add[v0, v1])
                                T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[5, 4, 1, 10, 5])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[34, 8, 2])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #31: "fused_nn_softmax"
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
        T_softmax_exp = T.alloc_buffer([1, 1000], dtype="float32")
        T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_softmax_maxelem"):
                i0_1, k = T.axis.remap("SR", [i0, i1])
                T.reads(placeholder[i0_1, k])
                T.writes(T_softmax_maxelem[i0_1])
                with T.init():
                    T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_softmax_exp"):
                i0_2, i1_1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[i0_2, i1_1], T_softmax_maxelem[i0_2])
                T.writes(T_softmax_exp[i0_2, i1_1])
                T_softmax_exp[i0_2, i1_1] = T.exp(placeholder[i0_2, i1_1] - T_softmax_maxelem[i0_2], dtype="float32")
        for i0_3, i1 in T.grid(1, 1000):
            with T.block("T_softmax_expsum"):
                i0_4, k = T.axis.remap("SR", [i0_3, i1])
                T.reads(T_softmax_exp[i0_4, k])
                T.writes(T_softmax_expsum[i0_4])
                with T.init():
                    T_softmax_expsum[i0_4] = T.float32(0)
                T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T_softmax_exp[i0_4, k]
        for i0_5, i1 in T.grid(1, 1000):
            with T.block("T_softmax_norm"):
                i0_6, i1_2 = T.axis.remap("SS", [i0_5, i1])
                T.reads(T_softmax_exp[i0_6, i1_2], T_softmax_expsum[i0_6])
                T.writes(T_softmax_norm[i0_6, i1_2])
                T.block_attr({"axis":1})
                T_softmax_norm[i0_6, i1_2] = T_softmax_exp[i0_6, i1_2] / T_softmax_expsum[i0_6]
    

[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 4 design space(s) generated
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            T_softmax_maxelem_shared = T.alloc_buffer([1], dtype="float32", scope="shared")
            T_softmax_expsum_shared = T.alloc_buffer([1], dtype="float32", scope="shared")
            for i0 in T.serial(1):
                for ax0, ax1_0 in T.grid(1, 16):
                    for ax1_1 in T.thread_binding(64, thread="threadIdx.x"):
                        with T.block("T_softmax_maxelem"):
                            i0_1 = T.axis.spatial(1, 0)
                            k = T.axis.reduce(1000, ax1_0 * 64 + ax1_1)
                            T.where(ax1_0 * 64 + ax1_1 < 1000)
                            T.reads(placeholder[i0_1, k])
                            T.writes(T_softmax_maxelem_shared[i0_1])
                            with T.init():
                                T_softmax_maxelem_shared[i0_1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[i0_1] = T.max(T_softmax_maxelem_shared[i0_1], placeholder[i0_1, k])
                for ax0, ax1_0 in T.grid(1, 16):
                    for ax1_1 in T.thread_binding(64, thread="threadIdx.x"):
                        with T.block("T_softmax_expsum"):
                            i0_2 = T.axis.spatial(1, 0)
                            k = T.axis.reduce(1000, ax1_0 * 64 + ax1_1)
                            T.where(ax1_0 * 64 + ax1_1 < 1000)
                            T.reads(placeholder[i0_2, k], T_softmax_maxelem_shared[i0_2])
                            T.writes(T_softmax_expsum_shared[i0_2])
                            with T.init():
                                T_softmax_expsum_shared[i0_2] = T.float32(0)
                            T_softmax_expsum_shared[i0_2] = T_softmax_expsum_shared[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem_shared[i0_2], dtype="float32")
                for i1_0 in T.serial(16):
                    for i1_1 in T.thread_binding(64, thread="threadIdx.x"):
                        with T.block("T_softmax_norm"):
                            i0_3 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(1000, i1_0 * 64 + i1_1)
                            T.where(i1_0 * 64 + i1_1 < 1000)
                            T.reads(placeholder[i0_3, i1], T_softmax_maxelem_shared[i0_3], T_softmax_expsum_shared[i0_3])
                            T.writes(T_softmax_norm[i0_3, i1])
                            T.block_attr({"axis":1})
                            T_softmax_norm[i0_3, i1] = T.exp(placeholder[i0_3, i1] - T_softmax_maxelem_shared[i0_3], dtype="float32") / T_softmax_expsum_shared[i0_3]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
b4, = sch.get_consumers(block=b2)
l5, l6 = sch.get_loops(block=b4)
v7 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)
l8, l9 = sch.split(loop=l6, factors=[None, v7])
sch.bind(loop=l9, thread_axis="threadIdx.x")
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
sch.set_scope(block=b2, buffer_index=0, storage_scope="shared")
l10, l11, l12 = sch.get_loops(block=b2)
l13, l14 = sch.split(loop=l12, factors=[None, v7])
sch.bind(loop=l14, thread_axis="threadIdx.x")
b15, b16 = sch.get_consumers(block=b0)
l17, l18, l19, l20 = sch.get_loops(block=b15)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l21, l22, l23 = sch.get_loops(block=b0)
l24, l25 = sch.split(loop=l23, factors=[None, v7])
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_shared = T.alloc_buffer([1], dtype="float32", scope="shared")
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0 in T.serial(1):
                for ax0, ax1_0 in T.grid(1, 16):
                    for ax1_1 in T.thread_binding(64, thread="threadIdx.x"):
                        with T.block("T_softmax_expsum"):
                            i0_2 = T.axis.spatial(1, 0)
                            k = T.axis.reduce(1000, ax1_0 * 64 + ax1_1)
                            T.where(ax1_0 * 64 + ax1_1 < 1000)
                            T.reads(placeholder[i0_2, k], T_softmax_maxelem[i0_2])
                            T.writes(T_softmax_expsum_shared[i0_2])
                            with T.init():
                                T_softmax_expsum_shared[i0_2] = T.float32(0)
                            T_softmax_expsum_shared[i0_2] = T_softmax_expsum_shared[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem[i0_2], dtype="float32")
                for i1_0 in T.serial(16):
                    for i1_1 in T.thread_binding(64, thread="threadIdx.x"):
                        with T.block("T_softmax_norm"):
                            i0_3 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(1000, i1_0 * 64 + i1_1)
                            T.where(i1_0 * 64 + i1_1 < 1000)
                            T.reads(placeholder[i0_3, i1], T_softmax_maxelem[i0_3], T_softmax_expsum_shared[i0_3])
                            T.writes(T_softmax_norm[i0_3, i1])
                            T.block_attr({"axis":1})
                            T_softmax_norm[i0_3, i1] = T.exp(placeholder[i0_3, i1] - T_softmax_maxelem[i0_3], dtype="float32") / T_softmax_expsum_shared[i0_3]
    

b0 = sch.get_block(name="T_softmax_exp", func_name="main")
b1 = sch.get_block(name="T_softmax_expsum", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
b3, = sch.get_consumers(block=b1)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)
l7, l8 = sch.split(loop=l5, factors=[None, v6])
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l4, preserve_unit_loops=True)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l9, l10, l11 = sch.get_loops(block=b1)
l12, l13 = sch.split(loop=l11, factors=[None, v6])
sch.bind(loop=l13, thread_axis="threadIdx.x")
v14 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v14)
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            for i0, i1_0 in T.grid(1, 250):
                for i1_1 in T.thread_binding(4, thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        i0_1 = T.axis.spatial(1, 0)
                        k = T.axis.reduce(1000, i1_0 * 4 + i1_1)
                        T.reads(placeholder[i0_1, k])
                        T.writes(T_softmax_maxelem[i0_1])
                        with T.init():
                            T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_expsum"):
                    i0_2, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_2, k], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_expsum[i0_2])
                    with T.init():
                        T_softmax_expsum[i0_2] = T.float32(0)
                    T_softmax_expsum[i0_2] = T_softmax_expsum[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_4, i1_2 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(placeholder[i0_4, i1_2], T_softmax_maxelem[i0_4], T_softmax_expsum[i0_4])
                    T.writes(T_softmax_norm[i0_4, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_4, i1_2] = T.exp(placeholder[i0_4, i1_2] - T_softmax_maxelem[i0_4], dtype="float32") / T_softmax_expsum[i0_4]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l4, l5 = sch.get_loops(block=b0)
l6, l7 = sch.split(loop=l5, factors=[None, v3])
sch.bind(loop=l7, thread_axis="threadIdx.x")
v8 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v8)
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #3:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_expsum"):
                    i0_2, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_2, k], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_expsum[i0_2])
                    with T.init():
                        T_softmax_expsum[i0_2] = T.float32(0)
                    T_softmax_expsum[i0_2] = T_softmax_expsum[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_4, i1_1 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(placeholder[i0_4, i1_1], T_softmax_maxelem[i0_4], T_softmax_expsum[i0_4])
                    T.writes(T_softmax_norm[i0_4, i1_1])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_4, i1_1] = T.exp(placeholder[i0_4, i1_1] - T_softmax_maxelem[i0_4], dtype="float32") / T_softmax_expsum[i0_4]
    

b0 = sch.get_block(name="T_softmax_exp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[16:16:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_nn_avg_pool2d"
[16:16:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:16:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:16:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_nn_avg_pool2d_1"
[16:16:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:16:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:17:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_nn_avg_pool2d_2"
[16:17:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:17:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:17:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_nn_conv2d_add_add_nn_relu"
[16:17:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:17:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_nn_max_pool2d"
[16:17:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:17:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:18:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_nn_conv2d_multiply_add_nn_relu"
[16:18:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:18:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:18:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_reshape_transpose_reshape"
[16:18:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:18:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:18:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_nn_conv2d_add"
[16:19:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:19:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:19:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_nn_conv2d_multiply_add"
[16:19:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:19:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:20:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_concatenate_nn_relu"
[16:20:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:20:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:20:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_nn_conv2d_add_1"
[16:20:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:20:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:20:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"
[16:21:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:21:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:21:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"
[16:21:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:21:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:21:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_reshape_transpose_reshape_1"
[16:21:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:21:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:21:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #14: "fused_nn_conv2d_add_2"
[16:21:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:22:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:22:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #15: "fused_nn_conv2d_multiply_add_1"
[16:22:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:22:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:22:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #16: "fused_concatenate_nn_relu_1"
[16:22:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:22:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:22:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #17: "fused_nn_conv2d_add_3"
[16:22:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:23:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:23:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"
[16:23:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:23:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [16:23:51] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x0000558779922bc2
  96: 0xffffffffffffffff


[16:23:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"
[16:23:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:24:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:24:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #20: "fused_reshape_transpose_reshape_2"
[16:24:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:24:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:24:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #21: "fused_nn_conv2d_add_4"
[16:24:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:24:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:24:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #22: "fused_nn_conv2d_multiply_add_2"
[16:24:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:24:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [16:24:59] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x0000555e0c426bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [16:25:02] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055c4ffc7abc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [16:25:03] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x00005618f7c85bc2
  96: 0xffffffffffffffff


[16:25:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #23: "fused_concatenate_nn_relu_2"
[16:25:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:25:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:25:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"
[16:25:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:25:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [16:25:25] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x00005557cd4babc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [16:25:28] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055b3b1238bc2
  96: 0xffffffffffffffff


[16:25:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #25: "fused_reshape_transpose_reshape_3"
[16:25:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:25:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:25:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #26: "fused_nn_conv2d_add_5"
[16:25:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:26:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:26:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"
[16:26:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:26:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [16:26:34] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x0000562ea6c5cbc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [16:26:36] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055a189ddabc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [16:26:38] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x00005593f3224bc2
  96: 0xffffffffffffffff


[16:26:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #28: "fused_nn_avg_pool2d_3"
[16:26:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:26:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:26:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #29: "fused_reshape"
[16:26:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:26:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:27:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #30: "fused_nn_dense_add"
[16:27:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:27:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [16:27:12] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x0000561f04fadbc2
  96: 0xffffffffffffffff


[16:27:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #31: "fused_nn_softmax"
[16:27:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[16:27:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #0: GFLOPs: 80.1791. Time: 0.0053 ms. Best GFLOPs: 80.1791
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #1: GFLOPs: 89.6697. Time: 0.0048 ms. Best GFLOPs: 89.6697
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #2: GFLOPs: 76.7744. Time: 0.0056 ms. Best GFLOPs: 89.6697
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #3: GFLOPs: 97.8948. Time: 0.0044 ms. Best GFLOPs: 97.8948
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #4: GFLOPs: 93.5076. Time: 0.0046 ms. Best GFLOPs: 97.8948
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #5: GFLOPs: 94.2435. Time: 0.0045 ms. Best GFLOPs: 97.8948
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #6: GFLOPs: 90.3318. Time: 0.0047 ms. Best GFLOPs: 97.8948
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #7: GFLOPs: 87.0431. Time: 0.0049 ms. Best GFLOPs: 97.8948
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #8: GFLOPs: 82.2539. Time: 0.0052 ms. Best GFLOPs: 97.8948
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #9: GFLOPs: 98.5220. Time: 0.0043 ms. Best GFLOPs: 98.5220
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #10: GFLOPs: 96.5051. Time: 0.0044 ms. Best GFLOPs: 98.5220
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #11: GFLOPs: 97.4775. Time: 0.0044 ms. Best GFLOPs: 98.5220
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #12: GFLOPs: 90.2441. Time: 0.0047 ms. Best GFLOPs: 98.5220
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #13: GFLOPs: 99.2456. Time: 0.0043 ms. Best GFLOPs: 99.2456
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #14: GFLOPs: 99.7392. Time: 0.0043 ms. Best GFLOPs: 99.7392
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #15: GFLOPs: 88.2121. Time: 0.0048 ms. Best GFLOPs: 99.7392
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #16: GFLOPs: 97.6839. Time: 0.0044 ms. Best GFLOPs: 99.7392
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #17: GFLOPs: 94.3939. Time: 0.0045 ms. Best GFLOPs: 99.7392
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #18: GFLOPs: 88.4507. Time: 0.0048 ms. Best GFLOPs: 99.7392
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #19: GFLOPs: 96.8707. Time: 0.0044 ms. Best GFLOPs: 99.7392
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #20: GFLOPs: 89.7315. Time: 0.0048 ms. Best GFLOPs: 99.7392
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #21: GFLOPs: 94.6539. Time: 0.0045 ms. Best GFLOPs: 99.7392
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #22: GFLOPs: 94.7557. Time: 0.0045 ms. Best GFLOPs: 99.7392
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #23: GFLOPs: 91.5930. Time: 0.0047 ms. Best GFLOPs: 99.7392
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #24: GFLOPs: 98.1301. Time: 0.0043 ms. Best GFLOPs: 99.7392
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #25: GFLOPs: 85.0842. Time: 0.0050 ms. Best GFLOPs: 99.7392
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #26: GFLOPs: 85.1536. Time: 0.0050 ms. Best GFLOPs: 99.7392
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #27: GFLOPs: 95.7530. Time: 0.0045 ms. Best GFLOPs: 99.7392
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #28: GFLOPs: 99.7439. Time: 0.0043 ms. Best GFLOPs: 99.7439
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #29: GFLOPs: 97.7838. Time: 0.0044 ms. Best GFLOPs: 99.7439
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #30: GFLOPs: 93.9700. Time: 0.0045 ms. Best GFLOPs: 99.7439
[16:27:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #31: GFLOPs: 94.2616. Time: 0.0045 ms. Best GFLOPs: 99.7439
/home/yj/anaconda3/lib/python3.9/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
[16:27:26] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_avg_pool2d"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 4.27591

[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #0: GFLOPs: 146.3679. Time: 0.0058 ms. Best GFLOPs: 146.3679
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #1: GFLOPs: 159.6026. Time: 0.0053 ms. Best GFLOPs: 159.6026
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #2: GFLOPs: 170.3937. Time: 0.0050 ms. Best GFLOPs: 170.3937
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #3: GFLOPs: 169.4948. Time: 0.0050 ms. Best GFLOPs: 170.3937
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #4: GFLOPs: 186.0858. Time: 0.0046 ms. Best GFLOPs: 186.0858
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #5: GFLOPs: 189.2236. Time: 0.0045 ms. Best GFLOPs: 189.2236
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #6: GFLOPs: 162.5836. Time: 0.0052 ms. Best GFLOPs: 189.2236
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #7: GFLOPs: 183.7568. Time: 0.0046 ms. Best GFLOPs: 189.2236
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #8: GFLOPs: 192.5784. Time: 0.0044 ms. Best GFLOPs: 192.5784
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #9: GFLOPs: 176.7360. Time: 0.0048 ms. Best GFLOPs: 192.5784
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #10: GFLOPs: 161.7042. Time: 0.0053 ms. Best GFLOPs: 192.5784
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #11: GFLOPs: 188.2977. Time: 0.0045 ms. Best GFLOPs: 192.5784
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #12: GFLOPs: 186.7794. Time: 0.0046 ms. Best GFLOPs: 192.5784
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #13: GFLOPs: 185.0190. Time: 0.0046 ms. Best GFLOPs: 192.5784
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #14: GFLOPs: 189.1286. Time: 0.0045 ms. Best GFLOPs: 192.5784
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #15: GFLOPs: 182.2762. Time: 0.0047 ms. Best GFLOPs: 192.5784
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #16: GFLOPs: 198.4875. Time: 0.0043 ms. Best GFLOPs: 198.4875
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #17: GFLOPs: 196.1156. Time: 0.0043 ms. Best GFLOPs: 198.4875
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #18: GFLOPs: 201.3370. Time: 0.0042 ms. Best GFLOPs: 201.3370
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #19: GFLOPs: 194.9458. Time: 0.0044 ms. Best GFLOPs: 201.3370
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #20: GFLOPs: 195.8330. Time: 0.0044 ms. Best GFLOPs: 201.3370
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #21: GFLOPs: 189.5210. Time: 0.0045 ms. Best GFLOPs: 201.3370
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #22: GFLOPs: 186.7017. Time: 0.0046 ms. Best GFLOPs: 201.3370
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #23: GFLOPs: 193.0513. Time: 0.0044 ms. Best GFLOPs: 201.3370
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #24: GFLOPs: 189.5068. Time: 0.0045 ms. Best GFLOPs: 201.3370
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #25: GFLOPs: 182.6618. Time: 0.0047 ms. Best GFLOPs: 201.3370
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #26: GFLOPs: 170.8905. Time: 0.0050 ms. Best GFLOPs: 201.3370
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #27: GFLOPs: 173.6809. Time: 0.0049 ms. Best GFLOPs: 201.3370
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #28: GFLOPs: 141.5366. Time: 0.0060 ms. Best GFLOPs: 201.3370
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #29: GFLOPs: 169.2453. Time: 0.0050 ms. Best GFLOPs: 201.3370
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #30: GFLOPs: 169.0037. Time: 0.0050 ms. Best GFLOPs: 201.3370
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #31: GFLOPs: 161.2902. Time: 0.0053 ms. Best GFLOPs: 201.3370
[16:27:26] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_avg_pool2d_1"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 64
Total latency (us): 8.51255

[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #0: GFLOPs: 135.8748. Time: 0.0044 ms. Best GFLOPs: 135.8748
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #1: GFLOPs: 128.9391. Time: 0.0047 ms. Best GFLOPs: 135.8748
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #2: GFLOPs: 125.4953. Time: 0.0048 ms. Best GFLOPs: 135.8748
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #3: GFLOPs: 130.2294. Time: 0.0046 ms. Best GFLOPs: 135.8748
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #4: GFLOPs: 125.3955. Time: 0.0048 ms. Best GFLOPs: 135.8748
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #5: GFLOPs: 118.4831. Time: 0.0051 ms. Best GFLOPs: 135.8748
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #6: GFLOPs: 115.7675. Time: 0.0052 ms. Best GFLOPs: 135.8748
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #7: GFLOPs: 128.6757. Time: 0.0047 ms. Best GFLOPs: 135.8748
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #8: GFLOPs: 131.8667. Time: 0.0046 ms. Best GFLOPs: 135.8748
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #9: GFLOPs: 113.2785. Time: 0.0053 ms. Best GFLOPs: 135.8748
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #10: GFLOPs: 115.1940. Time: 0.0052 ms. Best GFLOPs: 135.8748
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #11: GFLOPs: 119.8517. Time: 0.0050 ms. Best GFLOPs: 135.8748
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #12: GFLOPs: 117.4094. Time: 0.0051 ms. Best GFLOPs: 135.8748
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #13: GFLOPs: 138.1512. Time: 0.0044 ms. Best GFLOPs: 138.1512
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #14: GFLOPs: 126.5733. Time: 0.0048 ms. Best GFLOPs: 138.1512
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #15: GFLOPs: 132.5042. Time: 0.0045 ms. Best GFLOPs: 138.1512
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #16: GFLOPs: 136.4533. Time: 0.0044 ms. Best GFLOPs: 138.1512
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #17: GFLOPs: 110.5562. Time: 0.0054 ms. Best GFLOPs: 138.1512
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #18: GFLOPs: 116.9651. Time: 0.0051 ms. Best GFLOPs: 138.1512
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #19: GFLOPs: 115.8360. Time: 0.0052 ms. Best GFLOPs: 138.1512
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #20: GFLOPs: 122.5848. Time: 0.0049 ms. Best GFLOPs: 138.1512
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #21: GFLOPs: 135.7265. Time: 0.0044 ms. Best GFLOPs: 138.1512
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #22: GFLOPs: 129.7051. Time: 0.0046 ms. Best GFLOPs: 138.1512
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #23: GFLOPs: 138.5516. Time: 0.0043 ms. Best GFLOPs: 138.5516
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #24: GFLOPs: 138.9542. Time: 0.0043 ms. Best GFLOPs: 138.9542
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #25: GFLOPs: 134.1138. Time: 0.0045 ms. Best GFLOPs: 138.9542
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #26: GFLOPs: 127.7124. Time: 0.0047 ms. Best GFLOPs: 138.9542
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #27: GFLOPs: 135.8901. Time: 0.0044 ms. Best GFLOPs: 138.9542
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #28: GFLOPs: 133.1410. Time: 0.0045 ms. Best GFLOPs: 138.9542
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #29: GFLOPs: 136.8098. Time: 0.0044 ms. Best GFLOPs: 138.9542
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #30: GFLOPs: 132.8366. Time: 0.0045 ms. Best GFLOPs: 138.9542
[16:27:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_avg_pool2d_2"] Trial #31: GFLOPs: 143.5711. Time: 0.0042 ms. Best GFLOPs: 143.5711
[16:27:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_avg_pool2d_2"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 96
Total latency (us): 12.7064

[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #0: GFLOPs: 65.7688. Time: 0.2609 ms. Best GFLOPs: 65.7688
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #1: GFLOPs: 777.9939. Time: 0.0221 ms. Best GFLOPs: 777.9939
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #2: GFLOPs: 486.7716. Time: 0.0353 ms. Best GFLOPs: 777.9939
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(24, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 24, 1, 1), "float32"], T_relu: T.Buffer[(1, 24, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init, i3_3_init, i1_4_init in T.grid(2, 4, 3):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 14 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 3 + i1_4_init)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 2 + i2_3_init)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_3_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [24, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(3, 1):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(317):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 10125 // 3375)
                                        v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 14 * 16 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3375 // 225)
                                        v3 = T.axis.spatial(226, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 225)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 10125)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 14 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                            v2 = T.axis.spatial(3, i5_0)
                                            v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 108)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 2, 4, 1, 1, 3, 1, 3, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 14 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 3 + i1_4)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 2 + i2_3)
                                    xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_3)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_0, i6_2])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [24, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 14 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 3 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 4, 1, 3])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 2, 2, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 4, 4, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 32])
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l175)
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #4: GFLOPs: 94.0378. Time: 0.1825 ms. Best GFLOPs: 777.9939
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #5: GFLOPs: 302.6399. Time: 0.0567 ms. Best GFLOPs: 777.9939
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #6: GFLOPs: 613.5339. Time: 0.0280 ms. Best GFLOPs: 777.9939
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #7: GFLOPs: 478.7442. Time: 0.0358 ms. Best GFLOPs: 777.9939
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #8: GFLOPs: 1075.8478. Time: 0.0160 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #9: GFLOPs: 388.6273. Time: 0.0442 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(24, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 24, 1, 1), "float32"], T_relu: T.Buffer[(1, 24, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(168, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for i3_3_init, i1_4_init in T.grid(2, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused // 56 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4_init)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 56 // 8 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 8 * 2 + i3_3_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [24, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i6_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(329):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 10509 // 3503)
                                        v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused // 7 * 112 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3503 // 31)
                                        v3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 7 * 32 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 31)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 10509)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i6_0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 216)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused // 56 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 56 // 8 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 8 * 2 + i3_3)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_1, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [24, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused // 56 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 56 // 8 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 8 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 3, 4, 1, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 8, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[7, 8, 1, 2, 1])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 32])
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l176)
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #11: GFLOPs: 159.6027. Time: 0.1075 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #12: GFLOPs: 252.3411. Time: 0.0680 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #13: GFLOPs: 126.4233. Time: 0.1357 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #14: GFLOPs: 203.1385. Time: 0.0845 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #15: GFLOPs: 208.8300. Time: 0.0822 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #16: GFLOPs: 241.7440. Time: 0.0710 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #17: GFLOPs: 121.6738. Time: 0.1410 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #18: GFLOPs: 348.1482. Time: 0.0493 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #19: GFLOPs: 30.0728. Time: 0.5706 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #20: GFLOPs: 304.1399. Time: 0.0564 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #21: GFLOPs: 526.0070. Time: 0.0326 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #22: GFLOPs: 465.6991. Time: 0.0368 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(24, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 24, 1, 1), "float32"], T_relu: T.Buffer[(1, 24, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 3, 7, 8):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 8 * 6 + i1_3_init * 3 + i1_4_init)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + i2_4_init)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [24, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i6_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(329):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 10509 // 3503)
                                        v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused // 7 * 112 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3503 // 31)
                                        v3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 7 * 32 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 31)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 10509)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 216)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 2, 1, 1, 3, 1, 1, 1, 3, 7, 8):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 8 * 6 + i1_3 * 3 + i1_4)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + i2_4)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_1, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [24, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 7, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 8 * 6 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 2, 3])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 7])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 8])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 32])
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l174)
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #24: GFLOPs: 636.4093. Time: 0.0270 ms. Best GFLOPs: 1075.8478
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #25: GFLOPs: 1091.9020. Time: 0.0157 ms. Best GFLOPs: 1091.9020
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #26: GFLOPs: 168.3122. Time: 0.1020 ms. Best GFLOPs: 1091.9020
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #27: GFLOPs: 713.6437. Time: 0.0240 ms. Best GFLOPs: 1091.9020
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #28: GFLOPs: 118.5329. Time: 0.1448 ms. Best GFLOPs: 1091.9020
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #29: GFLOPs: 131.2485. Time: 0.1307 ms. Best GFLOPs: 1091.9020
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #30: GFLOPs: 767.7234. Time: 0.0224 ms. Best GFLOPs: 1091.9020
[16:27:27] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_add_nn_relu"] Trial #31: GFLOPs: 219.1943. Time: 0.0783 ms. Best GFLOPs: 1091.9020
[16:27:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_conv2d_add_add_nn_relu"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 128
Total latency (us): 28.4222

[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #0: GFLOPs: 170.7726. Time: 0.0040 ms. Best GFLOPs: 170.7726
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #1: GFLOPs: 164.2249. Time: 0.0041 ms. Best GFLOPs: 170.7726
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #2: GFLOPs: 179.6987. Time: 0.0038 ms. Best GFLOPs: 179.6987
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #3: GFLOPs: 186.9725. Time: 0.0036 ms. Best GFLOPs: 186.9725
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #4: GFLOPs: 170.2544. Time: 0.0040 ms. Best GFLOPs: 186.9725
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #5: GFLOPs: 188.4530. Time: 0.0036 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #6: GFLOPs: 183.5786. Time: 0.0037 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #7: GFLOPs: 170.5396. Time: 0.0040 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #8: GFLOPs: 183.6248. Time: 0.0037 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #9: GFLOPs: 183.9702. Time: 0.0037 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #10: GFLOPs: 170.8577. Time: 0.0040 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #11: GFLOPs: 182.9719. Time: 0.0037 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #12: GFLOPs: 175.0318. Time: 0.0039 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #13: GFLOPs: 182.3549. Time: 0.0037 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #14: GFLOPs: 170.7816. Time: 0.0040 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #15: GFLOPs: 176.2393. Time: 0.0038 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #16: GFLOPs: 174.2038. Time: 0.0039 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #17: GFLOPs: 181.5989. Time: 0.0037 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #18: GFLOPs: 186.1869. Time: 0.0036 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #19: GFLOPs: 184.8550. Time: 0.0037 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #20: GFLOPs: 174.8469. Time: 0.0039 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #21: GFLOPs: 148.3557. Time: 0.0046 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #22: GFLOPs: 160.9019. Time: 0.0042 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #23: GFLOPs: 145.3491. Time: 0.0047 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #24: GFLOPs: 160.2126. Time: 0.0042 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #25: GFLOPs: 178.6583. Time: 0.0038 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #26: GFLOPs: 128.9296. Time: 0.0053 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #27: GFLOPs: 142.1403. Time: 0.0048 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #28: GFLOPs: 138.6160. Time: 0.0049 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #29: GFLOPs: 130.0036. Time: 0.0052 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #30: GFLOPs: 135.5695. Time: 0.0050 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d"] Trial #31: GFLOPs: 124.9428. Time: 0.0054 ms. Best GFLOPs: 188.4530
[16:27:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_max_pool2d"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 160
Total latency (us): 32.0166

[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(112, 6, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1), "float32"], T_relu: T.Buffer[(1, 112, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        group_conv2d_nchw_local = T.alloc_buffer([1, 112, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([112, 6, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 2, 2):
                        with T.block("group_conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused // 14 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_4_init)
                            T.reads()
                            T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [112, 6, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                            group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(42):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 3 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 9408 // 448)
                                        v2 = T.axis.spatial(56, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 448 // 8)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(6, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2):
                            with T.block("group_conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused // 14 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_4)
                                rc = T.axis.reduce(6, i4_0 * 3 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(group_conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, ff // 28 * 6 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [112, 6, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 28 * 6 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 2):
                        with T.block("group_conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused // 14 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 7, 2, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 14, 2, 2, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 2])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[2, 3, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110, l111 = sch.split(loop=l108, factors=[None, 56, 4])
sch.vectorize(loop=l111)
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b88)
l119, l120, l121 = sch.split(loop=l118, factors=[None, 56, 2])
sch.vectorize(loop=l121)
sch.bind(loop=l120, thread_axis="threadIdx.x")
b122 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b122, ann_key="meta_schedule.unroll_explicit")
b123, b124, b125, b126 = sch.get_child_blocks(b122)
l127, l128, l129, l130, l131, l132, l133, l134, l135 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l136, l137, l138, l139, l140, l141, l142, l143, l144 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l136, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l136, ann_key="pragma_unroll_explicit", ann_val=1)
l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l145, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l145, ann_key="pragma_unroll_explicit", ann_val=1)
l165, l166, l167, l168, l169, l170, l171 = sch.get_loops(block=b126)
sch.annotate(block_or_loop=l165, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l165, ann_key="pragma_unroll_explicit", ann_val=1)
b172 = sch.get_block(name="group_conv2d_nchw", func_name="main")
l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191, l192 = sch.get_loops(block=b172)
b193 = sch.decompose_reduction(block=b172, loop=l176)
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #1: GFLOPs: 149.3384. Time: 0.0353 ms. Best GFLOPs: 149.3384
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #2: GFLOPs: 175.5550. Time: 0.0300 ms. Best GFLOPs: 175.5550
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #3: GFLOPs: 448.4287. Time: 0.0117 ms. Best GFLOPs: 448.4287
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #4: GFLOPs: 263.3641. Time: 0.0200 ms. Best GFLOPs: 448.4287
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #5: GFLOPs: 233.7343. Time: 0.0225 ms. Best GFLOPs: 448.4287
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #6: GFLOPs: 219.1317. Time: 0.0240 ms. Best GFLOPs: 448.4287
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #7: GFLOPs: 58.8606. Time: 0.0895 ms. Best GFLOPs: 448.4287
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #8: GFLOPs: 71.8101. Time: 0.0734 ms. Best GFLOPs: 448.4287
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #9: GFLOPs: 217.0135. Time: 0.0243 ms. Best GFLOPs: 448.4287
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #10: GFLOPs: 551.8742. Time: 0.0095 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #11: GFLOPs: 282.5686. Time: 0.0186 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #12: GFLOPs: 113.7587. Time: 0.0463 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #13: GFLOPs: 152.3904. Time: 0.0346 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #14: GFLOPs: 247.7200. Time: 0.0213 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #15: GFLOPs: 37.0947. Time: 0.1420 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #16: GFLOPs: 230.4561. Time: 0.0229 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #17: GFLOPs: 52.5262. Time: 0.1003 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #18: GFLOPs: 183.8558. Time: 0.0287 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #19: GFLOPs: 234.2173. Time: 0.0225 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #20: GFLOPs: 221.3307. Time: 0.0238 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #21: GFLOPs: 69.7388. Time: 0.0755 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #22: GFLOPs: 77.5749. Time: 0.0679 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #23: GFLOPs: 81.6964. Time: 0.0645 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #24: GFLOPs: 173.7011. Time: 0.0303 ms. Best GFLOPs: 551.8742
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #25: GFLOPs: 921.7444. Time: 0.0057 ms. Best GFLOPs: 921.7444
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #26: GFLOPs: 489.5625. Time: 0.0108 ms. Best GFLOPs: 921.7444
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #27: GFLOPs: 321.2000. Time: 0.0164 ms. Best GFLOPs: 921.7444
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #28: GFLOPs: 58.6328. Time: 0.0899 ms. Best GFLOPs: 921.7444
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #29: GFLOPs: 99.3109. Time: 0.0531 ms. Best GFLOPs: 921.7444
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #30: GFLOPs: 523.2383. Time: 0.0101 ms. Best GFLOPs: 921.7444
[16:27:29] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #31: GFLOPs: 296.3096. Time: 0.0178 ms. Best GFLOPs: 921.7444
[16:27:29] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_conv2d_multiply_add_nn_relu"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 192
Total latency (us): 37.7324

[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #0: GFLOPs: 0.0000. Time: 0.0052 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #1: GFLOPs: 0.0000. Time: 0.0047 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #2: GFLOPs: 0.0000. Time: 0.0048 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #3: GFLOPs: 0.0000. Time: 0.0045 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #4: GFLOPs: 0.0000. Time: 0.0051 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #5: GFLOPs: 0.0000. Time: 0.0047 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #6: GFLOPs: 0.0000. Time: 0.0047 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #7: GFLOPs: 0.0000. Time: 0.0054 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #8: GFLOPs: 0.0000. Time: 0.0049 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #9: GFLOPs: 0.0000. Time: 0.0051 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #10: GFLOPs: 0.0000. Time: 0.0051 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #11: GFLOPs: 0.0000. Time: 0.0051 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #12: GFLOPs: 0.0000. Time: 0.0049 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #13: GFLOPs: 0.0000. Time: 0.0049 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #14: GFLOPs: 0.0000. Time: 0.0054 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #15: GFLOPs: 0.0000. Time: 0.0057 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #16: GFLOPs: 0.0000. Time: 0.0055 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #17: GFLOPs: 0.0000. Time: 0.0055 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #18: GFLOPs: 0.0000. Time: 0.0045 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #19: GFLOPs: 0.0000. Time: 0.0056 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #20: GFLOPs: 0.0000. Time: 0.0055 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #21: GFLOPs: 0.0000. Time: 0.0047 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #22: GFLOPs: 0.0000. Time: 0.0048 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #23: GFLOPs: 0.0000. Time: 0.0048 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #24: GFLOPs: 0.0000. Time: 0.0056 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #25: GFLOPs: 0.0000. Time: 0.0047 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #26: GFLOPs: 0.0000. Time: 0.0048 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #27: GFLOPs: 0.0000. Time: 0.0047 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #28: GFLOPs: 0.0000. Time: 0.0056 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #29: GFLOPs: 0.0000. Time: 0.0049 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #30: GFLOPs: 0.0000. Time: 0.0055 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_transpose_reshape"] Trial #31: GFLOPs: 0.0000. Time: 0.0050 ms. Best GFLOPs: 0.0000
[16:27:30] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_reshape_transpose_reshape"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 224
Total latency (us): 42.2309

[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #0: GFLOPs: 93.8687. Time: 0.0178 ms. Best GFLOPs: 93.8687
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #1: GFLOPs: 36.6632. Time: 0.0455 ms. Best GFLOPs: 93.8687
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #2: GFLOPs: 15.1486. Time: 0.1101 ms. Best GFLOPs: 93.8687
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #3: GFLOPs: 73.0393. Time: 0.0228 ms. Best GFLOPs: 93.8687
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #4: GFLOPs: 40.7120. Time: 0.0410 ms. Best GFLOPs: 93.8687
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #5: GFLOPs: 96.2528. Time: 0.0173 ms. Best GFLOPs: 96.2528
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #6: GFLOPs: 56.8500. Time: 0.0293 ms. Best GFLOPs: 96.2528
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #7: GFLOPs: 120.2410. Time: 0.0139 ms. Best GFLOPs: 120.2410
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #8: GFLOPs: 29.0303. Time: 0.0575 ms. Best GFLOPs: 120.2410
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #9: GFLOPs: 270.4420. Time: 0.0062 ms. Best GFLOPs: 270.4420
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #10: GFLOPs: 16.3581. Time: 0.1020 ms. Best GFLOPs: 270.4420
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #11: GFLOPs: 205.7960. Time: 0.0081 ms. Best GFLOPs: 270.4420
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #12: GFLOPs: 20.4274. Time: 0.0817 ms. Best GFLOPs: 270.4420
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #13: GFLOPs: 50.5614. Time: 0.0330 ms. Best GFLOPs: 270.4420
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #14: GFLOPs: 272.1160. Time: 0.0061 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #15: GFLOPs: 102.2619. Time: 0.0163 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #16: GFLOPs: 109.6993. Time: 0.0152 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #17: GFLOPs: 78.4555. Time: 0.0213 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #18: GFLOPs: 7.8291. Time: 0.2131 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #19: GFLOPs: 140.0168. Time: 0.0119 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #20: GFLOPs: 88.7128. Time: 0.0188 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #21: GFLOPs: 47.0342. Time: 0.0355 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #22: GFLOPs: 105.5798. Time: 0.0158 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #23: GFLOPs: 12.4415. Time: 0.1341 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #24: GFLOPs: 65.8410. Time: 0.0253 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #25: GFLOPs: 63.3147. Time: 0.0264 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #26: GFLOPs: 66.5732. Time: 0.0251 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #27: GFLOPs: 179.5352. Time: 0.0093 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #28: GFLOPs: 141.1510. Time: 0.0118 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #29: GFLOPs: 19.5569. Time: 0.0853 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #30: GFLOPs: 141.6259. Time: 0.0118 ms. Best GFLOPs: 272.1160
[16:27:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add"] Trial #31: GFLOPs: 31.3673. Time: 0.0532 ms. Best GFLOPs: 272.1160
[16:27:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_conv2d_add"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 256
Total latency (us): 48.3619

[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #0: GFLOPs: 572.1836. Time: 0.0089 ms. Best GFLOPs: 572.1836
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #1: GFLOPs: 582.1873. Time: 0.0087 ms. Best GFLOPs: 582.1873
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #2: GFLOPs: 150.7252. Time: 0.0338 ms. Best GFLOPs: 582.1873
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #3: GFLOPs: 174.6589. Time: 0.0292 ms. Best GFLOPs: 582.1873
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #4: GFLOPs: 363.7933. Time: 0.0140 ms. Best GFLOPs: 582.1873
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #5: GFLOPs: 527.6963. Time: 0.0097 ms. Best GFLOPs: 582.1873
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #6: GFLOPs: 694.9553. Time: 0.0073 ms. Best GFLOPs: 694.9553
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #7: GFLOPs: 772.5482. Time: 0.0066 ms. Best GFLOPs: 772.5482
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #8: GFLOPs: 289.5893. Time: 0.0176 ms. Best GFLOPs: 772.5482
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #9: GFLOPs: 111.3134. Time: 0.0458 ms. Best GFLOPs: 772.5482
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #10: GFLOPs: 923.7090. Time: 0.0055 ms. Best GFLOPs: 923.7090
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #11: GFLOPs: 59.7390. Time: 0.0853 ms. Best GFLOPs: 923.7090
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #12: GFLOPs: 260.8760. Time: 0.0195 ms. Best GFLOPs: 923.7090
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #13: GFLOPs: 368.5132. Time: 0.0138 ms. Best GFLOPs: 923.7090
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #14: GFLOPs: 627.1676. Time: 0.0081 ms. Best GFLOPs: 923.7090
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #15: GFLOPs: 595.8778. Time: 0.0085 ms. Best GFLOPs: 923.7090
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #16: GFLOPs: 148.4650. Time: 0.0343 ms. Best GFLOPs: 923.7090
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #17: GFLOPs: 1212.6287. Time: 0.0042 ms. Best GFLOPs: 1212.6287
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #18: GFLOPs: 370.4533. Time: 0.0137 ms. Best GFLOPs: 1212.6287
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #19: GFLOPs: 367.3905. Time: 0.0139 ms. Best GFLOPs: 1212.6287
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #20: GFLOPs: 219.1770. Time: 0.0232 ms. Best GFLOPs: 1212.6287
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #21: GFLOPs: 419.4570. Time: 0.0121 ms. Best GFLOPs: 1212.6287
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #22: GFLOPs: 75.4310. Time: 0.0675 ms. Best GFLOPs: 1212.6287
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #23: GFLOPs: 42.0450. Time: 0.1211 ms. Best GFLOPs: 1212.6287
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #24: GFLOPs: 596.9688. Time: 0.0085 ms. Best GFLOPs: 1212.6287
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #25: GFLOPs: 602.3626. Time: 0.0085 ms. Best GFLOPs: 1212.6287
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #26: GFLOPs: 261.5055. Time: 0.0195 ms. Best GFLOPs: 1212.6287
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #27: GFLOPs: 691.7726. Time: 0.0074 ms. Best GFLOPs: 1212.6287
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #28: GFLOPs: 189.3586. Time: 0.0269 ms. Best GFLOPs: 1212.6287
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #29: GFLOPs: 964.3316. Time: 0.0053 ms. Best GFLOPs: 1212.6287
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #30: GFLOPs: 215.0517. Time: 0.0237 ms. Best GFLOPs: 1212.6287
[16:27:32] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_multiply_add"] Trial #31: GFLOPs: 579.0514. Time: 0.0088 ms. Best GFLOPs: 1212.6287
[16:27:34] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_conv2d_multiply_add"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 288
Total latency (us): 52.5618

[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #0: GFLOPs: 42.8016. Time: 0.0025 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #1: GFLOPs: 42.4891. Time: 0.0025 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #2: GFLOPs: 40.5124. Time: 0.0026 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #3: GFLOPs: 42.5709. Time: 0.0025 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #4: GFLOPs: 40.5438. Time: 0.0026 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #5: GFLOPs: 38.6536. Time: 0.0028 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #6: GFLOPs: 40.8550. Time: 0.0026 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #7: GFLOPs: 39.4216. Time: 0.0027 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #8: GFLOPs: 36.2959. Time: 0.0029 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #9: GFLOPs: 38.2409. Time: 0.0028 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #10: GFLOPs: 40.5941. Time: 0.0026 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #11: GFLOPs: 40.7792. Time: 0.0026 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #12: GFLOPs: 42.6944. Time: 0.0025 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #13: GFLOPs: 40.6232. Time: 0.0026 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #14: GFLOPs: 35.9716. Time: 0.0030 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #15: GFLOPs: 41.7763. Time: 0.0026 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #16: GFLOPs: 38.5969. Time: 0.0028 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #17: GFLOPs: 42.4720. Time: 0.0025 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #18: GFLOPs: 39.7145. Time: 0.0027 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #19: GFLOPs: 42.7700. Time: 0.0025 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #20: GFLOPs: 42.7718. Time: 0.0025 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #21: GFLOPs: 40.5117. Time: 0.0026 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #22: GFLOPs: 42.7752. Time: 0.0025 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #23: GFLOPs: 42.7309. Time: 0.0025 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #24: GFLOPs: 38.8046. Time: 0.0027 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #25: GFLOPs: 40.6535. Time: 0.0026 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #26: GFLOPs: 39.5968. Time: 0.0027 ms. Best GFLOPs: 42.8016
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #27: GFLOPs: 43.3552. Time: 0.0025 ms. Best GFLOPs: 43.3552
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #28: GFLOPs: 43.8238. Time: 0.0024 ms. Best GFLOPs: 43.8238
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #29: GFLOPs: 40.7190. Time: 0.0026 ms. Best GFLOPs: 43.8238
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #30: GFLOPs: 41.8786. Time: 0.0025 ms. Best GFLOPs: 43.8238
[16:27:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_concatenate_nn_relu"] Trial #31: GFLOPs: 40.5225. Time: 0.0026 ms. Best GFLOPs: 43.8238
[16:27:35] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_concatenate_nn_relu"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 320
Total latency (us): 54.9948

[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #0: GFLOPs: 44.0226. Time: 0.0460 ms. Best GFLOPs: 44.0226
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #1: GFLOPs: 33.0363. Time: 0.0613 ms. Best GFLOPs: 44.0226
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #2: GFLOPs: 157.9559. Time: 0.0128 ms. Best GFLOPs: 157.9559
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #3: GFLOPs: 197.6574. Time: 0.0102 ms. Best GFLOPs: 197.6574
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #4: GFLOPs: 89.9840. Time: 0.0225 ms. Best GFLOPs: 197.6574
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #5: GFLOPs: 141.4343. Time: 0.0143 ms. Best GFLOPs: 197.6574
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #6: GFLOPs: 229.5977. Time: 0.0088 ms. Best GFLOPs: 229.5977
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #7: GFLOPs: 81.3297. Time: 0.0249 ms. Best GFLOPs: 229.5977
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_1"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], T_add: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 136, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 136, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([136, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(34, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 14, 2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(136, i0_1_i1_1_i2_1_i3_1_fused * 68 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i2_3_init * 2 + i2_4_init)
                            j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_3_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(256):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(34, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(136, (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 8704 // 64)
                                    v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 64 // 16)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(34, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(136, (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 14, 1, 3, 1, 2, 2, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(136, i0_1_i1_1_i2_1_i3_1_fused * 68 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i2_3 * 2 + i2_4)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 14):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(136, i0_1_i1_1_i2_1_i3_1_fused * 68 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l4, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 34, 1, 2])
l25, l26, l27, l28, l29 = sch.split(loop=l5, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 2])
l35, l36, l37, l38, l39 = sch.split(loop=l6, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 14, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l7, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l53, l54, l55 = sch.split(loop=l8, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l59, l60, l61 = sch.split(loop=l9, factors=[v56, v57, v58])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l54, l60, l18, l28, l38, l48, l55, l61, l19, l29, l39, l49)
l62 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l62, thread_axis="blockIdx.x")
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="vthread.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b65 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b65, loop=l64, preserve_unit_loops=True)
b66 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b66, loop=l59, preserve_unit_loops=True)
l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b66)
l76 = sch.fuse(l72, l73, l74, l75)
v77 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b66, ann_key="meta_schedule.cooperative_fetch", ann_val=v77)
b78 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b78, loop=l59, preserve_unit_loops=True)
l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b78)
l88 = sch.fuse(l84, l85, l86, l87)
v89 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b78, ann_key="meta_schedule.cooperative_fetch", ann_val=v89)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v90 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v90)
sch.enter_postproc()
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.cooperative_fetch")
l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b66)
l97, l98 = sch.split(loop=l96, factors=[None, 34])
sch.bind(loop=l98, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b78, ann_key="meta_schedule.cooperative_fetch")
l99, l100, l101, l102, l103, l104 = sch.get_loops(block=b78)
l105, l106 = sch.split(loop=l104, factors=[None, 34])
sch.bind(loop=l106, thread_axis="threadIdx.x")
b107 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b107, ann_key="meta_schedule.unroll_explicit")
b108, b109, b110, b111 = sch.get_child_blocks(b107)
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b108)
sch.annotate(block_or_loop=l112, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l112, ann_key="pragma_unroll_explicit", ann_val=1)
l119, l120, l121, l122, l123, l124, l125 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l119, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l119, ann_key="pragma_unroll_explicit", ann_val=1)
l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
b150 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b150)
b168 = sch.decompose_reduction(block=b150, loop=l154)
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #9: GFLOPs: 203.7726. Time: 0.0099 ms. Best GFLOPs: 229.5977
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #10: GFLOPs: 58.7293. Time: 0.0345 ms. Best GFLOPs: 229.5977
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #11: GFLOPs: 105.2303. Time: 0.0193 ms. Best GFLOPs: 229.5977
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #12: GFLOPs: 26.2840. Time: 0.0771 ms. Best GFLOPs: 229.5977
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #13: GFLOPs: 261.7333. Time: 0.0077 ms. Best GFLOPs: 261.7333
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #14: GFLOPs: 11.6327. Time: 0.1742 ms. Best GFLOPs: 261.7333
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #15: GFLOPs: 229.5381. Time: 0.0088 ms. Best GFLOPs: 261.7333
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #16: GFLOPs: 348.9668. Time: 0.0058 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #17: GFLOPs: 164.4304. Time: 0.0123 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #18: GFLOPs: 108.0057. Time: 0.0188 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #19: GFLOPs: 316.6687. Time: 0.0064 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #20: GFLOPs: 152.3406. Time: 0.0133 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #21: GFLOPs: 43.4693. Time: 0.0466 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #22: GFLOPs: 33.2896. Time: 0.0609 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #23: GFLOPs: 147.8351. Time: 0.0137 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #24: GFLOPs: 98.5177. Time: 0.0206 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #25: GFLOPs: 9.8351. Time: 0.2060 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #26: GFLOPs: 114.4100. Time: 0.0177 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #27: GFLOPs: 165.6809. Time: 0.0122 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #28: GFLOPs: 266.3268. Time: 0.0076 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #29: GFLOPs: 76.5106. Time: 0.0265 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #30: GFLOPs: 110.2643. Time: 0.0184 ms. Best GFLOPs: 348.9668
[16:27:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_1"] Trial #31: GFLOPs: 91.8187. Time: 0.0221 ms. Best GFLOPs: 348.9668
[16:27:37] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_conv2d_add_1"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 352
Total latency (us): 72.4107

[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #0: GFLOPs: 61.3461. Time: 0.1251 ms. Best GFLOPs: 61.3461
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #1: GFLOPs: 252.9929. Time: 0.0303 ms. Best GFLOPs: 252.9929
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #2: GFLOPs: 400.9748. Time: 0.0191 ms. Best GFLOPs: 400.9748
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #3: GFLOPs: 20.8218. Time: 0.3687 ms. Best GFLOPs: 400.9748
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #4: GFLOPs: 129.9047. Time: 0.0591 ms. Best GFLOPs: 400.9748
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #5: GFLOPs: 427.0818. Time: 0.0180 ms. Best GFLOPs: 427.0818
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #6: GFLOPs: 112.8327. Time: 0.0680 ms. Best GFLOPs: 427.0818
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #7: GFLOPs: 465.9716. Time: 0.0165 ms. Best GFLOPs: 465.9716
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #8: GFLOPs: 457.2490. Time: 0.0168 ms. Best GFLOPs: 465.9716
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #9: GFLOPs: 419.0533. Time: 0.0183 ms. Best GFLOPs: 465.9716
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #10: GFLOPs: 673.6283. Time: 0.0114 ms. Best GFLOPs: 673.6283
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #11: GFLOPs: 646.9631. Time: 0.0119 ms. Best GFLOPs: 673.6283
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #12: GFLOPs: 245.5391. Time: 0.0313 ms. Best GFLOPs: 673.6283
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #13: GFLOPs: 97.8132. Time: 0.0785 ms. Best GFLOPs: 673.6283
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #14: GFLOPs: 1094.8859. Time: 0.0070 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #15: GFLOPs: 381.8073. Time: 0.0201 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #16: GFLOPs: 18.4711. Time: 0.4156 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #17: GFLOPs: 57.2514. Time: 0.1341 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #18: GFLOPs: 106.8393. Time: 0.0719 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #19: GFLOPs: 329.3246. Time: 0.0233 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #20: GFLOPs: 81.5443. Time: 0.0941 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #21: GFLOPs: 70.3550. Time: 0.1091 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #22: GFLOPs: 558.9398. Time: 0.0137 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #23: GFLOPs: 835.0866. Time: 0.0092 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #24: GFLOPs: 602.7051. Time: 0.0127 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #25: GFLOPs: 150.0783. Time: 0.0512 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #26: GFLOPs: 556.2639. Time: 0.0138 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #27: GFLOPs: 576.0509. Time: 0.0133 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #28: GFLOPs: 122.5244. Time: 0.0627 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #29: GFLOPs: 269.0403. Time: 0.0285 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #30: GFLOPs: 452.1542. Time: 0.0170 ms. Best GFLOPs: 1094.8859
[16:27:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #31: GFLOPs: 387.1838. Time: 0.0198 ms. Best GFLOPs: 1094.8859
[16:27:39] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 384
Total latency (us): 93.4455

[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #0: GFLOPs: 242.0943. Time: 0.0313 ms. Best GFLOPs: 242.0943
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #1: GFLOPs: 512.6407. Time: 0.0148 ms. Best GFLOPs: 512.6407
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #2: GFLOPs: 54.5814. Time: 0.1387 ms. Best GFLOPs: 512.6407
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #3: GFLOPs: 178.9158. Time: 0.0423 ms. Best GFLOPs: 512.6407
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #4: GFLOPs: 716.0406. Time: 0.0106 ms. Best GFLOPs: 716.0406
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #5: GFLOPs: 44.4066. Time: 0.1705 ms. Best GFLOPs: 716.0406
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #6: GFLOPs: 550.4674. Time: 0.0138 ms. Best GFLOPs: 716.0406
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #7: GFLOPs: 353.6629. Time: 0.0214 ms. Best GFLOPs: 716.0406
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #8: GFLOPs: 313.9528. Time: 0.0241 ms. Best GFLOPs: 716.0406
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #9: GFLOPs: 556.6449. Time: 0.0136 ms. Best GFLOPs: 716.0406
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #10: GFLOPs: 391.3443. Time: 0.0193 ms. Best GFLOPs: 716.0406
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #11: GFLOPs: 340.1973. Time: 0.0223 ms. Best GFLOPs: 716.0406
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #12: GFLOPs: 41.0553. Time: 0.1844 ms. Best GFLOPs: 716.0406
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #13: GFLOPs: 766.1901. Time: 0.0099 ms. Best GFLOPs: 766.1901
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #14: GFLOPs: 582.3321. Time: 0.0130 ms. Best GFLOPs: 766.1901
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #15: GFLOPs: 29.5448. Time: 0.2562 ms. Best GFLOPs: 766.1901
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #16: GFLOPs: 496.5288. Time: 0.0152 ms. Best GFLOPs: 766.1901
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #17: GFLOPs: 102.0526. Time: 0.0742 ms. Best GFLOPs: 766.1901
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #18: GFLOPs: 156.6758. Time: 0.0483 ms. Best GFLOPs: 766.1901
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #19: GFLOPs: 809.7167. Time: 0.0093 ms. Best GFLOPs: 809.7167
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #20: GFLOPs: 69.0862. Time: 0.1096 ms. Best GFLOPs: 809.7167
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #21: GFLOPs: 968.3829. Time: 0.0078 ms. Best GFLOPs: 968.3829
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #22: GFLOPs: 254.7121. Time: 0.0297 ms. Best GFLOPs: 968.3829
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #23: GFLOPs: 16.8712. Time: 0.4487 ms. Best GFLOPs: 968.3829
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #24: GFLOPs: 185.3887. Time: 0.0408 ms. Best GFLOPs: 968.3829
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #25: GFLOPs: 31.8682. Time: 0.2376 ms. Best GFLOPs: 968.3829
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #26: GFLOPs: 605.8099. Time: 0.0125 ms. Best GFLOPs: 968.3829
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #27: GFLOPs: 714.6519. Time: 0.0106 ms. Best GFLOPs: 968.3829
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #28: GFLOPs: 593.4245. Time: 0.0128 ms. Best GFLOPs: 968.3829
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #29: GFLOPs: 46.7153. Time: 0.1621 ms. Best GFLOPs: 968.3829
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #30: GFLOPs: 550.8103. Time: 0.0137 ms. Best GFLOPs: 968.3829
[16:27:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #31: GFLOPs: 685.3185. Time: 0.0110 ms. Best GFLOPs: 968.3829
[16:27:41] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 416
Total latency (us): 124.715

[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #0: GFLOPs: 0.0000. Time: 0.0032 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #1: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #2: GFLOPs: 0.0000. Time: 0.0031 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #3: GFLOPs: 0.0000. Time: 0.0033 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #4: GFLOPs: 0.0000. Time: 0.0030 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #5: GFLOPs: 0.0000. Time: 0.0031 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #6: GFLOPs: 0.0000. Time: 0.0030 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #7: GFLOPs: 0.0000. Time: 0.0032 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #8: GFLOPs: 0.0000. Time: 0.0028 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #9: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #10: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #11: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #12: GFLOPs: 0.0000. Time: 0.0030 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #13: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #14: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #15: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #16: GFLOPs: 0.0000. Time: 0.0032 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #17: GFLOPs: 0.0000. Time: 0.0030 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #18: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #19: GFLOPs: 0.0000. Time: 0.0031 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #20: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #21: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #22: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #23: GFLOPs: 0.0000. Time: 0.0031 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #24: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #25: GFLOPs: 0.0000. Time: 0.0031 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #26: GFLOPs: 0.0000. Time: 0.0031 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #27: GFLOPs: 0.0000. Time: 0.0030 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #28: GFLOPs: 0.0000. Time: 0.0031 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #29: GFLOPs: 0.0000. Time: 0.0031 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #30: GFLOPs: 0.0000. Time: 0.0032 ms. Best GFLOPs: 0.0000
[16:27:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_reshape_transpose_reshape_1"] Trial #31: GFLOPs: 0.0000. Time: 0.0032 ms. Best GFLOPs: 0.0000
[16:27:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_reshape_transpose_reshape_1"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 448
Total latency (us): 135.891

[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #0: GFLOPs: 64.4605. Time: 0.0079 ms. Best GFLOPs: 64.4605
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #1: GFLOPs: 13.4527. Time: 0.0376 ms. Best GFLOPs: 64.4605
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #2: GFLOPs: 20.6290. Time: 0.0246 ms. Best GFLOPs: 64.4605
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #3: GFLOPs: 6.3180. Time: 0.0802 ms. Best GFLOPs: 64.4605
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #4: GFLOPs: 3.3811. Time: 0.1498 ms. Best GFLOPs: 64.4605
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #5: GFLOPs: 46.3416. Time: 0.0109 ms. Best GFLOPs: 64.4605
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #6: GFLOPs: 3.3340. Time: 0.1519 ms. Best GFLOPs: 64.4605
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #7: GFLOPs: 34.6122. Time: 0.0146 ms. Best GFLOPs: 64.4605
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #8: GFLOPs: 52.5285. Time: 0.0096 ms. Best GFLOPs: 64.4605
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #9: GFLOPs: 30.5802. Time: 0.0166 ms. Best GFLOPs: 64.4605
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #10: GFLOPs: 21.7745. Time: 0.0233 ms. Best GFLOPs: 64.4605
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #11: GFLOPs: 78.1828. Time: 0.0065 ms. Best GFLOPs: 78.1828
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #12: GFLOPs: 36.2874. Time: 0.0140 ms. Best GFLOPs: 78.1828
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #13: GFLOPs: 13.2978. Time: 0.0381 ms. Best GFLOPs: 78.1828
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #14: GFLOPs: 73.1452. Time: 0.0069 ms. Best GFLOPs: 78.1828
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #15: GFLOPs: 137.5403. Time: 0.0037 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #16: GFLOPs: 38.7457. Time: 0.0131 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #17: GFLOPs: 26.6188. Time: 0.0190 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #18: GFLOPs: 11.5631. Time: 0.0438 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #19: GFLOPs: 14.9934. Time: 0.0338 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #20: GFLOPs: 12.9187. Time: 0.0392 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #21: GFLOPs: 101.1010. Time: 0.0050 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #22: GFLOPs: 63.7512. Time: 0.0079 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #23: GFLOPs: 83.7011. Time: 0.0061 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #24: GFLOPs: 42.4548. Time: 0.0119 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #25: GFLOPs: 6.9868. Time: 0.0725 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #26: GFLOPs: 37.4688. Time: 0.0135 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #27: GFLOPs: 4.4076. Time: 0.1149 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #28: GFLOPs: 119.5938. Time: 0.0042 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #29: GFLOPs: 34.9155. Time: 0.0145 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #30: GFLOPs: 76.1902. Time: 0.0066 ms. Best GFLOPs: 137.5403
[16:27:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_2"] Trial #31: GFLOPs: 8.4291. Time: 0.0601 ms. Best GFLOPs: 137.5403
[16:27:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_nn_conv2d_add_2"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 480
Total latency (us): 139.574

[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #0: GFLOPs: 47.0910. Time: 0.0396 ms. Best GFLOPs: 47.0910
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #1: GFLOPs: 16.7347. Time: 0.1115 ms. Best GFLOPs: 47.0910
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #2: GFLOPs: 10.4315. Time: 0.1789 ms. Best GFLOPs: 47.0910
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #3: GFLOPs: 12.1525. Time: 0.1535 ms. Best GFLOPs: 47.0910
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #4: GFLOPs: 26.9584. Time: 0.0692 ms. Best GFLOPs: 47.0910
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #5: GFLOPs: 46.0197. Time: 0.0405 ms. Best GFLOPs: 47.0910
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #6: GFLOPs: 348.2776. Time: 0.0054 ms. Best GFLOPs: 348.2776
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #7: GFLOPs: 61.5995. Time: 0.0303 ms. Best GFLOPs: 348.2776
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #8: GFLOPs: 325.7932. Time: 0.0057 ms. Best GFLOPs: 348.2776
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #9: GFLOPs: 83.5358. Time: 0.0223 ms. Best GFLOPs: 348.2776
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #10: GFLOPs: 111.6791. Time: 0.0167 ms. Best GFLOPs: 348.2776
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #11: GFLOPs: 108.5138. Time: 0.0172 ms. Best GFLOPs: 348.2776
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #12: GFLOPs: 56.2707. Time: 0.0332 ms. Best GFLOPs: 348.2776
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #13: GFLOPs: 2.6379. Time: 0.7074 ms. Best GFLOPs: 348.2776
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #14: GFLOPs: 68.8360. Time: 0.0271 ms. Best GFLOPs: 348.2776
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #15: GFLOPs: 216.5384. Time: 0.0086 ms. Best GFLOPs: 348.2776
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #16: GFLOPs: 139.5132. Time: 0.0134 ms. Best GFLOPs: 348.2776
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #17: GFLOPs: 69.6540. Time: 0.0268 ms. Best GFLOPs: 348.2776
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #18: GFLOPs: 375.9655. Time: 0.0050 ms. Best GFLOPs: 375.9655
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #19: GFLOPs: 66.3802. Time: 0.0281 ms. Best GFLOPs: 375.9655
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #20: GFLOPs: 73.3392. Time: 0.0254 ms. Best GFLOPs: 375.9655
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #21: GFLOPs: 141.0860. Time: 0.0132 ms. Best GFLOPs: 375.9655
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #22: GFLOPs: 93.8184. Time: 0.0199 ms. Best GFLOPs: 375.9655
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #23: GFLOPs: 54.5513. Time: 0.0342 ms. Best GFLOPs: 375.9655
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #24: GFLOPs: 133.1135. Time: 0.0140 ms. Best GFLOPs: 375.9655
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #25: GFLOPs: 35.9967. Time: 0.0518 ms. Best GFLOPs: 375.9655
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #26: GFLOPs: 425.0650. Time: 0.0044 ms. Best GFLOPs: 425.0650
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #27: GFLOPs: 66.0884. Time: 0.0282 ms. Best GFLOPs: 425.0650
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #28: GFLOPs: 47.5716. Time: 0.0392 ms. Best GFLOPs: 425.0650
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #29: GFLOPs: 267.3255. Time: 0.0070 ms. Best GFLOPs: 425.0650
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #30: GFLOPs: 142.4770. Time: 0.0131 ms. Best GFLOPs: 425.0650
[16:27:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_1"] Trial #31: GFLOPs: 208.8688. Time: 0.0089 ms. Best GFLOPs: 425.0650
[16:27:46] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_conv2d_multiply_add_1"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 512
Total latency (us): 143.963

[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #0: GFLOPs: 24.3782. Time: 0.0022 ms. Best GFLOPs: 24.3782
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #1: GFLOPs: 24.6017. Time: 0.0022 ms. Best GFLOPs: 24.6017
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #2: GFLOPs: 24.6405. Time: 0.0022 ms. Best GFLOPs: 24.6405
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #3: GFLOPs: 23.1807. Time: 0.0023 ms. Best GFLOPs: 24.6405
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #4: GFLOPs: 23.1355. Time: 0.0023 ms. Best GFLOPs: 24.6405
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #5: GFLOPs: 23.7452. Time: 0.0022 ms. Best GFLOPs: 24.6405
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #6: GFLOPs: 23.6377. Time: 0.0023 ms. Best GFLOPs: 24.6405
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #7: GFLOPs: 23.7803. Time: 0.0022 ms. Best GFLOPs: 24.6405
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #8: GFLOPs: 23.6932. Time: 0.0023 ms. Best GFLOPs: 24.6405
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #9: GFLOPs: 23.2834. Time: 0.0023 ms. Best GFLOPs: 24.6405
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #10: GFLOPs: 23.7221. Time: 0.0022 ms. Best GFLOPs: 24.6405
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #11: GFLOPs: 23.5533. Time: 0.0023 ms. Best GFLOPs: 24.6405
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #12: GFLOPs: 24.1443. Time: 0.0022 ms. Best GFLOPs: 24.6405
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #13: GFLOPs: 24.4372. Time: 0.0022 ms. Best GFLOPs: 24.6405
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #14: GFLOPs: 23.4228. Time: 0.0023 ms. Best GFLOPs: 24.6405
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #15: GFLOPs: 24.7213. Time: 0.0022 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #16: GFLOPs: 23.4670. Time: 0.0023 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #17: GFLOPs: 23.1560. Time: 0.0023 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #18: GFLOPs: 24.0275. Time: 0.0022 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #19: GFLOPs: 24.0287. Time: 0.0022 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #20: GFLOPs: 23.8621. Time: 0.0022 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #21: GFLOPs: 23.7962. Time: 0.0022 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #22: GFLOPs: 23.4924. Time: 0.0023 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #23: GFLOPs: 23.0293. Time: 0.0023 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #24: GFLOPs: 23.5308. Time: 0.0023 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #25: GFLOPs: 23.7682. Time: 0.0022 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #26: GFLOPs: 23.8591. Time: 0.0022 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #27: GFLOPs: 22.9281. Time: 0.0023 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #28: GFLOPs: 23.9833. Time: 0.0022 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #29: GFLOPs: 23.9177. Time: 0.0022 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #30: GFLOPs: 23.8674. Time: 0.0022 ms. Best GFLOPs: 24.7213
[16:27:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_concatenate_nn_relu_1"] Trial #31: GFLOPs: 20.3357. Time: 0.0026 ms. Best GFLOPs: 24.7213
[16:27:49] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_concatenate_nn_relu_1"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 544
Total latency (us): 146.12

[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #0: GFLOPs: 63.4608. Time: 0.0160 ms. Best GFLOPs: 63.4608
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #1: GFLOPs: 17.7924. Time: 0.0569 ms. Best GFLOPs: 63.4608
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #2: GFLOPs: 96.2607. Time: 0.0105 ms. Best GFLOPs: 96.2607
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #3: GFLOPs: 45.9564. Time: 0.0220 ms. Best GFLOPs: 96.2607
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #4: GFLOPs: 59.4141. Time: 0.0170 ms. Best GFLOPs: 96.2607
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #5: GFLOPs: 28.0651. Time: 0.0361 ms. Best GFLOPs: 96.2607
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #6: GFLOPs: 25.9636. Time: 0.0390 ms. Best GFLOPs: 96.2607
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #7: GFLOPs: 75.7895. Time: 0.0134 ms. Best GFLOPs: 96.2607
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #8: GFLOPs: 42.3354. Time: 0.0239 ms. Best GFLOPs: 96.2607
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #9: GFLOPs: 35.8128. Time: 0.0283 ms. Best GFLOPs: 96.2607
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #10: GFLOPs: 147.7931. Time: 0.0069 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #11: GFLOPs: 88.5871. Time: 0.0114 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #12: GFLOPs: 111.9535. Time: 0.0090 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #13: GFLOPs: 9.1660. Time: 0.1105 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #14: GFLOPs: 122.5573. Time: 0.0083 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #15: GFLOPs: 8.0151. Time: 0.1264 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #16: GFLOPs: 141.8507. Time: 0.0071 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #17: GFLOPs: 40.4854. Time: 0.0250 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #18: GFLOPs: 14.8231. Time: 0.0683 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #19: GFLOPs: 124.4664. Time: 0.0081 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #20: GFLOPs: 82.7864. Time: 0.0122 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #21: GFLOPs: 101.4437. Time: 0.0100 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #22: GFLOPs: 65.6744. Time: 0.0154 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #23: GFLOPs: 113.8133. Time: 0.0089 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #24: GFLOPs: 85.5710. Time: 0.0118 ms. Best GFLOPs: 147.7931
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #25: GFLOPs: 287.9353. Time: 0.0035 ms. Best GFLOPs: 287.9353
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #26: GFLOPs: 68.5265. Time: 0.0148 ms. Best GFLOPs: 287.9353
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #27: GFLOPs: 194.4514. Time: 0.0052 ms. Best GFLOPs: 287.9353
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #28: GFLOPs: 25.8084. Time: 0.0392 ms. Best GFLOPs: 287.9353
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #29: GFLOPs: 67.2243. Time: 0.0151 ms. Best GFLOPs: 287.9353
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #30: GFLOPs: 57.5427. Time: 0.0176 ms. Best GFLOPs: 287.9353
[16:27:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_3"] Trial #31: GFLOPs: 35.1902. Time: 0.0288 ms. Best GFLOPs: 287.9353
[16:27:51] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #17: "fused_nn_conv2d_add_3"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 576
Total latency (us): 170.745

[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #0: GFLOPs: 17.3205. Time: 0.4309 ms. Best GFLOPs: 17.3205
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #1: GFLOPs: 140.4128. Time: 0.0532 ms. Best GFLOPs: 140.4128
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #2: GFLOPs: 287.8199. Time: 0.0259 ms. Best GFLOPs: 287.8199
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #3: GFLOPs: 1126.6153. Time: 0.0066 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #4: GFLOPs: 336.1832. Time: 0.0222 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #5: GFLOPs: 70.1694. Time: 0.1064 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #6: GFLOPs: 8.9558. Time: 0.8334 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #7: GFLOPs: 236.9557. Time: 0.0315 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #8: GFLOPs: 171.0962. Time: 0.0436 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #9: GFLOPs: 61.8843. Time: 0.1206 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #10: GFLOPs: 745.2083. Time: 0.0100 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #11: GFLOPs: 684.9731. Time: 0.0109 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #12: GFLOPs: 631.0557. Time: 0.0118 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #13: GFLOPs: 258.3195. Time: 0.0289 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #14: GFLOPs: 620.6895. Time: 0.0120 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #15: GFLOPs: 220.0435. Time: 0.0339 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #16: GFLOPs: 158.1507. Time: 0.0472 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #17: GFLOPs: 473.8353. Time: 0.0158 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #18: GFLOPs: 40.0920. Time: 0.1862 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #19: GFLOPs: 388.8329. Time: 0.0192 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #20: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 272, 14, 14), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        group_conv2d_nchw_local = T.alloc_buffer([1, 272, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 272, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([272, 68, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(68, thread="threadIdx.x"):
                    for i1_4_init in T.serial(2):
                        with T.block("group_conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused // 7 * 136 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 14 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                            xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                            T.reads()
                            T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                            group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(68, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(272, i4_0 * 17 + (ax0_ax1_ax2_ax3_fused_0 * 68 + ax0_ax1_ax2_ax3_fused_1) % 1547 // 7)
                                    v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 14 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 68 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 68 + ax0_ax1_ax2_ax3_fused_1 < 1547)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(17):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(68, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(272, (ax0_ax1_ax2_ax3_fused_0 * 272 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 17)
                                        v1 = T.axis.spatial(68, i4_0 * 17 + (ax0_ax1_ax2_ax3_fused_0 * 272 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 17)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("group_conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused // 7 * 136 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 14 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7)
                                xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                rc = T.axis.reduce(68, i4_0 * 17 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(group_conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("group_conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused // 7 * 136 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 14 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + ax3)
                            T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0], placeholder_4[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0] + placeholder_4[v0, v1, v2, v3], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l19, l20, l21, l22, l23 = sch.split(loop=l7, factors=[v14, v15, v16, v17, v18])
v24, v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 68, 1, 2])
l29, l30, l31, l32, l33 = sch.split(loop=l8, factors=[v24, v25, v26, v27, v28])
v34, v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 1])
l39, l40, l41, l42, l43 = sch.split(loop=l9, factors=[v34, v35, v36, v37, v38])
v44, v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l10, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l49, l50, l51, l52, l53 = sch.split(loop=l10, factors=[v44, v45, v46, v47, v48])
v54, v55, v56 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[4, 17, 1])
l57, l58, l59 = sch.split(loop=l11, factors=[v54, v55, v56])
v60, v61, v62 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l63, l64, l65 = sch.split(loop=l12, factors=[v60, v61, v62])
v66, v67, v68 = sch.sample_perfect_tile(loop=l13, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l69, l70, l71 = sch.split(loop=l13, factors=[v66, v67, v68])
sch.reorder(l19, l29, l39, l49, l20, l30, l40, l50, l21, l31, l41, l51, l57, l63, l69, l58, l64, l70, l22, l32, l42, l52, l59, l65, l71, l23, l33, l43, l53)
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="vthread.x")
l74 = sch.fuse(l21, l31, l41, l51)
sch.bind(loop=l74, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b75 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b75, loop=l74, preserve_unit_loops=True)
b76 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b76, loop=l69, preserve_unit_loops=True)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b76)
l87 = sch.fuse(l83, l84, l85, l86)
v88 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch", ann_val=v88)
b89 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b89, loop=l69, preserve_unit_loops=True)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b89)
l100 = sch.fuse(l96, l97, l98, l99)
v101 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b76)
l110, l111 = sch.split(loop=l109, factors=[None, 68])
sch.bind(loop=l111, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b89)
l119, l120, l121 = sch.split(loop=l118, factors=[None, 68, 4])
sch.vectorize(loop=l121)
sch.bind(loop=l120, thread_axis="threadIdx.x")
b122 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b122, ann_key="meta_schedule.unroll_explicit")
b123, b124, b125, b126 = sch.get_child_blocks(b122)
l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b126)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="group_conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #21: GFLOPs: 65.4098. Time: 0.1141 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #22: GFLOPs: 164.3461. Time: 0.0454 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #23: GFLOPs: 528.5653. Time: 0.0141 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #24: GFLOPs: 420.3128. Time: 0.0178 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #25: GFLOPs: 87.7800. Time: 0.0850 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #26: GFLOPs: 631.7937. Time: 0.0118 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #27: GFLOPs: 97.9201. Time: 0.0762 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #28: GFLOPs: 926.3102. Time: 0.0081 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #29: GFLOPs: 471.5048. Time: 0.0158 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #30: GFLOPs: 228.8007. Time: 0.0326 ms. Best GFLOPs: 1126.6153
[16:27:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #31: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
  7: tvm::tir::StmtExprMutator::VisitExpr(tvm::PrimExpr const&)
        at /home/yj/tvm/src/tir/ir/stmt_functor.cc:503
  6: tvm::tir::ExprFunctor<tvm::PrimExpr (tvm::PrimExpr const&)>::VisitExpr(tvm::PrimExpr const&)
        at /home/yj/tvm/include/tvm/tir/stmt_functor.h:319
  5: tvm::NodeFunctor<tvm::PrimExpr (tvm::runtime::ObjectRef const&, tvm::tir::ExprFunctor<tvm::PrimExpr (tvm::PrimExpr const&)>*)>::operator()(tvm::runtime::ObjectRef const&, tvm::tir::ExprFunctor<tvm::PrimExpr (tvm::PrimExpr const&)>*) const
        at /home/yj/tvm/include/tvm/tir/expr_functor.h:114
  4: tvm::tir::ExprFunctor<tvm::PrimExpr (tvm::PrimExpr const&)>::InitVTable()::{lambda(tvm::runtime::ObjectRef const&, tvm::tir::ExprFunctor<tvm::PrimExpr (tvm::PrimExpr const&)>*)#7}::_FUN(tvm::runtime::ObjectRef const&, tvm::tir::ExprFunctor<tvm::PrimExpr (tvm::PrimExpr const&)>*)
        at /home/yj/tvm/include/tvm/node/functor.h:97
  3: tvm::tir::ExprFunctor<tvm::PrimExpr (tvm::PrimExpr const&)>::InitVTable()::{lambda(tvm::runtime::ObjectRef const&, tvm::tir::ExprFunctor<tvm::PrimExpr (tvm::PrimExpr const&)>*)#7}::operator()(tvm::runtime::ObjectRef const&, tvm::tir::ExprFunctor<tvm::PrimExpr (tvm::PrimExpr const&)>*) const
        at /home/yj/tvm/include/tvm/tir/expr_functor.h:170
  2: tvm::tir::BuiltinLower::VisitExpr_(tvm::tir::CallNode const*)
        at /home/yj/tvm/include/tvm/tir/expr_functor.h:170
  1: tvm::tir::BuiltinLower::MakeCallPacked(tvm::tir::CallNode const*, bool)
        at /home/yj/tvm/src/tir/transforms/lower_tvm_builtin.cc:292
  0: tvm::tir::APIType(tvm::runtime::DataType)
        at /home/yj/tvm/src/tir/transforms/lower_tvm_builtin.cc:410
  File "/home/yj/tvm/src/tir/transforms/ir_utils.h", line 153
        at /home/yj/tvm/src/target/llvm/../../tir/transforms/ir_utils.h:153
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: t.lanes() == 1 (4 vs. 1) : Cannot pass vector type through packed API.
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 272, 14, 14), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        group_conv2d_nchw_local = T.alloc_buffer([1, 272, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 272, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([272, 68, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(119, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(119, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused // 14 * 136 + (ax0_ax1_ax2_ax3_fused_0 * 476 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 14)
                                        v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 476 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14 // 2)
                                        v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 476 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(119, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused // 14 * 136 + (ax0_ax1_ax2_ax3_fused_0 * 476 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 68)
                                        v1 = T.axis.spatial(68, (ax0_ax1_ax2_ax3_fused_0 * 476 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 68)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 119 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 9248)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init in T.serial(2):
                            with T.block("group_conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused // 14 * 136 + i0_1_i1_1_i2_1_i3_1_fused * 17 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i3_3_init)
                                T.reads()
                                T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 2, 34, 1, 1, 1, 1, 1, 1):
                            with T.block("group_conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused // 14 * 136 + i0_1_i1_1_i2_1_i3_1_fused * 17 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i3_3)
                                rc = T.axis.reduce(68, i4_1 * 34 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(group_conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 2):
                        with T.block("group_conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused // 14 * 136 + i0_1_i1_1_i2_1_i3_1_fused * 17 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ax3)
                            T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0], placeholder_4[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0] + placeholder_4[v0, v1, v2, v3], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l19, l20, l21, l22, l23 = sch.split(loop=l7, factors=[v14, v15, v16, v17, v18])
v24, v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 8, 17, 1, 1])
l29, l30, l31, l32, l33 = sch.split(loop=l8, factors=[v24, v25, v26, v27, v28])
v34, v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l39, l40, l41, l42, l43 = sch.split(loop=l9, factors=[v34, v35, v36, v37, v38])
v44, v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l10, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l49, l50, l51, l52, l53 = sch.split(loop=l10, factors=[v44, v45, v46, v47, v48])
v54, v55, v56 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 2, 34])
l57, l58, l59 = sch.split(loop=l11, factors=[v54, v55, v56])
v60, v61, v62 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l63, l64, l65 = sch.split(loop=l12, factors=[v60, v61, v62])
v66, v67, v68 = sch.sample_perfect_tile(loop=l13, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l69, l70, l71 = sch.split(loop=l13, factors=[v66, v67, v68])
sch.reorder(l19, l29, l39, l49, l20, l30, l40, l50, l21, l31, l41, l51, l57, l63, l69, l58, l64, l70, l22, l32, l42, l52, l59, l65, l71, l23, l33, l43, l53)
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="vthread.x")
l74 = sch.fuse(l21, l31, l41, l51)
sch.bind(loop=l74, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b75 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b75, loop=l74, preserve_unit_loops=True)
b76 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b76, loop=l69, preserve_unit_loops=True)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b76)
l87 = sch.fuse(l83, l84, l85, l86)
v88 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch", ann_val=v88)
b89 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b89, loop=l69, preserve_unit_loops=True)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b89)
l100 = sch.fuse(l96, l97, l98, l99)
v101 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b76)
l110, l111, l112 = sch.split(loop=l109, factors=[None, 119, 4])
sch.vectorize(loop=l112)
sch.bind(loop=l111, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch")
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b89)
l120, l121, l122 = sch.split(loop=l119, factors=[None, 119, 4])
sch.vectorize(loop=l122)
sch.bind(loop=l121, thread_axis="threadIdx.x")
b123 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b123, ann_key="meta_schedule.unroll_explicit")
b124, b125, b126, b127 = sch.get_child_blocks(b123)
l128, l129, l130, l131, l132, l133, l134, l135, l136 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l128, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l128, ann_key="pragma_unroll_explicit", ann_val=1)
l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l137, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l137, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b126)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b127)
sch.annotate(block_or_loop=l166, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l166, ann_key="pragma_unroll_explicit", ann_val=1)
b173 = sch.get_block(name="group_conv2d_nchw", func_name="main")
l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191, l192, l193 = sch.get_loops(block=b173)
b194 = sch.decompose_reduction(block=b173, loop=l180)
[16:27:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |      1126.6153 |       6.6249 |               46.3741 |     32 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 608
Total latency (us): 217.119

[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #0: GFLOPs: 336.2648. Time: 0.0220 ms. Best GFLOPs: 336.2648
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #1: GFLOPs: 239.0218. Time: 0.0310 ms. Best GFLOPs: 336.2648
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #2: GFLOPs: 73.7303. Time: 0.1005 ms. Best GFLOPs: 336.2648
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #3: GFLOPs: 394.2008. Time: 0.0188 ms. Best GFLOPs: 394.2008
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #4: GFLOPs: 354.8489. Time: 0.0209 ms. Best GFLOPs: 394.2008
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #5: GFLOPs: 359.9351. Time: 0.0206 ms. Best GFLOPs: 394.2008
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #6: GFLOPs: 370.6460. Time: 0.0200 ms. Best GFLOPs: 394.2008
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #7: GFLOPs: 32.8200. Time: 0.2258 ms. Best GFLOPs: 394.2008
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #8: GFLOPs: 656.4130. Time: 0.0113 ms. Best GFLOPs: 656.4130
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #9: GFLOPs: 300.6110. Time: 0.0247 ms. Best GFLOPs: 656.4130
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #10: GFLOPs: 456.7368. Time: 0.0162 ms. Best GFLOPs: 656.4130
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #11: GFLOPs: 194.3939. Time: 0.0381 ms. Best GFLOPs: 656.4130
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #12: GFLOPs: 27.4988. Time: 0.2695 ms. Best GFLOPs: 656.4130
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #13: GFLOPs: 356.8317. Time: 0.0208 ms. Best GFLOPs: 656.4130
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #14: GFLOPs: 729.6828. Time: 0.0102 ms. Best GFLOPs: 729.6828
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #15: GFLOPs: 167.9073. Time: 0.0441 ms. Best GFLOPs: 729.6828
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #16: GFLOPs: 231.4514. Time: 0.0320 ms. Best GFLOPs: 729.6828
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #17: GFLOPs: 153.0556. Time: 0.0484 ms. Best GFLOPs: 729.6828
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #18: GFLOPs: 274.2550. Time: 0.0270 ms. Best GFLOPs: 729.6828
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #19: GFLOPs: 603.3369. Time: 0.0123 ms. Best GFLOPs: 729.6828
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #20: GFLOPs: 267.2766. Time: 0.0277 ms. Best GFLOPs: 729.6828
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #21: GFLOPs: 313.0449. Time: 0.0237 ms. Best GFLOPs: 729.6828
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #22: GFLOPs: 72.8992. Time: 0.1017 ms. Best GFLOPs: 729.6828
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #23: GFLOPs: 25.3627. Time: 0.2922 ms. Best GFLOPs: 729.6828
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #24: GFLOPs: 294.3845. Time: 0.0252 ms. Best GFLOPs: 729.6828
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #25: GFLOPs: 45.1148. Time: 0.1643 ms. Best GFLOPs: 729.6828
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #26: GFLOPs: 217.6482. Time: 0.0340 ms. Best GFLOPs: 729.6828
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #27: GFLOPs: 634.4449. Time: 0.0117 ms. Best GFLOPs: 729.6828
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #28: GFLOPs: 1311.6890. Time: 0.0056 ms. Best GFLOPs: 1311.6890
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #29: GFLOPs: 28.7446. Time: 0.2578 ms. Best GFLOPs: 1311.6890
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #30: GFLOPs: 317.5036. Time: 0.0233 ms. Best GFLOPs: 1311.6890
[16:27:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #31: GFLOPs: 55.9113. Time: 0.1325 ms. Best GFLOPs: 1311.6890
[16:27:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |      1126.6153 |       6.6249 |               46.3741 |     32 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |      1311.6890 |       5.6495 |               45.1959 |     32 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 640
Total latency (us): 262.315

[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #0: GFLOPs: 0.0000. Time: 0.0026 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #1: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #2: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #3: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #4: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #5: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #6: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #7: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #8: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #9: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #10: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #11: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #12: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #13: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #14: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #15: GFLOPs: 0.0000. Time: 0.0031 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #16: GFLOPs: 0.0000. Time: 0.0028 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #17: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #18: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #19: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #20: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #21: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #22: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #23: GFLOPs: 0.0000. Time: 0.0028 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #24: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #25: GFLOPs: 0.0000. Time: 0.0027 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #26: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #27: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #28: GFLOPs: 0.0000. Time: 0.0028 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #29: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #30: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:27:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_reshape_transpose_reshape_2"] Trial #31: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:27:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_reshape_transpose_reshape_2"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |      1126.6153 |       6.6249 |               46.3741 |     32 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |      1311.6890 |       5.6495 |               45.1959 |     32 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |         0.0004 |       2.2715 |               18.1718 |     32 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 672
Total latency (us): 280.487

[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #0: GFLOPs: 28.0758. Time: 0.0090 ms. Best GFLOPs: 28.0758
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #1: GFLOPs: 28.8254. Time: 0.0088 ms. Best GFLOPs: 28.8254
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #2: GFLOPs: 14.2402. Time: 0.0178 ms. Best GFLOPs: 28.8254
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #3: GFLOPs: 14.1305. Time: 0.0179 ms. Best GFLOPs: 28.8254
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #4: GFLOPs: 17.9399. Time: 0.0141 ms. Best GFLOPs: 28.8254
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #5: GFLOPs: 35.9134. Time: 0.0071 ms. Best GFLOPs: 35.9134
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #6: GFLOPs: 14.9086. Time: 0.0170 ms. Best GFLOPs: 35.9134
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #7: GFLOPs: 42.3933. Time: 0.0060 ms. Best GFLOPs: 42.3933
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #8: GFLOPs: 13.1480. Time: 0.0193 ms. Best GFLOPs: 42.3933
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #9: GFLOPs: 58.7552. Time: 0.0043 ms. Best GFLOPs: 58.7552
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #10: GFLOPs: 4.9899. Time: 0.0507 ms. Best GFLOPs: 58.7552
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #11: GFLOPs: 46.0827. Time: 0.0055 ms. Best GFLOPs: 58.7552
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #12: GFLOPs: 27.6788. Time: 0.0091 ms. Best GFLOPs: 58.7552
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #13: GFLOPs: 45.5364. Time: 0.0056 ms. Best GFLOPs: 58.7552
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #14: GFLOPs: 2.3336. Time: 0.1085 ms. Best GFLOPs: 58.7552
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #15: GFLOPs: 22.1063. Time: 0.0115 ms. Best GFLOPs: 58.7552
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #16: GFLOPs: 3.2626. Time: 0.0776 ms. Best GFLOPs: 58.7552
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #17: GFLOPs: 2.6162. Time: 0.0968 ms. Best GFLOPs: 58.7552
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #18: GFLOPs: 38.0684. Time: 0.0067 ms. Best GFLOPs: 58.7552
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #19: GFLOPs: 11.2168. Time: 0.0226 ms. Best GFLOPs: 58.7552
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #20: GFLOPs: 50.9411. Time: 0.0050 ms. Best GFLOPs: 58.7552
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #21: GFLOPs: 1.3013. Time: 0.1946 ms. Best GFLOPs: 58.7552
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #22: GFLOPs: 73.0954. Time: 0.0035 ms. Best GFLOPs: 73.0954
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #23: GFLOPs: 2.3134. Time: 0.1095 ms. Best GFLOPs: 73.0954
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #24: GFLOPs: 9.2347. Time: 0.0274 ms. Best GFLOPs: 73.0954
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #25: GFLOPs: 68.7216. Time: 0.0037 ms. Best GFLOPs: 73.0954
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #26: GFLOPs: 10.8183. Time: 0.0234 ms. Best GFLOPs: 73.0954
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #27: GFLOPs: 13.0553. Time: 0.0194 ms. Best GFLOPs: 73.0954
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #28: GFLOPs: 60.1344. Time: 0.0042 ms. Best GFLOPs: 73.0954
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #29: GFLOPs: 12.8000. Time: 0.0198 ms. Best GFLOPs: 73.0954
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #30: GFLOPs: 21.8841. Time: 0.0116 ms. Best GFLOPs: 73.0954
[16:27:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_4"] Trial #31: GFLOPs: 23.9309. Time: 0.0106 ms. Best GFLOPs: 73.0954
[16:27:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_nn_conv2d_add_4"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |      1126.6153 |       6.6249 |               46.3741 |     32 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |      1311.6890 |       5.6495 |               45.1959 |     32 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |         0.0004 |       2.2715 |               18.1718 |     32 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |        73.0954 |       3.4644 |                3.4644 |     32 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 704
Total latency (us): 283.951

[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #0: GFLOPs: 454.8653. Time: 0.0040 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #1: GFLOPs: 22.6525. Time: 0.0812 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #2: GFLOPs: 134.9086. Time: 0.0136 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #3: GFLOPs: 127.2584. Time: 0.0145 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #4: GFLOPs: 148.0420. Time: 0.0124 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #5: GFLOPs: 333.7658. Time: 0.0055 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #6: GFLOPs: 50.7022. Time: 0.0363 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #7: GFLOPs: 294.4197. Time: 0.0062 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #8: GFLOPs: 233.5887. Time: 0.0079 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #9: GFLOPs: 15.8114. Time: 0.1163 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #10: GFLOPs: 4.4037. Time: 0.4177 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #11: GFLOPs: 98.2627. Time: 0.0187 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #12: GFLOPs: 103.7851. Time: 0.0177 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #13: GFLOPs: 6.5862. Time: 0.2793 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #14: GFLOPs: 57.7305. Time: 0.0319 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #15: GFLOPs: 374.1400. Time: 0.0049 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #16: GFLOPs: 55.1078. Time: 0.0334 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #17: GFLOPs: 27.4671. Time: 0.0670 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #18: GFLOPs: 6.9004. Time: 0.2665 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #19: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        group_conv2d_nchw_local = T.alloc_buffer([1, 272, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 272, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([272, 68, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(34, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 7):
                        with T.block("group_conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused // 7 * 136 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(7, i2_4_init)
                            xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            T.reads()
                            T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 7, 7], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                            group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(34, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused // 7 * 136 + i4_0 * 34 + ((ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 714 // 7)
                                        v2 = T.axis.spatial(7, ((ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 714)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(34):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(34, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused // 7 * 136 + (ax0_ax1_ax2_ax3_fused_0 * 136 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 34)
                                        v1 = T.axis.spatial(68, i4_0 * 34 + (ax0_ax1_ax2_ax3_fused_0 * 136 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 34)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(17, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 7, 1):
                            with T.block("group_conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused // 7 * 136 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(7, i2_4)
                                xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                rc = T.axis.reduce(68, i4_0 * 34 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(group_conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 7, 7], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 1):
                        with T.block("group_conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused // 7 * 136 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(7, ax2)
                            v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                            T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 34, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 17, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 34, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 34, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="group_conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #20: GFLOPs: 17.3622. Time: 0.1059 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #21: GFLOPs: 13.1790. Time: 0.1396 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #22: GFLOPs: 154.0871. Time: 0.0119 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #23: GFLOPs: 41.8254. Time: 0.0440 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #24: GFLOPs: 170.1594. Time: 0.0108 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #25: GFLOPs: 126.2860. Time: 0.0146 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #26: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        group_conv2d_nchw_local = T.alloc_buffer([1, 272, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 272, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([272, 68, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(49, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(34, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 2):
                        with T.block("group_conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused * 136 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused // 7)
                            xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                            T.reads()
                            T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 7, 7], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                            group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(68, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(100):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(34, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused * 136 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 3381 // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1 < 3381)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(34, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused * 136 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(68, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("group_conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused * 136 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused // 7)
                                xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                rc = T.axis.reduce(68, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(group_conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 7, 7], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("group_conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused * 136 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused // 7 + ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                            T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 34, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[68, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 34])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 34, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="group_conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #27: GFLOPs: 19.4056. Time: 0.0948 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #28: GFLOPs: 4.1370. Time: 0.4446 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #29: GFLOPs: 68.5663. Time: 0.0268 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #30: GFLOPs: 322.3185. Time: 0.0057 ms. Best GFLOPs: 454.8653
[16:27:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_conv2d_multiply_add_2"] Trial #31: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        group_conv2d_nchw_local = T.alloc_buffer([1, 272, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 272, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([272, 68, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(34, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(2, 7):
                        with T.block("group_conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused * 68 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy, xx = T.axis.remap("SS", [i0_1_i1_1_i2_1_i3_1_fused, i3_3_init])
                            T.reads()
                            T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 7, 7], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                            group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(34, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused * 68 + i4_0 * 34 + (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(17):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(34, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused * 68 + (ax0_ax1_ax2_ax3_fused_0 * 136 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 34)
                                        v1 = T.axis.spatial(68, i4_0 * 34 + (ax0_ax1_ax2_ax3_fused_0 * 136 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 34)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 7, 34, 1, 1, 1, 1, 1, 1):
                            with T.block("group_conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused * 68 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy, xx = T.axis.remap("SS", [i0_1_i1_1_i2_1_i3_1_fused, i3_3])
                                rc = T.axis.reduce(68, i4_0 * 34 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(group_conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 272, 7, 7], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 68 * 68 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 7):
                        with T.block("group_conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused * 68 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 34, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 1, 34])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 34])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 34, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="group_conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[16:28:01] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_nn_conv2d_multiply_add_2"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |      1126.6153 |       6.6249 |               46.3741 |     32 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |      1311.6890 |       5.6495 |               45.1959 |     32 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |         0.0004 |       2.2715 |               18.1718 |     32 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |        73.0954 |       3.4644 |                3.4644 |     32 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |       454.8653 |       4.0435 |                4.0435 |     32 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 736
Total latency (us): 287.995

[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #0: GFLOPs: 12.1485. Time: 0.0022 ms. Best GFLOPs: 12.1485
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #1: GFLOPs: 12.1445. Time: 0.0022 ms. Best GFLOPs: 12.1485
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #2: GFLOPs: 12.2363. Time: 0.0022 ms. Best GFLOPs: 12.2363
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #3: GFLOPs: 12.1374. Time: 0.0022 ms. Best GFLOPs: 12.2363
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #4: GFLOPs: 12.2300. Time: 0.0022 ms. Best GFLOPs: 12.2363
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #5: GFLOPs: 12.1880. Time: 0.0022 ms. Best GFLOPs: 12.2363
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #6: GFLOPs: 12.2592. Time: 0.0022 ms. Best GFLOPs: 12.2592
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #7: GFLOPs: 12.2665. Time: 0.0022 ms. Best GFLOPs: 12.2665
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #8: GFLOPs: 11.7647. Time: 0.0023 ms. Best GFLOPs: 12.2665
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #9: GFLOPs: 12.1444. Time: 0.0022 ms. Best GFLOPs: 12.2665
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #10: GFLOPs: 12.1108. Time: 0.0022 ms. Best GFLOPs: 12.2665
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #11: GFLOPs: 12.0712. Time: 0.0022 ms. Best GFLOPs: 12.2665
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #12: GFLOPs: 12.1825. Time: 0.0022 ms. Best GFLOPs: 12.2665
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #13: GFLOPs: 12.0900. Time: 0.0022 ms. Best GFLOPs: 12.2665
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #14: GFLOPs: 11.8931. Time: 0.0022 ms. Best GFLOPs: 12.2665
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #15: GFLOPs: 12.1079. Time: 0.0022 ms. Best GFLOPs: 12.2665
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #16: GFLOPs: 12.1619. Time: 0.0022 ms. Best GFLOPs: 12.2665
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #17: GFLOPs: 12.2615. Time: 0.0022 ms. Best GFLOPs: 12.2665
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #18: GFLOPs: 12.1215. Time: 0.0022 ms. Best GFLOPs: 12.2665
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #19: GFLOPs: 12.2534. Time: 0.0022 ms. Best GFLOPs: 12.2665
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #20: GFLOPs: 12.2683. Time: 0.0022 ms. Best GFLOPs: 12.2683
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #21: GFLOPs: 12.2237. Time: 0.0022 ms. Best GFLOPs: 12.2683
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #22: GFLOPs: 12.1602. Time: 0.0022 ms. Best GFLOPs: 12.2683
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #23: GFLOPs: 12.2906. Time: 0.0022 ms. Best GFLOPs: 12.2906
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #24: GFLOPs: 11.7751. Time: 0.0023 ms. Best GFLOPs: 12.2906
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #25: GFLOPs: 11.5382. Time: 0.0023 ms. Best GFLOPs: 12.2906
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #26: GFLOPs: 12.1717. Time: 0.0022 ms. Best GFLOPs: 12.2906
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #27: GFLOPs: 11.4646. Time: 0.0023 ms. Best GFLOPs: 12.2906
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #28: GFLOPs: 12.1529. Time: 0.0022 ms. Best GFLOPs: 12.2906
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #29: GFLOPs: 12.2196. Time: 0.0022 ms. Best GFLOPs: 12.2906
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #30: GFLOPs: 11.6228. Time: 0.0023 ms. Best GFLOPs: 12.2906
[16:28:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_concatenate_nn_relu_2"] Trial #31: GFLOPs: 11.6076. Time: 0.0023 ms. Best GFLOPs: 12.2906
[16:28:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_concatenate_nn_relu_2"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |      1126.6153 |       6.6249 |               46.3741 |     32 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |      1311.6890 |       5.6495 |               45.1959 |     32 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |         0.0004 |       2.2715 |               18.1718 |     32 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |        73.0954 |       3.4644 |                3.4644 |     32 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |       454.8653 |       4.0435 |                4.0435 |     32 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |        12.2906 |       2.1688 |                2.1688 |     32 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 768
Total latency (us): 290.164

[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #0: GFLOPs: 87.6395. Time: 0.0836 ms. Best GFLOPs: 87.6395
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #1: GFLOPs: 84.5338. Time: 0.0867 ms. Best GFLOPs: 87.6395
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #2: GFLOPs: 154.8093. Time: 0.0474 ms. Best GFLOPs: 154.8093
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #3: GFLOPs: 72.7369. Time: 0.1008 ms. Best GFLOPs: 154.8093
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #4: GFLOPs: 307.2290. Time: 0.0239 ms. Best GFLOPs: 307.2290
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #5: GFLOPs: 92.4905. Time: 0.0793 ms. Best GFLOPs: 307.2290
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #6: GFLOPs: 59.0193. Time: 0.1242 ms. Best GFLOPs: 307.2290
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #7: GFLOPs: 924.8735. Time: 0.0079 ms. Best GFLOPs: 924.8735
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #8: GFLOPs: 1351.2928. Time: 0.0054 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #9: GFLOPs: 217.2889. Time: 0.0337 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #10: GFLOPs: 59.3113. Time: 0.1236 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #11: GFLOPs: 163.7102. Time: 0.0448 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #12: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        group_conv2d_nchw_local = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([544, 136, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(34, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(4, 2, 7):
                        with T.block("group_conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(544, i0_1_i1_1_i2_1_i3_1_fused * 272 + i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3_init * 2 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i0_0_i1_0_i2_0_i3_0_fused, i3_4_init])
                            T.reads()
                            T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                            group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(68, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(85):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(34, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(544, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 2870 // 7)
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused + 0)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1 < 2870)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(34, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(544, (ax0_ax1_ax2_ax3_fused_0 * 136 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(136, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 136 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 1, 7):
                            with T.block("group_conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(544, i0_1_i1_1_i2_1_i3_1_fused * 272 + i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3 * 2 + i1_4)
                                yy, xx = T.axis.remap("SS", [i0_0_i1_0_i2_0_i3_0_fused, i3_4])
                                rc = T.axis.reduce(136, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(group_conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, ff // 136 * 136 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 136 * 136 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 7):
                        with T.block("group_conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(544, i0_1_i1_1_i2_1_i3_1_fused * 272 + i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 34, 4, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[68, 2, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 34])
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 34, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="group_conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #13: GFLOPs: 640.1689. Time: 0.0115 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #14: GFLOPs: 929.1858. Time: 0.0079 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #15: GFLOPs: 682.9875. Time: 0.0107 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #16: GFLOPs: 48.2476. Time: 0.1519 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #17: GFLOPs: 882.8051. Time: 0.0083 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #18: GFLOPs: 556.0470. Time: 0.0132 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #19: GFLOPs: 472.2354. Time: 0.0155 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #20: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        group_conv2d_nchw_local = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([544, 136, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(34, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i3_4_init in T.grid(2, 7, 2, 7):
                        with T.block("group_conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i0_1_i1_1_i2_1_i3_1_fused * 136 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init * 2 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i2_3_init, i3_4_init])
                            T.reads()
                            T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                            group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(68, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(199):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(34, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 6762 // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1 < 6762)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(34, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + (ax0_ax1_ax2_ax3_fused_0 * 136 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(136, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 136 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 7, 1, 1, 1, 1, 1, 2, 1, 7):
                            with T.block("group_conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i0_1_i1_1_i2_1_i3_1_fused * 136 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3 * 2 + i1_4)
                                yy, xx = T.axis.remap("SS", [i2_3, i3_4])
                                rc = T.axis.reduce(136, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(group_conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, ff // 136 * 136 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 136 * 136 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 7):
                        with T.block("group_conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i0_1_i1_1_i2_1_i3_1_fused * 136 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_relu", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16, v17 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l18, l19, l20, l21, l22 = sch.split(loop=l6, factors=[v13, v14, v15, v16, v17])
v23, v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 34, 2, 2])
l28, l29, l30, l31, l32 = sch.split(loop=l7, factors=[v23, v24, v25, v26, v27])
v33, v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l38, l39, l40, l41, l42 = sch.split(loop=l8, factors=[v33, v34, v35, v36, v37])
v43, v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l48, l49, l50, l51, l52 = sch.split(loop=l9, factors=[v43, v44, v45, v46, v47])
v53, v54, v55 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[68, 2, 1])
l56, l57, l58 = sch.split(loop=l10, factors=[v53, v54, v55])
v59, v60, v61 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l62, l63, l64 = sch.split(loop=l11, factors=[v59, v60, v61])
v65, v66, v67 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l68, l69, l70 = sch.split(loop=l12, factors=[v65, v66, v67])
sch.reorder(l18, l28, l38, l48, l19, l29, l39, l49, l20, l30, l40, l50, l56, l62, l68, l57, l63, l69, l21, l31, l41, l51, l58, l64, l70, l22, l32, l42, l52)
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="blockIdx.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="vthread.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b74 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b74, loop=l73, preserve_unit_loops=True)
b75 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b75, loop=l68, preserve_unit_loops=True)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85 = sch.get_loops(block=b75)
l86 = sch.fuse(l82, l83, l84, l85)
v87 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch", ann_val=v87)
b88 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b88, loop=l68, preserve_unit_loops=True)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b88)
l99 = sch.fuse(l95, l96, l97, l98)
v100 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch", ann_val=v100)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v101 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v101)
sch.enter_postproc()
sch.unannotate(block_or_loop=b75, ann_key="meta_schedule.cooperative_fetch")
l102, l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b75)
l109, l110 = sch.split(loop=l108, factors=[None, 34])
sch.bind(loop=l110, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b88, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b88)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 34, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="group_conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #21: GFLOPs: 219.1821. Time: 0.0334 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #22: GFLOPs: 265.6312. Time: 0.0276 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #23: GFLOPs: 9.6737. Time: 0.7578 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #24: GFLOPs: 22.1411. Time: 0.3311 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #25: GFLOPs: 12.6485. Time: 0.5795 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #26: GFLOPs: 145.2793. Time: 0.0505 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #27: GFLOPs: 58.2650. Time: 0.1258 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #28: GFLOPs: 51.7572. Time: 0.1416 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #29: GFLOPs: 421.2887. Time: 0.0174 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #30: GFLOPs: 48.8168. Time: 0.1502 ms. Best GFLOPs: 1351.2928
[16:28:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #31: GFLOPs: 35.0533. Time: 0.2091 ms. Best GFLOPs: 1351.2928
[16:28:09] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |      1126.6153 |       6.6249 |               46.3741 |     32 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |      1311.6890 |       5.6495 |               45.1959 |     32 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |         0.0004 |       2.2715 |               18.1718 |     32 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |        73.0954 |       3.4644 |                3.4644 |     32 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |       454.8653 |       4.0435 |                4.0435 |     32 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |        12.2906 |       2.1688 |                2.1688 |     32 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |      1351.2928 |       5.4247 |               16.2742 |     32 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 800
Total latency (us): 306.438

[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #0: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #1: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #2: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #3: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #4: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #5: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #6: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #7: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #8: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #9: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #10: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #11: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #12: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #13: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #14: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #15: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #16: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #17: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #18: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #19: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #20: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #21: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #22: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #23: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #24: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #25: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #26: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #27: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #28: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #29: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #30: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_reshape_transpose_reshape_3"] Trial #31: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:11] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_reshape_transpose_reshape_3"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |      1126.6153 |       6.6249 |               46.3741 |     32 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |      1311.6890 |       5.6495 |               45.1959 |     32 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |         0.0004 |       2.2715 |               18.1718 |     32 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |        73.0954 |       3.4644 |                3.4644 |     32 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |       454.8653 |       4.0435 |                4.0435 |     32 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |        12.2906 |       2.1688 |                2.1688 |     32 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |      1351.2928 |       5.4247 |               16.2742 |     32 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |         0.0005 |       2.2001 |                6.6004 |     32 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 832
Total latency (us): 313.038

[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #0: GFLOPs: 4.7126. Time: 0.1075 ms. Best GFLOPs: 4.7126
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #1: GFLOPs: 94.8868. Time: 0.0053 ms. Best GFLOPs: 94.8868
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #2: GFLOPs: 40.5442. Time: 0.0125 ms. Best GFLOPs: 94.8868
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #26: "fused_nn_conv2d_add_5"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], T_add: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 544, 9, 9], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([544, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(68, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(75):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 136 + (ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 81)
                                        v2 = T.axis.spatial(9, (ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 81 // 9)
                                        v3 = T.axis.spatial(9, (ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 11016)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 136 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1224)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 2, 1, 1):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 136 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i1_3)
                                i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused // 7)
                                j = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 136 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i1_3)
                                    i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    j = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 136 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused // 7 + ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l4, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 68, 1, 2, 1])
l25, l26, l27, l28, l29 = sch.split(loop=l5, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l6, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l7, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l53, l54, l55 = sch.split(loop=l8, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l59, l60, l61 = sch.split(loop=l9, factors=[v56, v57, v58])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l54, l60, l18, l28, l38, l48, l55, l61, l19, l29, l39, l49)
l62 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l62, thread_axis="blockIdx.x")
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="vthread.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b65 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b65, loop=l64, preserve_unit_loops=True)
b66 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b66, loop=l59, preserve_unit_loops=True)
l67, l68, l69, l70, l71, l72, l73, l74, l75 = sch.get_loops(block=b66)
l76 = sch.fuse(l72, l73, l74, l75)
v77 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b66, ann_key="meta_schedule.cooperative_fetch", ann_val=v77)
b78 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b78, loop=l59, preserve_unit_loops=True)
l79, l80, l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b78)
l88 = sch.fuse(l84, l85, l86, l87)
v89 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b78, ann_key="meta_schedule.cooperative_fetch", ann_val=v89)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v90 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v90)
sch.enter_postproc()
sch.unannotate(block_or_loop=b66, ann_key="meta_schedule.cooperative_fetch")
l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b66)
l97, l98, l99 = sch.split(loop=l96, factors=[None, 49, 3])
sch.vectorize(loop=l99)
sch.bind(loop=l98, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b78, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b78)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 49, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l130, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l130, ann_key="pragma_unroll_explicit", ann_val=1)
l147, l148, l149, l150, l151, l152, l153 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l147, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l147, ann_key="pragma_unroll_explicit", ann_val=1)
b154 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171 = sch.get_loops(block=b154)
b172 = sch.decompose_reduction(block=b154, loop=l166)
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #4: GFLOPs: 99.9448. Time: 0.0051 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #5: GFLOPs: 18.4769. Time: 0.0274 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #6: GFLOPs: 50.0208. Time: 0.0101 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #7: GFLOPs: 69.6500. Time: 0.0073 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #8: GFLOPs: 11.6115. Time: 0.0436 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #9: GFLOPs: 91.9077. Time: 0.0055 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #10: GFLOPs: 22.2061. Time: 0.0228 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #11: GFLOPs: 63.1935. Time: 0.0080 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #12: GFLOPs: 60.1786. Time: 0.0084 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #13: GFLOPs: 43.2264. Time: 0.0117 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #14: GFLOPs: 57.9302. Time: 0.0087 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #15: GFLOPs: 47.7474. Time: 0.0106 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #16: GFLOPs: 49.3202. Time: 0.0103 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #17: GFLOPs: 17.6643. Time: 0.0287 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #18: GFLOPs: 17.7929. Time: 0.0285 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #19: GFLOPs: 51.1694. Time: 0.0099 ms. Best GFLOPs: 99.9448
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #20: GFLOPs: 120.7441. Time: 0.0042 ms. Best GFLOPs: 120.7441
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #21: GFLOPs: 28.3098. Time: 0.0179 ms. Best GFLOPs: 120.7441
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #22: GFLOPs: 48.2830. Time: 0.0105 ms. Best GFLOPs: 120.7441
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #23: GFLOPs: 82.4330. Time: 0.0061 ms. Best GFLOPs: 120.7441
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #24: GFLOPs: 84.4530. Time: 0.0060 ms. Best GFLOPs: 120.7441
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #25: GFLOPs: 26.6996. Time: 0.0190 ms. Best GFLOPs: 120.7441
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #26: GFLOPs: 62.9608. Time: 0.0080 ms. Best GFLOPs: 120.7441
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #27: GFLOPs: 81.5401. Time: 0.0062 ms. Best GFLOPs: 120.7441
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #28: GFLOPs: 63.2458. Time: 0.0080 ms. Best GFLOPs: 120.7441
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #29: GFLOPs: 82.3157. Time: 0.0062 ms. Best GFLOPs: 120.7441
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #30: GFLOPs: 9.0923. Time: 0.0557 ms. Best GFLOPs: 120.7441
[16:28:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_5"] Trial #31: GFLOPs: 34.7664. Time: 0.0146 ms. Best GFLOPs: 120.7441
[16:28:14] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_nn_conv2d_add_5"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |      1126.6153 |       6.6249 |               46.3741 |     32 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |      1311.6890 |       5.6495 |               45.1959 |     32 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |         0.0004 |       2.2715 |               18.1718 |     32 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |        73.0954 |       3.4644 |                3.4644 |     32 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |       454.8653 |       4.0435 |                4.0435 |     32 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |        12.2906 |       2.1688 |                2.1688 |     32 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |      1351.2928 |       5.4247 |               16.2742 |     32 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |         0.0005 |       2.2001 |                6.6004 |     32 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |       120.7441 |       4.1945 |               12.5836 |     32 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 864
Total latency (us): 325.622

[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #0: GFLOPs: 33.0411. Time: 0.2227 ms. Best GFLOPs: 33.0411
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #1: GFLOPs: 113.1655. Time: 0.0650 ms. Best GFLOPs: 113.1655
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #2: GFLOPs: 63.8240. Time: 0.1153 ms. Best GFLOPs: 113.1655
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #3: GFLOPs: 338.8569. Time: 0.0217 ms. Best GFLOPs: 338.8569
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #4: GFLOPs: 104.2753. Time: 0.0706 ms. Best GFLOPs: 338.8569
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #5: GFLOPs: 142.5179. Time: 0.0516 ms. Best GFLOPs: 338.8569
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #6: GFLOPs: 148.1547. Time: 0.0497 ms. Best GFLOPs: 338.8569
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #7: GFLOPs: 653.3104. Time: 0.0113 ms. Best GFLOPs: 653.3104
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #8: GFLOPs: 207.0188. Time: 0.0355 ms. Best GFLOPs: 653.3104
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #9: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 544, 7, 7), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        group_conv2d_nchw_local = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([544, 136, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(68, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(4, 7):
                        with T.block("group_conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(544, i0_1_i1_1_i2_1_i3_1_fused * 272 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i0_0_i1_0_i2_0_i3_0_fused, i3_4_init])
                            T.reads()
                            T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                            group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(44):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(68, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(544, i4_0 * 17 + (ax0_ax1_ax2_ax3_fused_0 * 68 + ax0_ax1_ax2_ax3_fused_1) % 2975 // 7)
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused + 0)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 68 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 68 + ax0_ax1_ax2_ax3_fused_1 < 2975)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(34):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(68, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(544, (ax0_ax1_ax2_ax3_fused_0 * 272 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 17)
                                        v1 = T.axis.spatial(136, i4_0 * 17 + (ax0_ax1_ax2_ax3_fused_0 * 272 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 17)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 7):
                            with T.block("group_conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(544, i0_1_i1_1_i2_1_i3_1_fused * 272 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_4)
                                yy, xx = T.axis.remap("SS", [i0_0_i1_0_i2_0_i3_0_fused, i3_4])
                                rc = T.axis.reduce(136, i4_0 * 17 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(group_conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, ff // 136 * 136 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 136 * 136 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 7):
                        with T.block("group_conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(544, i0_1_i1_1_i2_1_i3_1_fused * 272 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0], placeholder_4[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0] + placeholder_4[v0, v1, v2, v3], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l19, l20, l21, l22, l23 = sch.split(loop=l7, factors=[v14, v15, v16, v17, v18])
v24, v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 68, 1, 4])
l29, l30, l31, l32, l33 = sch.split(loop=l8, factors=[v24, v25, v26, v27, v28])
v34, v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l39, l40, l41, l42, l43 = sch.split(loop=l9, factors=[v34, v35, v36, v37, v38])
v44, v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l10, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l49, l50, l51, l52, l53 = sch.split(loop=l10, factors=[v44, v45, v46, v47, v48])
v54, v55, v56 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[8, 17, 1])
l57, l58, l59 = sch.split(loop=l11, factors=[v54, v55, v56])
v60, v61, v62 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l63, l64, l65 = sch.split(loop=l12, factors=[v60, v61, v62])
v66, v67, v68 = sch.sample_perfect_tile(loop=l13, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l69, l70, l71 = sch.split(loop=l13, factors=[v66, v67, v68])
sch.reorder(l19, l29, l39, l49, l20, l30, l40, l50, l21, l31, l41, l51, l57, l63, l69, l58, l64, l70, l22, l32, l42, l52, l59, l65, l71, l23, l33, l43, l53)
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="vthread.x")
l74 = sch.fuse(l21, l31, l41, l51)
sch.bind(loop=l74, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b75 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b75, loop=l74, preserve_unit_loops=True)
b76 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b76, loop=l69, preserve_unit_loops=True)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b76)
l87 = sch.fuse(l83, l84, l85, l86)
v88 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch", ann_val=v88)
b89 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b89, loop=l69, preserve_unit_loops=True)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b89)
l100 = sch.fuse(l96, l97, l98, l99)
v101 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b76)
l110, l111 = sch.split(loop=l109, factors=[None, 68])
sch.bind(loop=l111, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b89)
l119, l120, l121 = sch.split(loop=l118, factors=[None, 68, 4])
sch.vectorize(loop=l121)
sch.bind(loop=l120, thread_axis="threadIdx.x")
b122 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b122, ann_key="meta_schedule.unroll_explicit")
b123, b124, b125, b126 = sch.get_child_blocks(b122)
l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b126)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="group_conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #10: GFLOPs: 293.9765. Time: 0.0250 ms. Best GFLOPs: 653.3104
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #11: GFLOPs: 571.9445. Time: 0.0129 ms. Best GFLOPs: 653.3104
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #12: GFLOPs: 181.1461. Time: 0.0406 ms. Best GFLOPs: 653.3104
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #13: GFLOPs: 207.0625. Time: 0.0355 ms. Best GFLOPs: 653.3104
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #14: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 544, 7, 7), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        group_conv2d_nchw_local = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([544, 136, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(68, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 7):
                        with T.block("group_conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init * 2 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i0_1_i1_1_i2_1_i3_1_fused, i3_4_init])
                            T.reads()
                            T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                            group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(111):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(68, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i4_0 * 17 + (ax0_ax1_ax2_ax3_fused_0 * 68 + ax0_ax1_ax2_ax3_fused_1) % 7497 // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 68 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 68 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 68 + ax0_ax1_ax2_ax3_fused_1 < 7497)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(17):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(68, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + (ax0_ax1_ax2_ax3_fused_0 * 272 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 17)
                                        v1 = T.axis.spatial(136, i4_0 * 17 + (ax0_ax1_ax2_ax3_fused_0 * 272 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 17)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 17, 1, 1, 1, 2, 1, 7):
                            with T.block("group_conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3 * 2 + i1_4)
                                yy, xx = T.axis.remap("SS", [i0_1_i1_1_i2_1_i3_1_fused, i3_4])
                                rc = T.axis.reduce(136, i4_0 * 17 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(group_conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, ff // 136 * 136 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 136 * 136 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 7):
                        with T.block("group_conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0], placeholder_4[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0] + placeholder_4[v0, v1, v2, v3], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l19, l20, l21, l22, l23 = sch.split(loop=l7, factors=[v14, v15, v16, v17, v18])
v24, v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 68, 2, 2])
l29, l30, l31, l32, l33 = sch.split(loop=l8, factors=[v24, v25, v26, v27, v28])
v34, v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l39, l40, l41, l42, l43 = sch.split(loop=l9, factors=[v34, v35, v36, v37, v38])
v44, v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l10, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l49, l50, l51, l52, l53 = sch.split(loop=l10, factors=[v44, v45, v46, v47, v48])
v54, v55, v56 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[8, 1, 17])
l57, l58, l59 = sch.split(loop=l11, factors=[v54, v55, v56])
v60, v61, v62 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l63, l64, l65 = sch.split(loop=l12, factors=[v60, v61, v62])
v66, v67, v68 = sch.sample_perfect_tile(loop=l13, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l69, l70, l71 = sch.split(loop=l13, factors=[v66, v67, v68])
sch.reorder(l19, l29, l39, l49, l20, l30, l40, l50, l21, l31, l41, l51, l57, l63, l69, l58, l64, l70, l22, l32, l42, l52, l59, l65, l71, l23, l33, l43, l53)
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="vthread.x")
l74 = sch.fuse(l21, l31, l41, l51)
sch.bind(loop=l74, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b75 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b75, loop=l74, preserve_unit_loops=True)
b76 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b76, loop=l69, preserve_unit_loops=True)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b76)
l87 = sch.fuse(l83, l84, l85, l86)
v88 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch", ann_val=v88)
b89 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b89, loop=l69, preserve_unit_loops=True)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b89)
l100 = sch.fuse(l96, l97, l98, l99)
v101 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b76)
l110, l111 = sch.split(loop=l109, factors=[None, 68])
sch.bind(loop=l111, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b89)
l119, l120, l121 = sch.split(loop=l118, factors=[None, 68, 4])
sch.vectorize(loop=l121)
sch.bind(loop=l120, thread_axis="threadIdx.x")
b122 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b122, ann_key="meta_schedule.unroll_explicit")
b123, b124, b125, b126 = sch.get_child_blocks(b122)
l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b126)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="group_conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #15: GFLOPs: 709.0937. Time: 0.0104 ms. Best GFLOPs: 709.0937
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #16: GFLOPs: 108.9155. Time: 0.0675 ms. Best GFLOPs: 709.0937
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #17: GFLOPs: 144.0603. Time: 0.0511 ms. Best GFLOPs: 709.0937
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #18: GFLOPs: 617.8987. Time: 0.0119 ms. Best GFLOPs: 709.0937
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #19: GFLOPs: 32.9261. Time: 0.2234 ms. Best GFLOPs: 709.0937
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #20: GFLOPs: 233.5816. Time: 0.0315 ms. Best GFLOPs: 709.0937
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #21: GFLOPs: 161.1900. Time: 0.0456 ms. Best GFLOPs: 709.0937
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #22: GFLOPs: 79.0911. Time: 0.0930 ms. Best GFLOPs: 709.0937
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #23: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 544, 7, 7), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        group_conv2d_nchw_local = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 544, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([544, 136, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(34, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init in T.grid(2, 7, 7):
                        with T.block("group_conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i0_1_i1_1_i2_1_i3_1_fused * 68 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy, xx = T.axis.remap("SS", [i2_3_init, i3_3_init])
                            T.reads()
                            T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                            group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(221):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(34, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i4_0 * 17 + (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 7497 // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 34 + ax0_ax1_ax2_ax3_fused_1 < 7497)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(68):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(34, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + (ax0_ax1_ax2_ax3_fused_0 * 68 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 17)
                                        v1 = T.axis.spatial(136, i4_0 * 17 + (ax0_ax1_ax2_ax3_fused_0 * 68 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 17)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 7, 7, 17, 1, 1, 1, 1, 1, 1):
                            with T.block("group_conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i0_1_i1_1_i2_1_i3_1_fused * 68 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy, xx = T.axis.remap("SS", [i2_3, i3_3])
                                rc = T.axis.reduce(136, i4_0 * 17 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(group_conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, ff // 136 * 136 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                                group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 136 * 136 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 7):
                        with T.block("group_conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(544, i0_0_i1_0_i2_0_i3_0_fused * 272 + i0_1_i1_1_i2_1_i3_1_fused * 68 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, 0, 0], placeholder_4[v0, v1, v2, v3])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] * placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, 0, 0] + placeholder_4[v0, v1, v2, v3], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_multiply", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="T_relu", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l7, l8, l9, l10, l11, l12, l13 = sch.get_loops(block=b1)
v14, v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l19, l20, l21, l22, l23 = sch.split(loop=l7, factors=[v14, v15, v16, v17, v18])
v24, v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 4, 34, 2, 1])
l29, l30, l31, l32, l33 = sch.split(loop=l8, factors=[v24, v25, v26, v27, v28])
v34, v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l9, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l39, l40, l41, l42, l43 = sch.split(loop=l9, factors=[v34, v35, v36, v37, v38])
v44, v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l10, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l49, l50, l51, l52, l53 = sch.split(loop=l10, factors=[v44, v45, v46, v47, v48])
v54, v55, v56 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[8, 1, 17])
l57, l58, l59 = sch.split(loop=l11, factors=[v54, v55, v56])
v60, v61, v62 = sch.sample_perfect_tile(loop=l12, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l63, l64, l65 = sch.split(loop=l12, factors=[v60, v61, v62])
v66, v67, v68 = sch.sample_perfect_tile(loop=l13, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l69, l70, l71 = sch.split(loop=l13, factors=[v66, v67, v68])
sch.reorder(l19, l29, l39, l49, l20, l30, l40, l50, l21, l31, l41, l51, l57, l63, l69, l58, l64, l70, l22, l32, l42, l52, l59, l65, l71, l23, l33, l43, l53)
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="blockIdx.x")
l73 = sch.fuse(l20, l30, l40, l50)
sch.bind(loop=l73, thread_axis="vthread.x")
l74 = sch.fuse(l21, l31, l41, l51)
sch.bind(loop=l74, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b75 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b75, loop=l74, preserve_unit_loops=True)
b76 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b76, loop=l69, preserve_unit_loops=True)
l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b76)
l87 = sch.fuse(l83, l84, l85, l86)
v88 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch", ann_val=v88)
b89 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b89, loop=l69, preserve_unit_loops=True)
l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b89)
l100 = sch.fuse(l96, l97, l98, l99)
v101 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch", ann_val=v101)
sch.reverse_compute_inline(block=b5)
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v102 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v102)
sch.enter_postproc()
sch.unannotate(block_or_loop=b76, ann_key="meta_schedule.cooperative_fetch")
l103, l104, l105, l106, l107, l108, l109 = sch.get_loops(block=b76)
l110, l111 = sch.split(loop=l109, factors=[None, 34])
sch.bind(loop=l111, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b89, ann_key="meta_schedule.cooperative_fetch")
l112, l113, l114, l115, l116, l117, l118 = sch.get_loops(block=b89)
l119, l120, l121 = sch.split(loop=l118, factors=[None, 34, 2])
sch.vectorize(loop=l121)
sch.bind(loop=l120, thread_axis="threadIdx.x")
b122 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b122, ann_key="meta_schedule.unroll_explicit")
b123, b124, b125, b126 = sch.get_child_blocks(b122)
l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b126)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="group_conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #24: GFLOPs: 50.0465. Time: 0.1470 ms. Best GFLOPs: 709.0937
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #25: GFLOPs: 957.4466. Time: 0.0077 ms. Best GFLOPs: 957.4466
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #26: GFLOPs: 29.0720. Time: 0.2531 ms. Best GFLOPs: 957.4466
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #27: GFLOPs: 132.5838. Time: 0.0555 ms. Best GFLOPs: 957.4466
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #28: GFLOPs: 209.1595. Time: 0.0352 ms. Best GFLOPs: 957.4466
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #29: GFLOPs: 103.4685. Time: 0.0711 ms. Best GFLOPs: 957.4466
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #30: GFLOPs: 75.0053. Time: 0.0981 ms. Best GFLOPs: 957.4466
[16:28:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #31: GFLOPs: 204.3219. Time: 0.0360 ms. Best GFLOPs: 957.4466
[16:28:17] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |      1126.6153 |       6.6249 |               46.3741 |     32 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |      1311.6890 |       5.6495 |               45.1959 |     32 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |         0.0004 |       2.2715 |               18.1718 |     32 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |        73.0954 |       3.4644 |                3.4644 |     32 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |       454.8653 |       4.0435 |                4.0435 |     32 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |        12.2906 |       2.1688 |                2.1688 |     32 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |      1351.2928 |       5.4247 |               16.2742 |     32 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |         0.0005 |       2.2001 |                6.6004 |     32 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |       120.7441 |       4.1945 |               12.5836 |     32 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |       957.4466 |       7.6840 |               23.0521 |     32 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 896
Total latency (us): 348.674

[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #0: GFLOPs: 3.4489. Time: 0.0098 ms. Best GFLOPs: 3.4489
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #1: GFLOPs: 3.4422. Time: 0.0098 ms. Best GFLOPs: 3.4489
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #2: GFLOPs: 3.8121. Time: 0.0088 ms. Best GFLOPs: 3.8121
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #3: GFLOPs: 14.8219. Time: 0.0023 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #4: GFLOPs: 14.0409. Time: 0.0024 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #5: GFLOPs: 4.1150. Time: 0.0082 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #6: GFLOPs: 14.6433. Time: 0.0023 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #7: GFLOPs: 12.7163. Time: 0.0027 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #8: GFLOPs: 3.6682. Time: 0.0092 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #9: GFLOPs: 14.4971. Time: 0.0023 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #10: GFLOPs: 14.5614. Time: 0.0023 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #11: GFLOPs: 3.8786. Time: 0.0087 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #12: GFLOPs: 3.8896. Time: 0.0087 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #13: GFLOPs: 4.1010. Time: 0.0082 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #14: GFLOPs: 3.8794. Time: 0.0087 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #15: GFLOPs: 4.1633. Time: 0.0081 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #16: GFLOPs: 3.9468. Time: 0.0085 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #17: GFLOPs: 14.6052. Time: 0.0023 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #18: GFLOPs: 3.6783. Time: 0.0092 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #19: GFLOPs: 12.6233. Time: 0.0027 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #20: GFLOPs: 14.0030. Time: 0.0024 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #21: GFLOPs: 8.6779. Time: 0.0039 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #22: GFLOPs: 13.8344. Time: 0.0024 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #23: GFLOPs: 3.8628. Time: 0.0087 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #24: GFLOPs: 14.2594. Time: 0.0024 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #25: GFLOPs: 14.3730. Time: 0.0023 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #26: GFLOPs: 4.0098. Time: 0.0084 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #27: GFLOPs: 14.7181. Time: 0.0023 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #28: GFLOPs: 13.9049. Time: 0.0024 ms. Best GFLOPs: 14.8219
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #29: GFLOPs: 14.9176. Time: 0.0023 ms. Best GFLOPs: 14.9176
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #30: GFLOPs: 13.9902. Time: 0.0024 ms. Best GFLOPs: 14.9176
[16:28:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_avg_pool2d_3"] Trial #31: GFLOPs: 11.1763. Time: 0.0030 ms. Best GFLOPs: 14.9176
[16:28:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #28: "fused_nn_avg_pool2d_3"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |      1126.6153 |       6.6249 |               46.3741 |     32 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |      1311.6890 |       5.6495 |               45.1959 |     32 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |         0.0004 |       2.2715 |               18.1718 |     32 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |        73.0954 |       3.4644 |                3.4644 |     32 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |       454.8653 |       4.0435 |                4.0435 |     32 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |        12.2906 |       2.1688 |                2.1688 |     32 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |      1351.2928 |       5.4247 |               16.2742 |     32 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |         0.0005 |       2.2001 |                6.6004 |     32 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |       120.7441 |       4.1945 |               12.5836 |     32 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |       957.4466 |       7.6840 |               23.0521 |     32 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |        14.9176 |       2.2610 |                2.2610 |     32 |            
 29 |                              fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 928
Total latency (us): 350.935

[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #0: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #1: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #2: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #3: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #4: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #5: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #6: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #7: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #8: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #9: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #10: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #11: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #12: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #13: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #14: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #15: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #16: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #17: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #18: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #19: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #20: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #21: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #22: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #23: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #24: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #25: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #26: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #27: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #28: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #29: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #30: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_reshape"] Trial #31: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[16:28:30] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #29: "fused_reshape"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |      1126.6153 |       6.6249 |               46.3741 |     32 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |      1311.6890 |       5.6495 |               45.1959 |     32 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |         0.0004 |       2.2715 |               18.1718 |     32 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |        73.0954 |       3.4644 |                3.4644 |     32 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |       454.8653 |       4.0435 |                4.0435 |     32 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |        12.2906 |       2.1688 |                2.1688 |     32 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |      1351.2928 |       5.4247 |               16.2742 |     32 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |         0.0005 |       2.2001 |                6.6004 |     32 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |       120.7441 |       4.1945 |               12.5836 |     32 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |       957.4466 |       7.6840 |               23.0521 |     32 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |        14.9176 |       2.2610 |                2.2610 |     32 |            
 29 |                              fused_reshape |        1 |      1 |         0.0005 |       2.1621 |                2.1621 |     32 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 960
Total latency (us): 353.097

[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #0: GFLOPs: 12.5049. Time: 0.0871 ms. Best GFLOPs: 12.5049
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #1: GFLOPs: 31.0240. Time: 0.0351 ms. Best GFLOPs: 31.0240
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #2: GFLOPs: 28.5050. Time: 0.0382 ms. Best GFLOPs: 31.0240
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #3: GFLOPs: 94.1839. Time: 0.0116 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #4: GFLOPs: 13.3377. Time: 0.0816 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #5: GFLOPs: 4.5127. Time: 0.2413 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #6: GFLOPs: 14.8558. Time: 0.0733 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #7: GFLOPs: 11.2449. Time: 0.0968 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #8: GFLOPs: 2.9741. Time: 0.3662 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #9: GFLOPs: 26.8641. Time: 0.0405 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #10: GFLOPs: 26.5740. Time: 0.0410 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #11: GFLOPs: 29.4154. Time: 0.0370 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #12: GFLOPs: 7.7843. Time: 0.1399 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #13: GFLOPs: 6.9190. Time: 0.1574 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #14: GFLOPs: 15.2548. Time: 0.0714 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #15: GFLOPs: 41.7703. Time: 0.0261 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #16: GFLOPs: 73.5721. Time: 0.0148 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #17: GFLOPs: 71.4367. Time: 0.0152 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #18: GFLOPs: 19.7462. Time: 0.0551 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #19: GFLOPs: 5.7992. Time: 0.1878 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #20: GFLOPs: 11.8888. Time: 0.0916 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #21: GFLOPs: 2.5794. Time: 0.4222 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #22: GFLOPs: 17.5221. Time: 0.0622 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #23: GFLOPs: 20.9122. Time: 0.0521 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #24: GFLOPs: 7.0251. Time: 0.1550 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #25: GFLOPs: 31.7294. Time: 0.0343 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #26: GFLOPs: 8.3406. Time: 0.1306 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #30: "fused_nn_dense_add"] Trial #27: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544), "float32"], placeholder_1: T.Buffer[(1000, 544), "float32"], placeholder_2: T.Buffer[(1, 1000), "float32"], T_add: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 1000], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 544], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([1000, 544], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(125, thread="threadIdx.x"):
                    with T.block("T_matmul_NT_init"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(1000, i0_0_i1_0_fused * 250 + i0_1_i1_1_fused * 125 + i0_2_i1_2_fused)
                        T.reads()
                        T.writes(T_matmul_NT_local[i, j])
                        T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 544], "float32"], ["TENSOR", [1000, 544], "float32"], None, "float32"]})
                        T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(272):
                        for ax0_ax1_fused_0 in T.serial(1):
                            for ax0_ax1_fused_1 in T.thread_binding(125, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(544, i2_0 * 2 + ax0_ax1_fused_1)
                                    T.where(ax0_ax1_fused_1 < 2)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(1):
                            for ax0_ax1_fused_1 in T.thread_binding(125, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1000, i0_0_i1_0_fused * 250 + (ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) // 2)
                                        v1 = T.axis.spatial(544, i2_0 * 2 + (ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) % 2)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(2, 1, 1, 1, 1, 1):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(1000, i0_0_i1_0_fused * 250 + i0_1_i1_1_fused * 125 + i0_2_i1_2_fused)
                                k = T.axis.reduce(544, i2_0 * 2 + i2_1)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 544], "float32"], ["TENSOR", [1000, 544], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 1):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1000, i0_0_i1_0_fused * 250 + i0_1_i1_1_fused * 125 + i0_2_i1_2_fused + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_add[v0, v1])
                            T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[4, 2, 125, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[272, 2, 1])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
sch.enter_postproc()
sch.unannotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch")
l55, l56, l57, l58, l59 = sch.get_loops(block=b36)
l60, l61 = sch.split(loop=l59, factors=[None, 125])
sch.bind(loop=l61, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b45)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 125, 4])
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86, l87 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l98, l99, l100, l101, l102 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l98, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l98, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="T_matmul_NT", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b103)
b114 = sch.decompose_reduction(block=b103, loop=l107)
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #28: GFLOPs: 5.4125. Time: 0.2012 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #29: GFLOPs: 10.1037. Time: 0.1078 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #30: GFLOPs: 1.5149. Time: 0.7189 ms. Best GFLOPs: 94.1839
[16:28:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_dense_add"] Trial #31: GFLOPs: 6.4609. Time: 0.1686 ms. Best GFLOPs: 94.1839
[16:28:33] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #30: "fused_nn_dense_add"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |      1126.6153 |       6.6249 |               46.3741 |     32 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |      1311.6890 |       5.6495 |               45.1959 |     32 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |         0.0004 |       2.2715 |               18.1718 |     32 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |        73.0954 |       3.4644 |                3.4644 |     32 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |       454.8653 |       4.0435 |                4.0435 |     32 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |        12.2906 |       2.1688 |                2.1688 |     32 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |      1351.2928 |       5.4247 |               16.2742 |     32 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |         0.0005 |       2.2001 |                6.6004 |     32 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |       120.7441 |       4.1945 |               12.5836 |     32 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |       957.4466 |       7.6840 |               23.0521 |     32 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |        14.9176 |       2.2610 |                2.2610 |     32 |            
 29 |                              fused_reshape |        1 |      1 |         0.0005 |       2.1621 |                2.1621 |     32 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |        94.1839 |      11.5625 |               11.5625 |     32 |            
 31 |                           fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 992
Total latency (us): 364.66

[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #0: GFLOPs: 0.3059. Time: 0.0131 ms. Best GFLOPs: 0.3059
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #1: GFLOPs: 0.1335. Time: 0.0300 ms. Best GFLOPs: 0.3059
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #2: GFLOPs: 0.1119. Time: 0.0358 ms. Best GFLOPs: 0.3059
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #3: GFLOPs: 0.1113. Time: 0.0359 ms. Best GFLOPs: 0.3059
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #4: GFLOPs: 0.1273. Time: 0.0314 ms. Best GFLOPs: 0.3059
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #5: GFLOPs: 0.1354. Time: 0.0295 ms. Best GFLOPs: 0.3059
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #6: GFLOPs: 0.1283. Time: 0.0312 ms. Best GFLOPs: 0.3059
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #7: GFLOPs: 1.5199. Time: 0.0026 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #8: GFLOPs: 0.1847. Time: 0.0217 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #9: GFLOPs: 0.1146. Time: 0.0349 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #10: GFLOPs: 0.1419. Time: 0.0282 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #11: GFLOPs: 1.4379. Time: 0.0028 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #12: GFLOPs: 0.1387. Time: 0.0288 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #13: GFLOPs: 1.4577. Time: 0.0027 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #14: GFLOPs: 0.1365. Time: 0.0293 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #15: GFLOPs: 0.3756. Time: 0.0106 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #16: GFLOPs: 0.1276. Time: 0.0314 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #17: GFLOPs: 0.1078. Time: 0.0371 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #18: GFLOPs: 0.1069. Time: 0.0374 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #19: GFLOPs: 1.4905. Time: 0.0027 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #20: GFLOPs: 0.1056. Time: 0.0379 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #21: GFLOPs: 0.1101. Time: 0.0363 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #22: GFLOPs: 0.1279. Time: 0.0313 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #23: GFLOPs: 1.2972. Time: 0.0031 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #24: GFLOPs: 0.1348. Time: 0.0297 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #25: GFLOPs: 0.1062. Time: 0.0377 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #26: GFLOPs: 0.3836. Time: 0.0104 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #27: GFLOPs: 0.3543. Time: 0.0113 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #28: GFLOPs: 0.1405. Time: 0.0285 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #29: GFLOPs: 0.1102. Time: 0.0363 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #30: GFLOPs: 0.3762. Time: 0.0106 ms. Best GFLOPs: 1.5199
[16:28:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_softmax"] Trial #31: GFLOPs: 0.6285. Time: 0.0064 ms. Best GFLOPs: 1.5199
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #31: "fused_nn_softmax"
 ID |                                       Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                        fused_nn_avg_pool2d |   426496 |      1 |        99.7439 |       4.2759 |                4.2759 |     32 |            
  1 |                      fused_nn_avg_pool2d_1 |   852992 |      1 |       201.3370 |       4.2366 |                4.2366 |     32 |            
  2 |                      fused_nn_avg_pool2d_2 |   602112 |      1 |       143.5711 |       4.1938 |                4.1938 |     32 |            
  3 |            fused_nn_conv2d_add_add_nn_relu | 17160192 |      1 |      1091.9020 |      15.7159 |               15.7159 |     32 |            
  4 |                        fused_nn_max_pool2d |   677376 |      1 |       188.4530 |       3.5944 |                3.5944 |     32 |            
  5 |       fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |       921.7444 |       5.7158 |                5.7158 |     32 |            
  6 |            fused_reshape_transpose_reshape |        1 |      1 |         0.0002 |       4.4984 |                4.4984 |     32 |            
  7 |                        fused_nn_conv2d_add |  1668352 |      1 |       272.1160 |       6.1310 |                6.1310 |     32 |            
  8 |               fused_nn_conv2d_multiply_add |  5092864 |      1 |      1212.6287 |       4.1999 |                4.1999 |     32 |            
  9 |                  fused_concatenate_nn_relu |   106624 |      1 |        43.8238 |       2.4330 |                2.4330 |     32 |            
 10 |                      fused_nn_conv2d_add_1 |  2025856 |      3 |       348.9668 |       5.8053 |               17.4159 |     32 |            
 11 |   fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |      1094.8859 |       7.0116 |               21.0349 |     32 |            
 12 |     fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |       968.3829 |       7.8175 |               31.2699 |     32 |            
 13 |          fused_reshape_transpose_reshape_1 |        1 |      4 |         0.0004 |       2.7940 |               11.1759 |     32 |            
 14 |                      fused_nn_conv2d_add_2 |   506464 |      1 |       137.5403 |       3.6823 |                3.6823 |     32 |            
 15 |             fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |       425.0650 |       4.3897 |                4.3897 |     32 |            
 16 |                fused_concatenate_nn_relu_1 |    53312 |      1 |        24.7213 |       2.1565 |                2.1565 |     32 |            
 17 |                      fused_nn_conv2d_add_3 |  1012928 |      7 |       287.9353 |       3.5179 |               24.6253 |     32 |            
 18 | fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |      1126.6153 |       6.6249 |               46.3741 |     32 |            
 19 |     fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |      1311.6890 |       5.6495 |               45.1959 |     32 |            
 20 |          fused_reshape_transpose_reshape_2 |        1 |      8 |         0.0004 |       2.2715 |               18.1718 |     32 |            
 21 |                      fused_nn_conv2d_add_4 |   253232 |      1 |        73.0954 |       3.4644 |                3.4644 |     32 |            
 22 |             fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |       454.8653 |       4.0435 |                4.0435 |     32 |            
 23 |                fused_concatenate_nn_relu_2 |    26656 |      1 |        12.2906 |       2.1688 |                2.1688 |     32 |            
 24 |     fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |      1351.2928 |       5.4247 |               16.2742 |     32 |            
 25 |          fused_reshape_transpose_reshape_3 |        1 |      3 |         0.0005 |       2.2001 |                6.6004 |     32 |            
 26 |                      fused_nn_conv2d_add_5 |   506464 |      3 |       120.7441 |       4.1945 |               12.5836 |     32 |            
 27 | fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |       957.4466 |       7.6840 |               23.0521 |     32 |            
 28 |                      fused_nn_avg_pool2d_3 |    33728 |      1 |        14.9176 |       2.2610 |                2.2610 |     32 |            
 29 |                              fused_reshape |        1 |      1 |         0.0005 |       2.1621 |                2.1621 |     32 |            
 30 |                         fused_nn_dense_add |  1089000 |      1 |        94.1839 |      11.5625 |               11.5625 |     32 |            
 31 |                           fused_nn_softmax |     4000 |      1 |         1.5199 |       2.6318 |                2.6318 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1024
Total latency (us): 367.291

[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #18: "fused_nn_conv2d_multiply_add_add_nn_relu_1"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #18 has finished. Remaining task(s): 31
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #19: "fused_nn_conv2d_multiply_add_nn_relu_2"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #19 has finished. Remaining task(s): 30
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_nn_conv2d_multiply_add_nn_relu_1"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #12 has finished. Remaining task(s): 29
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #17: "fused_nn_conv2d_add_3"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #17 has finished. Remaining task(s): 28
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #27: "fused_nn_conv2d_multiply_add_add_nn_relu_2"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #27 has finished. Remaining task(s): 27
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_nn_conv2d_multiply_add_add_nn_relu"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #11 has finished. Remaining task(s): 26
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #20: "fused_reshape_transpose_reshape_2"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #20 has finished. Remaining task(s): 25
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_nn_conv2d_add_1"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #10 has finished. Remaining task(s): 24
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #24: "fused_nn_conv2d_multiply_add_nn_relu_3"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #24 has finished. Remaining task(s): 23
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_nn_conv2d_add_add_nn_relu"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #3 has finished. Remaining task(s): 22
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #26: "fused_nn_conv2d_add_5"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #26 has finished. Remaining task(s): 21
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #30: "fused_nn_dense_add"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #30 has finished. Remaining task(s): 20
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_reshape_transpose_reshape_1"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #13 has finished. Remaining task(s): 19
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #25: "fused_reshape_transpose_reshape_3"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #25 has finished. Remaining task(s): 18
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_nn_conv2d_add"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #7 has finished. Remaining task(s): 17
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_nn_conv2d_multiply_add_nn_relu"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #5 has finished. Remaining task(s): 16
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_reshape_transpose_reshape"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #6 has finished. Remaining task(s): 15
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #15: "fused_nn_conv2d_multiply_add_1"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #15 has finished. Remaining task(s): 14
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_nn_avg_pool2d"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #0 has finished. Remaining task(s): 13
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_nn_avg_pool2d_1"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #1 has finished. Remaining task(s): 12
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_nn_conv2d_multiply_add"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #8 has finished. Remaining task(s): 11
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_nn_avg_pool2d_2"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #2 has finished. Remaining task(s): 10
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #22: "fused_nn_conv2d_multiply_add_2"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #22 has finished. Remaining task(s): 9
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #14: "fused_nn_conv2d_add_2"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #14 has finished. Remaining task(s): 8
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_nn_max_pool2d"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #4 has finished. Remaining task(s): 7
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #21: "fused_nn_conv2d_add_4"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #21 has finished. Remaining task(s): 6
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #31: "fused_nn_softmax"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #31 has finished. Remaining task(s): 5
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_concatenate_nn_relu"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #9 has finished. Remaining task(s): 4
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #28: "fused_nn_avg_pool2d_3"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #28 has finished. Remaining task(s): 3
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #23: "fused_concatenate_nn_relu_2"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #23 has finished. Remaining task(s): 2
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #29: "fused_reshape"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #29 has finished. Remaining task(s): 1
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #16: "fused_concatenate_nn_relu_1"
[16:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #16 has finished. Remaining task(s): 0
